{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# IMPORTS\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras import layers, models, Input\n",
    "from keras import backend as K\n",
    "from keras import regularizers\n",
    "from keras.utils import Sequence\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from collections import namedtuple\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import datetime, time\n",
    "import os, sys\n",
    "import tqdm\n",
    "import gc\n",
    "from multiprocessing import Process\n",
    "Point = namedtuple('Point', ('x', 'y'))\n",
    "Circle = namedtuple('Circle', ('r'))\n",
    "Square = namedtuple('Square', ('side'))\n",
    "Rectangle = namedtuple('Rectangle', ('length', 'width'))\n",
    "PointWithDistance = namedtuple('PointWithDistance', ('p', 'dist'))\n",
    "float_memory_used = 'float16'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INIT\n",
    "# PART 1\n",
    "# number_samples = [5] + list(range(10, 101, 10)) + [120, 150, 200, 250, 300, 400, 500, 700] + list(range(1000, 10001, 1000))\n",
    "number_samples = [256, 512, 1024, 2048, 4096, 8192] \n",
    "number_samples = [8192]\n",
    "\n",
    "# cnn_type = \"classification\"  # {\"classification\", \"regression\"}\n",
    "validation_size, noise_floor = 0.33, -90.0\n",
    "su_power = 0 # this is not actually su power just a number to show there is an SU in its image\n",
    "max_x, max_y, number_image_channels, su_szie = 100, 100, 7, 10  # su_size:30 for 1000, 10 for 100\n",
    "pu_shape, su_shape = 'circle', 'circle' # shape = {'circle', 'square', 'point'}\n",
    "style = \"raw_power_min_max_norm\"  # {\"raw_power_zscore_norm\", \"image_intensity\", \"raw_power_min_max_norm\"}\n",
    "intensity_degradation, slope = 'log', 5  # 'log', 'linear', slope 3 for 1000, 5 for 100\n",
    "max_pus_num, max_sus_num = 20, 1\n",
    "propagation_model = 'splat' # 'splat', 'log', 'testbed'\n",
    "noise, std = False, 1 # False for splat\n",
    "if su_shape == 'circle':\n",
    "    su_param = Circle(su_szie)\n",
    "elif su_shape == 'square':\n",
    "    su_param = Square(su_szie)\n",
    "else:\n",
    "    su_param = None\n",
    "    \n",
    "sensors = True\n",
    "if sensors:\n",
    "    sensors_num = 900\n",
    "    sensors_file_path = \"rsc/sensors/\" + str(max(max_x, max_y)) + \"/\" + str(sensors_num) + \"/sensors\"\n",
    "    sensors_file_path = \"../../java_workspace/research/commons/resources/sensors/square\" + str(max(max_x, max_y)) \\\n",
    "        + \"/\" + str(sensors_num) + \"/sensors.txt\"\n",
    "# num_pus = (data_reg.shape[1] - 3)//3\n",
    "\n",
    "# PART 2\n",
    "number_of_proccessors = 6\n",
    "memory_size_allowed = 4 # in Gigabyte\n",
    "float_size = 0\n",
    "if float_memory_used == \"float16\":\n",
    "    float_size = 16\n",
    "elif float_memory_used == \"float\" or \"float32\":\n",
    "    float_size = 32\n",
    "elif float_memory_used == \"float8\":\n",
    "    float_size = 8\n",
    "\n",
    "\n",
    "batch_size = int(memory_size_allowed / (max_x * max_y * number_image_channels * float_size/(8 * 1024 ** 3)))\n",
    "\n",
    "\n",
    "dtime = datetime.datetime.now().strftime('_%Y%m_%d%H_%M')\n",
    "color = \"color\" if number_image_channels > 1 else \"gray\"\n",
    "image_dir = 'ML/data/pictures_' + str(max_x) + '_' + str(max_y) + '/' + propagation_model + (\n",
    "    \"/noisy_std_\" + str(std) if noise else \"\") + '/pu_' + pu_shape + '_su_' + su_shape + '_' + (\n",
    "    \"\" if su_shape == 'point' else str(su_szie)) + \"/\" + style + \"/\" + color +'/' + (\n",
    "    \"\" if pu_shape == 'point' and su_shape == 'point' else (intensity_degradation + '_' + str(slope))) + (\n",
    "    \"/\" + str(sensors_num) + \"sensors\" if sensors else \"/pus\") + \"/images\"\n",
    "\n",
    "if not os.path.exists(image_dir):\n",
    "        os.makedirs(image_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ML/data/pictures_100_100/splat/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/900sensors/images'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD DATA\n",
    "num_columns = (sensors_num if sensors else max_pus_num * 3 + 1) + max_sus_num * 3 + 2\n",
    "cols = [i for i in range(num_columns)]\n",
    "dataset_name = \"dynamic_pus_900sensor_60000_min10_max20PUs_1SUs_square100grid_splat_2020_06_28_13_45.txt\"\n",
    "max_dataset_name = \"dynamic_pus_max_power_60000_min10_max20PUs_1SUs_square100grid_splat_2020_06_28_13_45.txt\"\n",
    "with open('/'.join(image_dir.split('/')[:-1]) + '/datasets' + dtime + '.txt', 'w') as set_file:\n",
    "    set_file.write(dataset_name + \"\\n\")\n",
    "    set_file.write(max_dataset_name)\n",
    "\n",
    "dataframe = pd.read_csv('../../java_workspace/research/spectrum_allocation/resources/data/' + dataset_name, \n",
    "                        delimiter=',', header=None, names=cols)\n",
    "dataframe_max = pd.read_csv('../../java_workspace/research/spectrum_allocation/resources/data/' + \n",
    "                            max_dataset_name, delimiter=',', header=None)\n",
    "\n",
    "dataframe.reset_index(drop=True, inplace=True)\n",
    "dataframe_max.reset_index(drop=True, inplace=True)\n",
    "dataframe_max[dataframe_max.shape[1] - 1] = dataframe_max[dataframe_max.shape[1] - 1].astype(float)\n",
    "\n",
    "dataframe_tot = pd.concat([dataframe, dataframe_max.iloc[:, dataframe_max.columns.values[-1:]]], axis=1,\n",
    "                        ignore_index=True)\n",
    "\n",
    "idx = dataframe_tot[dataframe_tot[dataframe_tot.columns[-1]] == -float('inf')].index\n",
    "dataframe_tot.drop(idx, inplace=True)\n",
    "\n",
    "data_reg = dataframe_tot.values\n",
    "# data_reg = np.concatenate((dataframe_tot.values[:, 0:dataframe_tot.shape[1]-3], \n",
    "#                            dataframe_tot.values[:, dataframe_tot.shape[1]-1:dataframe_tot.shape[1]]), axis=1)\n",
    "# data_class = dataframe_tot.values[:, 0:dataframe_tot.shape[1]-1]\n",
    "# y_class_power = dataframe_tot.values[:, -1]\n",
    "\n",
    "if sensors:\n",
    "    sensors_location = []\n",
    "    with open(sensors_file_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            line = line.split(',')\n",
    "            sensors_location.append(Point(int(float(line[0])), int(float(line[1]))))\n",
    "del dataframe, dataframe_tot, dataframe_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38076, 906)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_reg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1.   ,  75.   ,  31.   , -25.276,   1.   ,   5.966])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_reg[0, sensors_num:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_reg = data_reg[0:4096][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidian_distance(p1: Point, p2: Point):\n",
    "    return ((p1.x - p2.x) ** 2 + (p1.y - p2.y) ** 2) ** 0.5\n",
    "\n",
    "def calculate_mu_sigma(data, num_pus):\n",
    "    sum_non_noise = 0\n",
    "    for pu_n in range(num_pus): # calculate mu\n",
    "        sum_non_noise += data[pu_n*3+2]\n",
    "    mu = ((max_x * max_y - num_pus) * noise_floor + sum_non_noise)/(max_x * max_y)\n",
    "    sum_square = 0\n",
    "    for pu_n in range(num_pus): # calculate sigma\n",
    "        sum_square += (data[pu_n*3+2]-mu)**2\n",
    "    sum_square += (max_x * max_y - num_pus) * (noise_floor - mu)**2\n",
    "    sigma = math.sqrt(sum_square/(max_x * max_y))\n",
    "    return mu, sigma\n",
    "\n",
    "def get_pu_param(pu_shape: str, intensity_degradation: str, pu_p: float, noise_floor: float, slope: float):\n",
    "    pu_param = None\n",
    "    if pu_shape == 'circle':\n",
    "        if intensity_degradation == \"linear\":\n",
    "            pu_param = Circle(int((pu_p - noise_floor) / slope)) # linear\n",
    "        elif intensity_degradation == \"log\":\n",
    "            pu_param = Circle(int(10 ** ((pu_p - noise_floor) / (10 *slope)))) # log_based\n",
    "    elif pu_shape == 'square':\n",
    "        if intensity_degradation == \"linear\":\n",
    "            pu_param = Square(int(2 ** 0.5 * (pu_p - noise_floor) / slope)) # linear\n",
    "        elif intensity_degradation == \"log\":\n",
    "            pu_param = Square(int(2 ** 0.5 * 10 ** ((pu_p - noise_floor) / (10 *slope)))) # log_based\n",
    "    elif pu_shape == 'point':\n",
    "        pu_param = None\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported PU shape(create_image)! \", pu_shape)\n",
    "    return pu_param\n",
    "\n",
    "def create_image(data, slope, sensors_num, style=\"raw_power_z_score\", noise_floor=-90, pu_shape= 'circle', pu_param=None, \n",
    "                 su_shape='circle', su_param=None, intensity_degradation=\"log\", max_pu_power: float=0):  \n",
    "    # style = {\"raw_power_zscore_norm\", \"image_intensity\", \"raw_power_min_max_norm\"}\n",
    "    # intensity_degradation= {\"log\", \"linear\"}\n",
    "    # if param is None, it's automatically calculated. Highest brightness(or power value) (255 or 1.) would\n",
    "    # assigned to the center(PU location) and radius(side) would be calculated based on its power, slope, and noise floor.\n",
    "    # If it is given, intensity(power) of pixel beside center would be calculated in the same fashin with an exception that \n",
    "    # intensity below zero(noise_floor) would be replaced by zero(noise_floor)\n",
    "    if style == \"raw_power_min_max_norm\":\n",
    "        # In this way, PUs' location are replaced with their power(dBm) and the power would fade with \n",
    "        # slope till gets noise_floor(in circle shape)\n",
    "        \n",
    "        # creating pu matrix\n",
    "        image = np.zeros((1,number_image_channels,max_x, max_y), dtype=float_memory_used)\n",
    "        if not sensors:\n",
    "            pus_num = int(data[0])\n",
    "#             print(pus_num)\n",
    "            for pu_i in range(pus_num):\n",
    "                pu_x = max(0, min(max_x-1, int(data[pu_i * 3 + 1]))) \n",
    "                pu_y = max(0, min(max_x-1, int(data[pu_i * 3 + 2])))\n",
    "                pu_p = data[pu_i * 3 + 3]\n",
    "#                 print(pu_x, pu_y, pu_p)\n",
    "                if pu_param is None:\n",
    "                    pu_param_p = get_pu_param(pu_shape, intensity_degradation, pu_p, noise_floor, slope)\n",
    "                else:\n",
    "                    pu_param_p = pu_param\n",
    "                points = points_inside_shape(center=Point(pu_x, pu_y), shape=pu_shape, param=pu_param_p)\n",
    "                for point in points:\n",
    "                    if 0 <= point.p.x < max_x and 0 <= point.p.y < max_y: # TODO should pass image size\n",
    "                        if intensity_degradation == \"linear\":\n",
    "                            image[0][int(abs(pu_p))//5][point.p.x][point.p.y] += (pu_p - slope * point.dist - noise_floor)/(\n",
    "                                max_pu_power - noise_floor)\n",
    "                        elif intensity_degradation == \"log\":\n",
    "                            if point.dist < 1:\n",
    "                                image[0][int(abs(pu_p))//5][point.p.x][point.p.y] += (pu_p - noise_floor) / (max_pu_power - noise_floor)\n",
    "                            else:\n",
    "                                image[0][int(abs(pu_p))//5][point.p.x][point.p.y] += (pu_p - slope * 10*math.log10(point.dist) - noise_floor)/(\n",
    "                                    max_pu_power - noise_floor)\n",
    "        else:\n",
    "            ss_param, ss_shape = pu_param, pu_shape\n",
    "            for ss_i in range(sensors_num):\n",
    "                ss_x, ss_y, ss_p = max(0, min(max_x-1, int(sensors_location[ss_i].x))), max(0, min(max_x-1, int(\n",
    "                    sensors_location[ss_i].y))), max(noise_floor, data[ss_i])\n",
    "                ss_channel = 0 \n",
    "                if -66.0 <= ss_p < -60.0:\n",
    "                    ss_channel = 1\n",
    "                elif -72.0 <= ss_p < -66.0:\n",
    "                    ss_channel = 2\n",
    "                elif -78.0 <= ss_p < -72.0:\n",
    "                    ss_channel = 3\n",
    "                elif -84.0 <= ss_p < -78.0:\n",
    "                    ss_channel = 4\n",
    "#                 elif -70.0 <= ss_p < -65.0:\n",
    "#                     ss_channel = 5\n",
    "                elif ss_p < -84.0:\n",
    "                    ss_channel = 5\n",
    "                if ss_param is None:\n",
    "                    ss_param_p = get_pu_param(ss_shape, intensity_degradation, ss_p, noise_floor, slope)\n",
    "                else:\n",
    "                    ss_param_p = ss_param\n",
    "                points = points_inside_shape(center=Point(ss_x, ss_y), shape=ss_shape, param=ss_param_p)\n",
    "                for point in points:\n",
    "                    if 0 <= point.p.x < max_x and 0 <= point.p.y < max_y: # TODO should pass image size\n",
    "                        if intensity_degradation == \"linear\":\n",
    "                            image[0][ss_channel][point.p.x][point.p.y] += (ss_p - slope * point.dist - noise_floor)/(\n",
    "                                max_pu_power - noise_floor)\n",
    "                        elif intensity_degradation == \"log\":\n",
    "                            if point.dist < 1:\n",
    "                                image[0][ss_channel][point.p.x][point.p.y] += (ss_p - noise_floor) / (max_pu_power - noise_floor)\n",
    "                            else:\n",
    "                                image[0][ss_channel][point.p.x][point.p.y] += (ss_p - slope * 10*math.log10(point.dist) - noise_floor)/(\n",
    "                                    max_pu_power - noise_floor)\n",
    "        del points\n",
    "        # creating su matrix\n",
    "        su_num_idx = sensors_num if sensors else (pus_num * 3 + 1)\n",
    "        su_num = int(data[su_num_idx])\n",
    "#         print(su_num)\n",
    "#         su_num = (len(data) - pus_num * (3 if not sensors else 1)) // 2\n",
    "#         if not (len(data) - pus_num * (3 if not sensors else 1)) % 2:\n",
    "#             raise ValueError(\"Data provided is not correct; can't get SUs' information(create_image)\")\n",
    "        if su_param is None:\n",
    "            # if su_param is unavailable, a circle(square) with radius(side) 1 is created\n",
    "            if su_shape == 'circle':\n",
    "                su_param = Circle(1)\n",
    "            elif su_shape == 'square':\n",
    "                su_param = Square(1)\n",
    "            elif su_shape == 'point':\n",
    "                su_param = None\n",
    "            else:\n",
    "                raise ValueError(\"Unsupported SU shape(create_image)! \", su_shape)\n",
    "        \n",
    "        for su_i in range(su_num - 1):\n",
    "            su_x = max(0, min(max_x-1, int(data[su_num_idx + su_i * 3 + 1])))\n",
    "            su_y = max(0, min(max_x-1, int(data[su_num_idx + su_i * 3 + 2])))\n",
    "            su_p = data[su_num_idx + su_i * 3 + 3]\n",
    "#             su_p = su_intensity\n",
    "            points = points_inside_shape(center=Point(su_x, su_y), param=su_param, shape=su_shape)\n",
    "            su_channel = 0 if number_image_channels == 1 else -1\n",
    "            for point in points:\n",
    "                if 0 <= point.p.x < max_x and 0 <= point.p.y < max_y: # TODO should pass image size\n",
    "                    if intensity_degradation == \"linear\":\n",
    "                            su_val = (su_p - slope * point.dist - noise_floor)/(max_pu_power - noise_floor)\n",
    "                    elif intensity_degradation == \"log\":\n",
    "                        if point.dist < 1:\n",
    "                            su_val = (su_p - noise_floor) / (max_pu_power - noise_floor)\n",
    "                        else:\n",
    "                            su_val = (su_p - slope * 10*math.log10(point.dist) - noise_floor)/(\n",
    "                                max_pu_power - noise_floor)\n",
    "                    image[0][su_channel][point.p.x][point.p.y] += su_val\n",
    "            del points\n",
    "        # the last and  target SU\n",
    "        su_intensity = 1.\n",
    "        su_x = max(0, min(max_x-1, int(data[su_num_idx + (su_num - 1) * 3 + 1])))\n",
    "        su_y = max(0, min(max_x-1, int(data[su_num_idx + (su_num - 1) * 3 + 2])))\n",
    "#         print(su_x, su_y)\n",
    "        points = points_inside_shape(center=Point(su_x, su_y), param=su_param, shape=su_shape)\n",
    "        su_channel = 0 if number_image_channels == 1 else -1\n",
    "        for point in points:\n",
    "            if 0 <= point.p.x < max_x and 0 <= point.p.y < max_y: # TODO should pass image size\n",
    "                image[0][su_channel][point.p.x][point.p.y] += su_intensity\n",
    "        del points\n",
    "        return image\n",
    "        \n",
    "#         pu_image = [[(noise_floor - mu)/sigma] * max_y for _ in range(max_x)]\n",
    "    elif style == \"image_intensity\":\n",
    "        # creating PU image\n",
    "        image = np.zeros((1,number_image_channels,max_x, max_y), dtype=float_memory_used)\n",
    "        for pu_i in range(pus_num):\n",
    "            pu_x, pu_y, pu_p = max(0, min(max_x-1, int(data[pu_i*3]))), max(0, min(max_x-1, int(data[pu_i*3+1]))), data[pu_i*3+2]\n",
    "            if pu_param is None:\n",
    "                pu_param_p = get_pu_param(pu_shape, intensity_degradation, pu_p, noise_floor, slope)\n",
    "            else:\n",
    "                pu_param_p = pu_param\n",
    "            points = points_inside_shape(center=Point(pu_x, pu_y), shape=pu_shape, param=pu_param_p)\n",
    "            for point in points:\n",
    "                if 0 <= point.p.x < max_x and 0 <= point.p.y < max_y: # TODO should pass image size\n",
    "                    if intensity_degradation == \"linear\":\n",
    "                        image[0][0][point.p.x][point.p.y] += max((pu_p - slope * point.dist + abs(noise_floor))\n",
    "                                                              /(pu_p + abs(noise_floor)), 0)\n",
    "                    elif intensity_degradation == \"log\":\n",
    "                        if point.dist < 1:\n",
    "                            image[0][0][point.p.x][point.p.y] = 1\n",
    "                        else:\n",
    "                            image[0][0][point.p.x][point.p.y] += max((pu_p - slope * 10*math.log10(point.dist) + abs(noise_floor))\n",
    "                                                                 /(pu_p + abs(noise_floor)), 0)\n",
    "                    image[0][0][point.p.x][point.p.y] = min(image[0][0][point.p.x][point.p.y], 1.0)\n",
    "                        \n",
    "        # creating SU image\n",
    "        su_num = (len(data) - pus_num * 3) // 2\n",
    "        if not (len(data) - pus_num * 3) % 2:\n",
    "            raise ValueError(\"Data provided is not correct; can't get SUs' information(create_image)\")\n",
    "#         su_image = np.zeros((max_x, max_y), dtype=float_memory_used)\n",
    "        if su_param is None:\n",
    "            # if su_param is unavailable, a circle(square) with radius(side) 1 is created\n",
    "            if su_shape == 'circle':\n",
    "                su_param = Circle(1)\n",
    "            elif su_shape == 'square':\n",
    "                su_param = Square(1)\n",
    "            elif su_shape == 'point':\n",
    "                su_param = None\n",
    "            else:\n",
    "                raise ValueError(\"Unsupported SU shape(create_image)! \", su_shape)\n",
    "        su_intensity = 1.\n",
    "        for su_i in range(su_num):\n",
    "            su_x, su_y, su_p = max(0, min(max_x-1, int(data[pus_num * (3 if not sensors else 1) +su_i*2]))\n",
    "                                  ), max(0, min(max_x-1, int(data[pus_num * (3 if not sensors else 1) + su_i*2+1]))), su_intensity\n",
    "            points = points_inside_shape(center=Point(su_x, su_y), param=su_param, shape=su_shape)\n",
    "            for point in points:\n",
    "                if 0 <= point.p.x < max_x and 0 <= point.p.y < max_y: # TODO should pass image size\n",
    "                    if number_image_channels > 1:\n",
    "                        image[0][1][point.p.x][point.p.y] = su_intensity\n",
    "                    elif number_image_channels == 1:\n",
    "                        image[0][0][point.p.x][point.p.y] = su_intensity\n",
    "#         return np.array([pu_image, su_image, [[0.] * max_y for _ in range(max_x)]], dtype='float32') # return like this to be able to display as an RGB image with pyplot.imshow(imsave)\n",
    "#         return np.append(pu_image, su_image, axis=0)\n",
    "        return image\n",
    "        \n",
    "            \n",
    "    else:\n",
    "        raise ValueError(\"Unsupported style(create_image)! \", style)\n",
    "        \n",
    "def points_inside_shape(center: Point, shape: str, param)-> list:\n",
    "    # This function returns points+distance around center with defined shape\n",
    "    if shape == 'circle':\n",
    "        # First creates points inside a square(around orgigin) with 2*r side and then remove those with distance > r.\n",
    "        # Shift all remaining around center. O(4r^2)\n",
    "        r, origin = param.r, Point(0, 0)\n",
    "        square_points = set((Point(x, y) for x in range(max(-r, -max_x), min(r, max_x) + 1) \n",
    "                             for y in range(max(-r, -max_y), min(r, max_y) + 1)))\n",
    "        points = []\n",
    "        while square_points:\n",
    "            p = square_points.pop()\n",
    "            dist = euclidian_distance(p, origin)\n",
    "            if dist <= r:\n",
    "                points.append(PointWithDistance(Point(p.x + center.x, p.y + center.y), dist))\n",
    "                if p.x != 0:\n",
    "                    points.append(PointWithDistance(Point(-p.x + center.x, p.y + center.y), dist))\n",
    "                    square_points.remove(Point(-p.x, p.y))\n",
    "                if p.y != 0:\n",
    "                    points.append(PointWithDistance(Point(p.x + center.x, -p.y + center.y), dist))\n",
    "                    square_points.remove(Point(p.x, -p.y))\n",
    "                if p.x != 0 and p.y != 0:\n",
    "                    points.append(PointWithDistance(Point(-p.x + center.x, -p.y + center.y), dist))\n",
    "                    square_points.remove(Point(-p.x, -p.y))\n",
    "        del square_points\n",
    "        return points\n",
    "    elif shape == 'square':\n",
    "        half_side = param.side // 2\n",
    "        return [PointWithDistance(Point(x, y), euclidian_distance(Point(x, y), center)) for x in range(-half_side + center.x,\n",
    "                                                                                               half_side + center.x+1) \n",
    "                         for y in range(-half_side + center.y, half_side + center.y + 1)]\n",
    "    elif shape == 'point':\n",
    "        return [PointWithDistance(center, 0)]\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported shape(points_inside_shape)! \", shape)\n",
    "        \n",
    "def read_image(image_num):\n",
    "    if style == \"image_intensity\":\n",
    "        image = plt.imread(image_dir + '/image' + str(image_num)+'.png')\n",
    "        image = np.swapaxes(image, 0, 2)\n",
    "        image = np.array(image[:number_image_channels], dtype=float_memory_used).reshape(1, number_image_channels, max_x, max_y)\n",
    "    elif  style == \"raw_power_min_max_norm\" or style == \"raw_power_zscore_norm\":\n",
    "        suffix = 'npz'  # npy, npz\n",
    "        image = np.load(image_dir + '/image' + str(image_num) + '.' + suffix)  \n",
    "        if type(image) == np.lib.npyio.NpzFile:\n",
    "            image = image['a']\n",
    "    \n",
    "    return image\n",
    "    \n",
    "# TODO: Consider using min_max normalization becasue difference between values using\n",
    "# z-score is huge since most of the pixels have the same value, noise floor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  1.,   2.,  13.,  25.,  42.,  70., 122., 159., 233., 277., 294.,\n",
       "        289., 296., 272., 234., 221., 224., 225., 191., 167., 154., 159.,\n",
       "        135.,  83.,  42.,  47.,  33.,  34.,  21.,  11.,  13.,   7.]),\n",
       " array([-104.599     , -103.11265625, -101.6263125 , -100.13996875,\n",
       "         -98.653625  ,  -97.16728125,  -95.6809375 ,  -94.19459375,\n",
       "         -92.70825   ,  -91.22190625,  -89.7355625 ,  -88.24921875,\n",
       "         -86.762875  ,  -85.27653125,  -83.7901875 ,  -82.30384375,\n",
       "         -80.8175    ,  -79.33115625,  -77.8448125 ,  -76.35846875,\n",
       "         -74.872125  ,  -73.38578125,  -71.8994375 ,  -70.41309375,\n",
       "         -68.92675   ,  -67.44040625,  -65.9540625 ,  -64.46771875,\n",
       "         -62.981375  ,  -61.49503125,  -60.0086875 ,  -58.52234375,\n",
       "         -57.036     ]),\n",
       " <a list of 32 Patch objects>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(data_reg[:,0:1:sensors_num], bins='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving images once to save time\n",
    "# run this cell just for creating images\n",
    "def creating_image(start, end):\n",
    "    # for image_num in range(115, data_reg.shape[0]):\n",
    "    # for image_num in range(1625, 5000):\n",
    "    for image_num in tqdm.tqdm(range(start, end+1)):  #4463, data_reg.shape[0]\n",
    "        image = create_image(data=data_reg[image_num], slope=slope, style=style, noise_floor=noise_floor,\n",
    "                             pu_shape=pu_shape, su_shape=su_shape, su_param=su_param, \n",
    "                             sensors_num=(sensors_num if sensors else 0), \n",
    "                             intensity_degradation=intensity_degradation, \n",
    "                             max_pu_power=-60.0)\n",
    "        if style == \"image_intensity\":\n",
    "            if number_image_channels != 3:\n",
    "                image = np.append(np.array(image[0]), np.zeros((3-number_image_channels,max_x, max_y), \n",
    "                                                               dtype=float_memory_used), axis=0)\n",
    "            image_save = np.swapaxes(image, 0, 2)\n",
    "            plt.imsave(image_dir + '/image' + str(image_num)+'.png', image_save)\n",
    "        elif style == \"raw_power_min_max_norm\" or style == \"raw_power_zscore_norm\":\n",
    "    #         np.save(image_dir + '/image' + str(image_num), image)\n",
    "            np.savez_compressed(image_dir + '/image' + str(image_num), a=image)\n",
    "        del image\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6346/6346 [11:13<00:00,  9.42it/s]\n",
      "100%|██████████| 6346/6346 [11:13<00:00,  9.42it/s]\n",
      "100%|██████████| 6346/6346 [11:13<00:00,  9.42it/s]\n",
      "100%|██████████| 6346/6346 [11:14<00:00,  9.41it/s]\n",
      "100%|██████████| 6346/6346 [11:15<00:00,  9.40it/s]\n",
      "100%|██████████| 6346/6346 [11:15<00:00,  9.39it/s]\n"
     ]
    }
   ],
   "source": [
    "jobs = []\n",
    "proc_sizes = [data_reg.shape[0]//number_of_proccessors] * (number_of_proccessors)\n",
    "proc_sizes[-1] += data_reg.shape[0]%number_of_proccessors\n",
    "proc_idx = [(sum(proc_sizes[:i]), sum(proc_sizes[:i+1])-1) for i in range(number_of_proccessors)]\n",
    "\n",
    "for i in range(number_of_proccessors):\n",
    "    p = Process(target=creating_image, args=(proc_idx[i][0], proc_idx[i][1]))\n",
    "    jobs.append(p)\n",
    "    p.start()\n",
    "for i in range(number_of_proccessors):\n",
    "    jobs[i].join()\n",
    "\n",
    "for i in range(number_of_proccessors):\n",
    "    jobs[i].terminate()\n",
    "    jobs[i].close()\n",
    "del jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Point(x=25, y=25)\n",
      "2 Point(x=25, y=75)\n",
      "3 Point(x=25, y=125)\n",
      "4 Point(x=25, y=175)\n",
      "5 Point(x=25, y=225)\n",
      "6 Point(x=25, y=275)\n",
      "7 Point(x=25, y=325)\n",
      "8 Point(x=25, y=375)\n",
      "9 Point(x=25, y=425)\n",
      "10 Point(x=25, y=475)\n",
      "11 Point(x=25, y=525)\n",
      "12 Point(x=25, y=575)\n",
      "13 Point(x=25, y=625)\n",
      "14 Point(x=25, y=675)\n",
      "15 Point(x=25, y=725)\n",
      "16 Point(x=25, y=775)\n",
      "17 Point(x=25, y=825)\n",
      "18 Point(x=25, y=875)\n",
      "19 Point(x=25, y=925)\n",
      "20 Point(x=25, y=975)\n",
      "21 Point(x=75, y=25)\n",
      "22 Point(x=75, y=75)\n",
      "23 Point(x=75, y=125)\n",
      "24 Point(x=75, y=175)\n",
      "25 Point(x=75, y=225)\n",
      "26 Point(x=75, y=275)\n",
      "27 Point(x=75, y=325)\n",
      "28 Point(x=75, y=375)\n",
      "29 Point(x=75, y=425)\n",
      "30 Point(x=75, y=475)\n",
      "31 Point(x=75, y=525)\n",
      "32 Point(x=75, y=575)\n",
      "33 Point(x=75, y=625)\n",
      "34 Point(x=75, y=675)\n",
      "35 Point(x=75, y=725)\n",
      "36 Point(x=75, y=775)\n",
      "37 Point(x=75, y=825)\n",
      "38 Point(x=75, y=875)\n",
      "39 Point(x=75, y=925)\n",
      "40 Point(x=75, y=975)\n",
      "41 Point(x=125, y=25)\n",
      "42 Point(x=125, y=75)\n",
      "43 Point(x=125, y=125)\n",
      "44 Point(x=125, y=175)\n",
      "45 Point(x=125, y=225)\n",
      "46 Point(x=125, y=275)\n",
      "47 Point(x=125, y=325)\n",
      "48 Point(x=125, y=375)\n",
      "49 Point(x=125, y=425)\n",
      "50 Point(x=125, y=475)\n",
      "51 Point(x=125, y=525)\n",
      "52 Point(x=125, y=575)\n",
      "53 Point(x=125, y=625)\n",
      "54 Point(x=125, y=675)\n",
      "55 Point(x=125, y=725)\n",
      "56 Point(x=125, y=775)\n",
      "57 Point(x=125, y=825)\n",
      "58 Point(x=125, y=875)\n",
      "59 Point(x=125, y=925)\n",
      "60 Point(x=125, y=975)\n",
      "61 Point(x=175, y=25)\n",
      "62 Point(x=175, y=75)\n",
      "63 Point(x=175, y=125)\n",
      "64 Point(x=175, y=175)\n",
      "65 Point(x=175, y=225)\n",
      "66 Point(x=175, y=275)\n",
      "67 Point(x=175, y=325)\n",
      "68 Point(x=175, y=375)\n",
      "69 Point(x=175, y=425)\n",
      "70 Point(x=175, y=475)\n",
      "71 Point(x=175, y=525)\n",
      "72 Point(x=175, y=575)\n",
      "73 Point(x=175, y=625)\n",
      "74 Point(x=175, y=675)\n",
      "75 Point(x=175, y=725)\n",
      "76 Point(x=175, y=775)\n",
      "77 Point(x=175, y=825)\n",
      "78 Point(x=175, y=875)\n",
      "79 Point(x=175, y=925)\n",
      "80 Point(x=175, y=975)\n",
      "81 Point(x=225, y=25)\n",
      "82 Point(x=225, y=75)\n",
      "83 Point(x=225, y=125)\n",
      "84 Point(x=225, y=175)\n",
      "85 Point(x=225, y=225)\n",
      "86 Point(x=225, y=275)\n",
      "87 Point(x=225, y=325)\n",
      "88 Point(x=225, y=375)\n",
      "89 Point(x=225, y=425)\n",
      "90 Point(x=225, y=475)\n",
      "91 Point(x=225, y=525)\n",
      "92 Point(x=225, y=575)\n",
      "93 Point(x=225, y=625)\n",
      "94 Point(x=225, y=675)\n",
      "95 Point(x=225, y=725)\n",
      "96 Point(x=225, y=775)\n",
      "97 Point(x=225, y=825)\n",
      "98 Point(x=225, y=875)\n",
      "99 Point(x=225, y=925)\n",
      "100 Point(x=225, y=975)\n",
      "101 Point(x=275, y=25)\n",
      "102 Point(x=275, y=75)\n",
      "103 Point(x=275, y=125)\n",
      "104 Point(x=275, y=175)\n",
      "105 Point(x=275, y=225)\n",
      "106 Point(x=275, y=275)\n",
      "107 Point(x=275, y=325)\n",
      "108 Point(x=275, y=375)\n",
      "109 Point(x=275, y=425)\n",
      "110 Point(x=275, y=475)\n",
      "111 Point(x=275, y=525)\n",
      "112 Point(x=275, y=575)\n",
      "113 Point(x=275, y=625)\n",
      "114 Point(x=275, y=675)\n",
      "115 Point(x=275, y=725)\n",
      "116 Point(x=275, y=775)\n",
      "117 Point(x=275, y=825)\n",
      "118 Point(x=275, y=875)\n",
      "119 Point(x=275, y=925)\n",
      "120 Point(x=275, y=975)\n",
      "121 Point(x=325, y=25)\n",
      "122 Point(x=325, y=75)\n",
      "123 Point(x=325, y=125)\n",
      "124 Point(x=325, y=175)\n",
      "125 Point(x=325, y=225)\n",
      "126 Point(x=325, y=275)\n",
      "127 Point(x=325, y=325)\n",
      "128 Point(x=325, y=375)\n",
      "129 Point(x=325, y=425)\n",
      "130 Point(x=325, y=475)\n",
      "131 Point(x=325, y=525)\n",
      "132 Point(x=325, y=575)\n",
      "133 Point(x=325, y=625)\n",
      "134 Point(x=325, y=675)\n",
      "135 Point(x=325, y=725)\n",
      "136 Point(x=325, y=775)\n",
      "137 Point(x=325, y=825)\n",
      "138 Point(x=325, y=875)\n",
      "139 Point(x=325, y=925)\n",
      "140 Point(x=325, y=975)\n",
      "141 Point(x=375, y=25)\n",
      "142 Point(x=375, y=75)\n",
      "143 Point(x=375, y=125)\n",
      "144 Point(x=375, y=175)\n",
      "145 Point(x=375, y=225)\n",
      "146 Point(x=375, y=275)\n",
      "147 Point(x=375, y=325)\n",
      "148 Point(x=375, y=375)\n",
      "149 Point(x=375, y=425)\n",
      "150 Point(x=375, y=475)\n",
      "151 Point(x=375, y=525)\n",
      "152 Point(x=375, y=575)\n",
      "153 Point(x=375, y=625)\n",
      "154 Point(x=375, y=675)\n",
      "155 Point(x=375, y=725)\n",
      "156 Point(x=375, y=775)\n",
      "157 Point(x=375, y=825)\n",
      "158 Point(x=375, y=875)\n",
      "159 Point(x=375, y=925)\n",
      "160 Point(x=375, y=975)\n",
      "161 Point(x=425, y=25)\n",
      "162 Point(x=425, y=75)\n",
      "163 Point(x=425, y=125)\n",
      "164 Point(x=425, y=175)\n",
      "165 Point(x=425, y=225)\n",
      "166 Point(x=425, y=275)\n",
      "167 Point(x=425, y=325)\n",
      "168 Point(x=425, y=375)\n",
      "169 Point(x=425, y=425)\n",
      "170 Point(x=425, y=475)\n",
      "171 Point(x=425, y=525)\n",
      "172 Point(x=425, y=575)\n",
      "173 Point(x=425, y=625)\n",
      "174 Point(x=425, y=675)\n",
      "175 Point(x=425, y=725)\n",
      "176 Point(x=425, y=775)\n",
      "177 Point(x=425, y=825)\n",
      "178 Point(x=425, y=875)\n",
      "179 Point(x=425, y=925)\n",
      "180 Point(x=425, y=975)\n",
      "181 Point(x=475, y=25)\n",
      "182 Point(x=475, y=75)\n",
      "183 Point(x=475, y=125)\n",
      "184 Point(x=475, y=175)\n",
      "185 Point(x=475, y=225)\n",
      "186 Point(x=475, y=275)\n",
      "187 Point(x=475, y=325)\n",
      "188 Point(x=475, y=375)\n",
      "189 Point(x=475, y=425)\n",
      "190 Point(x=475, y=475)\n",
      "191 Point(x=475, y=525)\n",
      "192 Point(x=475, y=575)\n",
      "193 Point(x=475, y=625)\n",
      "194 Point(x=475, y=675)\n",
      "195 Point(x=475, y=725)\n",
      "196 Point(x=475, y=775)\n",
      "197 Point(x=475, y=825)\n",
      "198 Point(x=475, y=875)\n",
      "199 Point(x=475, y=925)\n",
      "200 Point(x=475, y=975)\n",
      "201 Point(x=525, y=25)\n",
      "202 Point(x=525, y=75)\n",
      "203 Point(x=525, y=125)\n",
      "204 Point(x=525, y=175)\n",
      "205 Point(x=525, y=225)\n",
      "206 Point(x=525, y=275)\n",
      "207 Point(x=525, y=325)\n",
      "208 Point(x=525, y=375)\n",
      "209 Point(x=525, y=425)\n",
      "210 Point(x=525, y=475)\n",
      "211 Point(x=525, y=525)\n",
      "212 Point(x=525, y=575)\n",
      "213 Point(x=525, y=625)\n",
      "214 Point(x=525, y=675)\n",
      "215 Point(x=525, y=725)\n",
      "216 Point(x=525, y=775)\n",
      "217 Point(x=525, y=825)\n",
      "218 Point(x=525, y=875)\n",
      "219 Point(x=525, y=925)\n",
      "220 Point(x=525, y=975)\n",
      "221 Point(x=575, y=25)\n",
      "222 Point(x=575, y=75)\n",
      "223 Point(x=575, y=125)\n",
      "224 Point(x=575, y=175)\n",
      "225 Point(x=575, y=225)\n",
      "226 Point(x=575, y=275)\n",
      "227 Point(x=575, y=325)\n",
      "228 Point(x=575, y=375)\n",
      "229 Point(x=575, y=425)\n",
      "230 Point(x=575, y=475)\n",
      "231 Point(x=575, y=525)\n",
      "232 Point(x=575, y=575)\n",
      "233 Point(x=575, y=625)\n",
      "234 Point(x=575, y=675)\n",
      "235 Point(x=575, y=725)\n",
      "236 Point(x=575, y=775)\n",
      "237 Point(x=575, y=825)\n",
      "238 Point(x=575, y=875)\n",
      "239 Point(x=575, y=925)\n",
      "240 Point(x=575, y=975)\n",
      "241 Point(x=625, y=25)\n",
      "242 Point(x=625, y=75)\n",
      "243 Point(x=625, y=125)\n",
      "244 Point(x=625, y=175)\n",
      "245 Point(x=625, y=225)\n",
      "246 Point(x=625, y=275)\n",
      "247 Point(x=625, y=325)\n",
      "248 Point(x=625, y=375)\n",
      "249 Point(x=625, y=425)\n",
      "250 Point(x=625, y=475)\n",
      "251 Point(x=625, y=525)\n",
      "252 Point(x=625, y=575)\n",
      "253 Point(x=625, y=625)\n",
      "254 Point(x=625, y=675)\n",
      "255 Point(x=625, y=725)\n",
      "256 Point(x=625, y=775)\n",
      "257 Point(x=625, y=825)\n",
      "258 Point(x=625, y=875)\n",
      "259 Point(x=625, y=925)\n",
      "260 Point(x=625, y=975)\n",
      "261 Point(x=675, y=25)\n",
      "262 Point(x=675, y=75)\n",
      "263 Point(x=675, y=125)\n",
      "264 Point(x=675, y=175)\n",
      "265 Point(x=675, y=225)\n",
      "266 Point(x=675, y=275)\n",
      "267 Point(x=675, y=325)\n",
      "268 Point(x=675, y=375)\n",
      "269 Point(x=675, y=425)\n",
      "270 Point(x=675, y=475)\n",
      "271 Point(x=675, y=525)\n",
      "272 Point(x=675, y=575)\n",
      "273 Point(x=675, y=625)\n",
      "274 Point(x=675, y=675)\n",
      "275 Point(x=675, y=725)\n",
      "276 Point(x=675, y=775)\n",
      "277 Point(x=675, y=825)\n",
      "278 Point(x=675, y=875)\n",
      "279 Point(x=675, y=925)\n",
      "280 Point(x=675, y=975)\n",
      "281 Point(x=725, y=25)\n",
      "282 Point(x=725, y=75)\n",
      "283 Point(x=725, y=125)\n",
      "284 Point(x=725, y=175)\n",
      "285 Point(x=725, y=225)\n",
      "286 Point(x=725, y=275)\n",
      "287 Point(x=725, y=325)\n",
      "288 Point(x=725, y=375)\n",
      "289 Point(x=725, y=425)\n",
      "290 Point(x=725, y=475)\n",
      "291 Point(x=725, y=525)\n",
      "292 Point(x=725, y=575)\n",
      "293 Point(x=725, y=625)\n",
      "294 Point(x=725, y=675)\n",
      "295 Point(x=725, y=725)\n",
      "296 Point(x=725, y=775)\n",
      "297 Point(x=725, y=825)\n",
      "298 Point(x=725, y=875)\n",
      "299 Point(x=725, y=925)\n",
      "300 Point(x=725, y=975)\n",
      "301 Point(x=775, y=25)\n",
      "302 Point(x=775, y=75)\n",
      "303 Point(x=775, y=125)\n",
      "304 Point(x=775, y=175)\n",
      "305 Point(x=775, y=225)\n",
      "306 Point(x=775, y=275)\n",
      "307 Point(x=775, y=325)\n",
      "308 Point(x=775, y=375)\n",
      "309 Point(x=775, y=425)\n",
      "310 Point(x=775, y=475)\n",
      "311 Point(x=775, y=525)\n",
      "312 Point(x=775, y=575)\n",
      "313 Point(x=775, y=625)\n",
      "314 Point(x=775, y=675)\n",
      "315 Point(x=775, y=725)\n",
      "316 Point(x=775, y=775)\n",
      "317 Point(x=775, y=825)\n",
      "318 Point(x=775, y=875)\n",
      "319 Point(x=775, y=925)\n",
      "320 Point(x=775, y=975)\n",
      "321 Point(x=825, y=25)\n",
      "322 Point(x=825, y=75)\n",
      "323 Point(x=825, y=125)\n",
      "324 Point(x=825, y=175)\n",
      "325 Point(x=825, y=225)\n",
      "326 Point(x=825, y=275)\n",
      "327 Point(x=825, y=325)\n",
      "328 Point(x=825, y=375)\n",
      "329 Point(x=825, y=425)\n",
      "330 Point(x=825, y=475)\n",
      "331 Point(x=825, y=525)\n",
      "332 Point(x=825, y=575)\n",
      "333 Point(x=825, y=625)\n",
      "334 Point(x=825, y=675)\n",
      "335 Point(x=825, y=725)\n",
      "336 Point(x=825, y=775)\n",
      "337 Point(x=825, y=825)\n",
      "338 Point(x=825, y=875)\n",
      "339 Point(x=825, y=925)\n",
      "340 Point(x=825, y=975)\n",
      "341 Point(x=875, y=25)\n",
      "342 Point(x=875, y=75)\n",
      "343 Point(x=875, y=125)\n",
      "344 Point(x=875, y=175)\n",
      "345 Point(x=875, y=225)\n",
      "346 Point(x=875, y=275)\n",
      "347 Point(x=875, y=325)\n",
      "348 Point(x=875, y=375)\n",
      "349 Point(x=875, y=425)\n",
      "350 Point(x=875, y=475)\n",
      "351 Point(x=875, y=525)\n",
      "352 Point(x=875, y=575)\n",
      "353 Point(x=875, y=625)\n",
      "354 Point(x=875, y=675)\n",
      "355 Point(x=875, y=725)\n",
      "356 Point(x=875, y=775)\n",
      "357 Point(x=875, y=825)\n",
      "358 Point(x=875, y=875)\n",
      "359 Point(x=875, y=925)\n",
      "360 Point(x=875, y=975)\n",
      "361 Point(x=925, y=25)\n",
      "362 Point(x=925, y=75)\n",
      "363 Point(x=925, y=125)\n",
      "364 Point(x=925, y=175)\n",
      "365 Point(x=925, y=225)\n",
      "366 Point(x=925, y=275)\n",
      "367 Point(x=925, y=325)\n",
      "368 Point(x=925, y=375)\n",
      "369 Point(x=925, y=425)\n",
      "370 Point(x=925, y=475)\n",
      "371 Point(x=925, y=525)\n",
      "372 Point(x=925, y=575)\n",
      "373 Point(x=925, y=625)\n",
      "374 Point(x=925, y=675)\n",
      "375 Point(x=925, y=725)\n",
      "376 Point(x=925, y=775)\n",
      "377 Point(x=925, y=825)\n",
      "378 Point(x=925, y=875)\n",
      "379 Point(x=925, y=925)\n",
      "380 Point(x=925, y=975)\n",
      "381 Point(x=975, y=25)\n",
      "382 Point(x=975, y=75)\n",
      "383 Point(x=975, y=125)\n",
      "384 Point(x=975, y=175)\n",
      "385 Point(x=975, y=225)\n",
      "386 Point(x=975, y=275)\n",
      "387 Point(x=975, y=325)\n",
      "388 Point(x=975, y=375)\n",
      "389 Point(x=975, y=425)\n",
      "390 Point(x=975, y=475)\n",
      "391 Point(x=975, y=525)\n",
      "392 Point(x=975, y=575)\n",
      "393 Point(x=975, y=625)\n",
      "394 Point(x=975, y=675)\n",
      "395 Point(x=975, y=725)\n",
      "396 Point(x=975, y=775)\n",
      "397 Point(x=975, y=825)\n",
      "398 Point(x=975, y=875)\n",
      "399 Point(x=975, y=925)\n",
      "400 Point(x=975, y=975)\n"
     ]
    }
   ],
   "source": [
    "for idx, point in enumerate(sensors_location):\n",
    "    print(idx+1, point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "823 Point(x=916, y=416) close\n"
     ]
    }
   ],
   "source": [
    "for idx, point in enumerate(sensors_location):\n",
    "    print(idx+1, point,\"close\") if math.sqrt((point.x-917)**2+(point.y-415)**2)<=1.5 else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = [0, 0, 0, 0]\n",
    "idxx = [[],[],[],[]]\n",
    "for i in range(data_reg.shape[0]):\n",
    "    pus_c = int(data_reg[i][0]) * 3 + 1\n",
    "    idx = int(data_reg[i][pus_c]) - 1\n",
    "    count[idx] += 1\n",
    "    idxx[idx].append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[24178, 12235, 5997, 3024]\n",
      "[18, 37, 42, 58, 79, 82, 89, 90, 103, 124, 142, 149, 171, 173, 189, 207, 233, 238, 269, 277, 293, 319, 331, 341, 387, 396, 404, 410, 418, 420, 432, 440, 444, 497, 525, 539, 544, 614, 616, 619, 627, 632, 633, 645, 648, 651, 663, 732, 775, 778, 843, 902, 923, 949, 965, 967, 968, 988, 996, 1018, 1020, 1024, 1030, 1036, 1075, 1084, 1105, 1116, 1129, 1133, 1140, 1142, 1152, 1168, 1171, 1188, 1250, 1251, 1264, 1279, 1288, 1306, 1329, 1367, 1369, 1389, 1390, 1421, 1430, 1435, 1440, 1447, 1450, 1456, 1463, 1534, 1539, 1550, 1562, 1594, 1606, 1621, 1632, 1654, 1658, 1664, 1683, 1714, 1754, 1773, 1796, 1820, 1821, 1843, 1848, 1865, 1871, 1897, 1905, 1938, 1943, 1948, 1997, 2017, 2023, 2040, 2053, 2062, 2087, 2103, 2110, 2116, 2119, 2148, 2157, 2169, 2182, 2220, 2227, 2288, 2326, 2364, 2372, 2387, 2402, 2426, 2431, 2434, 2436, 2441, 2447, 2472, 2478, 2491, 2497, 2532, 2543, 2557, 2582, 2588, 2619, 2628, 2706, 2741, 2742, 2743, 2747, 2751, 2762, 2788, 2807, 2870, 2874, 2894, 2915, 2922, 2931, 2957, 2966, 2974, 2989, 3026, 3030, 3046, 3049, 3063, 3082, 3083, 3105, 3119, 3154, 3159, 3178, 3181, 3182, 3196, 3212, 3229, 3232, 3249, 3258, 3260, 3284, 3285, 3290, 3292, 3304, 3307, 3309, 3333, 3340, 3346, 3364, 3365, 3380, 3397, 3437, 3443, 3472, 3499, 3503, 3517, 3518, 3520, 3524, 3572, 3588, 3600, 3608, 3616, 3626, 3641, 3649, 3665, 3666, 3667, 3669, 3676, 3682, 3684, 3729, 3738, 3744, 3749, 3764, 3765, 3784, 3788, 3809, 3812, 3822, 3845, 3852, 3862, 3881, 3891, 3899, 3910, 3942, 3953, 3974, 3978, 4045, 4047, 4052, 4087, 4135, 4140, 4144, 4159, 4160, 4168, 4177, 4183, 4193, 4195, 4207, 4208, 4251, 4264, 4281, 4282, 4287, 4295, 4314, 4320, 4335, 4395, 4406, 4416, 4450, 4467, 4513, 4514, 4521, 4547, 4568, 4578, 4587, 4607, 4622, 4646, 4648, 4672, 4681, 4702, 4708, 4723, 4727, 4728, 4742, 4743, 4748, 4774, 4780, 4799, 4813, 4817, 4822, 4870, 4905, 4913, 4925, 4930, 4952, 4975, 4976, 4982, 4992, 5003, 5028, 5033, 5039, 5051, 5056, 5058, 5063, 5081, 5084, 5095, 5098, 5146, 5174, 5190, 5197, 5240, 5245, 5263, 5278, 5305, 5324, 5339, 5347, 5352, 5365, 5374, 5384, 5385, 5390, 5395, 5427, 5436, 5439, 5447, 5464, 5490, 5502, 5510, 5512, 5514, 5531, 5532, 5533, 5536, 5539, 5557, 5593, 5606, 5626, 5681, 5694, 5713, 5769, 5821, 5842, 5846, 5859, 5863, 5889, 5903, 5942, 5960, 5964, 5989, 5997, 6001, 6016, 6022, 6024, 6037, 6048, 6058, 6077, 6081, 6082, 6099, 6106, 6125, 6146, 6165, 6194, 6208, 6213, 6219, 6225, 6232, 6242, 6245, 6252, 6294, 6308, 6318, 6325, 6327, 6342, 6366, 6370, 6372, 6373, 6386, 6400, 6419, 6423, 6432, 6436, 6443, 6501, 6511, 6520, 6541, 6573, 6601, 6605, 6612, 6616, 6617, 6619, 6631, 6656, 6680, 6718, 6726, 6754, 6762, 6764, 6768, 6783, 6796, 6823, 6828, 6840, 6846, 6847, 6853, 6859, 6881, 6925, 6926, 6929, 6930, 6939, 6955, 6956, 6958, 6967, 7021, 7037, 7050, 7057, 7071, 7078, 7082, 7115, 7118, 7143, 7172, 7198, 7202, 7214, 7227, 7235, 7254, 7268, 7308, 7331, 7382, 7410, 7426, 7427, 7434, 7447, 7458, 7487, 7509, 7512, 7521, 7547, 7557, 7571, 7579, 7583, 7588, 7589, 7591, 7618, 7625, 7632, 7691, 7700, 7705, 7717, 7731, 7733, 7738, 7744, 7757, 7767, 7771, 7777, 7812, 7828, 7889, 7910, 7921, 7922, 7963, 7999, 8016, 8024, 8050, 8051, 8066, 8076, 8108, 8116, 8146, 8173, 8180, 8181, 8183, 8214, 8220, 8228, 8232, 8235, 8263, 8297, 8310, 8311, 8331, 8371, 8372, 8388, 8389, 8393, 8407, 8468, 8472, 8494, 8500, 8509, 8531, 8556, 8571, 8579, 8584, 8622, 8623, 8629, 8655, 8675, 8677, 8687, 8727, 8736, 8738, 8746, 8753, 8786, 8789, 8792, 8816, 8829, 8835, 8839, 8847, 8866, 8899, 8906, 8909, 8930, 8940, 8944, 8952, 8967, 8970, 8987, 8997, 9034, 9061, 9080, 9087, 9095, 9102, 9110, 9127, 9131, 9148, 9158, 9161, 9175, 9178, 9208, 9217, 9242, 9248, 9258, 9266, 9273, 9283, 9288, 9305, 9335, 9342, 9347, 9357, 9371, 9399, 9428, 9441, 9458, 9464, 9473, 9490, 9539, 9541, 9552, 9603, 9608, 9625, 9629, 9635, 9662, 9670, 9671, 9672, 9688, 9699, 9710, 9763, 9774, 9790, 9793, 9865, 9876, 9889, 9896, 9903, 9907, 9952, 10010, 10020, 10021, 10034, 10043, 10046, 10050, 10056, 10074, 10084, 10090, 10105, 10115, 10159, 10170, 10181, 10192, 10213, 10218, 10256, 10259, 10295, 10296, 10303, 10320, 10331, 10336, 10351, 10372, 10421, 10430, 10438, 10458, 10471, 10529, 10531, 10557, 10571, 10591, 10593, 10623, 10629, 10637, 10642, 10662, 10667, 10723, 10769, 10794, 10800, 10812, 10829, 10837, 10838, 10844, 10855, 10865, 10880, 10885, 10887, 10895, 10909, 10914, 10941, 10978, 10980, 10981, 10986, 11010, 11017, 11048, 11056, 11068, 11069, 11116, 11117, 11118, 11135, 11148, 11156, 11166, 11174, 11203, 11204, 11256, 11284, 11287, 11297, 11322, 11341, 11355, 11375, 11403, 11406, 11430, 11442, 11443, 11470, 11484, 11487, 11491, 11508, 11520, 11559, 11578, 11582, 11583, 11590, 11616, 11626, 11627, 11636, 11650, 11672, 11673, 11681, 11693, 11700, 11711, 11733, 11737, 11748, 11751, 11757, 11765, 11781, 11791, 11796, 11802, 11805, 11810, 11832, 11838, 11842, 11852, 11874, 11895, 11935, 11939, 11940, 11943, 11980, 11989, 12002, 12017, 12020, 12021, 12033, 12034, 12037, 12077, 12080, 12089, 12091, 12125, 12136, 12166, 12184, 12193, 12194, 12195, 12212, 12244, 12276, 12312, 12324, 12330, 12361, 12362, 12371, 12374, 12386, 12404, 12417, 12455, 12475, 12515, 12518, 12525, 12529, 12541, 12545, 12546, 12560, 12567, 12569, 12592, 12611, 12651, 12656, 12673, 12676, 12678, 12722, 12725, 12736, 12742, 12757, 12758, 12772, 12782, 12785, 12806, 12833, 12835, 12837, 12849, 12851, 12858, 12888, 12892, 12895, 12922, 12956, 12987, 13013, 13030, 13033, 13063, 13066, 13069, 13078, 13091, 13111, 13112, 13150, 13180, 13196, 13199, 13220, 13235, 13239, 13242, 13244, 13286, 13288, 13296, 13298, 13308, 13313, 13338, 13347, 13360, 13384, 13386, 13412, 13444, 13478, 13489, 13490, 13526, 13529, 13560, 13561, 13570, 13571, 13572, 13576, 13592, 13613, 13635, 13641, 13702, 13746, 13750, 13757, 13773, 13814, 13835, 13869, 13878, 13881, 13904, 13906, 13918, 13923, 13928, 13931, 13937, 13979, 13998, 14039, 14052, 14069, 14075, 14090, 14095, 14098, 14121, 14129, 14145, 14179, 14180, 14194, 14200, 14226, 14247, 14249, 14254, 14261, 14271, 14280, 14309, 14313, 14319, 14326, 14345, 14357, 14409, 14457, 14467, 14485, 14491, 14495, 14499, 14504, 14520, 14526, 14530, 14540, 14541, 14552, 14576, 14581, 14584, 14591, 14594, 14614, 14641, 14665, 14679, 14686, 14757, 14782, 14786, 14793, 14796, 14804, 14813, 14842, 14846, 14874, 14901, 14917, 14930, 15024, 15038, 15040, 15051, 15054, 15072, 15078, 15084, 15091, 15096, 15111, 15122, 15133, 15137, 15149, 15180, 15218, 15259, 15263, 15264, 15273, 15275, 15279, 15299, 15304, 15305, 15318, 15320, 15346, 15361, 15365, 15395, 15442, 15457, 15459, 15480, 15516, 15528, 15539, 15571, 15578, 15585, 15630, 15632, 15635, 15660, 15670, 15679, 15714, 15740, 15764, 15794, 15894, 15926, 15930, 15946, 15956, 15981, 15987, 15989, 15998, 16028, 16034, 16037, 16060, 16067, 16087, 16092, 16096, 16145, 16159, 16173, 16183, 16186, 16189, 16191, 16201, 16206, 16208, 16219, 16231, 16237, 16241, 16272, 16313, 16323, 16349, 16361, 16366, 16367, 16373, 16390, 16410, 16417, 16479, 16489, 16505, 16506, 16507, 16523, 16536, 16581, 16583, 16588, 16607, 16612, 16617, 16637, 16644, 16650, 16651, 16658, 16667, 16673, 16675, 16687, 16724, 16748, 16783, 16795, 16882, 16927, 16941, 16954, 16963, 16989, 17018, 17040, 17075, 17120, 17128, 17134, 17135, 17147, 17149, 17155, 17162, 17246, 17251, 17258, 17261, 17269, 17299, 17305, 17320, 17336, 17347, 17361, 17370, 17371, 17380, 17395, 17396, 17404, 17428, 17467, 17497, 17527, 17539, 17563, 17573, 17576, 17578, 17599, 17621, 17622, 17647, 17661, 17665, 17692, 17726, 17735, 17737, 17738, 17748, 17762, 17791, 17797, 17824, 17841, 17842, 17864, 17881, 17894, 17897, 17898, 17904, 17921, 17938, 17939, 17943, 17956, 17982, 17992, 17998, 18011, 18028, 18071, 18076, 18086, 18095, 18135, 18136, 18143, 18154, 18168, 18205, 18216, 18226, 18229, 18231, 18238, 18241, 18296, 18301, 18337, 18360, 18366, 18369, 18381, 18392, 18430, 18474, 18483, 18486, 18492, 18506, 18508, 18511, 18524, 18535, 18536, 18564, 18576, 18632, 18643, 18653, 18673, 18679, 18686, 18727, 18750, 18754, 18765, 18769, 18774, 18780, 18790, 18842, 18853, 18875, 18906, 18918, 18928, 19041, 19043, 19062, 19068, 19097, 19100, 19110, 19131, 19144, 19162, 19170, 19192, 19207, 19211, 19213, 19221, 19223, 19246, 19266, 19282, 19283, 19311, 19312, 19316, 19319, 19336, 19371, 19379, 19383, 19390, 19428, 19438, 19466, 19482, 19498, 19511, 19520, 19525, 19601, 19610, 19611, 19632, 19640, 19645, 19657, 19660, 19662, 19674, 19675, 19696, 19705, 19768, 19772, 19774, 19785, 19787, 19790, 19805, 19818, 19846, 19860, 19866, 19895, 19898, 19911, 19948, 19951, 19958, 19983, 20043, 20050, 20071, 20094, 20097, 20101, 20114, 20205, 20207, 20209, 20217, 20229, 20236, 20239, 20258, 20262, 20304, 20318, 20358, 20372, 20392, 20407, 20420, 20424, 20429, 20559, 20563, 20580, 20590, 20595, 20624, 20637, 20643, 20647, 20655, 20659, 20663, 20708, 20712, 20728, 20734, 20739, 20752, 20763, 20768, 20779, 20795, 20817, 20823, 20831, 20852, 20858, 20905, 20938, 20946, 20962, 20989, 20991, 20992, 21008, 21014, 21058, 21075, 21084, 21100, 21109, 21150, 21175, 21207, 21208, 21210, 21229, 21237, 21245, 21250, 21264, 21265, 21273, 21284, 21294, 21322, 21341, 21348, 21350, 21379, 21393, 21400, 21478, 21484, 21491, 21494, 21505, 21506, 21514, 21555, 21607, 21613, 21616, 21619, 21654, 21661, 21665, 21708, 21716, 21721, 21734, 21756, 21783, 21784, 21786, 21792, 21800, 21811, 21840, 21884, 21926, 21955, 21983, 22004, 22012, 22016, 22030, 22039, 22042, 22051, 22054, 22057, 22078, 22087, 22090, 22094, 22125, 22131, 22153, 22172, 22193, 22205, 22206, 22208, 22225, 22231, 22236, 22244, 22257, 22258, 22264, 22289, 22296, 22307, 22325, 22333, 22370, 22432, 22433, 22441, 22442, 22453, 22473, 22482, 22484, 22487, 22513, 22516, 22586, 22623, 22626, 22638, 22679, 22683, 22694, 22696, 22697, 22736, 22737, 22759, 22760, 22765, 22778, 22779, 22782, 22809, 22879, 22893, 22914, 22916, 22972, 22978, 22979, 22985, 22989, 22993, 23009, 23013, 23019, 23028, 23045, 23056, 23071, 23092, 23096, 23122, 23127, 23154, 23182, 23199, 23205, 23223, 23232, 23235, 23239, 23256, 23261, 23263, 23274, 23293, 23299, 23330, 23336, 23337, 23354, 23384, 23430, 23462, 23469, 23489, 23492, 23501, 23504, 23543, 23593, 23600, 23631, 23644, 23654, 23663, 23680, 23695, 23699, 23719, 23731, 23733, 23759, 23762, 23778, 23783, 23803, 23805, 23809, 23826, 23865, 23868, 23909, 23926, 23928, 23943, 23945, 23991, 24004, 24008, 24034, 24037, 24051, 24060, 24074, 24085, 24092, 24094, 24140, 24149, 24150, 24166, 24169, 24182, 24184, 24203, 24213, 24219, 24222, 24229, 24247, 24249, 24255, 24267, 24280, 24300, 24306, 24314, 24325, 24352, 24362, 24376, 24397, 24405, 24415, 24417, 24429, 24441, 24446, 24464, 24493, 24509, 24512, 24515, 24533, 24551, 24560, 24562, 24570, 24572, 24580, 24612, 24623, 24647, 24657, 24668, 24669, 24675, 24723, 24732, 24742, 24771, 24784, 24789, 24811, 24818, 24821, 24829, 24832, 24842, 24860, 24919, 24936, 24939, 24978, 24984, 25005, 25046, 25053, 25057, 25060, 25068, 25089, 25106, 25132, 25146, 25182, 25197, 25200, 25213, 25231, 25233, 25251, 25256, 25258, 25268, 25297, 25319, 25356, 25378, 25381, 25385, 25386, 25425, 25436, 25453, 25465, 25484, 25495, 25517, 25520, 25526, 25540, 25560, 25568, 25589, 25623, 25626, 25639, 25686, 25687, 25745, 25753, 25767, 25779, 25786, 25787, 25802, 25831, 25833, 25850, 25861, 25894, 25903, 25905, 25906, 25908, 25926, 25935, 25942, 25951, 25994, 26002, 26011, 26021, 26023, 26037, 26075, 26087, 26089, 26097, 26101, 26107, 26121, 26141, 26147, 26185, 26186, 26204, 26271, 26272, 26277, 26289, 26297, 26310, 26319, 26324, 26332, 26341, 26378, 26396, 26418, 26425, 26426, 26456, 26499, 26514, 26519, 26523, 26527, 26535, 26545, 26555, 26577, 26584, 26600, 26627, 26632, 26651, 26653, 26657, 26662, 26664, 26677, 26678, 26681, 26690, 26697, 26698, 26708, 26714, 26736, 26753, 26765, 26795, 26812, 26828, 26835, 26847, 26862, 26866, 26873, 26899, 26917, 26939, 26974, 26988, 26994, 27008, 27097, 27102, 27105, 27107, 27123, 27139, 27181, 27196, 27197, 27224, 27235, 27238, 27249, 27263, 27297, 27300, 27371, 27378, 27405, 27442, 27477, 27535, 27537, 27548, 27566, 27577, 27595, 27601, 27606, 27618, 27619, 27630, 27636, 27654, 27657, 27702, 27713, 27739, 27752, 27754, 27786, 27804, 27818, 27821, 27825, 27832, 27858, 27863, 27877, 27894, 27907, 27920, 27936, 27940, 27943, 27960, 27966, 27979, 27997, 28010, 28024, 28078, 28103, 28107, 28121, 28129, 28132, 28166, 28171, 28175, 28176, 28183, 28196, 28200, 28213, 28232, 28246, 28247, 28268, 28304, 28360, 28361, 28386, 28401, 28410, 28413, 28435, 28440, 28455, 28478, 28479, 28480, 28481, 28482, 28496, 28528, 28543, 28560, 28573, 28580, 28589, 28597, 28639, 28672, 28675, 28693, 28785, 28789, 28797, 28800, 28807, 28816, 28846, 28858, 28882, 28892, 28897, 28899, 28904, 28916, 28942, 28943, 28948, 28963, 29031, 29120, 29143, 29145, 29147, 29155, 29181, 29226, 29254, 29284, 29325, 29331, 29340, 29342, 29356, 29358, 29380, 29414, 29432, 29445, 29449, 29459, 29466, 29472, 29489, 29572, 29636, 29659, 29666, 29677, 29678, 29680, 29699, 29722, 29726, 29744, 29763, 29782, 29784, 29790, 29809, 29823, 29856, 29885, 29902, 29912, 29954, 29963, 29973, 30011, 30122, 30131, 30154, 30168, 30173, 30192, 30206, 30228, 30239, 30260, 30265, 30266, 30281, 30282, 30300, 30326, 30332, 30335, 30336, 30340, 30346, 30376, 30393, 30397, 30436, 30439, 30478, 30523, 30528, 30540, 30552, 30564, 30567, 30572, 30591, 30592, 30596, 30599, 30617, 30621, 30625, 30645, 30671, 30715, 30731, 30739, 30758, 30764, 30779, 30780, 30783, 30792, 30800, 30819, 30829, 30833, 30840, 30871, 30882, 30887, 30895, 30905, 30915, 30917, 30919, 30925, 30941, 30946, 30947, 30956, 30971, 30973, 30974, 30978, 31025, 31041, 31044, 31072, 31083, 31093, 31118, 31143, 31170, 31187, 31219, 31226, 31248, 31253, 31261, 31316, 31323, 31330, 31344, 31375, 31402, 31403, 31404, 31415, 31426, 31445, 31454, 31455, 31473, 31503, 31509, 31512, 31529, 31549, 31555, 31561, 31569, 31574, 31582, 31587, 31631, 31656, 31700, 31713, 31716, 31740, 31769, 31781, 31787, 31790, 31833, 31865, 31876, 31880, 31894, 31916, 31923, 31947, 31950, 31980, 32008, 32012, 32028, 32030, 32032, 32042, 32051, 32064, 32122, 32131, 32146, 32154, 32180, 32197, 32200, 32201, 32264, 32295, 32301, 32318, 32330, 32372, 32379, 32398, 32433, 32436, 32437, 32439, 32441, 32446, 32448, 32461, 32474, 32479, 32480, 32511, 32515, 32517, 32519, 32548, 32558, 32559, 32579, 32581, 32584, 32612, 32623, 32639, 32644, 32648, 32652, 32653, 32687, 32703, 32708, 32720, 32723, 32765, 32780, 32784, 32787, 32817, 32859, 32867, 32872, 32895, 32908, 32917, 32919, 32936, 32957, 32968, 32971, 33000, 33011, 33020, 33030, 33053, 33059, 33069, 33081, 33102, 33109, 33128, 33146, 33160, 33188, 33298, 33306, 33310, 33319, 33325, 33362, 33371, 33381, 33453, 33470, 33488, 33495, 33507, 33517, 33526, 33529, 33536, 33553, 33599, 33616, 33631, 33652, 33661, 33666, 33674, 33693, 33701, 33702, 33713, 33717, 33719, 33748, 33750, 33755, 33768, 33779, 33785, 33791, 33872, 33889, 33920, 33936, 33937, 33963, 33970, 33972, 33998, 34000, 34002, 34037, 34043, 34065, 34081, 34096, 34118, 34136, 34152, 34202, 34213, 34237, 34239, 34256, 34274, 34279, 34281, 34292, 34315, 34318, 34354, 34362, 34374, 34375, 34387, 34405, 34406, 34428, 34438, 34463, 34477, 34483, 34515, 34548, 34558, 34571, 34602, 34614, 34615, 34627, 34657, 34670, 34701, 34725, 34747, 34751, 34765, 34777, 34790, 34799, 34818, 34859, 34863, 34890, 34926, 34937, 34958, 34976, 34986, 34998, 35002, 35005, 35009, 35031, 35053, 35063, 35066, 35084, 35086, 35106, 35110, 35119, 35128, 35158, 35191, 35218, 35265, 35270, 35295, 35305, 35330, 35344, 35365, 35366, 35369, 35384, 35397, 35424, 35434, 35438, 35477, 35493, 35529, 35540, 35574, 35595, 35609, 35610, 35613, 35614, 35617, 35630, 35633, 35641, 35660, 35661, 35671, 35672, 35675, 35678, 35724, 35727, 35732, 35761, 35770, 35785, 35788, 35789, 35800, 35807, 35810, 35812, 35814, 35855, 35865, 35867, 35868, 35872, 35904, 35923, 35937, 35997, 36014, 36017, 36030, 36041, 36058, 36060, 36082, 36086, 36090, 36106, 36136, 36176, 36197, 36223, 36241, 36259, 36268, 36271, 36283, 36300, 36302, 36318, 36325, 36328, 36339, 36341, 36354, 36369, 36373, 36375, 36400, 36410, 36415, 36418, 36423, 36436, 36498, 36509, 36527, 36553, 36555, 36574, 36579, 36607, 36635, 36662, 36663, 36684, 36697, 36707, 36713, 36719, 36722, 36730, 36733, 36744, 36748, 36765, 36769, 36793, 36794, 36801, 36809, 36811, 36828, 36836, 36850, 36887, 36893, 36901, 36913, 36963, 36964, 36971, 36986, 36993, 36997, 37002, 37003, 37064, 37089, 37090, 37098, 37106, 37109, 37114, 37116, 37131, 37132, 37193, 37199, 37215, 37218, 37235, 37285, 37293, 37304, 37320, 37329, 37348, 37351, 37360, 37378, 37388, 37437, 37440, 37450, 37456, 37495, 37502, 37521, 37532, 37549, 37572, 37593, 37610, 37620, 37653, 37655, 37660, 37692, 37698, 37705, 37728, 37750, 37757, 37765, 37791, 37805, 37807, 37823, 37825, 37867, 37887, 37925, 37933, 37947, 37951, 37991, 38002, 38022, 38088, 38094, 38111, 38136, 38193, 38213, 38240, 38266, 38275, 38283, 38308, 38331, 38346, 38348, 38355, 38380, 38386, 38413, 38449, 38450, 38454, 38462, 38474, 38495, 38500, 38534, 38565, 38568, 38596, 38618, 38620, 38628, 38653, 38659, 38661, 38684, 38688, 38699, 38702, 38711, 38712, 38736, 38740, 38749, 38774, 38782, 38827, 38841, 38848, 38853, 38866, 38882, 38898, 38907, 38914, 38922, 38928, 38931, 38958, 38986, 38989, 38999, 39004, 39008, 39009, 39030, 39067, 39077, 39102, 39119, 39121, 39140, 39161, 39176, 39199, 39212, 39215, 39275, 39291, 39309, 39318, 39349, 39354, 39372, 39374, 39375, 39387, 39412, 39418, 39426, 39430, 39457, 39466, 39474, 39491, 39513, 39516, 39520, 39529, 39537, 39558, 39562, 39585, 39587, 39592, 39602, 39604, 39613, 39620, 39649, 39676, 39677, 39703, 39717, 39720, 39768, 39769, 39775, 39794, 39810, 39820, 39833, 39853, 39855, 39872, 39877, 39881, 39892, 39895, 39907, 39912, 39914, 39924, 39925, 39946, 39960, 39961, 39982, 39993, 40019, 40022, 40039, 40056, 40070, 40118, 40137, 40138, 40145, 40146, 40149, 40151, 40178, 40185, 40190, 40222, 40239, 40245, 40260, 40261, 40268, 40287, 40312, 40321, 40331, 40343, 40344, 40363, 40369, 40373, 40450, 40451, 40462, 40492, 40499, 40501, 40521, 40530, 40574, 40586, 40598, 40601, 40615, 40618, 40627, 40709, 40749, 40752, 40774, 40799, 40825, 40853, 40886, 40887, 40889, 40894, 40906, 40915, 40946, 40952, 40964, 40966, 40998, 41018, 41028, 41031, 41033, 41037, 41055, 41056, 41086, 41088, 41109, 41145, 41162, 41164, 41184, 41199, 41205, 41229, 41279, 41286, 41288, 41300, 41310, 41311, 41313, 41326, 41337, 41355, 41358, 41359, 41364, 41397, 41414, 41418, 41449, 41470, 41477, 41494, 41501, 41517, 41525, 41534, 41542, 41552, 41562, 41573, 41590, 41618, 41628, 41629, 41668, 41678, 41680, 41685, 41694, 41701, 41713, 41732, 41780, 41781, 41799, 41805, 41808, 41819, 41850, 41857, 41885, 41915, 41919, 41945, 41947, 41952, 41979, 41983, 41988, 41999, 42013, 42015, 42032, 42046, 42055, 42082, 42109, 42135, 42141, 42162, 42169, 42187, 42210, 42213, 42239, 42244, 42259, 42279, 42314, 42341, 42345, 42353, 42357, 42382, 42384, 42405, 42433, 42464, 42465, 42468, 42491, 42504, 42509, 42514, 42518, 42547, 42573, 42593, 42621, 42629, 42632, 42634, 42652, 42704, 42715, 42725, 42760, 42767, 42796, 42816, 42823, 42832, 42838, 42853, 42856, 42882, 42888, 42958, 42960, 42974, 42991, 43004, 43011, 43023, 43028, 43034, 43051, 43076, 43078, 43079, 43116, 43121, 43169, 43174, 43200, 43204, 43213, 43220, 43252, 43262, 43293, 43304, 43324, 43337, 43339, 43375, 43399, 43403, 43416, 43417, 43465, 43508, 43509, 43513, 43517, 43562, 43582, 43584, 43595, 43601, 43649, 43655, 43657, 43668, 43676, 43677, 43681, 43702, 43721, 43736, 43763, 43766, 43796, 43800, 43814, 43843, 43848, 43862, 43894, 43896, 43920, 43926, 43947, 43962, 43984, 43996, 44002, 44022, 44056, 44069, 44085, 44103, 44111, 44124, 44139, 44148, 44153, 44249, 44265, 44282, 44292, 44328, 44332, 44340, 44376, 44395, 44401, 44439, 44489, 44500, 44516, 44527, 44541, 44555, 44570, 44587, 44646, 44653, 44658, 44668, 44669, 44674, 44682, 44691, 44785, 44792, 44796, 44828, 44832, 44838, 44843, 44851, 44860, 44873, 44889, 44909, 44917, 44925, 44947, 44949, 44966, 44973, 44982, 44995, 45026, 45043, 45047, 45049, 45055, 45060, 45091, 45111, 45122, 45135, 45152, 45155, 45173, 45179, 45182, 45189, 45230, 45277, 45279, 45293, 45298, 45305, 45311, 45342, 45354, 45377, 45380, 45388, 45396, 45422, 45426]\n"
     ]
    }
   ],
   "source": [
    "print(count)\n",
    "print(idxx[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "imm = read_image(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-93.964   1.     37.     80.     22.378   0.      9.42 ]\n"
     ]
    }
   ],
   "source": [
    "print(data_reg[1][-7:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7ff56ed59ad0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2160x2160 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(30,30))         # Sample figsize in inches\n",
    "sns.heatmap(imm[0][6], ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_model(num_filters, kernel_lam, bias_lam):\n",
    "#     num_filters, lam = 5, 5\n",
    "    data_format = 'channels_first'\n",
    "    convolution_init, dense_init = \"lecun_normal\", \"RandomNormal\"\n",
    "    convolution_filter, dense_filter = 'selu', 'linear' #softsign, sigmoid; relu, linear\n",
    "    filter_shape, pool_size = (3, 3), (2,2)\n",
    "    cnn = models.Sequential()\n",
    "    cnn.add(layers.Conv2D(num_filters, filter_shape, padding='same', activation=convolution_filter, \n",
    "                          input_shape=(number_image_channels, max_x, max_y), data_format=data_format,\n",
    "                          kernel_regularizer=regularizers.l2(kernel_lam), bias_regularizer=regularizers.l2(bias_lam),\n",
    "                          kernel_initializer=convolution_init))\n",
    "#     cnn.add(BatchNormalization(axis=1))\n",
    "#     cnn.add(layers.Conv2D(4*num_filters, filter_shape, padding='same', activation=convolution_filter, \n",
    "#                           input_shape=(number_image_channels, max_x, max_y), data_format=data_format,\n",
    "#                           kernel_regularizer=regularizers.l2(kernel_lam), bias_regularizer=regularizers.l2(bias_lam),\n",
    "#                           kernel_initializer='lecun_normal'))\n",
    "    cnn.add(layers.MaxPooling2D(pool_size=pool_size, data_format=data_format))\n",
    "    cnn.add(BatchNormalization(axis=1))\n",
    "#     cnn.add(layers.Dropout(0.25))\n",
    "    \n",
    "    cnn.add(layers.Conv2D(2*num_filters, filter_shape,padding='same', activation=convolution_filter, data_format=data_format, \n",
    "                         kernel_regularizer=regularizers.l2(kernel_lam), bias_regularizer=regularizers.l2(bias_lam),\n",
    "                         kernel_initializer=convolution_init))\n",
    "#     cnn.add(BatchNormalization(axis=1))\n",
    "#     cnn.add(layers.Conv2D(3*num_filters, filter_shape, padding='same', activation=convolution_filter, \n",
    "#                           input_shape=(number_image_channels, max_x, max_y), data_format=data_format,\n",
    "#                           kernel_regularizer=regularizers.l2(kernel_lam), bias_regularizer=regularizers.l2(bias_lam),\n",
    "#                           kernel_initializer='lecun_normal'))\n",
    "    cnn.add(layers.MaxPooling2D(pool_size=pool_size, data_format=data_format))\n",
    "    cnn.add(BatchNormalization(axis=1))\n",
    "#     cnn.add(layers.Dropout(0.25))\n",
    "    \n",
    "    cnn.add(layers.Conv2D(3*num_filters, filter_shape, padding='same', activation=convolution_filter, data_format=data_format, \n",
    "                         kernel_regularizer=regularizers.l2(kernel_lam), bias_regularizer=regularizers.l2(bias_lam),\n",
    "                         kernel_initializer=convolution_init))\n",
    "#     cnn.add(BatchNormalization(axis=1))\n",
    "#     cnn.add(layers.Conv2D(2*num_filters, filter_shape, padding='same', activation=convolution_filter, \n",
    "#                           input_shape=(number_image_channels, max_x, max_y), data_format=data_format,\n",
    "#                           kernel_regularizer=regularizers.l2(kernel_lam), bias_regularizer=regularizers.l2(bias_lam),\n",
    "#                           kernel_initializer='lecun_normal'))\n",
    "    cnn.add(layers.MaxPooling2D(pool_size, data_format=data_format))\n",
    "    cnn.add(BatchNormalization(axis=1))\n",
    "#     cnn.add(layers.Dropout(0.25))\n",
    "    \n",
    "    cnn.add(layers.Conv2D(4*num_filters, filter_shape, padding='same', activation=convolution_filter, data_format=data_format, \n",
    "                         kernel_regularizer=regularizers.l2(kernel_lam), bias_regularizer=regularizers.l2(bias_lam),\n",
    "                         kernel_initializer=convolution_init))\n",
    "#     cnn.add(BatchNormalization(axis=1))\n",
    "#     cnn.add(layers.Conv2D(num_filters, filter_shape, padding='same', activation=convolution_filter, data_format=data_format, \n",
    "#                          kernel_regularizer=regularizers.l2(kernel_lam), bias_regularizer=regularizers.l2(bias_lam),\n",
    "#                          kernel_initializer='lecun_normal'))\n",
    "    cnn.add(layers.MaxPooling2D(pool_size, data_format=data_format))\n",
    "    cnn.add(BatchNormalization(axis=1))\n",
    "#     cnn.add(layers.Dropout(0.25))\n",
    "    \n",
    "#     cnn.add(layers.Conv2D(4*num_filters, filter_shape, padding='same', activation=convolution_filter, data_format=data_format, \n",
    "#                          kernel_regularizer=regularizers.l2(kernel_lam), bias_regularizer=regularizers.l2(bias_lam),\n",
    "#                          kernel_initializer=convolution_init))\n",
    "# #     cnn.add(BatchNormalization(axis=1))\n",
    "# #     cnn.add(layers.Conv2D(num_filters, filter_shape, padding='same', activation=convolution_filter, data_format=data_format, \n",
    "# #                          kernel_regularizer=regularizers.l2(kernel_lam), bias_regularizer=regularizers.l2(bias_lam),\n",
    "# #                          kernel_initializer='lecun_normal'))\n",
    "#     cnn.add(layers.MaxPooling2D(pool_size, data_format=data_format))\n",
    "#     cnn.add(BatchNormalization(axis=1))\n",
    "# #     cnn.add(layers.Dropout(0.25))\n",
    "    \n",
    "#     cnn.add(layers.Conv2D(4*num_filters, filter_shape, padding='same', activation=convolution_filter, data_format=data_format, \n",
    "#                          kernel_regularizer=regularizers.l2(kernel_lam), bias_regularizer=regularizers.l2(bias_lam), \n",
    "#                          kernel_initializer=convolution_init))\n",
    "# #     cnn.add(BatchNormalization(axis=1))\n",
    "# #     cnn.add(layers.Conv2D(2*num_filters, filter_shape, padding='same', activation=convolution_filter, data_format=data_format, \n",
    "# #                          kernel_regularizer=regularizers.l2(kernel_lam), bias_regularizer=regularizers.l2(bias_lam),\n",
    "# #                          kernel_initializer='lecun_normal'))\n",
    "#     cnn.add(layers.MaxPooling2D(pool_size, data_format=data_format))\n",
    "#     cnn.add(BatchNormalization(axis=1))\n",
    "# #     cnn.add(layers.Dropout(0.25))\n",
    "    \n",
    "#     cnn.add(layers.Conv2D(4*num_filters, filter_shape, padding='same', activation=convolution_filter, data_format=data_format, \n",
    "#                          kernel_regularizer=regularizers.l2(kernel_lam), bias_regularizer=regularizers.l2(bias_lam), \n",
    "#                          kernel_initializer=convolution_init))\n",
    "# #     cnn.add(BatchNormalization(axis=1))\n",
    "# #     cnn.add(layers.Conv2D(3*num_filters, filter_shape, padding='same', activation=convolution_filter, data_format=data_format, \n",
    "# #                          kernel_regularizer=regularizers.l2(kernel_lam), bias_regularizer=regularizers.l2(bias_lam),\n",
    "# #                          kernel_initializer='lecun_normal'))\n",
    "#     cnn.add(layers.MaxPooling2D(pool_size, data_format=data_format))\n",
    "#     cnn.add(BatchNormalization(axis=1))\n",
    "# #     cnn.add(layers.Dropout(0.25))\n",
    "    \n",
    "    cnn.add(layers.Flatten())\n",
    "    cnn.add(layers.Dense(20, activation=convolution_filter, kernel_regularizer=regularizers.l2(kernel_lam),\n",
    "                         bias_regularizer=regularizers.l2(bias_lam), kernel_initializer=convolution_init))\n",
    "    cnn.add(BatchNormalization())\n",
    "    cnn.add(layers.Dense(20, activation=convolution_filter, kernel_regularizer=regularizers.l2(kernel_lam),\n",
    "                         bias_regularizer=regularizers.l2(bias_lam), kernel_initializer=convolution_init))\n",
    "#     cnn.add(BatchNormalization())\n",
    "    cnn.add(layers.Dense(1, activation=dense_filter, kernel_regularizer=regularizers.l2(kernel_lam),\n",
    "                         bias_regularizer=regularizers.l2(bias_lam), kernel_initializer=dense_init))\n",
    "    return cnn\n",
    "\n",
    "\n",
    "class DataBatchGenerator(Sequence):\n",
    "    def __init__(self, dataset:np.ndarray, batch_size:int, start_idx:int, number_image_channels:int,\n",
    "                 max_x, max_y, float_memory_used, conserve=0):\n",
    "#         print(dataset.shape[0])\n",
    "        self.dataset, self.batch_size, self.start_idx = dataset, batch_size, start_idx\n",
    "        self.number_image_channels, self.max_x, self.max_y = number_image_channels, max_x, max_y\n",
    "        self.float_memory_used = float_memory_used\n",
    "        self.conserve = conserve\n",
    "    \n",
    "    def __len__(self):\n",
    "        return np.ceil(self.dataset.shape[0] / self.batch_size).astype(np.int)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        size = min(self.dataset.shape[0] - idx * self.batch_size, self.batch_size)\n",
    "        batch_x = np.empty((size, self.number_image_channels, self.max_x, self.max_y), dtype=self.float_memory_used)\n",
    "        batch_y = np.empty((size), dtype=self.float_memory_used)\n",
    "        for i in range(size):\n",
    "            batch_x[i] = read_image(self.start_idx + idx * self.batch_size + i)\n",
    "            batch_y[i] = self.dataset[idx * self.batch_size + i][- 1 - self.conserve]\n",
    "        return batch_x, batch_y\n",
    "    \n",
    "def custom_loss(fp_penalty_coef, fn_penalty_coef):\n",
    "    # custom loss function that penalize false positive and negative differently\n",
    "    def loss(y_true, y_pred):\n",
    "        res = y_pred - y_true\n",
    "        res = tf.where(res > 0, res * fp_penalty_coef, res * fn_penalty_coef)\n",
    "        return K.mean(K.square(res))\n",
    "    return loss\n",
    "\n",
    "def fp_mae(y_true, y_pred):\n",
    "    # custom metric that replace false negative with zero and return the mean of new vector\n",
    "    res = y_pred - y_true\n",
    "    res = tf.nn.relu(res)\n",
    "#     res = tf.where(res <= 0, 0, res)\n",
    "    return K.mean(res)\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = cnn_model(10, 0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 10, 100, 100)      640       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 10, 50, 50)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 10, 50, 50)        40        \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 20, 50, 50)        1820      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 20, 25, 25)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 20, 25, 25)        80        \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 30, 25, 25)        5430      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 30, 12, 12)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 30, 12, 12)        120       \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 40, 12, 12)        10840     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 40, 6, 6)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 40, 6, 6)          160       \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1440)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 20)                28820     \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 20)                80        \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 48,471\n",
      "Trainable params: 48,231\n",
      "Non-trainable params: 240\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number_samples: 8192 , New samples: 8192\n",
      "Validation size: 2704 , starts: 8192 , ends: 10895\n",
      "\n",
      "Epoch 00001: val_mae improved from inf to 8.74685, saving model to ML/data/pictures_100_100/splat/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/900sensors/models/8192/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00002: val_mae improved from 8.74685 to 8.22004, saving model to ML/data/pictures_100_100/splat/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/900sensors/models/8192/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00003: val_mae improved from 8.22004 to 7.62096, saving model to ML/data/pictures_100_100/splat/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/900sensors/models/8192/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00004: val_mae improved from 7.62096 to 7.53113, saving model to ML/data/pictures_100_100/splat/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/900sensors/models/8192/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00005: val_mae improved from 7.53113 to 7.21488, saving model to ML/data/pictures_100_100/splat/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/900sensors/models/8192/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00006: val_mae did not improve from 7.21488\n",
      "\n",
      "Epoch 00007: val_mae did not improve from 7.21488\n",
      "\n",
      "Epoch 00008: val_mae did not improve from 7.21488\n",
      "\n",
      "Epoch 00009: val_mae did not improve from 7.21488\n",
      "\n",
      "Epoch 00010: val_mae did not improve from 7.21488\n",
      "\n",
      "Epoch 00011: val_mae did not improve from 7.21488\n",
      "\n",
      "Epoch 00012: val_mae did not improve from 7.21488\n",
      "\n",
      "Epoch 00013: val_mae did not improve from 7.21488\n",
      "\n",
      "Epoch 00014: val_mae did not improve from 7.21488\n",
      "\n",
      "Epoch 00015: val_mae did not improve from 7.21488\n",
      "\n",
      "Epoch 00016: val_mae did not improve from 7.21488\n",
      "\n",
      "Epoch 00017: val_mae did not improve from 7.21488\n",
      "\n",
      "Epoch 00018: val_mae did not improve from 7.21488\n",
      "\n",
      "Epoch 00019: val_mae did not improve from 7.21488\n",
      "\n",
      "Epoch 00020: val_mae did not improve from 7.21488\n",
      "\n",
      "Epoch 00021: val_mae did not improve from 7.21488\n",
      "\n",
      "Epoch 00022: val_mae did not improve from 7.21488\n",
      "\n",
      "Epoch 00023: val_mae did not improve from 7.21488\n",
      "\n",
      "Epoch 00024: val_mae did not improve from 7.21488\n",
      "\n",
      "Epoch 00025: val_mae did not improve from 7.21488\n",
      "\n",
      "Epoch 00026: val_mae did not improve from 7.21488\n",
      "\n",
      "Epoch 00027: val_mae did not improve from 7.21488\n",
      "\n",
      "Epoch 00028: val_mae did not improve from 7.21488\n",
      "\n",
      "Epoch 00029: val_mae did not improve from 7.21488\n",
      "\n",
      "Epoch 00030: val_mae did not improve from 7.21488\n",
      "\n",
      "Epoch 00031: val_mae did not improve from 7.21488\n",
      "\n",
      "Epoch 00032: val_mae did not improve from 7.21488\n",
      "\n",
      "Epoch 00033: val_mae did not improve from 7.21488\n",
      "\n",
      "Epoch 00034: val_mae did not improve from 7.21488\n",
      "\n",
      "Epoch 00035: val_mae did not improve from 7.21488\n",
      "\n",
      "Epoch 00036: val_mae did not improve from 7.21488\n",
      "\n",
      "Epoch 00037: val_mae did not improve from 7.21488\n",
      "\n",
      "Epoch 00038: val_mae did not improve from 7.21488\n",
      "\n",
      "Epoch 00039: val_mae did not improve from 7.21488\n",
      "\n",
      "Epoch 00040: val_mae did not improve from 7.21488\n",
      "\n",
      "Lambda: 0 , Time: 0:42:18\n",
      "Train Error(all epochs): 0.9010352 \n",
      " [9.22, 8.981, 8.476, 7.634, 6.726, 5.958, 5.387, 4.77, 4.244, 3.804, 3.372, 3.063, 2.738, 2.453, 2.216, 2.072, 1.909, 1.813, 1.727, 1.597, 1.521, 1.385, 1.301, 1.233, 1.223, 1.24, 1.159, 1.114, 1.075, 1.069, 1.078, 1.046, 1.086, 1.077, 1.039, 0.974, 0.963, 0.901, 0.904, 0.919]\n",
      "Train FP Error(all epochs): 0.44410473 \n",
      " [1.018, 1.078, 1.244, 1.559, 2.027, 2.353, 2.436, 2.246, 2.023, 1.885, 1.655, 1.54, 1.361, 1.252, 1.108, 1.029, 0.99, 0.916, 0.859, 0.809, 0.769, 0.705, 0.64, 0.609, 0.631, 0.617, 0.588, 0.549, 0.534, 0.541, 0.524, 0.529, 0.535, 0.55, 0.516, 0.489, 0.489, 0.444, 0.451, 0.464]\n",
      "Val Error(all epochs): 7.214880466461182 \n",
      " [8.747, 8.22, 7.621, 7.531, 7.215, 7.299, 7.715, 7.439, 7.901, 8.006, 7.762, 7.751, 7.966, 8.176, 8.266, 8.212, 8.383, 8.365, 8.606, 8.541, 8.544, 8.567, 8.617, 8.627, 8.611, 8.675, 8.691, 8.679, 8.797, 8.673, 8.668, 8.702, 8.664, 8.632, 8.659, 8.631, 8.562, 8.619, 8.612, 8.628]\n",
      "Val FP Error(all epochs): 1.1538649797439575 \n",
      " [1.154, 1.407, 1.948, 2.149, 3.188, 2.772, 2.106, 2.617, 2.374, 2.204, 2.909, 3.239, 3.197, 3.064, 3.291, 4.005, 3.748, 4.116, 3.944, 4.094, 3.861, 4.242, 4.395, 4.288, 4.369, 4.133, 4.611, 4.195, 4.21, 4.408, 4.263, 4.392, 3.953, 4.506, 4.517, 4.438, 4.387, 4.251, 4.293, 4.056]\n",
      "\n",
      "Epoch 00001: val_mae improved from inf to 9.15669, saving model to ML/data/pictures_100_100/splat/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/900sensors/models/8192/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00002: val_mae improved from 9.15669 to 8.72697, saving model to ML/data/pictures_100_100/splat/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/900sensors/models/8192/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00003: val_mae improved from 8.72697 to 8.05491, saving model to ML/data/pictures_100_100/splat/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/900sensors/models/8192/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00004: val_mae improved from 8.05491 to 7.54871, saving model to ML/data/pictures_100_100/splat/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/900sensors/models/8192/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00005: val_mae did not improve from 7.54871\n",
      "\n",
      "Epoch 00006: val_mae improved from 7.54871 to 7.50808, saving model to ML/data/pictures_100_100/splat/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/900sensors/models/8192/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00007: val_mae did not improve from 7.50808\n",
      "\n",
      "Epoch 00008: val_mae did not improve from 7.50808\n",
      "\n",
      "Epoch 00009: val_mae did not improve from 7.50808\n",
      "\n",
      "Epoch 00010: val_mae did not improve from 7.50808\n",
      "\n",
      "Epoch 00011: val_mae did not improve from 7.50808\n",
      "\n",
      "Epoch 00012: val_mae did not improve from 7.50808\n",
      "\n",
      "Epoch 00013: val_mae did not improve from 7.50808\n",
      "\n",
      "Epoch 00014: val_mae did not improve from 7.50808\n",
      "\n",
      "Epoch 00015: val_mae did not improve from 7.50808\n",
      "\n",
      "Epoch 00016: val_mae did not improve from 7.50808\n",
      "\n",
      "Epoch 00017: val_mae did not improve from 7.50808\n",
      "\n",
      "Epoch 00018: val_mae did not improve from 7.50808\n",
      "\n",
      "Epoch 00019: val_mae did not improve from 7.50808\n",
      "\n",
      "Epoch 00020: val_mae did not improve from 7.50808\n",
      "\n",
      "Epoch 00021: val_mae did not improve from 7.50808\n",
      "\n",
      "Epoch 00022: val_mae did not improve from 7.50808\n",
      "\n",
      "Epoch 00023: val_mae did not improve from 7.50808\n",
      "\n",
      "Epoch 00024: val_mae did not improve from 7.50808\n",
      "\n",
      "Epoch 00025: val_mae did not improve from 7.50808\n",
      "\n",
      "Epoch 00026: val_mae did not improve from 7.50808\n",
      "\n",
      "Epoch 00027: val_mae did not improve from 7.50808\n",
      "\n",
      "Epoch 00028: val_mae did not improve from 7.50808\n",
      "\n",
      "Epoch 00029: val_mae did not improve from 7.50808\n",
      "\n",
      "Epoch 00030: val_mae did not improve from 7.50808\n",
      "\n",
      "Epoch 00031: val_mae did not improve from 7.50808\n",
      "\n",
      "Epoch 00032: val_mae did not improve from 7.50808\n",
      "\n",
      "Epoch 00033: val_mae did not improve from 7.50808\n",
      "\n",
      "Epoch 00034: val_mae did not improve from 7.50808\n",
      "\n",
      "Epoch 00035: val_mae did not improve from 7.50808\n",
      "\n",
      "Epoch 00036: val_mae did not improve from 7.50808\n",
      "\n",
      "Epoch 00037: val_mae did not improve from 7.50808\n",
      "\n",
      "Epoch 00038: val_mae did not improve from 7.50808\n",
      "\n",
      "Epoch 00039: val_mae did not improve from 7.50808\n",
      "\n",
      "Epoch 00040: val_mae did not improve from 7.50808\n",
      "\n",
      "Lambda: 0.001 , Time: 0:43:05\n",
      "Train Error(all epochs): 0.8973973 \n",
      " [9.227, 8.949, 8.421, 7.526, 6.452, 5.512, 4.803, 4.193, 3.658, 3.21, 2.821, 2.546, 2.288, 2.115, 1.904, 1.792, 1.755, 1.672, 1.534, 1.397, 1.3, 1.229, 1.194, 1.097, 1.048, 1.057, 0.999, 0.996, 1.003, 1.075, 1.076, 1.041, 1.017, 1.038, 1.014, 0.965, 0.969, 0.946, 0.97, 0.897]\n",
      "Train FP Error(all epochs): 0.45617464 \n",
      " [1.014, 1.073, 1.222, 1.479, 1.848, 2.147, 2.189, 2.02, 1.779, 1.604, 1.437, 1.286, 1.164, 1.073, 0.993, 0.912, 0.86, 0.871, 0.744, 0.72, 0.659, 0.627, 0.599, 0.534, 0.543, 0.529, 0.502, 0.495, 0.505, 0.535, 0.55, 0.527, 0.518, 0.509, 0.52, 0.48, 0.483, 0.479, 0.485, 0.456]\n",
      "Val Error(all epochs): 7.508084297180176 \n",
      " [9.157, 8.727, 8.055, 7.549, 8.921, 7.508, 7.761, 7.984, 7.865, 8.071, 8.596, 8.503, 8.603, 8.659, 8.732, 8.825, 8.78, 8.919, 8.883, 8.897, 8.953, 8.916, 8.939, 9.009, 8.994, 8.925, 9.009, 8.924, 8.917, 8.993, 8.856, 8.951, 8.892, 8.805, 8.865, 8.842, 8.84, 8.821, 8.794, 8.812]\n",
      "Val FP Error(all epochs): 0.9978484511375427 \n",
      " [0.998, 1.155, 1.478, 1.874, 1.17, 2.223, 2.284, 2.305, 2.922, 3.245, 2.824, 3.294, 3.433, 3.906, 3.691, 4.066, 4.253, 4.141, 4.308, 4.504, 4.55, 4.331, 4.361, 4.754, 4.395, 4.636, 4.532, 4.631, 4.307, 4.754, 4.49, 4.687, 4.631, 4.596, 4.558, 4.505, 4.631, 4.162, 4.571, 4.482]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: val_mae improved from inf to 9.85906, saving model to ML/data/pictures_100_100/splat/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/900sensors/models/8192/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00002: val_mae improved from 9.85906 to 8.71032, saving model to ML/data/pictures_100_100/splat/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/900sensors/models/8192/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00003: val_mae improved from 8.71032 to 7.75013, saving model to ML/data/pictures_100_100/splat/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/900sensors/models/8192/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00004: val_mae did not improve from 7.75013\n",
      "\n",
      "Epoch 00005: val_mae did not improve from 7.75013\n",
      "\n",
      "Epoch 00006: val_mae did not improve from 7.75013\n",
      "\n",
      "Epoch 00007: val_mae did not improve from 7.75013\n",
      "\n",
      "Epoch 00008: val_mae did not improve from 7.75013\n",
      "\n",
      "Epoch 00009: val_mae did not improve from 7.75013\n",
      "\n",
      "Epoch 00010: val_mae did not improve from 7.75013\n",
      "\n",
      "Epoch 00011: val_mae did not improve from 7.75013\n",
      "\n",
      "Epoch 00012: val_mae did not improve from 7.75013\n",
      "\n",
      "Epoch 00013: val_mae did not improve from 7.75013\n",
      "\n",
      "Epoch 00014: val_mae did not improve from 7.75013\n",
      "\n",
      "Epoch 00015: val_mae did not improve from 7.75013\n",
      "\n",
      "Epoch 00016: val_mae did not improve from 7.75013\n",
      "\n",
      "Epoch 00017: val_mae did not improve from 7.75013\n",
      "\n",
      "Epoch 00018: val_mae did not improve from 7.75013\n",
      "\n",
      "Epoch 00019: val_mae did not improve from 7.75013\n",
      "\n",
      "Epoch 00020: val_mae did not improve from 7.75013\n",
      "\n",
      "Epoch 00021: val_mae did not improve from 7.75013\n",
      "\n",
      "Epoch 00022: val_mae did not improve from 7.75013\n",
      "\n",
      "Epoch 00023: val_mae did not improve from 7.75013\n",
      "\n",
      "Epoch 00024: val_mae did not improve from 7.75013\n",
      "\n",
      "Epoch 00025: val_mae did not improve from 7.75013\n",
      "\n",
      "Epoch 00026: val_mae did not improve from 7.75013\n",
      "\n",
      "Epoch 00027: val_mae did not improve from 7.75013\n",
      "\n",
      "Epoch 00028: val_mae did not improve from 7.75013\n",
      "\n",
      "Epoch 00029: val_mae did not improve from 7.75013\n",
      "\n",
      "Epoch 00030: val_mae did not improve from 7.75013\n",
      "\n",
      "Epoch 00031: val_mae did not improve from 7.75013\n",
      "\n",
      "Epoch 00032: val_mae did not improve from 7.75013\n",
      "\n",
      "Epoch 00033: val_mae did not improve from 7.75013\n",
      "\n",
      "Epoch 00034: val_mae did not improve from 7.75013\n",
      "\n",
      "Epoch 00035: val_mae did not improve from 7.75013\n",
      "\n",
      "Epoch 00036: val_mae did not improve from 7.75013\n",
      "\n",
      "Epoch 00037: val_mae did not improve from 7.75013\n",
      "\n",
      "Epoch 00038: val_mae did not improve from 7.75013\n",
      "\n",
      "Epoch 00039: val_mae did not improve from 7.75013\n",
      "\n",
      "Epoch 00040: val_mae did not improve from 7.75013\n",
      "\n",
      "Lambda: 0.01 , Time: 0:43:07\n",
      "Train Error(all epochs): 1.0036687 \n",
      " [9.196, 8.892, 8.39, 7.644, 6.814, 5.961, 5.271, 4.673, 4.086, 3.587, 3.125, 2.756, 2.481, 2.196, 2.02, 1.888, 1.727, 1.638, 1.552, 1.552, 1.558, 1.389, 1.294, 1.246, 1.23, 1.219, 1.158, 1.065, 1.014, 1.015, 1.047, 1.092, 1.105, 1.044, 1.017, 1.032, 1.004, 1.049, 1.059, 1.033]\n",
      "Train FP Error(all epochs): 0.4955247 \n",
      " [1.03, 1.12, 1.322, 1.623, 2.022, 2.287, 2.37, 2.245, 2.02, 1.801, 1.606, 1.43, 1.243, 1.147, 1.043, 0.966, 0.876, 0.827, 0.776, 0.783, 0.785, 0.71, 0.659, 0.592, 0.638, 0.613, 0.595, 0.533, 0.498, 0.505, 0.552, 0.54, 0.525, 0.531, 0.512, 0.52, 0.496, 0.53, 0.527, 0.511]\n",
      "Val Error(all epochs): 7.750132083892822 \n",
      " [9.859, 8.71, 7.75, 8.738, 8.054, 10.738, 10.384, 9.753, 9.881, 9.124, 9.416, 9.294, 8.661, 8.814, 8.636, 8.695, 8.662, 8.595, 8.754, 8.651, 8.683, 8.629, 8.689, 8.769, 8.724, 8.637, 8.638, 8.612, 8.663, 8.623, 8.568, 8.694, 8.65, 8.603, 8.616, 8.601, 8.657, 8.678, 8.537, 8.59]\n",
      "Val FP Error(all epochs): 0.6673442125320435 \n",
      " [0.798, 1.186, 1.87, 1.217, 1.597, 0.667, 0.797, 1.015, 1.116, 1.511, 1.514, 1.763, 2.573, 2.483, 2.774, 3.153, 3.329, 3.6, 3.683, 3.499, 4.169, 3.852, 4.265, 4.391, 4.416, 4.623, 4.362, 4.407, 4.443, 4.455, 3.941, 4.369, 4.329, 4.299, 4.231, 4.422, 4.485, 4.421, 4.321, 4.266]\n",
      "\n",
      "Epoch 00001: val_mae improved from inf to 9.06989, saving model to ML/data/pictures_100_100/splat/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/900sensors/models/8192/best_model_lambda_3.h5\n",
      "\n",
      "Epoch 00002: val_mae improved from 9.06989 to 8.41200, saving model to ML/data/pictures_100_100/splat/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/900sensors/models/8192/best_model_lambda_3.h5\n",
      "\n",
      "Epoch 00003: val_mae improved from 8.41200 to 7.53068, saving model to ML/data/pictures_100_100/splat/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/900sensors/models/8192/best_model_lambda_3.h5\n",
      "\n",
      "Epoch 00004: val_mae improved from 7.53068 to 7.18016, saving model to ML/data/pictures_100_100/splat/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/900sensors/models/8192/best_model_lambda_3.h5\n",
      "\n",
      "Epoch 00005: val_mae improved from 7.18016 to 6.94553, saving model to ML/data/pictures_100_100/splat/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/900sensors/models/8192/best_model_lambda_3.h5\n",
      "\n",
      "Epoch 00006: val_mae did not improve from 6.94553\n",
      "\n",
      "Epoch 00007: val_mae did not improve from 6.94553\n",
      "\n",
      "Epoch 00008: val_mae did not improve from 6.94553\n",
      "\n",
      "Epoch 00009: val_mae did not improve from 6.94553\n",
      "\n",
      "Epoch 00010: val_mae did not improve from 6.94553\n",
      "\n",
      "Epoch 00011: val_mae did not improve from 6.94553\n",
      "\n",
      "Epoch 00012: val_mae did not improve from 6.94553\n",
      "\n",
      "Epoch 00013: val_mae did not improve from 6.94553\n",
      "\n",
      "Epoch 00014: val_mae did not improve from 6.94553\n",
      "\n",
      "Epoch 00015: val_mae did not improve from 6.94553\n",
      "\n",
      "Epoch 00016: val_mae did not improve from 6.94553\n",
      "\n",
      "Epoch 00017: val_mae did not improve from 6.94553\n",
      "\n",
      "Epoch 00018: val_mae did not improve from 6.94553\n",
      "\n",
      "Epoch 00019: val_mae did not improve from 6.94553\n",
      "\n",
      "Epoch 00020: val_mae did not improve from 6.94553\n",
      "\n",
      "Epoch 00021: val_mae did not improve from 6.94553\n",
      "\n",
      "Epoch 00022: val_mae did not improve from 6.94553\n",
      "\n",
      "Epoch 00023: val_mae did not improve from 6.94553\n",
      "\n",
      "Epoch 00024: val_mae did not improve from 6.94553\n",
      "\n",
      "Epoch 00025: val_mae did not improve from 6.94553\n",
      "\n",
      "Epoch 00026: val_mae did not improve from 6.94553\n",
      "\n",
      "Epoch 00027: val_mae did not improve from 6.94553\n",
      "\n",
      "Epoch 00028: val_mae did not improve from 6.94553\n",
      "\n",
      "Epoch 00029: val_mae did not improve from 6.94553\n",
      "\n",
      "Epoch 00030: val_mae did not improve from 6.94553\n",
      "\n",
      "Epoch 00031: val_mae did not improve from 6.94553\n",
      "\n",
      "Epoch 00032: val_mae did not improve from 6.94553\n",
      "\n",
      "Epoch 00033: val_mae did not improve from 6.94553\n",
      "\n",
      "Epoch 00034: val_mae did not improve from 6.94553\n",
      "\n",
      "Epoch 00035: val_mae did not improve from 6.94553\n",
      "\n",
      "Epoch 00036: val_mae did not improve from 6.94553\n",
      "\n",
      "Epoch 00037: val_mae did not improve from 6.94553\n",
      "\n",
      "Epoch 00038: val_mae did not improve from 6.94553\n",
      "\n",
      "Epoch 00039: val_mae did not improve from 6.94553\n",
      "\n",
      "Epoch 00040: val_mae did not improve from 6.94553\n",
      "\n",
      "Lambda: 0.1 , Time: 0:43:03\n",
      "Train Error(all epochs): 1.3618047 \n",
      " [9.176, 8.922, 8.519, 7.82, 7.093, 6.488, 5.979, 5.423, 4.857, 4.355, 3.971, 3.563, 3.251, 2.962, 2.74, 2.524, 2.243, 2.088, 1.983, 1.824, 1.701, 1.683, 1.662, 1.661, 1.731, 1.712, 1.684, 1.614, 1.567, 1.507, 1.438, 1.459, 1.49, 1.489, 1.432, 1.39, 1.386, 1.362, 1.434, 1.598]\n",
      "Train FP Error(all epochs): 0.657034 \n",
      " [1.029, 1.106, 1.272, 1.598, 2.079, 2.513, 2.659, 2.529, 2.285, 2.103, 1.933, 1.749, 1.578, 1.455, 1.359, 1.255, 1.117, 1.033, 0.972, 0.912, 0.842, 0.82, 0.835, 0.828, 0.846, 0.853, 0.816, 0.806, 0.768, 0.732, 0.702, 0.73, 0.721, 0.734, 0.719, 0.684, 0.657, 0.673, 0.716, 0.756]\n",
      "Val Error(all epochs): 6.945526123046875 \n",
      " [9.07, 8.412, 7.531, 7.18, 6.946, 6.971, 7.144, 7.055, 7.288, 7.421, 7.762, 7.763, 7.985, 8.023, 8.064, 8.152, 8.2, 8.254, 8.278, 8.472, 8.364, 8.388, 8.453, 8.358, 8.376, 8.329, 8.412, 8.28, 8.41, 8.38, 8.444, 8.32, 8.464, 8.402, 8.411, 8.344, 8.497, 8.304, 8.292, 8.316]\n",
      "Val FP Error(all epochs): 1.0405049324035645 \n",
      " [1.041, 1.3, 1.926, 2.333, 3.023, 3.082, 2.648, 3.475, 3.07, 3.493, 2.928, 3.715, 3.115, 4.018, 3.579, 3.644, 3.59, 3.79, 3.878, 3.853, 3.938, 3.908, 3.794, 4.105, 4.213, 4.329, 4.318, 4.11, 3.845, 3.966, 3.964, 4.181, 4.28, 4.474, 4.094, 4.297, 3.958, 4.283, 4.346, 4.389]\n",
      "\n",
      "Epoch 00001: val_mae improved from inf to 9.35321, saving model to ML/data/pictures_100_100/splat/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/900sensors/models/8192/best_model_lambda_4.h5\n",
      "\n",
      "Epoch 00002: val_mae improved from 9.35321 to 9.17773, saving model to ML/data/pictures_100_100/splat/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/900sensors/models/8192/best_model_lambda_4.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00003: val_mae improved from 9.17773 to 8.40302, saving model to ML/data/pictures_100_100/splat/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/900sensors/models/8192/best_model_lambda_4.h5\n",
      "\n",
      "Epoch 00004: val_mae did not improve from 8.40302\n",
      "\n",
      "Epoch 00005: val_mae improved from 8.40302 to 7.98434, saving model to ML/data/pictures_100_100/splat/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/900sensors/models/8192/best_model_lambda_4.h5\n",
      "\n",
      "Epoch 00006: val_mae did not improve from 7.98434\n",
      "\n",
      "Epoch 00007: val_mae improved from 7.98434 to 7.35051, saving model to ML/data/pictures_100_100/splat/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/900sensors/models/8192/best_model_lambda_4.h5\n",
      "\n",
      "Epoch 00008: val_mae did not improve from 7.35051\n",
      "\n",
      "Epoch 00009: val_mae did not improve from 7.35051\n",
      "\n",
      "Epoch 00010: val_mae did not improve from 7.35051\n",
      "\n",
      "Epoch 00011: val_mae did not improve from 7.35051\n",
      "\n",
      "Epoch 00012: val_mae did not improve from 7.35051\n",
      "\n",
      "Epoch 00013: val_mae did not improve from 7.35051\n",
      "\n",
      "Epoch 00014: val_mae did not improve from 7.35051\n",
      "\n",
      "Epoch 00015: val_mae did not improve from 7.35051\n",
      "\n",
      "Epoch 00016: val_mae did not improve from 7.35051\n",
      "\n",
      "Epoch 00017: val_mae did not improve from 7.35051\n",
      "\n",
      "Epoch 00018: val_mae did not improve from 7.35051\n",
      "\n",
      "Epoch 00019: val_mae did not improve from 7.35051\n",
      "\n",
      "Epoch 00020: val_mae did not improve from 7.35051\n",
      "\n",
      "Epoch 00021: val_mae did not improve from 7.35051\n",
      "\n",
      "Epoch 00022: val_mae did not improve from 7.35051\n",
      "\n",
      "Epoch 00023: val_mae did not improve from 7.35051\n",
      "\n",
      "Epoch 00024: val_mae did not improve from 7.35051\n",
      "\n",
      "Epoch 00025: val_mae did not improve from 7.35051\n",
      "\n",
      "Epoch 00026: val_mae did not improve from 7.35051\n",
      "\n",
      "Epoch 00027: val_mae did not improve from 7.35051\n",
      "\n",
      "Epoch 00028: val_mae did not improve from 7.35051\n",
      "\n",
      "Epoch 00029: val_mae did not improve from 7.35051\n",
      "\n",
      "Epoch 00030: val_mae did not improve from 7.35051\n",
      "\n",
      "Epoch 00031: val_mae did not improve from 7.35051\n",
      "\n",
      "Epoch 00032: val_mae did not improve from 7.35051\n",
      "\n",
      "Epoch 00033: val_mae did not improve from 7.35051\n",
      "\n",
      "Epoch 00034: val_mae did not improve from 7.35051\n",
      "\n",
      "Epoch 00035: val_mae did not improve from 7.35051\n",
      "\n",
      "Epoch 00036: val_mae did not improve from 7.35051\n",
      "\n",
      "Epoch 00037: val_mae did not improve from 7.35051\n",
      "\n",
      "Epoch 00038: val_mae did not improve from 7.35051\n",
      "\n",
      "Epoch 00039: val_mae did not improve from 7.35051\n",
      "\n",
      "Epoch 00040: val_mae did not improve from 7.35051\n",
      "\n",
      "Lambda: 1 , Time: 0:42:55\n",
      "Train Error(all epochs): 2.588532 \n",
      " [9.25, 9.095, 8.813, 8.357, 7.723, 7.033, 6.415, 5.82, 5.342, 4.912, 4.465, 4.101, 3.668, 3.534, 3.532, 3.254, 3.172, 2.983, 2.939, 2.911, 2.922, 2.884, 2.807, 2.728, 2.737, 2.801, 2.832, 2.648, 2.817, 2.828, 2.715, 2.672, 2.679, 2.656, 2.769, 2.741, 2.69, 2.589, 2.716, 2.79]\n",
      "Train FP Error(all epochs): 1.007685 \n",
      " [1.008, 1.038, 1.131, 1.303, 1.582, 1.836, 2.095, 2.132, 2.089, 1.97, 1.851, 1.757, 1.588, 1.546, 1.553, 1.489, 1.42, 1.379, 1.329, 1.347, 1.355, 1.363, 1.3, 1.269, 1.282, 1.278, 1.352, 1.259, 1.347, 1.318, 1.323, 1.276, 1.268, 1.276, 1.307, 1.317, 1.297, 1.246, 1.304, 1.367]\n",
      "Val Error(all epochs): 7.350513935089111 \n",
      " [9.353, 9.178, 8.403, 8.732, 7.984, 9.025, 7.351, 9.597, 10.895, 10.504, 11.255, 8.33, 9.741, 8.922, 7.933, 8.563, 8.287, 8.044, 8.033, 8.024, 8.073, 8.135, 8.172, 8.203, 8.47, 8.302, 8.261, 8.443, 8.241, 8.063, 8.205, 7.949, 8.394, 8.24, 8.266, 8.165, 8.342, 8.148, 7.977, 8.158]\n",
      "Val FP Error(all epochs): 0.594662606716156 \n",
      " [0.939, 0.991, 1.308, 1.152, 1.543, 1.049, 2.133, 0.888, 0.605, 0.719, 0.595, 1.68, 1.077, 1.495, 2.345, 1.788, 2.267, 2.875, 3.09, 3.179, 3.184, 2.944, 2.974, 3.134, 2.85, 3.689, 3.349, 3.329, 3.317, 4.245, 3.461, 3.389, 3.151, 3.122, 3.187, 3.717, 4.202, 4.322, 4.026, 3.46]\n",
      "\n",
      "Trainig set size: 8192 , Time: 3:34:30 , best_lambda: 0.1 , min_  error: 6.946\n",
      "Test starts:  10896 , ends:  38075\n",
      "107/107 [==============================] - 52s 484ms/step\n",
      "average_error:  7.077 , fp_average_error:  3.269\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# CNN: support batching\n",
    "TEST, CONSERVE = True, False\n",
    "mini_batch, epochs = 256, 40\n",
    "MAX_QUEUE_SIZE, WORKERS = 6, 1\n",
    "fp_penalty_coef, fn_penalty_coef = 1, 1\n",
    "hyper_metric, mode = 'val_mae', 'min'  # the metric that hyper parameters are tuned with\n",
    "prev_sample = 0\n",
    "lambda_vec = [0, 0.001, 0.01, 0.1, 1]  #0.003, 0.01, 0.03, 0.1, 0.3, 1, 3\n",
    "# MODEL_PATH = 'models/'\n",
    "average_diff_power, fp_mean_power = [21.622],[16.506] #[7.177, 8.088, 8.183], [3.438, 3.506, 2.662]\n",
    "best_lambda = [0.01]\n",
    "average_diff_power_conserve, fp_mean_power_conserve = [], []\n",
    "# average_diff_power, fp_mean_power = [7.568, 7.916],[3.357, 2.705] \n",
    "# checkpointers = [ModelCheckpoint(filepath=MODEL_PATH + str(lamb_idx)+ '.h5',\n",
    "#                                  verbose=1, save_best_only=True, monitor=hyper_metric, mode=mode, period=1)\n",
    "#                  for lamb_idx in range(len(lambda_vec))]\n",
    "\n",
    "for num_sample_idx, number_sample in enumerate(number_samples):\n",
    "#     if num_sample_idx < 3:\n",
    "#         continue\n",
    "#     if num_sample_idx == 0:\n",
    "    MODEL_PATH = '/'.join(image_dir.split('/')[:-1]) + '/models/' + str(number_sample)\n",
    "    if not os.path.exists(MODEL_PATH):\n",
    "        os.makedirs(MODEL_PATH)\n",
    "    MODEL_PATH += \"/best_model_lambda_\"\n",
    "    if True:\n",
    "        cnns = [cnn_model(10, lamb, 0) for lamb in lambda_vec]\n",
    "        for cnn in cnns:\n",
    "#             cnn.compile(loss='mean_squared_error', optimizer='adam', metrics=['mse','mae', fp_mean])\n",
    "            cnn.compile(loss=custom_loss(fp_penalty_coef, fn_penalty_coef), \n",
    "                        optimizer='adam', \n",
    "                        metrics=['mse', 'mae', fp_mae])\n",
    "        checkpointers = [ModelCheckpoint(filepath=MODEL_PATH + str(lamb_idx)+ '.h5',\n",
    "                                         verbose=1, save_best_only=True, monitor=hyper_metric, mode=mode, period=1)\n",
    "                         for lamb_idx in range(len(lambda_vec))]\n",
    "    else:\n",
    "        cnns = []\n",
    "        cnns = [models.load_model(MODEL_PATH + str(lamb_idx) + '.h5', \n",
    "                                  custom_objects={ 'loss': custom_loss(fp_penalty_coef, fn_penalty_coef), \n",
    "                                                  'fp_mae': fp_mae }) \n",
    "                for lamb_idx in range(len(lambda_vec))]\n",
    "    number_start = time.time()\n",
    "    train_generator = DataBatchGenerator(dataset=data_reg[prev_sample:number_sample], batch_size=mini_batch,\n",
    "                                         start_idx=prev_sample, number_image_channels=number_image_channels,\n",
    "                                         max_x=max_x, max_y=max_y, float_memory_used=float_memory_used)\n",
    "    \n",
    "\n",
    "    val_size = math.ceil(number_sample * validation_size)\n",
    "    val_generator = DataBatchGenerator(dataset=data_reg[number_sample:number_sample+val_size], \n",
    "                                       batch_size=mini_batch,\n",
    "                                       start_idx=number_sample, number_image_channels=number_image_channels,\n",
    "                                       max_x=max_x, max_y=max_y, float_memory_used=float_memory_used)\n",
    "  \n",
    "    print('number_samples:', number_sample, \", New samples:\", number_sample - prev_sample)\n",
    "    print(\"Validation size:\", val_size, \", starts:\", number_sample, \", ends:\", number_sample + val_size - 1)\n",
    "\n",
    "    for lamb_idx, lamb in enumerate(lambda_vec[:len(lambda_vec) - num_sample_idx//2]):\n",
    "#         if num_sample_idx == 3 and lamb_idx < 4:\n",
    "#             continue\n",
    "        lambda_start = time.time()\n",
    "        cnns[lamb_idx].fit(train_generator, epochs=epochs, verbose=0, validation_data=val_generator, \n",
    "                           shuffle=True, callbacks=[checkpointers[lamb_idx]], \n",
    "                           workers=WORKERS, max_queue_size=MAX_QUEUE_SIZE, use_multiprocessing=False)\n",
    "        \n",
    "        print(\"\\nLambda:\", lamb, \", Time:\", str(datetime.timedelta(seconds=int(time.time() - lambda_start))))\n",
    "        print(\"Train Error(all epochs):\", min(cnns[lamb_idx].history.history['mae']), '\\n', \n",
    "              [round(val, 3) for val in cnns[lamb_idx].history.history['mae']])\n",
    "        print(\"Train FP Error(all epochs):\", min(cnns[lamb_idx].history.history['fp_mae']), '\\n',\n",
    "              [round(val,3) for val in cnns[lamb_idx].history.history['fp_mae']])\n",
    "        print(\"Val Error(all epochs):\", min(cnns[lamb_idx].history.history['val_mae']), '\\n', \n",
    "              [round(val,3) for val in cnns[lamb_idx].history.history['val_mae']])\n",
    "        print(\"Val FP Error(all epochs):\", min(cnns[lamb_idx].history.history['val_fp_mae']), '\\n',\n",
    "              [round(val,3) for val in cnns[lamb_idx].history.history['val_fp_mae']])\n",
    "#     if num_sample_idx == 3:    \n",
    "#         models_min_mae = [8.27781, 8.23545, 8.20838, 7.74743]\n",
    "#         models_min_mae += [min(cnns[lamb_idx].history.history[hyper_metric]) for lamb_idx in range(4,lamb_idx+1)]\n",
    "#     else:\n",
    "    models_min_mae = [min(cnns[lamb_idx].history.history[hyper_metric]) for lamb_idx in range(lamb_idx+1)]\n",
    "    best_lamb_idx = models_min_mae.index(min(models_min_mae))\n",
    "    best_lambda.append(lambda_vec[best_lamb_idx])\n",
    "    print(\"\\nTrainig set size:\", number_sample, \", Time:\", str(datetime.timedelta(seconds=int(time.time() - \n",
    "                                                                                              number_start)))\n",
    "          ,\", best_lambda:\", lambda_vec[best_lamb_idx], \", min_\" , (\"fp_\" if hyper_metric == \"val_fp_mae\" else \"\"),\n",
    "          \"error:\", round(min(models_min_mae), 3))\n",
    "    del cnns, train_generator, val_generator, checkpointers\n",
    "    \n",
    "    if TEST:\n",
    "        # evaluating test images\n",
    "        best_model = None\n",
    "        best_model = models.load_model(MODEL_PATH + str(best_lamb_idx) + '.h5', \n",
    "                                       custom_objects={ 'loss': custom_loss(fp_penalty_coef, fn_penalty_coef), \n",
    "                                                       'fp_mae': fp_mae })\n",
    "        test_generator = DataBatchGenerator(dataset=data_reg[number_sample + val_size:], \n",
    "                                            batch_size=mini_batch,\n",
    "                                            start_idx=number_sample + val_size, \n",
    "                                            number_image_channels=number_image_channels,\n",
    "                                            max_x=max_x, max_y=max_y, float_memory_used=float_memory_used)\n",
    "\n",
    "        print(\"Test starts: \", number_sample + val_size, \", ends: \", data_reg.shape[0] - 1)\n",
    "        time.sleep(1)\n",
    "        test_res = best_model.evaluate(test_generator, verbose=1, \n",
    "                                       workers=WORKERS, max_queue_size=MAX_QUEUE_SIZE, use_multiprocessing=False)\n",
    "        \n",
    "        test_mae_idx, test_fp_mae_idx = [best_model.metrics_names.index(mtrc) for mtrc in ['mae','fp_mae']]\n",
    "        test_mae, test_fp_mae = test_res[test_mae_idx], test_res[test_fp_mae_idx]\n",
    "        average_diff_power.append(round(test_mae, 3))\n",
    "        fp_mean_power.append(round(test_fp_mae, 3))\n",
    "        print('average_error: ', average_diff_power[-1], ', fp_average_error: ', \n",
    "              fp_mean_power[-1])\n",
    "        \n",
    "        if CONSERVE:\n",
    "            test_generator_conserve = DataBatchGenerator(dataset=data_reg[number_sample + val_size:], \n",
    "                                                         batch_size=mini_batch,\n",
    "                                                         start_idx=number_sample + val_size, \n",
    "                                                         number_image_channels=number_image_channels,\n",
    "                                                         max_x=max_x, max_y=max_y, \n",
    "                                                         float_memory_used=float_memory_used, \n",
    "                                                         conserve=1)\n",
    "            test_res_conserve = best_model.evaluate(test_generator_conserve, verbose=1, \n",
    "                                                    workers=WORKERS, max_queue_size=MAX_QUEUE_SIZE, \n",
    "                                                    use_multiprocessing=False)\n",
    "            test_mae_cons, test_fp_mae_cons = test_res_conserve[test_mae_idx], test_res_conserve[test_fp_mae_idx]\n",
    "            average_diff_power_conserve.append(round(test_mae_cons, 3))\n",
    "            fp_mean_power_conserve.append(round(test_fp_mae_cons, 3))\n",
    "            print('Conserve, average_error: ', average_diff_power_conserve[-1], ', fp_average_error: ',\n",
    "                 fp_mean_power_conserve[-1])\n",
    "        print(\"\\n\\n\")\n",
    "\n",
    "        \n",
    "        var_f = open('/'.join(image_dir.split('/')[:-1]) +  '/' + intensity_degradation + '_' + str(slope) + '_' + \n",
    "                     dtime + \".dat\", \"wb\") # file for saving results\n",
    "        pickle.dump([average_diff_power, fp_mean_power, number_samples, best_lambda, \n",
    "                     dataset_name, max_dataset_name, average_diff_power_conserve, fp_mean_power_conserve],\n",
    "                    file=var_f)\n",
    "        var_f.close()\n",
    "        del best_model, test_generator\n",
    "#     prev_sample = number_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_min_mae = [8.27781, 8.23545, 8.20838]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_diff_power = [8.166, 7.844, 7.592]\n",
    "fp_mean_power = [4.56, 4.42, 4.37]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[256, 512, 1024, 2048, 4096, 8192]\n",
      "[21.947, 21.696, 21.466, 21.369, 21.251, 21.07]\n",
      "[15.964, 16.258, 17.124, 17.177, 17.29, 15.634]\n",
      "[]\n",
      "[]\n",
      "[0.01, 0.01, 0.1, 0, 0.001, 0.001]\n"
     ]
    }
   ],
   "source": [
    "print(number_samples)\n",
    "print(average_diff_power)\n",
    "print(fp_mean_power)\n",
    "# print(best_lambda)\n",
    "print(average_diff_power_conserve)\n",
    "print(fp_mean_power_conserve)\n",
    "print(best_lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "SystemError",
     "evalue": "<built-in function GetOperationInputs> returned a result with an error set",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/research/lib/python3.7/site-packages/tensorflow_core/python/pywrap_tensorflow_internal.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(self, name, value)\u001b[0m\n\u001b[1;32m   1443\u001b[0m     \u001b[0m__swig_setmethods__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1444\u001b[0;31m     \u001b[0m__setattr__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_swig_setattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTF_Output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1445\u001b[0m     \u001b[0m__swig_getmethods__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/research/lib/python3.7/site-packages/tensorflow_core/python/pywrap_tensorflow_internal.py\u001b[0m in \u001b[0;36m_swig_setattr\u001b[0;34m(self, class_type, name, value)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_swig_setattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_swig_setattr_nondynamic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/research/lib/python3.7/site-packages/tensorflow_core/python/pywrap_tensorflow_internal.py\u001b[0m in \u001b[0;36m_swig_setattr_nondynamic\u001b[0;34m(self, class_type, name, value, static)\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'SwigPyObject'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mSystemError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-2bfc622779b0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mlambda_vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.003\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.03\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m#, 0.3, 1, 3, 10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0maverage_diff_power\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp_mean_power\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mcnns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcnn_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlamb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlamb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlambda_vec\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcnn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcnns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mcnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'mean_squared_error'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mse'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'mae'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-2bfc622779b0>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mlambda_vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.003\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.03\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m#, 0.3, 1, 3, 10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0maverage_diff_power\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp_mean_power\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mcnns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcnn_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlamb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlamb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlambda_vec\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcnn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcnns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mcnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'mean_squared_error'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mse'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'mae'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-21260a02aa4c>\u001b[0m in \u001b[0;36mcnn_model\u001b[0;34m(num_filters, kernel_lam, bias_lam)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;31m#                          kernel_initializer='lecun_normal'))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0mcnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMaxPooling2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpool_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m     \u001b[0mcnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBatchNormalization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;31m#     cnn.add(layers.Dropout(0.25))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/research/lib/python3.7/site-packages/keras/engine/sequential.py\u001b[0m in \u001b[0;36madd\u001b[0;34m(self, layer)\u001b[0m\n\u001b[1;32m    180\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_source_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m             \u001b[0moutput_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 raise TypeError('All layers in a Sequential model '\n",
      "\u001b[0;32m~/anaconda3/envs/research/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36msymbolic_fn_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_SYMBOLIC_SCOPE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mget_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/research/lib/python3.7/site-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0;31m# Actually call the layer,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m             \u001b[0;31m# collecting output(s), mask(s), and shape(s).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m             \u001b[0moutput_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprevious_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/research/lib/python3.7/site-packages/keras/layers/normalization.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, training)\u001b[0m\n\u001b[1;32m    187\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'cntk'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m             sample_size = K.prod([K.shape(inputs)[axis]\n\u001b[0;32m--> 189\u001b[0;31m                                   for axis in reduction_axes])\n\u001b[0m\u001b[1;32m    190\u001b[0m             \u001b[0msample_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'tensorflow'\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msample_size\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'float32'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/research/lib/python3.7/site-packages/keras/layers/normalization.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    187\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'cntk'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m             sample_size = K.prod([K.shape(inputs)[axis]\n\u001b[0;32m--> 189\u001b[0;31m                                   for axis in reduction_axes])\n\u001b[0m\u001b[1;32m    190\u001b[0m             \u001b[0msample_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'tensorflow'\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msample_size\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'float32'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/research/lib/python3.7/site-packages/tensorflow_core/python/ops/array_ops.py\u001b[0m in \u001b[0;36m_slice_helper\u001b[0;34m(tensor, slice_spec, var)\u001b[0m\n\u001b[1;32m    811\u001b[0m         \u001b[0mellipsis_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mellipsis_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[0mvar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 813\u001b[0;31m         name=name)\n\u001b[0m\u001b[1;32m    814\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    815\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/research/lib/python3.7/site-packages/tensorflow_core/python/ops/array_ops.py\u001b[0m in \u001b[0;36mstrided_slice\u001b[0;34m(input_, begin, end, strides, begin_mask, end_mask, ellipsis_mask, new_axis_mask, shrink_axis_mask, var, name)\u001b[0m\n\u001b[1;32m    977\u001b[0m       \u001b[0mellipsis_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mellipsis_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    978\u001b[0m       \u001b[0mnew_axis_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnew_axis_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 979\u001b[0;31m       shrink_axis_mask=shrink_axis_mask)\n\u001b[0m\u001b[1;32m    980\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m   \u001b[0mparent_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/research/lib/python3.7/site-packages/tensorflow_core/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36mstrided_slice\u001b[0;34m(input, begin, end, strides, begin_mask, end_mask, ellipsis_mask, new_axis_mask, shrink_axis_mask, name)\u001b[0m\n\u001b[1;32m  10392\u001b[0m                         \u001b[0mellipsis_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mellipsis_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  10393\u001b[0m                         \u001b[0mnew_axis_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnew_axis_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m> 10394\u001b[0;31m                         shrink_axis_mask=shrink_axis_mask, name=name)\n\u001b[0m\u001b[1;32m  10395\u001b[0m   \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  10396\u001b[0m   \u001b[0m_inputs_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/research/lib/python3.7/site-packages/tensorflow_core/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    791\u001b[0m         op = g.create_op(op_type_name, inputs, dtypes=None, name=scope,\n\u001b[1;32m    792\u001b[0m                          \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 793\u001b[0;31m                          op_def=op_def)\n\u001b[0m\u001b[1;32m    794\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0moutput_structure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_stateful\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/research/lib/python3.7/site-packages/tensorflow_core/python/framework/func_graph.py\u001b[0m in \u001b[0;36mcreate_op\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    546\u001b[0m     return super(FuncGraph, self)._create_op_internal(  # pylint: disable=protected-access\n\u001b[1;32m    547\u001b[0m         \u001b[0mop_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_def\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 548\u001b[0;31m         compute_device)\n\u001b[0m\u001b[1;32m    549\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcapture\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/research/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_op_internal\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[1;32m   3427\u001b[0m           \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3428\u001b[0m           \u001b[0moriginal_op\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_default_original_op\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3429\u001b[0;31m           op_def=op_def)\n\u001b[0m\u001b[1;32m   3430\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_op_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompute_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3431\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/research/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)\u001b[0m\n\u001b[1;32m   1790\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1791\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mc_op\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1792\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_control_flow_post_processing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1793\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1794\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_control_flow_post_processing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/research/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36m_control_flow_post_processing\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1798\u001b[0m     \u001b[0mavailable\u001b[0m \u001b[0mbefore\u001b[0m \u001b[0mcalling\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1799\u001b[0m     \"\"\"\n\u001b[0;32m-> 1800\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0minput_tensor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1801\u001b[0m       \u001b[0mcontrol_flow_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCheckInputFromValidContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1802\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_control_flow_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/research/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36minputs\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2161\u001b[0m     \u001b[0;34m\"\"\"The list of `Tensor` objects representing the data inputs of this op.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2162\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inputs_val\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2163\u001b[0;31m       \u001b[0mtf_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGetOperationInputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_c_op\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2164\u001b[0m       \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2165\u001b[0m       retval = [\n",
      "\u001b[0;31mSystemError\u001b[0m: <built-in function GetOperationInputs> returned a result with an error set"
     ]
    }
   ],
   "source": [
    "# CNN: support batching\n",
    "TEST = True\n",
    "mini_batch, epochs = 16, 30\n",
    "batch_size = (batch_size // mini_batch) * mini_batch\n",
    "prev_sample = 0\n",
    "lambda_vec = [0, 0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1, 3, 10]  #, 0.3, 1, 3, 10\n",
    "average_diff_power, fp_mean_power = [], []\n",
    "cnns = [cnn_model(10, lamb, 0) for lamb in lambda_vec]\n",
    "for cnn in cnns:\n",
    "    cnn.compile(loss='mean_squared_error', optimizer='adam', metrics=['mse','mae'])\n",
    "for number_sample in number_samples:\n",
    "    number_start = time.time()\n",
    "    current_sample = number_sample - prev_sample\n",
    "    train_samples = [batch_size] * (current_sample//batch_size) + ([current_sample%batch_size] if \n",
    "                                                                    current_sample%batch_size else [])\n",
    "    val_size = math.ceil(number_sample * validation_size)\n",
    "#     val_samples = [batch_size] * (val_size//batch_size) + ([val_size%batch_size] if \n",
    "#                                                                val_size%batch_size else [])\n",
    "    \n",
    "    print('number_samples:', number_sample)\n",
    "    print(\"Train batches:\", train_samples)\n",
    "    for i, train_sample in enumerate(train_samples):\n",
    "        print(\"Train batch#:\", i, \", batch size:\", train_sample, \", starts:\", prev_sample + i * batch_size,\n",
    "                      \", ends:\", prev_sample + i * batch_size + train_sample - 1)\n",
    "    print(\"Validation size:\", val_size, \", starts:\", number_sample, \", ends:\", number_sample + val_size - 1)\n",
    "#     print(\"Validation Batches:\", val_samples)\n",
    "#     for i, val_sample in enumerate(val_samples):\n",
    "#         print(\"Validation batch#:\", i, \", batch size:\", val_sample, \", starts:\", number_sample + i * batch_size,\n",
    "#                       \", ends:\", number_sample + i * batch_size + val_sample - 1)\n",
    "        \n",
    "    min_error = float('inf')\n",
    "    best_model, best_lam = None, None\n",
    "    for lamb_idx, lamb in enumerate(lambda_vec):\n",
    "        lambda_start = time.time()\n",
    "        \n",
    "#         cnn = cnn_model(10, lamb, 0)\n",
    "#         cnn.compile(loss='mean_squared_error', optimizer='adam', metrics=['mse','mae'])\n",
    "        \n",
    "        # training on all batches\n",
    "        for i, train_sample in enumerate(train_samples):\n",
    "#             if lamb_idx == 0:\n",
    "#                 print(\"Train batch#:\", i, \", batch size:\", train_sample, \", starts:\", prev_sample + i * batch_size,\n",
    "#                       \", ends:\", prev_sample + i * batch_size + train_sample - 1)\n",
    "            x_train = np.empty((train_sample, number_image_channels, max_x, max_y), dtype=float_memory_used)\n",
    "            y_train = np.empty((train_sample), dtype=float_memory_used)\n",
    "            for image_num in range(prev_sample + i * batch_size, prev_sample + i * batch_size + train_sample):\n",
    "                x_train[(image_num - prev_sample) % batch_size] = read_image(image_num)\n",
    "                y_train[(image_num - prev_sample) % batch_size] = np.asarray(data_reg[image_num][-1], \n",
    "                                                                             dtype=float_memory_used)\n",
    "            cnns[lamb_idx].fit(x_train, y_train, epochs=epochs, verbose=2, batch_size=mini_batch,\n",
    "                               validation_split=0.2, \n",
    "                               shuffle=True)\n",
    "            del x_train, y_train\n",
    "#         if lamb_idx == 0:\n",
    "#             print(\"Validation size:\", val_size, \", starts:\", number_sample, \", ends:\", \n",
    "#                   number_sample + val_size - 1)\n",
    "        print(\"\\nLambda:\", lamb)\n",
    "        print(\"Train Error(all epochs): \", cnns[lamb_idx].history.history['mae'])\n",
    "        \n",
    "        # validating\n",
    "        val_mae, val_fp_mae = 0.0, 0.0\n",
    "#         for i, val_sample in enumerate(val_samples):\n",
    "#             x_val = np.empty((val_sample, number_image_channels, max_x, max_y), dtype=float_memory_used)\n",
    "#             for image_num in range(val_sample):\n",
    "#                 x_val[image_num] = read_image(image_num + number_sample + i * batch_size)\n",
    "#             yp_val = cnns[lamb_idx].predict(x_val)\n",
    "        for image_num in range(val_size):\n",
    "            val_y = data_reg[image_num + number_sample][-1]\n",
    "            image = read_image(image_num + number_sample)\n",
    "            val_yp = cnns[lamb_idx].predict(image)[0][0]\n",
    "#             for image_num in range(val_sample):\n",
    "#                 val_yp = yp_val[image_num][0]\n",
    "#                 val_y = data_reg[image_num + number_sample + i * batch_size][-1]\n",
    "            val_mae += abs(val_y - val_yp)\n",
    "            if val_yp > val_y:\n",
    "                val_fp_mae += abs(val_yp - val_y)\n",
    "        val_mae /= val_size\n",
    "        val_fp_mae /= val_size\n",
    "        print(\"Val Error:\", round(val_mae, 3), \", Time:\", str(datetime.timedelta(seconds=int(time.time() - lambda_start))))\n",
    "        if val_mae < min_error:\n",
    "            min_error = val_mae\n",
    "            best_model = cnns[lamb_idx]\n",
    "            best_lam = lamb\n",
    "            best_lam_idx = lamb_idx\n",
    "    print(\"\\nTrainig set size:\", number_sample, \", Time:\", str(datetime.timedelta(seconds=int(time.time() - number_start)))\n",
    "          ,\", best_lambda:\", best_lam, \", min_error:\", round(min_error, 3))\n",
    "    \n",
    "    \n",
    "    if TEST:\n",
    "        # evaluating test images\n",
    "        sum_mae, sum_fp_mae = 0, 0\n",
    "        test_size = 0\n",
    "\n",
    "        y_test_p = np.empty((data_reg.shape[0] - (number_sample + val_size)), dtype=float_memory_used)\n",
    "    #     test_size = data_reg.shape[0] - (number_sample + val_size)\n",
    "    #     test_samples = [batch_size] * (test_size//batch_size) + ([test_size%batch_size] if \n",
    "    #                                                              test_size%batch_size else [])\n",
    "        print(\"Test starts: \", number_sample + val_size, \", ends: \", data_reg.shape[0] - 1)\n",
    "        time.sleep(1)\n",
    "    #     for i, test_sample in tqdm.tqdm(enumerate(test_samples)):\n",
    "    #         x_test = np.empty((test_sample, number_image_channels, max_x, max_y), dtype=float_memory_used)\n",
    "    #         for image_num in range(test_sample):\n",
    "    #             x_test[image_num] = read_image(number_sample + val_size + i * batch_size)\n",
    "    #         yp_test = cnns[best_lam_idx].predict(x_test)\n",
    "    #         for image_num in range(test_sample):\n",
    "    #             test_y = data_reg[number_sample + val_size + i * batch_size][-1]\n",
    "    #             test_yp = yp_test[image_num][0]\n",
    "    #             sum_mae += abs(test_yp - test_y)\n",
    "    #             if test_yp > test_y:\n",
    "    #                 sum_fp_mae += abs(test_yp - test_y)\n",
    "\n",
    "        for test_num in tqdm.tqdm(range(number_sample + val_size, data_reg.shape[0])):\n",
    "            test_size += 1\n",
    "            test_image = read_image(test_num)\n",
    "            test_y = data_reg[test_num][-1]\n",
    "            test_yp = best_model.predict(test_image)[0][0]\n",
    "            y_test_p[test_num - (number_sample + val_size)] = test_yp\n",
    "            sum_mae += abs(test_yp - test_y)\n",
    "            if test_yp > test_y:\n",
    "                sum_fp_mae += abs(test_yp - test_y)\n",
    "        fp_mean_power.append(round(sum_fp_mae/ test_size, 3))\n",
    "        average_diff_power.append(round(sum_mae / test_size, 3))\n",
    "        print('average_error: ', average_diff_power[-1], ', fp_average_error: ', \n",
    "              fp_mean_power[-1])\n",
    "        print(\"\\n\\n\")\n",
    "        var_f = open('/'.join(image_dir.split('/')[:-1]) +  '/' + intensity_degradation + '_' + str(slope) + '_' + \n",
    "                     dtime + \".dat\", \"wb\") # file for saving results\n",
    "        pickle.dump([average_diff_power, fp_mean_power, number_samples], file=var_f)\n",
    "        var_f.close()\n",
    "    prev_sample = number_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'conv2d_216_1/kernel:0' shape=(3, 3, 7, 10) dtype=float32, numpy=\n",
       " array([[[[ 0.13334414, -0.0261361 , -0.07441936,  0.16142276,\n",
       "            0.10993325,  0.04581179,  0.24121895,  0.23147854,\n",
       "            0.17021964, -0.0591266 ],\n",
       "          [ 0.09241038,  0.10264444,  0.00484879, -0.02517582,\n",
       "            0.06216534, -0.06764037,  0.02806018,  0.08384448,\n",
       "            0.13874047,  0.15180951],\n",
       "          [ 0.1893204 , -0.05741746,  0.00313846,  0.03854534,\n",
       "           -0.20698836,  0.00660574, -0.1372445 , -0.11774021,\n",
       "            0.10552079, -0.1360089 ],\n",
       "          [-0.1331031 ,  0.02123114,  0.00091115, -0.03792882,\n",
       "            0.22543769, -0.07044397, -0.09563252, -0.04734167,\n",
       "           -0.09483308,  0.04207201],\n",
       "          [ 0.06165536,  0.04747773,  0.00436929,  0.0485286 ,\n",
       "           -0.16992377,  0.06714778,  0.20848735,  0.16978653,\n",
       "           -0.07686065, -0.09264071],\n",
       "          [ 0.08572359, -0.19217153, -0.08705788,  0.12219634,\n",
       "           -0.27800012,  0.1237585 , -0.21454357,  0.04010623,\n",
       "           -0.13206151, -0.07199094],\n",
       "          [-0.13942277,  0.12991397,  0.05158436,  0.20681241,\n",
       "            0.0927059 , -0.06157902,  0.09529742, -0.06181924,\n",
       "           -0.20937485,  0.08383203]],\n",
       " \n",
       "         [[ 0.14524975, -0.05317885,  0.07710187, -0.06027135,\n",
       "           -0.03889057, -0.13648398, -0.11979997,  0.1689479 ,\n",
       "           -0.1522798 , -0.05902102],\n",
       "          [ 0.06488625,  0.12912498,  0.09674022,  0.20966475,\n",
       "           -0.04465631,  0.12140161,  0.14916958, -0.02789529,\n",
       "            0.03566265, -0.07232169],\n",
       "          [-0.07508589,  0.11446775, -0.13920276,  0.22177145,\n",
       "            0.08524486,  0.0902052 , -0.10775251,  0.08795813,\n",
       "           -0.0761055 , -0.09227429],\n",
       "          [ 0.02373275, -0.11338315,  0.1032308 , -0.07559496,\n",
       "            0.04385605, -0.1355971 ,  0.10978308,  0.10370436,\n",
       "            0.03486014,  0.04627065],\n",
       "          [-0.21984474,  0.05307737,  0.04838712,  0.16839921,\n",
       "            0.11966983,  0.14054751, -0.10383917, -0.17774169,\n",
       "           -0.08481254,  0.01736768],\n",
       "          [ 0.04152398, -0.06096472, -0.00492838,  0.1368062 ,\n",
       "            0.1721607 ,  0.07199505, -0.14697716, -0.23859444,\n",
       "           -0.1461731 , -0.03527378],\n",
       "          [-0.27617452,  0.15010485,  0.11511505, -0.12417073,\n",
       "           -0.08122088,  0.14036025,  0.1422506 ,  0.17473486,\n",
       "            0.08986371,  0.01991365]],\n",
       " \n",
       "         [[ 0.04023483,  0.01661961, -0.08479083, -0.28282636,\n",
       "            0.12767118,  0.0509973 , -0.05391447,  0.19763674,\n",
       "            0.16401489,  0.02086166],\n",
       "          [ 0.02320998,  0.21452   ,  0.02131915, -0.22229502,\n",
       "            0.07397044,  0.08494943,  0.04313029, -0.19079794,\n",
       "           -0.15622707, -0.12654568],\n",
       "          [-0.16471341,  0.07649319,  0.01780317,  0.17973316,\n",
       "            0.00282395,  0.01653246,  0.14217037, -0.07009459,\n",
       "            0.2320501 , -0.25558722],\n",
       "          [-0.13088815, -0.03935688, -0.01927   ,  0.06664042,\n",
       "            0.05449986, -0.27412698,  0.04830834,  0.05888246,\n",
       "           -0.11925068, -0.29004106],\n",
       "          [ 0.09491049, -0.02301907,  0.0727259 , -0.1380526 ,\n",
       "            0.06161747,  0.08663608,  0.00969613, -0.02390122,\n",
       "           -0.07748399, -0.11728296],\n",
       "          [-0.28847724, -0.17691025,  0.10943054,  0.17433746,\n",
       "           -0.01487851, -0.18751724, -0.12222388,  0.05808705,\n",
       "           -0.11082985, -0.17230581],\n",
       "          [-0.04870008, -0.2375922 ,  0.0976412 ,  0.22535303,\n",
       "            0.01409588, -0.02963496,  0.2672263 ,  0.00094256,\n",
       "            0.05225825,  0.03488791]]],\n",
       " \n",
       " \n",
       "        [[[-0.19926119, -0.03603117, -0.05551852, -0.10433564,\n",
       "           -0.14335166,  0.06645513, -0.2795412 , -0.17509606,\n",
       "            0.15266582, -0.05958281],\n",
       "          [ 0.00352683, -0.05189499,  0.14581002, -0.09802195,\n",
       "           -0.0185049 , -0.19627838, -0.17125873, -0.11177973,\n",
       "           -0.15869416,  0.23808493],\n",
       "          [ 0.07290543, -0.04298121,  0.01518185,  0.24427563,\n",
       "           -0.04966308,  0.13753985,  0.09585362,  0.13073985,\n",
       "            0.14580618, -0.05254631],\n",
       "          [ 0.04517658, -0.0560442 , -0.04483185, -0.19104496,\n",
       "           -0.02319299, -0.01029402, -0.15246606,  0.26141602,\n",
       "           -0.09231006,  0.27955005],\n",
       "          [-0.1809828 ,  0.06103823,  0.14722025,  0.08918977,\n",
       "            0.05734708, -0.11969002,  0.06482836, -0.1559477 ,\n",
       "            0.0741372 ,  0.04197035],\n",
       "          [-0.06164405,  0.10421135,  0.11966567, -0.11889777,\n",
       "            0.02610188, -0.02379085,  0.25728026,  0.22226161,\n",
       "            0.05567339, -0.09093149],\n",
       "          [-0.0941878 , -0.13269533,  0.14010622, -0.15810324,\n",
       "            0.09464984,  0.04383229,  0.11680618, -0.05575173,\n",
       "            0.05689744,  0.12688859]],\n",
       " \n",
       "         [[ 0.09463172,  0.07173032, -0.17775638,  0.12823975,\n",
       "            0.22743171, -0.03096067, -0.07845164,  0.10056094,\n",
       "            0.1375068 , -0.00266118],\n",
       "          [-0.13137127, -0.05241982,  0.1078676 , -0.1591938 ,\n",
       "            0.14385167,  0.17707717,  0.15017428, -0.05183896,\n",
       "            0.08562967,  0.07045954],\n",
       "          [ 0.00724656,  0.04945549, -0.00321115, -0.10365249,\n",
       "           -0.11740654, -0.03923345,  0.05962155,  0.10888351,\n",
       "           -0.09101029,  0.13877763],\n",
       "          [ 0.07962049,  0.05093174, -0.0761327 ,  0.12048989,\n",
       "            0.03043546, -0.10803499,  0.03744312,  0.02285842,\n",
       "            0.03543871, -0.05152218],\n",
       "          [-0.07119177, -0.07241955,  0.02490062,  0.00423546,\n",
       "           -0.04316133,  0.19139971, -0.01283269, -0.07494903,\n",
       "           -0.02201682, -0.06245283],\n",
       "          [ 0.00799846,  0.18765801, -0.09534817, -0.077469  ,\n",
       "           -0.11586822, -0.0051675 , -0.0102727 , -0.11339141,\n",
       "            0.03436622,  0.09942368],\n",
       "          [ 0.07523795,  0.03675905,  0.02437227, -0.01435202,\n",
       "           -0.02034545, -0.09359565, -0.02359232,  0.04803864,\n",
       "            0.20680504, -0.00644547]],\n",
       " \n",
       "         [[-0.1005403 ,  0.11378611,  0.21102735,  0.06846891,\n",
       "            0.09442588,  0.09777017, -0.11909521,  0.12265773,\n",
       "           -0.06148191, -0.07069711],\n",
       "          [ 0.01405865, -0.10141152, -0.11822031, -0.13485955,\n",
       "           -0.16953868, -0.21654141, -0.05722266, -0.01606212,\n",
       "           -0.07794228, -0.1521405 ],\n",
       "          [-0.08736753,  0.02725688, -0.01609255, -0.1761448 ,\n",
       "           -0.02491948, -0.07835825, -0.02474505, -0.2482235 ,\n",
       "            0.19618578, -0.08405739],\n",
       "          [-0.13201593,  0.07316361,  0.14873287,  0.03337387,\n",
       "            0.04706023, -0.03948716, -0.15505248,  0.06093074,\n",
       "            0.05425106,  0.18854883],\n",
       "          [ 0.12107033, -0.19670665,  0.03158212,  0.08311516,\n",
       "            0.01700125, -0.04055984, -0.08617479,  0.03713043,\n",
       "           -0.04209922, -0.01615153],\n",
       "          [ 0.0248774 , -0.02845379, -0.07260788, -0.1052746 ,\n",
       "            0.15719673, -0.07497114,  0.0163184 ,  0.15407056,\n",
       "           -0.19900005, -0.0528684 ],\n",
       "          [-0.09391885, -0.03256474,  0.02134794, -0.06547134,\n",
       "            0.02687071,  0.0165927 , -0.21028309,  0.18223254,\n",
       "            0.1601678 ,  0.06802534]]],\n",
       " \n",
       " \n",
       "        [[[-0.11673861, -0.02761208, -0.05152625,  0.26689234,\n",
       "            0.05464312, -0.0165887 , -0.15953052, -0.20351106,\n",
       "           -0.00750204, -0.04550588],\n",
       "          [-0.18615605,  0.19847395, -0.22109997, -0.04208753,\n",
       "            0.0015387 , -0.08712109, -0.12053566,  0.04012857,\n",
       "            0.12612605,  0.0755921 ],\n",
       "          [-0.19893515,  0.09130076, -0.02334492,  0.10549977,\n",
       "            0.10952222, -0.20525998,  0.13201837,  0.09622242,\n",
       "           -0.27899146, -0.1937835 ],\n",
       "          [ 0.02993972,  0.06206925, -0.02340401,  0.1940042 ,\n",
       "           -0.27053413, -0.02356468,  0.0402792 ,  0.00274105,\n",
       "            0.10014538,  0.1180291 ],\n",
       "          [ 0.09827848, -0.05272344, -0.24212418,  0.18239398,\n",
       "            0.28893584,  0.03425925,  0.18894415, -0.16935891,\n",
       "           -0.19899593, -0.17445645],\n",
       "          [-0.16428833,  0.15912978, -0.03123697,  0.19525754,\n",
       "            0.1313913 , -0.1464028 ,  0.19579571, -0.03027558,\n",
       "            0.02622594,  0.05283794],\n",
       "          [ 0.00537856, -0.02129651,  0.01653795, -0.03506261,\n",
       "            0.06030509,  0.0077933 , -0.26723355,  0.08907922,\n",
       "           -0.23224692,  0.15809113]],\n",
       " \n",
       "         [[-0.04699924,  0.21412279, -0.13859469,  0.15074492,\n",
       "           -0.05260769,  0.02953377,  0.21503067,  0.09594385,\n",
       "            0.18201022,  0.18532227],\n",
       "          [ 0.0813347 ,  0.1836983 , -0.05178611,  0.14537628,\n",
       "            0.13789822,  0.1196593 , -0.13453369, -0.05221471,\n",
       "           -0.26833838,  0.20581754],\n",
       "          [ 0.26625836, -0.22261377,  0.03792743,  0.14181626,\n",
       "           -0.13818225, -0.03733314, -0.2065161 , -0.11526174,\n",
       "            0.0504146 , -0.08647973],\n",
       "          [ 0.09377266,  0.2062351 ,  0.05943884,  0.03329353,\n",
       "           -0.10891343, -0.11080927,  0.23227565,  0.23207027,\n",
       "            0.15182719, -0.16632546],\n",
       "          [-0.01662905,  0.14488567,  0.1271512 ,  0.02320437,\n",
       "            0.01788143,  0.05921528, -0.0353229 , -0.188283  ,\n",
       "            0.1494947 , -0.18092439],\n",
       "          [-0.13061637,  0.1592543 ,  0.00978735, -0.05683037,\n",
       "            0.17805217,  0.02800326,  0.26188764, -0.11778035,\n",
       "            0.06277441, -0.12744053],\n",
       "          [ 0.00465929, -0.04557341, -0.15533756, -0.05069922,\n",
       "           -0.14725295, -0.0853371 , -0.20699254,  0.07564644,\n",
       "            0.02780418,  0.2634452 ]],\n",
       " \n",
       "         [[ 0.01776883, -0.03514121, -0.12700932,  0.12356611,\n",
       "           -0.10552283,  0.20057616,  0.04670141,  0.12937702,\n",
       "           -0.05964081,  0.2509442 ],\n",
       "          [-0.03483937,  0.04906264, -0.0862985 , -0.22901045,\n",
       "            0.00186068, -0.07967521, -0.02993756, -0.03580993,\n",
       "            0.23193611, -0.02563061],\n",
       "          [ 0.01709478,  0.07445383, -0.19091251, -0.0331335 ,\n",
       "            0.0253082 ,  0.02352164,  0.2859322 ,  0.10892042,\n",
       "            0.18633689, -0.0595445 ],\n",
       "          [-0.20758714,  0.07152744, -0.01677686, -0.01780074,\n",
       "           -0.12487143,  0.00212223, -0.01722952,  0.2022124 ,\n",
       "           -0.2645012 , -0.04340143],\n",
       "          [ 0.13472383,  0.00098047, -0.10209464,  0.14819503,\n",
       "           -0.08379597, -0.18391946, -0.01674595,  0.14736894,\n",
       "            0.11205667, -0.14535202],\n",
       "          [-0.07028721, -0.13803037, -0.09943354, -0.27029005,\n",
       "            0.04495002, -0.04398946,  0.05618115,  0.1637726 ,\n",
       "           -0.21474266, -0.09334109],\n",
       "          [-0.15808658, -0.23804595,  0.18949424,  0.04920245,\n",
       "           -0.20425159, -0.18336712, -0.08576456, -0.13084741,\n",
       "            0.00551501,  0.05699825]]]], dtype=float32)>,\n",
       " <tf.Variable 'conv2d_216_1/bias:0' shape=(10,) dtype=float32, numpy=\n",
       " array([ 0.00394855,  0.01156156,  0.00672093, -0.00029083,  0.0033301 ,\n",
       "        -0.00794703, -0.01068521, -0.00623209, -0.00136579,  0.0004727 ],\n",
       "       dtype=float32)>,\n",
       " <tf.Variable 'batch_normalization_246_1/gamma:0' shape=(10,) dtype=float32, numpy=\n",
       " array([0.99889743, 1.0080869 , 0.9918417 , 0.99180245, 0.99210924,\n",
       "        1.0056474 , 1.0041177 , 1.0091603 , 0.999746  , 0.99256486],\n",
       "       dtype=float32)>,\n",
       " <tf.Variable 'batch_normalization_246_1/beta:0' shape=(10,) dtype=float32, numpy=\n",
       " array([ 1.2581445e-02,  7.9289870e-03, -7.9357335e-03, -1.1723038e-02,\n",
       "        -7.7971257e-03,  2.2734562e-03,  5.4945835e-05, -6.2508588e-03,\n",
       "         4.6733394e-04,  5.2858326e-03], dtype=float32)>,\n",
       " <tf.Variable 'conv2d_217_1/kernel:0' shape=(3, 3, 10, 20) dtype=float32, numpy=\n",
       " array([[[[-0.00613516, -0.07706599,  0.006248  , ...,  0.01483129,\n",
       "           -0.13148054,  0.01379959],\n",
       "          [-0.06125846, -0.04463966,  0.15691034, ...,  0.06724139,\n",
       "           -0.08870574,  0.04128341],\n",
       "          [ 0.18929154, -0.07877082,  0.09513605, ..., -0.06041372,\n",
       "           -0.03992198, -0.18455972],\n",
       "          ...,\n",
       "          [ 0.02003453, -0.11194929, -0.11064813, ..., -0.08062167,\n",
       "            0.13399614, -0.022826  ],\n",
       "          [ 0.10247879, -0.09553227,  0.04918528, ...,  0.23149642,\n",
       "           -0.09936932, -0.01014787],\n",
       "          [-0.06446787,  0.0508823 ,  0.03458132, ...,  0.09098805,\n",
       "            0.05103992, -0.22692856]],\n",
       " \n",
       "         [[ 0.00899815, -0.01982983,  0.2108407 , ..., -0.11129528,\n",
       "            0.04713659,  0.02240109],\n",
       "          [ 0.0239722 ,  0.03624769, -0.13892104, ..., -0.1916124 ,\n",
       "            0.11251629,  0.05872869],\n",
       "          [-0.14052154, -0.02749509,  0.067243  , ..., -0.07384579,\n",
       "            0.1350736 ,  0.04331046],\n",
       "          ...,\n",
       "          [-0.1179302 , -0.04421049,  0.02058602, ..., -0.18133816,\n",
       "            0.14051902, -0.00200996],\n",
       "          [-0.13089395, -0.02914838,  0.02653082, ...,  0.09342858,\n",
       "           -0.08133891,  0.13094549],\n",
       "          [ 0.06657103, -0.18192683,  0.2327268 , ...,  0.06579488,\n",
       "            0.16109888, -0.0417343 ]],\n",
       " \n",
       "         [[ 0.14598422,  0.02474008, -0.2001503 , ..., -0.05117525,\n",
       "           -0.08759225,  0.09137008],\n",
       "          [ 0.11352185,  0.10408597,  0.05276801, ...,  0.02367296,\n",
       "            0.0709122 , -0.05202084],\n",
       "          [ 0.04591953,  0.00177405, -0.05923538, ..., -0.04373968,\n",
       "            0.09335176, -0.23466058],\n",
       "          ...,\n",
       "          [ 0.04117727,  0.09622082, -0.00483591, ..., -0.00573619,\n",
       "           -0.00544672, -0.07039722],\n",
       "          [-0.15966627,  0.13912055,  0.04043337, ...,  0.18531808,\n",
       "            0.12856238,  0.12779331],\n",
       "          [-0.11824572, -0.00606683, -0.02776809, ...,  0.03119941,\n",
       "           -0.06217549, -0.1021296 ]]],\n",
       " \n",
       " \n",
       "        [[[ 0.11843395, -0.11313166,  0.10206358, ...,  0.1424125 ,\n",
       "            0.02883888,  0.10749669],\n",
       "          [-0.0932157 ,  0.08986368,  0.08855385, ..., -0.04054831,\n",
       "            0.20761569,  0.17895086],\n",
       "          [ 0.21287711, -0.13902958, -0.05071876, ..., -0.10464473,\n",
       "            0.13464488,  0.00796567],\n",
       "          ...,\n",
       "          [-0.01583735, -0.10220268,  0.16242856, ..., -0.22936785,\n",
       "           -0.02587985,  0.05995097],\n",
       "          [-0.05231554, -0.07904537,  0.09455277, ...,  0.03787832,\n",
       "            0.05256969, -0.03759636],\n",
       "          [-0.09452608, -0.05021729, -0.00096467, ..., -0.1103365 ,\n",
       "            0.10697437, -0.14381793]],\n",
       " \n",
       "         [[-0.04669933,  0.01557779, -0.0037258 , ...,  0.12755205,\n",
       "            0.11051998,  0.04037065],\n",
       "          [ 0.07087644,  0.08811377,  0.0231888 , ..., -0.06829704,\n",
       "            0.07152358, -0.01255593],\n",
       "          [ 0.06456453,  0.1441479 ,  0.21550633, ...,  0.11959073,\n",
       "           -0.21130289, -0.01518227],\n",
       "          ...,\n",
       "          [ 0.06106412,  0.13728103, -0.02890644, ...,  0.07025255,\n",
       "           -0.0409243 , -0.0301967 ],\n",
       "          [-0.10983857,  0.08415116, -0.16161825, ..., -0.03688908,\n",
       "           -0.1048198 ,  0.06283347],\n",
       "          [-0.02078158,  0.07726054,  0.22857217, ..., -0.02705708,\n",
       "           -0.21459246,  0.20110175]],\n",
       " \n",
       "         [[ 0.10718577, -0.03096308, -0.03471959, ..., -0.15640563,\n",
       "           -0.17416269,  0.1678829 ],\n",
       "          [ 0.23790742, -0.12122889,  0.05106934, ..., -0.14332658,\n",
       "            0.13123561, -0.17268288],\n",
       "          [-0.07216983,  0.02125906,  0.04503383, ...,  0.02436657,\n",
       "            0.05501794,  0.01041913],\n",
       "          ...,\n",
       "          [-0.23144129, -0.04420161, -0.1883411 , ...,  0.06308959,\n",
       "            0.07754758, -0.0654578 ],\n",
       "          [ 0.0442895 ,  0.0457861 ,  0.08194239, ..., -0.15825216,\n",
       "            0.04440489, -0.0206511 ],\n",
       "          [-0.028714  ,  0.13239929, -0.03383066, ...,  0.09529423,\n",
       "           -0.22242351, -0.07872296]]],\n",
       " \n",
       " \n",
       "        [[[-0.09778915,  0.05839903,  0.02432927, ..., -0.04064826,\n",
       "           -0.10225639,  0.10601223],\n",
       "          [ 0.10263681, -0.01321077, -0.14533637, ..., -0.07213327,\n",
       "           -0.20481627, -0.02181217],\n",
       "          [-0.05705408, -0.16224189,  0.06692079, ..., -0.13675866,\n",
       "            0.03165859,  0.08559091],\n",
       "          ...,\n",
       "          [ 0.1333062 , -0.01051405,  0.10658982, ..., -0.13980907,\n",
       "           -0.13984981, -0.13669503],\n",
       "          [ 0.14150469,  0.06306539, -0.049619  , ..., -0.08240972,\n",
       "           -0.17580837, -0.15314321],\n",
       "          [ 0.12678467,  0.00361737,  0.12182796, ...,  0.05166734,\n",
       "            0.01910091,  0.19070801]],\n",
       " \n",
       "         [[-0.07456422,  0.04228234, -0.04476889, ...,  0.01102671,\n",
       "            0.03837417,  0.00232783],\n",
       "          [-0.1743557 ,  0.03958088,  0.21821131, ..., -0.06465518,\n",
       "           -0.05084631,  0.07745536],\n",
       "          [ 0.07760292,  0.0065304 , -0.1256236 , ...,  0.01159124,\n",
       "            0.18457702, -0.15225986],\n",
       "          ...,\n",
       "          [ 0.0826866 , -0.04612084, -0.08189004, ..., -0.1406778 ,\n",
       "           -0.15322529,  0.01678441],\n",
       "          [-0.07755306, -0.01942021, -0.04269481, ...,  0.20054315,\n",
       "            0.04770061, -0.06134127],\n",
       "          [-0.06864616, -0.02571295, -0.01662292, ..., -0.1724913 ,\n",
       "            0.01081838,  0.00735117]],\n",
       " \n",
       "         [[ 0.04716585, -0.04663199, -0.22949034, ...,  0.06523322,\n",
       "           -0.03782558,  0.11918577],\n",
       "          [-0.04422992,  0.02638413, -0.03934672, ..., -0.00764357,\n",
       "           -0.06596323,  0.00637577],\n",
       "          [-0.13985588,  0.04368904, -0.13685353, ..., -0.07130355,\n",
       "            0.13632149,  0.02693932],\n",
       "          ...,\n",
       "          [ 0.16433991, -0.18598363,  0.0482639 , ...,  0.07945465,\n",
       "           -0.00088441, -0.07022502],\n",
       "          [-0.0102343 ,  0.03097694,  0.01139166, ...,  0.00272   ,\n",
       "            0.09357451, -0.04334646],\n",
       "          [-0.10491278, -0.01409396, -0.11717147, ...,  0.08654676,\n",
       "           -0.1032054 ,  0.03606071]]]], dtype=float32)>,\n",
       " <tf.Variable 'conv2d_217_1/bias:0' shape=(20,) dtype=float32, numpy=\n",
       " array([-0.0060811 , -0.0068215 ,  0.00610762,  0.00381076, -0.00332611,\n",
       "         0.01825029, -0.00569285,  0.0057989 , -0.00631325, -0.00456895,\n",
       "        -0.00355404, -0.00981498, -0.00726559,  0.01567949,  0.00570573,\n",
       "         0.00865319,  0.00934281, -0.00607597,  0.0064635 ,  0.0063222 ],\n",
       "       dtype=float32)>,\n",
       " <tf.Variable 'batch_normalization_247_1/gamma:0' shape=(20,) dtype=float32, numpy=\n",
       " array([1.0085537 , 0.9999157 , 1.0029037 , 1.0036793 , 0.9965737 ,\n",
       "        0.9955619 , 0.99597555, 0.9951744 , 0.99155873, 0.99123126,\n",
       "        1.0021704 , 1.0111369 , 1.0073707 , 0.9978239 , 0.9897377 ,\n",
       "        0.9979535 , 1.0064371 , 1.0076557 , 0.99272054, 0.9983158 ],\n",
       "       dtype=float32)>,\n",
       " <tf.Variable 'batch_normalization_247_1/beta:0' shape=(20,) dtype=float32, numpy=\n",
       " array([-0.00536959, -0.00913435,  0.00736503, -0.00202524, -0.00472902,\n",
       "         0.00195614, -0.00162657, -0.00393156, -0.00975933, -0.00361821,\n",
       "         0.0016747 , -0.01143507,  0.00245739,  0.00032578, -0.00409207,\n",
       "        -0.00324961,  0.00862126, -0.01241841,  0.00541203,  0.00482147],\n",
       "       dtype=float32)>,\n",
       " <tf.Variable 'conv2d_218_1/kernel:0' shape=(3, 3, 20, 10) dtype=float32, numpy=\n",
       " array([[[[ 0.02090567, -0.01063356,  0.01568592, ..., -0.00421554,\n",
       "           -0.04153974, -0.04014684],\n",
       "          [ 0.04146984, -0.06783944, -0.0200508 , ...,  0.09385803,\n",
       "            0.04323528,  0.01106868],\n",
       "          [ 0.03960693,  0.05715526,  0.05039765, ...,  0.13124132,\n",
       "           -0.01877139,  0.12398075],\n",
       "          ...,\n",
       "          [ 0.01227695, -0.00760612,  0.05333555, ..., -0.06424541,\n",
       "           -0.01374738,  0.04552693],\n",
       "          [ 0.0417938 , -0.02966972,  0.0077204 , ...,  0.10324682,\n",
       "           -0.0336564 ,  0.08304278],\n",
       "          [ 0.06021956, -0.07890533, -0.05801883, ...,  0.08471491,\n",
       "            0.14794236, -0.03558924]],\n",
       " \n",
       "         [[-0.06004382,  0.14762361, -0.14524157, ..., -0.05869973,\n",
       "            0.03716639,  0.10489431],\n",
       "          [-0.12088605,  0.00969887, -0.15406442, ..., -0.0068292 ,\n",
       "           -0.09708827, -0.15776071],\n",
       "          [ 0.00866788,  0.02003344, -0.00541908, ...,  0.0622683 ,\n",
       "            0.06663585,  0.05296098],\n",
       "          ...,\n",
       "          [-0.02643398,  0.07877985,  0.02854742, ..., -0.05691448,\n",
       "           -0.10097322, -0.0748076 ],\n",
       "          [ 0.0615546 ,  0.03269725,  0.14803933, ...,  0.14153697,\n",
       "            0.11893072,  0.05922618],\n",
       "          [ 0.12306278,  0.01435436,  0.0020571 , ..., -0.12050573,\n",
       "           -0.0762575 ,  0.16076486]],\n",
       " \n",
       "         [[-0.1540486 ,  0.11167903,  0.03835633, ...,  0.11817916,\n",
       "           -0.15860094, -0.05598804],\n",
       "          [-0.15525052, -0.14420928,  0.03094636, ..., -0.04865462,\n",
       "            0.0121097 ,  0.02022653],\n",
       "          [-0.06025796,  0.01669158, -0.02238417, ..., -0.04274051,\n",
       "            0.10321181, -0.09857506],\n",
       "          ...,\n",
       "          [-0.00701016,  0.1400375 , -0.13470443, ...,  0.10114507,\n",
       "           -0.12888566, -0.04554447],\n",
       "          [ 0.07841129,  0.00090785, -0.01532124, ..., -0.12396181,\n",
       "           -0.0439156 , -0.02015512],\n",
       "          [ 0.00565134, -0.02282265,  0.04575562, ...,  0.12269929,\n",
       "            0.11260708,  0.0627861 ]]],\n",
       " \n",
       " \n",
       "        [[[ 0.03517975, -0.09904189,  0.08187799, ...,  0.09351594,\n",
       "            0.05125515, -0.00984296],\n",
       "          [ 0.00697843,  0.08033408, -0.10224243, ..., -0.04909829,\n",
       "           -0.04039364,  0.16326733],\n",
       "          [-0.07986713, -0.11045446, -0.0182618 , ...,  0.03425604,\n",
       "            0.04857165, -0.01289561],\n",
       "          ...,\n",
       "          [-0.01944935,  0.16133569, -0.13886565, ..., -0.04946548,\n",
       "           -0.09193281,  0.07258816],\n",
       "          [ 0.03535461, -0.01553783, -0.12240314, ..., -0.03787487,\n",
       "           -0.03542279, -0.10071688],\n",
       "          [-0.02973528, -0.0235173 , -0.14208344, ...,  0.01603398,\n",
       "           -0.10384908, -0.03715665]],\n",
       " \n",
       "         [[-0.00868847,  0.09747783,  0.05609084, ..., -0.06709271,\n",
       "           -0.0365287 ,  0.02167607],\n",
       "          [ 0.00520192,  0.04545872, -0.00385959, ..., -0.10382078,\n",
       "            0.09922986,  0.04590368],\n",
       "          [-0.12633488, -0.09125228,  0.01445932, ..., -0.03902439,\n",
       "            0.0587509 ,  0.13841613],\n",
       "          ...,\n",
       "          [-0.16209798,  0.13320892, -0.07741296, ...,  0.07428703,\n",
       "           -0.10164525,  0.02643708],\n",
       "          [ 0.02213026,  0.01649134,  0.0436659 , ...,  0.0232906 ,\n",
       "            0.04629252, -0.00076802],\n",
       "          [ 0.03799146,  0.0559881 , -0.02978235, ...,  0.00977015,\n",
       "            0.11580397, -0.03857177]],\n",
       " \n",
       "         [[ 0.06608044, -0.0416215 ,  0.06630466, ...,  0.055699  ,\n",
       "            0.00784587,  0.09676087],\n",
       "          [-0.00490601,  0.12060454, -0.05847608, ...,  0.03492145,\n",
       "            0.00243892, -0.02397208],\n",
       "          [ 0.02921079, -0.00870965, -0.02787584, ...,  0.00931264,\n",
       "           -0.10823767,  0.05034735],\n",
       "          ...,\n",
       "          [ 0.03446243, -0.1045508 ,  0.06442372, ...,  0.07789522,\n",
       "            0.01189602,  0.08175033],\n",
       "          [ 0.00793133, -0.10998193, -0.02340578, ...,  0.06498767,\n",
       "           -0.15805757, -0.12531461],\n",
       "          [ 0.06383936,  0.04361217,  0.01095787, ...,  0.00465115,\n",
       "           -0.04824472,  0.00653344]]],\n",
       " \n",
       " \n",
       "        [[[ 0.00245395,  0.03652324,  0.16486716, ...,  0.05380679,\n",
       "           -0.05727434, -0.11223233],\n",
       "          [ 0.05858332,  0.06456722,  0.00737718, ..., -0.05184856,\n",
       "            0.06343709, -0.00745704],\n",
       "          [-0.02949599, -0.04968589, -0.10517275, ..., -0.05019437,\n",
       "            0.05241147,  0.0156231 ],\n",
       "          ...,\n",
       "          [ 0.01725427,  0.15474428, -0.06188389, ...,  0.00435201,\n",
       "           -0.00123665,  0.10892878],\n",
       "          [ 0.01494043, -0.03885448,  0.01336966, ..., -0.0267803 ,\n",
       "            0.06688994, -0.14476316],\n",
       "          [ 0.14485203, -0.10829151, -0.07587951, ...,  0.06992213,\n",
       "           -0.10144979,  0.02104662]],\n",
       " \n",
       "         [[ 0.02576772,  0.01628014,  0.08864211, ..., -0.0828298 ,\n",
       "            0.03301449,  0.0850352 ],\n",
       "          [ 0.07903824,  0.04840345,  0.05991028, ...,  0.08987902,\n",
       "           -0.10944261,  0.03650283],\n",
       "          [ 0.03825856, -0.00832999, -0.04583719, ...,  0.06594151,\n",
       "           -0.07302982,  0.07432107],\n",
       "          ...,\n",
       "          [-0.04050666, -0.12797402,  0.07386046, ..., -0.10249872,\n",
       "            0.04911845,  0.06743121],\n",
       "          [ 0.07613551,  0.00332004, -0.07916892, ..., -0.00123999,\n",
       "            0.05332122, -0.13459298],\n",
       "          [-0.02192901,  0.04683131,  0.00082733, ..., -0.00446338,\n",
       "           -0.12011667,  0.03525013]],\n",
       " \n",
       "         [[-0.04359539, -0.06374618,  0.1202983 , ...,  0.00113101,\n",
       "            0.02773231,  0.0731922 ],\n",
       "          [ 0.02177648,  0.07440197, -0.06188597, ..., -0.09979589,\n",
       "           -0.00345127,  0.15094717],\n",
       "          [-0.00584744, -0.031125  ,  0.06415071, ...,  0.03767491,\n",
       "            0.05926217, -0.12075986],\n",
       "          ...,\n",
       "          [ 0.04383463, -0.05182926,  0.03458637, ..., -0.11864363,\n",
       "            0.01741709, -0.15909383],\n",
       "          [-0.12114664, -0.08047332,  0.09436321, ...,  0.02143348,\n",
       "            0.14162898, -0.03481112],\n",
       "          [ 0.01134355,  0.00687216,  0.02118197, ...,  0.0458432 ,\n",
       "            0.02889669, -0.01219227]]]], dtype=float32)>,\n",
       " <tf.Variable 'conv2d_218_1/bias:0' shape=(10,) dtype=float32, numpy=\n",
       " array([ 4.9985615e-03, -6.2674466e-03,  2.9960005e-03, -5.8403853e-03,\n",
       "        -7.8184283e-05, -2.7212226e-03,  1.2709799e-03,  8.3391936e-03,\n",
       "         1.3473940e-02,  1.5454404e-03], dtype=float32)>,\n",
       " <tf.Variable 'batch_normalization_248_1/gamma:0' shape=(10,) dtype=float32, numpy=\n",
       " array([0.99919415, 1.0033033 , 0.99802935, 1.000345  , 0.99773407,\n",
       "        1.0104444 , 1.0010378 , 0.9978654 , 1.0017155 , 0.9996311 ],\n",
       "       dtype=float32)>,\n",
       " <tf.Variable 'batch_normalization_248_1/beta:0' shape=(10,) dtype=float32, numpy=\n",
       " array([ 0.01331361, -0.00868953,  0.00726795,  0.00127022,  0.00731085,\n",
       "        -0.0044188 , -0.0090006 ,  0.0002409 , -0.00658916,  0.0044319 ],\n",
       "       dtype=float32)>,\n",
       " <tf.Variable 'conv2d_219_1/kernel:0' shape=(3, 3, 10, 10) dtype=float32, numpy=\n",
       " array([[[[ 2.83129606e-02, -1.62238032e-02,  1.03980504e-01,\n",
       "            1.16387211e-01, -1.06103413e-01,  5.61737977e-02,\n",
       "           -1.46659836e-01, -1.29340915e-02,  1.27901323e-02,\n",
       "            5.08181863e-02],\n",
       "          [ 9.80623253e-03, -9.37586352e-02, -1.98928304e-02,\n",
       "           -1.94695219e-01,  2.00361088e-01,  1.11831360e-01,\n",
       "            2.42758051e-01, -5.33776470e-02,  1.82724893e-01,\n",
       "           -2.01382041e-01],\n",
       "          [-8.73721913e-02,  4.33610752e-03,  4.48629409e-02,\n",
       "            5.48482053e-02,  8.65596533e-02,  2.04726696e-01,\n",
       "            2.42778962e-03,  7.55755603e-02,  1.12725645e-01,\n",
       "           -8.08122940e-03],\n",
       "          [ 1.87131241e-02,  2.89434455e-02, -9.44256485e-02,\n",
       "           -7.55071547e-03, -3.37521769e-02, -9.08817127e-02,\n",
       "           -1.92754716e-01, -3.49253858e-03, -7.13579729e-02,\n",
       "           -1.08991116e-01],\n",
       "          [ 1.05075799e-01, -2.05045298e-01, -1.68525428e-02,\n",
       "            2.29099188e-02, -1.71253443e-01, -1.90423220e-01,\n",
       "           -8.28932002e-02, -8.61670077e-02, -7.59391934e-02,\n",
       "            1.02766417e-01],\n",
       "          [ 1.50967371e-02, -1.48425549e-01, -1.78931765e-02,\n",
       "            1.73622563e-01,  7.03308210e-02, -1.47450194e-01,\n",
       "            3.78412120e-02, -1.73187628e-01,  1.86909065e-02,\n",
       "            1.24283642e-01],\n",
       "          [-8.66790563e-02,  2.08363533e-01, -2.15889933e-03,\n",
       "            1.31178916e-01,  5.47453202e-02, -1.24806426e-01,\n",
       "           -1.41787799e-02, -2.28378568e-02, -2.13128418e-01,\n",
       "            9.85392258e-02],\n",
       "          [ 2.00720485e-02, -5.98793551e-02, -1.57219812e-01,\n",
       "            1.22725964e-02, -1.11461073e-01, -1.41769096e-01,\n",
       "           -6.00360036e-02, -1.04730532e-01, -2.06204563e-01,\n",
       "           -9.85325128e-02],\n",
       "          [ 1.13914095e-01, -4.76640165e-02, -9.19190645e-02,\n",
       "            1.26577899e-01,  1.16801009e-01, -1.39470711e-01,\n",
       "            1.13734327e-01, -3.09173437e-03,  4.84448113e-02,\n",
       "           -2.80253701e-02],\n",
       "          [-7.73362964e-02,  4.04877998e-02,  8.21664631e-02,\n",
       "            1.34849161e-01, -5.47892861e-02, -1.04941633e-02,\n",
       "            1.29061297e-01,  1.57013908e-01,  1.52578220e-01,\n",
       "            7.48907030e-03]],\n",
       " \n",
       "         [[ 6.11932902e-03, -2.01341957e-01, -6.55429363e-02,\n",
       "            2.36354306e-01, -1.28261894e-01,  1.24785705e-02,\n",
       "           -6.12892136e-02,  1.89868510e-02, -1.59349188e-01,\n",
       "           -1.18715525e-01],\n",
       "          [ 4.51506525e-02, -4.20441888e-02, -7.08670635e-03,\n",
       "           -4.88252798e-03, -7.80906156e-02, -2.43846208e-01,\n",
       "           -2.31665194e-01, -1.65995300e-01,  2.43167460e-01,\n",
       "            2.33564109e-01],\n",
       "          [-1.00800224e-01,  8.09214711e-02,  9.82760489e-02,\n",
       "           -4.53637056e-02, -7.35944510e-02,  4.42068614e-02,\n",
       "            3.55523340e-02, -9.24341157e-02,  1.25541553e-01,\n",
       "            8.92291367e-02],\n",
       "          [ 1.51286796e-01, -3.83212566e-02,  4.72230464e-02,\n",
       "            3.73164192e-02,  1.42405853e-01,  6.37520179e-02,\n",
       "            8.13184679e-02, -3.31907496e-02, -2.84174234e-02,\n",
       "            6.62456304e-02],\n",
       "          [ 1.42802238e-01, -6.38124533e-03,  3.67526300e-02,\n",
       "            2.14254007e-01, -4.47148345e-02, -1.12954974e-01,\n",
       "           -2.18075924e-02, -7.93670267e-02, -3.08635482e-03,\n",
       "            8.13045874e-02],\n",
       "          [-1.21790268e-01,  2.42588706e-02,  1.54956177e-01,\n",
       "            2.93280147e-02, -9.38752573e-03,  8.03945288e-02,\n",
       "           -6.01228513e-02,  1.97463080e-01, -9.00273323e-02,\n",
       "           -2.03024633e-02],\n",
       "          [-5.71390167e-02,  1.48167282e-01, -2.24257652e-02,\n",
       "            1.80787429e-01, -1.91152856e-01,  7.82837421e-02,\n",
       "           -9.25571695e-02, -1.24554791e-01,  1.04387037e-01,\n",
       "            4.86497506e-02],\n",
       "          [ 1.21415062e-02, -1.17459297e-01,  1.29809305e-01,\n",
       "            4.45848657e-03,  2.13454694e-01, -2.84447726e-02,\n",
       "           -6.14425167e-02,  1.73679944e-02,  7.73714185e-02,\n",
       "           -1.43704489e-01],\n",
       "          [ 3.15138586e-02, -9.94348302e-02,  9.24090073e-02,\n",
       "            9.74579379e-02,  8.72250944e-02,  3.41705121e-02,\n",
       "            1.99354161e-02, -2.02842817e-01,  1.89016894e-01,\n",
       "           -1.42573178e-01],\n",
       "          [-1.25684381e-01, -1.00860551e-01,  1.66402146e-01,\n",
       "            1.84860025e-02,  5.38686551e-02, -7.63851032e-02,\n",
       "            2.20281482e-01, -7.81994238e-02, -2.29272675e-02,\n",
       "           -5.43190204e-02]],\n",
       " \n",
       "         [[-4.01767008e-02, -3.81764583e-02, -9.16618761e-03,\n",
       "            2.57409681e-02, -1.05128229e-01,  6.99722916e-02,\n",
       "           -2.14407697e-01,  1.73669323e-04,  1.60017461e-01,\n",
       "           -6.00143410e-02],\n",
       "          [ 1.34740472e-01, -9.66027975e-02, -2.19858973e-03,\n",
       "            1.11641444e-01,  1.19974781e-02,  1.43931573e-02,\n",
       "            8.19125250e-02,  2.36033320e-01,  7.85621479e-02,\n",
       "            3.52069363e-02],\n",
       "          [-1.61485925e-01,  2.34725237e-01,  2.11585134e-01,\n",
       "           -1.98489696e-01,  4.65236567e-02, -4.63504046e-02,\n",
       "           -7.07902387e-02, -6.04882911e-02,  8.41681063e-02,\n",
       "           -4.46977094e-02],\n",
       "          [ 1.33270755e-01, -1.77671947e-02,  3.69175598e-02,\n",
       "           -2.01188549e-01,  2.16184463e-02, -1.80943087e-01,\n",
       "           -4.58246395e-02,  8.48286897e-02, -1.54296026e-01,\n",
       "           -2.31164377e-02],\n",
       "          [ 1.79150149e-01, -6.76531792e-02, -1.56912953e-01,\n",
       "            4.04863358e-02,  4.27235328e-02, -6.50712028e-02,\n",
       "           -1.93436593e-01, -6.32430017e-02,  1.10042244e-01,\n",
       "           -1.39101848e-01],\n",
       "          [-3.99310365e-02,  2.42456421e-02,  4.05376889e-02,\n",
       "           -7.04867914e-02,  2.25961015e-01,  1.17363364e-01,\n",
       "           -5.90360612e-02, -6.30392730e-02, -6.30781129e-02,\n",
       "           -1.29879162e-01],\n",
       "          [ 8.54523629e-02, -2.15316445e-01,  6.61987662e-02,\n",
       "            1.52793795e-01, -1.21433884e-01,  7.31147528e-02,\n",
       "            3.67294401e-02,  2.71249209e-02, -8.48096684e-02,\n",
       "            9.98852998e-02],\n",
       "          [ 3.34720127e-02, -1.25386119e-02, -1.13391310e-01,\n",
       "           -4.81449850e-02, -1.01651937e-01, -5.83157837e-02,\n",
       "            6.98072910e-02, -2.51918249e-02,  1.01924531e-01,\n",
       "            1.03339896e-01],\n",
       "          [-1.21283501e-01, -1.44275418e-02, -1.04601830e-01,\n",
       "            1.18158452e-01, -1.30609661e-01, -8.51946846e-02,\n",
       "            1.06896400e-01,  7.87511989e-02, -1.38920724e-01,\n",
       "            5.03838398e-02],\n",
       "          [ 9.79290053e-06,  2.07273707e-01, -1.56843942e-02,\n",
       "            1.53424710e-01,  1.43007105e-02,  2.11922497e-01,\n",
       "           -8.35004970e-02, -3.15120220e-02, -8.58905464e-02,\n",
       "            9.53425840e-02]]],\n",
       " \n",
       " \n",
       "        [[[ 2.00899113e-02,  1.06507167e-01,  1.12569794e-01,\n",
       "            5.49545279e-03,  7.51778856e-02, -4.37212503e-03,\n",
       "            1.04209311e-01, -7.66417310e-02,  6.01512287e-03,\n",
       "           -1.04138918e-01],\n",
       "          [ 3.18270549e-03,  8.21854025e-02,  5.57398312e-02,\n",
       "            1.91434324e-01,  3.84605117e-02,  4.93508503e-02,\n",
       "            1.62451621e-02,  1.71981305e-01, -1.04712464e-01,\n",
       "            6.04983866e-02],\n",
       "          [-1.49672627e-01,  1.13039106e-01, -1.94703713e-01,\n",
       "           -2.60298867e-02,  1.79217413e-01,  5.45258038e-02,\n",
       "           -2.98164245e-02,  2.23772824e-02,  7.14413151e-02,\n",
       "           -2.02146575e-01],\n",
       "          [-5.72881997e-02,  8.11996907e-02, -3.08678262e-02,\n",
       "            2.38460228e-01,  3.87122408e-02, -9.00933594e-02,\n",
       "           -2.40432024e-02, -6.07253201e-02,  4.65678126e-02,\n",
       "            2.07203515e-02],\n",
       "          [ 1.56572223e-01, -2.69654170e-02, -8.72026458e-02,\n",
       "           -2.38066599e-01, -2.35741660e-01, -1.40856817e-01,\n",
       "            5.61638176e-02,  6.03501461e-02, -3.35139968e-02,\n",
       "           -6.20052814e-02],\n",
       "          [ 3.56668308e-02, -2.23486260e-01,  9.45441350e-02,\n",
       "            1.73284292e-01,  1.98778555e-01, -6.15402311e-02,\n",
       "           -5.03478609e-02,  2.19090562e-02, -9.25640985e-02,\n",
       "           -3.44366953e-02],\n",
       "          [ 9.05511230e-02, -7.97437280e-02, -1.51533827e-01,\n",
       "           -2.45877672e-02,  7.41868913e-02,  5.57781160e-02,\n",
       "           -1.38228804e-01,  1.38877630e-01,  6.86035911e-03,\n",
       "           -1.35780334e-01],\n",
       "          [ 3.76973152e-02,  4.19895798e-02,  4.39788215e-02,\n",
       "           -2.73699574e-02, -1.23707771e-01, -5.33239543e-02,\n",
       "            9.20776799e-02, -1.05991952e-01,  2.45116472e-01,\n",
       "            1.70258984e-01],\n",
       "          [-3.85737978e-02,  3.77977937e-02, -1.34565502e-01,\n",
       "            1.67865772e-02,  1.79697387e-02,  6.42088726e-02,\n",
       "            3.02807149e-02, -1.91765353e-01,  2.07179412e-01,\n",
       "            7.59577900e-02],\n",
       "          [-2.60505918e-02, -5.73843345e-03, -1.47325858e-01,\n",
       "           -3.20808589e-02,  1.05278902e-01,  3.87905575e-02,\n",
       "           -1.10959955e-01, -9.13213938e-02,  2.18660124e-02,\n",
       "           -3.05902194e-02]],\n",
       " \n",
       "         [[-1.23221174e-01, -6.07353577e-04, -1.35187283e-01,\n",
       "           -6.13944381e-02, -1.17465109e-01,  1.52044594e-01,\n",
       "            1.19614944e-01,  9.87146720e-02, -2.00308576e-01,\n",
       "           -2.29040042e-01],\n",
       "          [ 7.88311511e-02, -1.09998554e-01,  2.38945156e-01,\n",
       "           -1.34254187e-01, -7.02443253e-03, -2.36085467e-02,\n",
       "           -1.93788297e-02,  6.25462681e-02, -1.07806124e-01,\n",
       "            2.26303991e-02],\n",
       "          [ 6.59231991e-02,  6.75223544e-02,  5.03149517e-02,\n",
       "            4.55942526e-02, -3.14266160e-02, -5.69219217e-02,\n",
       "           -5.68435080e-02,  9.75023434e-02, -2.26747200e-01,\n",
       "           -2.86582746e-02],\n",
       "          [ 1.80034190e-02, -1.98804736e-01, -8.07423797e-03,\n",
       "            2.42109708e-02,  7.15936301e-03,  8.01295117e-02,\n",
       "           -1.32244006e-01,  1.37002021e-01, -5.79625405e-02,\n",
       "           -1.88237011e-01],\n",
       "          [-5.71402311e-02, -4.22745757e-02,  7.14820847e-02,\n",
       "            1.14351451e-01, -5.98136932e-02,  5.01520969e-02,\n",
       "            3.45501900e-02, -6.32073283e-02, -1.18889302e-01,\n",
       "           -2.23582551e-01],\n",
       "          [-4.12171558e-02, -1.63696483e-01,  5.49158789e-02,\n",
       "            9.33221802e-02,  7.14174509e-02, -6.26139119e-02,\n",
       "            4.56676520e-02,  1.05864212e-01,  1.53272048e-01,\n",
       "            8.63303095e-02],\n",
       "          [ 7.59049505e-02,  3.00823450e-02, -3.52703817e-02,\n",
       "            1.51237443e-01,  9.42618772e-02, -5.78911714e-02,\n",
       "           -1.88533396e-01,  1.85683534e-01, -6.58352524e-02,\n",
       "            1.11291893e-02],\n",
       "          [-1.81884207e-02,  1.38931096e-01, -6.68178685e-03,\n",
       "           -9.54097435e-02, -1.86438905e-03, -1.56380624e-01,\n",
       "            6.25710264e-02,  8.60823989e-02,  1.05746709e-01,\n",
       "           -1.59648314e-01],\n",
       "          [ 4.11932468e-02, -3.02847736e-02,  4.39207405e-02,\n",
       "            2.05051631e-01,  1.54822562e-02, -1.42320961e-01,\n",
       "           -4.87674773e-02, -1.02113135e-01,  2.12309901e-02,\n",
       "           -1.37282208e-01],\n",
       "          [-4.61015292e-02,  4.73923571e-02, -1.28647164e-01,\n",
       "            3.61755155e-02,  1.92175701e-01, -1.08970247e-01,\n",
       "           -1.67172533e-02, -1.03741273e-01, -9.14234817e-02,\n",
       "           -4.69418876e-02]],\n",
       " \n",
       "         [[ 1.97339877e-01, -7.92988986e-02,  3.61178927e-02,\n",
       "            8.75269324e-02, -1.27176102e-02,  1.71080425e-01,\n",
       "            2.23619431e-01, -1.64433405e-01, -9.96130854e-02,\n",
       "            3.44512835e-02],\n",
       "          [ 6.31227195e-02,  1.86223760e-01,  6.69227913e-02,\n",
       "            8.71588066e-02,  1.62240833e-01,  9.18638241e-03,\n",
       "            3.68096232e-02,  1.32041704e-02,  6.75824413e-05,\n",
       "           -4.22172621e-02],\n",
       "          [-1.10416561e-01,  3.98903862e-02, -7.48601779e-02,\n",
       "           -1.74434975e-01, -1.80871427e-01, -1.01983920e-01,\n",
       "           -9.76700410e-02,  1.06864423e-01,  3.02613038e-03,\n",
       "           -2.24782843e-02],\n",
       "          [ 6.83157220e-02, -1.52197881e-02,  9.79706496e-02,\n",
       "            7.68000484e-02, -2.31230766e-01, -9.90973087e-04,\n",
       "           -2.65486985e-02,  1.15314730e-01,  3.98474634e-02,\n",
       "            8.69876966e-02],\n",
       "          [ 1.40313685e-01,  1.46533594e-01, -6.51161075e-02,\n",
       "           -1.79117575e-01, -2.81689912e-02,  2.70518716e-02,\n",
       "           -1.32394224e-01,  2.51960419e-02, -7.56366625e-02,\n",
       "           -1.09888412e-01],\n",
       "          [-4.34448533e-02, -1.00457175e-02,  3.21859419e-02,\n",
       "           -6.23803623e-02, -1.33000359e-01, -1.15383923e-01,\n",
       "           -2.52384469e-02, -1.09912001e-01, -8.42400715e-02,\n",
       "           -1.65273443e-01],\n",
       "          [-6.06370643e-02, -3.23680602e-02,  4.55051772e-02,\n",
       "           -3.92534509e-02,  1.06640838e-01, -1.74291939e-01,\n",
       "           -6.02066368e-02, -2.57290117e-02, -4.42988090e-02,\n",
       "            6.82166964e-02],\n",
       "          [-7.94913396e-02,  2.23139040e-02, -9.35547501e-02,\n",
       "            5.75350299e-02,  5.55182993e-02,  2.99230199e-02,\n",
       "            2.28930354e-01, -4.30358425e-02, -1.76778406e-01,\n",
       "           -3.75630744e-02],\n",
       "          [-1.52267501e-01, -6.42918870e-02,  2.45847646e-02,\n",
       "            1.20171353e-01, -1.07269645e-01,  1.77991539e-01,\n",
       "           -1.71826363e-01,  1.47688374e-01,  5.74155292e-03,\n",
       "            6.37402758e-02],\n",
       "          [-1.28713310e-01,  4.82489914e-02, -4.67829108e-02,\n",
       "            1.05251744e-01, -2.60181464e-02,  1.74795583e-01,\n",
       "           -2.31962115e-01, -4.60058488e-02, -5.17882071e-02,\n",
       "           -7.63179883e-02]]],\n",
       " \n",
       " \n",
       "        [[[-1.44817248e-01,  2.26501569e-01, -6.66333511e-02,\n",
       "            2.10161716e-01, -3.24297585e-02,  1.21231750e-01,\n",
       "            1.04640760e-02,  1.14110850e-01, -2.48254873e-02,\n",
       "           -8.51678401e-02],\n",
       "          [ 6.60309047e-02, -5.13744131e-02, -4.83217426e-02,\n",
       "            2.13335156e-01, -5.97906373e-02, -1.41597008e-02,\n",
       "            2.04290561e-02,  1.16385892e-01, -1.05206758e-01,\n",
       "            7.37921372e-02],\n",
       "          [-1.19293474e-01, -3.85328941e-03, -9.86413732e-02,\n",
       "           -1.63997129e-01, -3.44718359e-02,  9.56578255e-02,\n",
       "           -1.20063625e-01,  8.15209597e-02, -2.21521333e-01,\n",
       "            1.03575267e-01],\n",
       "          [ 1.95028171e-01, -1.26510382e-01, -7.74103701e-02,\n",
       "           -1.69102192e-01,  7.19323978e-02, -4.03051041e-02,\n",
       "            1.62848845e-01,  3.21326181e-02,  8.33434463e-02,\n",
       "           -1.62626430e-01],\n",
       "          [-8.75652730e-02, -1.49491966e-01,  4.28811368e-03,\n",
       "            3.23839635e-02,  1.37482926e-01,  1.60996914e-01,\n",
       "           -8.75477493e-02,  7.81943724e-02,  6.33267462e-02,\n",
       "           -1.30919129e-01],\n",
       "          [-1.89303473e-01, -1.87263578e-01, -2.62267813e-02,\n",
       "            1.76763430e-01, -1.15484316e-02,  8.36394876e-02,\n",
       "            5.89777790e-02, -1.11105904e-01,  5.87093942e-02,\n",
       "            5.47035299e-02],\n",
       "          [-2.08298549e-01,  2.30278913e-03,  1.16173305e-01,\n",
       "           -5.41599058e-02, -8.23790729e-02, -1.35533875e-02,\n",
       "            1.33200869e-01, -6.03954755e-02,  1.96013507e-02,\n",
       "            3.12862471e-02],\n",
       "          [-1.85225368e-01, -8.42582434e-02, -1.15580134e-01,\n",
       "           -9.96347889e-02, -3.65294293e-02,  1.04543768e-01,\n",
       "           -8.10248107e-02,  9.70584601e-02,  6.56780452e-02,\n",
       "            1.60754502e-01],\n",
       "          [ 3.11631411e-02,  2.21990682e-02,  7.80974329e-02,\n",
       "           -1.48668915e-01,  1.43399224e-01, -4.82461564e-02,\n",
       "            3.16404887e-02,  9.08872262e-02, -4.38953340e-02,\n",
       "            4.53006756e-03],\n",
       "          [-1.97347045e-01,  5.47768846e-02, -7.89908171e-02,\n",
       "           -1.58007741e-01,  1.25662357e-01,  1.09114312e-01,\n",
       "           -1.11749873e-01,  1.00479506e-01,  8.23318139e-02,\n",
       "           -1.01387180e-01]],\n",
       " \n",
       "         [[ 2.33938098e-02, -9.43448618e-02, -6.36696368e-02,\n",
       "            1.74629074e-02, -3.73220965e-02, -9.87456739e-02,\n",
       "            6.62156707e-03,  8.11567530e-02,  1.52127802e-01,\n",
       "            1.19627461e-01],\n",
       "          [-1.69830605e-01, -9.37439203e-02,  1.05857998e-01,\n",
       "           -4.16716896e-02,  6.91281334e-02, -1.68573167e-02,\n",
       "            1.69957429e-01,  7.32689165e-03,  5.44025302e-02,\n",
       "            5.06813191e-02],\n",
       "          [-2.28049960e-02, -7.82734454e-02,  3.35688926e-02,\n",
       "            3.65514010e-02, -3.18110362e-02,  1.49431005e-01,\n",
       "            8.57109651e-02,  2.67472747e-03,  1.60440847e-01,\n",
       "            7.99737051e-02],\n",
       "          [ 4.91654128e-02,  3.52480523e-02, -3.43381353e-02,\n",
       "            2.15078983e-03, -1.40986711e-01, -1.70772851e-01,\n",
       "           -2.80163810e-02,  7.13505433e-05,  5.86924492e-04,\n",
       "           -1.14723993e-02],\n",
       "          [ 8.04784521e-02,  3.89645323e-02, -3.97195108e-03,\n",
       "           -4.08654623e-02, -3.92556228e-02,  1.07515231e-01,\n",
       "           -4.24126647e-02, -1.86153591e-01, -7.11496472e-02,\n",
       "            2.82673053e-02],\n",
       "          [-4.15638052e-02,  1.19508067e-02,  1.61667794e-01,\n",
       "           -5.98270074e-02,  1.59446061e-01, -1.54596463e-01,\n",
       "           -7.53921568e-02,  9.29858722e-03, -1.06049754e-01,\n",
       "            8.28401148e-02],\n",
       "          [ 1.21008217e-01, -4.27485742e-02,  1.39313042e-01,\n",
       "           -1.36166960e-01, -3.57744209e-02, -2.24507570e-01,\n",
       "           -1.47161499e-01, -7.15941638e-02, -1.14483694e-02,\n",
       "           -7.64013873e-03],\n",
       "          [ 4.46193703e-02,  6.67277798e-02, -3.73464776e-03,\n",
       "           -8.73474255e-02,  1.32458001e-01,  1.00006141e-01,\n",
       "           -1.37554884e-01, -7.53911659e-02, -5.00236899e-02,\n",
       "            2.74028201e-02],\n",
       "          [-4.06835079e-02,  7.67779797e-02, -3.67930345e-03,\n",
       "            1.60293102e-01, -9.20196995e-02, -1.03736660e-02,\n",
       "            7.64677152e-02, -3.93676311e-02, -1.77490652e-01,\n",
       "           -4.54227999e-02],\n",
       "          [ 1.16835363e-01, -2.86053400e-02, -1.81422345e-02,\n",
       "            3.87039520e-02, -2.17868648e-02,  1.51334047e-01,\n",
       "           -1.48274109e-01,  1.91059664e-01, -2.39984598e-02,\n",
       "            5.10003045e-02]],\n",
       " \n",
       "         [[ 2.61268318e-02, -2.80166529e-02,  1.15276687e-01,\n",
       "           -2.17348672e-02, -1.20558091e-01, -9.46394801e-02,\n",
       "            1.15609176e-01, -2.09887475e-01,  1.63544729e-01,\n",
       "           -1.23697519e-01],\n",
       "          [ 6.32945597e-02,  9.43811436e-04, -9.15335044e-02,\n",
       "            1.71730921e-01, -3.68504822e-02, -1.29017040e-01,\n",
       "            1.35943172e-02, -5.16752079e-02,  1.34774551e-01,\n",
       "            2.17116047e-02],\n",
       "          [ 4.05768231e-02, -7.65066892e-02, -9.12971124e-02,\n",
       "           -8.82846713e-02, -2.78011169e-02,  4.94951792e-02,\n",
       "           -1.06464466e-02, -5.60789481e-02,  3.15501541e-02,\n",
       "           -1.46239996e-01],\n",
       "          [-7.02964887e-02, -2.11720690e-01,  5.88321835e-02,\n",
       "            6.51261583e-02, -1.23016223e-01,  6.94340691e-02,\n",
       "           -4.82710600e-02,  8.33265558e-02, -1.01150116e-02,\n",
       "           -1.12979785e-01],\n",
       "          [ 9.89388525e-02, -1.67651594e-01, -1.74290240e-01,\n",
       "           -3.42526436e-02,  7.54584670e-02, -9.62401628e-02,\n",
       "           -2.77377870e-02, -2.37906516e-01, -9.84840393e-02,\n",
       "            1.66777018e-02],\n",
       "          [-7.32936561e-02, -2.10263096e-02, -1.26178876e-01,\n",
       "            1.32635962e-02,  2.07519475e-02,  1.24720454e-01,\n",
       "            1.93842184e-02,  2.52824835e-02, -1.62117369e-02,\n",
       "            6.89930022e-02],\n",
       "          [-9.29243788e-02,  1.63770676e-01,  1.31339535e-01,\n",
       "           -6.65975874e-03,  1.06957085e-01, -6.68346807e-02,\n",
       "            7.12891072e-02, -1.65606692e-01,  1.10420033e-01,\n",
       "           -2.30020117e-02],\n",
       "          [-2.39494704e-02, -2.72205845e-02, -9.64355692e-02,\n",
       "           -7.17165768e-02, -1.00173734e-01, -5.15200421e-02,\n",
       "           -3.80541496e-02, -6.95788935e-02,  4.53404896e-02,\n",
       "            8.21581408e-02],\n",
       "          [-1.19920053e-01, -7.69955590e-02, -1.02899656e-01,\n",
       "            9.14903358e-02, -5.14953025e-02,  8.42553675e-02,\n",
       "           -3.65705676e-02, -2.82598846e-03,  1.30101413e-01,\n",
       "            4.11561430e-02],\n",
       "          [ 4.03209701e-02, -1.04061730e-01, -2.15942726e-01,\n",
       "            9.21981782e-03,  2.14306235e-01,  7.95683637e-02,\n",
       "            8.54182988e-03, -1.10809542e-02, -1.15611874e-01,\n",
       "            9.50715840e-02]]]], dtype=float32)>,\n",
       " <tf.Variable 'conv2d_219_1/bias:0' shape=(10,) dtype=float32, numpy=\n",
       " array([-0.00359031,  0.00930855,  0.0001581 , -0.01199857,  0.01158896,\n",
       "         0.00556783, -0.00089094,  0.00080338, -0.00311097, -0.01332143],\n",
       "       dtype=float32)>,\n",
       " <tf.Variable 'batch_normalization_249_1/gamma:0' shape=(10,) dtype=float32, numpy=\n",
       " array([0.9998816 , 1.0022944 , 0.99472344, 1.0043478 , 0.9926456 ,\n",
       "        1.0033965 , 1.0001464 , 1.0021745 , 0.9974514 , 1.0024154 ],\n",
       "       dtype=float32)>,\n",
       " <tf.Variable 'batch_normalization_249_1/beta:0' shape=(10,) dtype=float32, numpy=\n",
       " array([ 0.00724354, -0.00172935, -0.00293744,  0.00704751, -0.00181972,\n",
       "         0.00639493, -0.01191419,  0.01088994,  0.00031498,  0.00212661],\n",
       "       dtype=float32)>,\n",
       " <tf.Variable 'conv2d_220_1/kernel:0' shape=(3, 3, 10, 10) dtype=float32, numpy=\n",
       " array([[[[-0.04071948, -0.10485952,  0.01714608, -0.00553399,\n",
       "           -0.04601976, -0.15614097,  0.01015718, -0.01447301,\n",
       "           -0.08060953, -0.01971467],\n",
       "          [ 0.10052959,  0.12239464, -0.04159877, -0.01063146,\n",
       "            0.01214133, -0.09487503,  0.1215203 ,  0.15190482,\n",
       "            0.02280762,  0.15189311],\n",
       "          [ 0.06481729,  0.02730891, -0.10479853, -0.22123197,\n",
       "            0.01312567,  0.02179761,  0.02754615,  0.07520343,\n",
       "           -0.15668066,  0.05704654],\n",
       "          [ 0.06913419,  0.16359174, -0.12802126,  0.06997892,\n",
       "           -0.15446733, -0.14898828,  0.02798152, -0.07788127,\n",
       "           -0.0007164 , -0.2034076 ],\n",
       "          [ 0.0710422 ,  0.01030962, -0.22790797,  0.06738345,\n",
       "            0.00239257, -0.17682263,  0.06270509, -0.11894358,\n",
       "           -0.11957943,  0.11605055],\n",
       "          [ 0.19660985,  0.03488865, -0.02263503, -0.18835084,\n",
       "            0.0132584 ,  0.11811081, -0.06893843, -0.10773569,\n",
       "            0.10691939, -0.00357162],\n",
       "          [-0.16067961,  0.09657294, -0.04251616,  0.09307007,\n",
       "            0.1380811 , -0.07799418,  0.13037837, -0.07571174,\n",
       "           -0.11876293,  0.15251985],\n",
       "          [-0.1596559 , -0.10186213, -0.05905106, -0.00574584,\n",
       "            0.04571532,  0.11937636, -0.16703257,  0.15985481,\n",
       "            0.09312058,  0.01634785],\n",
       "          [-0.1570705 , -0.05822331,  0.01686043, -0.15325579,\n",
       "            0.10246415, -0.15451688,  0.17035854, -0.12048357,\n",
       "            0.03490257,  0.0536119 ],\n",
       "          [-0.11687016, -0.07540125,  0.02444064, -0.17479359,\n",
       "           -0.14535907, -0.02633318,  0.22531435,  0.2060504 ,\n",
       "           -0.01372607, -0.19613992]],\n",
       " \n",
       "         [[ 0.03584301, -0.17299747, -0.04163562,  0.09959974,\n",
       "            0.00024959, -0.19808501, -0.01001026, -0.03504771,\n",
       "            0.09851305,  0.08893088],\n",
       "          [-0.04703859, -0.01428755, -0.12625444,  0.03017144,\n",
       "            0.03471938, -0.01430298,  0.13372639, -0.06289706,\n",
       "           -0.11124481,  0.14173086],\n",
       "          [-0.05502491, -0.00274278, -0.05702636,  0.21902218,\n",
       "           -0.19908437,  0.07024401, -0.07636495, -0.05490524,\n",
       "            0.12540717,  0.06467061],\n",
       "          [-0.15175089,  0.02079302,  0.09135033, -0.20282026,\n",
       "           -0.00815917,  0.02331526, -0.03340711, -0.2332676 ,\n",
       "           -0.17544949,  0.05481697],\n",
       "          [ 0.06899571,  0.02624624,  0.05847004, -0.0886029 ,\n",
       "            0.03417983, -0.20107238, -0.10540645, -0.08783702,\n",
       "            0.0434263 ,  0.13393244],\n",
       "          [ 0.1185165 , -0.05626282, -0.15149537,  0.09938962,\n",
       "           -0.0516636 , -0.02684043, -0.16565251, -0.03567411,\n",
       "            0.06572168, -0.01274624],\n",
       "          [ 0.06294444,  0.11881127,  0.08658954, -0.22784126,\n",
       "            0.05728424,  0.07628844,  0.06929845,  0.13603081,\n",
       "           -0.06107324, -0.04483739],\n",
       "          [ 0.08387396,  0.03528679, -0.03536478,  0.1490879 ,\n",
       "            0.05306144, -0.00647309, -0.16366966, -0.12046362,\n",
       "           -0.21901882, -0.09095283],\n",
       "          [-0.15251149, -0.15765509, -0.14195757,  0.06762421,\n",
       "           -0.15752289, -0.01296381, -0.02569367, -0.07656731,\n",
       "            0.16692142, -0.07053657],\n",
       "          [-0.03376301,  0.01094438,  0.19577473,  0.01565439,\n",
       "            0.10653786, -0.09451327, -0.03955395,  0.0212913 ,\n",
       "           -0.07982403,  0.1518836 ]],\n",
       " \n",
       "         [[ 0.06127471,  0.12454933, -0.09871164, -0.02527269,\n",
       "           -0.24028088, -0.05731723, -0.09198619,  0.19669445,\n",
       "            0.09193442,  0.04997336],\n",
       "          [-0.15162796,  0.07103198, -0.2218252 , -0.06372958,\n",
       "            0.06751615, -0.11743066, -0.02287547, -0.02629605,\n",
       "            0.09435048, -0.06150759],\n",
       "          [ 0.03620581,  0.00529569, -0.1737143 ,  0.18374074,\n",
       "            0.14244598,  0.04220079, -0.02583253,  0.12407978,\n",
       "           -0.06102292, -0.24223   ],\n",
       "          [ 0.1340856 ,  0.10628098,  0.10994002,  0.08776351,\n",
       "           -0.08998794,  0.03906384,  0.1969851 ,  0.07152422,\n",
       "            0.10800077,  0.01129551],\n",
       "          [-0.10427738, -0.14131749, -0.09314509,  0.15294255,\n",
       "            0.00875395, -0.19724834,  0.07097492, -0.19248793,\n",
       "            0.11776944, -0.18697022],\n",
       "          [-0.21740752,  0.17058925,  0.02181068, -0.00537843,\n",
       "           -0.11766393,  0.02217668, -0.00633296,  0.18343277,\n",
       "           -0.02777596, -0.14459547],\n",
       "          [ 0.03907492, -0.03859345, -0.02074963,  0.16998057,\n",
       "           -0.05929814, -0.12201289,  0.10748371, -0.04498356,\n",
       "            0.02333236,  0.21452245],\n",
       "          [-0.13871734,  0.0768059 , -0.08137463,  0.03905621,\n",
       "           -0.02389872, -0.08273219,  0.178774  ,  0.01808699,\n",
       "            0.09966321, -0.02609221],\n",
       "          [ 0.12111259,  0.17795041, -0.17967065, -0.08150757,\n",
       "            0.09129849,  0.02136693,  0.08330797, -0.12106644,\n",
       "           -0.01447172,  0.04401728],\n",
       "          [-0.05973915, -0.13731964,  0.00412354, -0.20115303,\n",
       "            0.11221953,  0.12197675,  0.11637232, -0.01218905,\n",
       "            0.10711353,  0.04065799]]],\n",
       " \n",
       " \n",
       "        [[[ 0.11289847, -0.15464102, -0.00503754, -0.20162842,\n",
       "            0.05786053,  0.00533249, -0.18682201,  0.07544801,\n",
       "            0.02992214,  0.1668975 ],\n",
       "          [-0.10690458, -0.10048852, -0.02218767,  0.11023614,\n",
       "            0.09695517, -0.14174752,  0.06219684, -0.1919142 ,\n",
       "           -0.11960484,  0.01888142],\n",
       "          [-0.09511629, -0.18400536, -0.01591347, -0.06207559,\n",
       "           -0.10421262, -0.00059714,  0.16427276, -0.04733974,\n",
       "            0.06939485,  0.06274066],\n",
       "          [ 0.17805433,  0.06046224, -0.19576204, -0.22586821,\n",
       "            0.02761406,  0.01868501,  0.01100948,  0.11048465,\n",
       "            0.03097244, -0.1579483 ],\n",
       "          [-0.01618839, -0.15260719, -0.0739207 ,  0.15400434,\n",
       "           -0.03664052, -0.04921087,  0.09785999, -0.06049857,\n",
       "            0.10606468, -0.02535314],\n",
       "          [ 0.00225398, -0.01712299, -0.09167674,  0.06150055,\n",
       "            0.04941973, -0.15490845,  0.0133996 , -0.20383632,\n",
       "           -0.06137588, -0.01955652],\n",
       "          [-0.10831454,  0.14457142,  0.08378258, -0.07117862,\n",
       "            0.08462255, -0.06013887,  0.01102784, -0.09678876,\n",
       "           -0.04734738, -0.11299018],\n",
       "          [ 0.12706931, -0.16220117,  0.01146989,  0.16133499,\n",
       "           -0.11934187, -0.09769115, -0.01804904, -0.0156853 ,\n",
       "           -0.02324687, -0.07757235],\n",
       "          [ 0.02574534, -0.14199553, -0.03150017, -0.03116885,\n",
       "            0.07719991,  0.07375062,  0.10555299, -0.05428745,\n",
       "            0.10313854,  0.07112016],\n",
       "          [-0.11598127, -0.0091943 , -0.08493185, -0.09912208,\n",
       "            0.13042375, -0.04010193, -0.00853848,  0.07666725,\n",
       "           -0.06170472, -0.04473584]],\n",
       " \n",
       "         [[ 0.18541652, -0.08166829,  0.05931174,  0.0687143 ,\n",
       "            0.0206749 , -0.00349296,  0.02489174, -0.2250181 ,\n",
       "            0.02561674,  0.01654119],\n",
       "          [ 0.02235785,  0.15290965,  0.16797732, -0.10170828,\n",
       "            0.06666873,  0.06588035, -0.02047775, -0.04356691,\n",
       "            0.09203377, -0.15294786],\n",
       "          [ 0.12796025,  0.00598191,  0.09989008, -0.0790161 ,\n",
       "            0.12956847, -0.04990802, -0.15634565, -0.13368894,\n",
       "            0.1074358 ,  0.00722903],\n",
       "          [-0.07765963,  0.09684461,  0.10540554, -0.17978643,\n",
       "            0.01679352,  0.02194397, -0.19048038,  0.05023158,\n",
       "           -0.10282219, -0.0738554 ],\n",
       "          [ 0.04523521,  0.12468612, -0.00278854, -0.06440792,\n",
       "            0.01214961,  0.03171487, -0.15661153,  0.10195048,\n",
       "           -0.05681071,  0.09453958],\n",
       "          [-0.24545331,  0.03853856, -0.15444192,  0.0375212 ,\n",
       "           -0.17065908,  0.02467153,  0.11351469,  0.05238007,\n",
       "            0.08989196,  0.05569572],\n",
       "          [ 0.02132878,  0.09569966, -0.01469224,  0.05609416,\n",
       "            0.09769177,  0.09231827,  0.07557043,  0.01838791,\n",
       "           -0.12677646,  0.01195089],\n",
       "          [ 0.06954648, -0.22083355, -0.01797156, -0.14597464,\n",
       "            0.06826974, -0.081863  ,  0.11297221, -0.0428413 ,\n",
       "            0.10324905,  0.1393388 ],\n",
       "          [ 0.06487879, -0.03823928,  0.11502053, -0.01297036,\n",
       "           -0.11708786,  0.05787999, -0.1587728 , -0.16029215,\n",
       "            0.17177664, -0.06030221],\n",
       "          [-0.08974628, -0.07788979, -0.04751116, -0.11183922,\n",
       "           -0.04852448,  0.2261977 ,  0.03013522,  0.16078137,\n",
       "           -0.03389005,  0.02587434]],\n",
       " \n",
       "         [[-0.22064258, -0.1908499 ,  0.1619993 ,  0.00171418,\n",
       "            0.00499647,  0.0356807 , -0.03211457,  0.0227444 ,\n",
       "            0.01557114,  0.07679296],\n",
       "          [ 0.16405907, -0.10235137,  0.19470684,  0.06001658,\n",
       "           -0.01353316, -0.05786043, -0.00766128,  0.0117112 ,\n",
       "            0.09051093, -0.01511177],\n",
       "          [ 0.08177408, -0.10976675, -0.00812047, -0.00562421,\n",
       "           -0.00303697, -0.15946068,  0.08626227, -0.00499012,\n",
       "           -0.1173096 , -0.03632371],\n",
       "          [-0.14015436, -0.06482852, -0.15247655, -0.07005783,\n",
       "            0.10436854, -0.14747691,  0.1761564 , -0.17428459,\n",
       "            0.05565731,  0.0011449 ],\n",
       "          [-0.0943415 ,  0.0610148 , -0.02266471, -0.06868771,\n",
       "           -0.00603163,  0.09057862, -0.18225738,  0.0421923 ,\n",
       "           -0.15266865,  0.01043644],\n",
       "          [ 0.14794429,  0.07333588,  0.03740503, -0.02257978,\n",
       "            0.06609224,  0.19682123, -0.1163616 , -0.08159977,\n",
       "            0.0072358 ,  0.06852735],\n",
       "          [-0.0651397 ,  0.01005876, -0.04705166, -0.12529229,\n",
       "           -0.09129009, -0.23530817,  0.04610407, -0.02511613,\n",
       "            0.23942219, -0.14881833],\n",
       "          [ 0.07650554, -0.03960873,  0.05548701, -0.06427865,\n",
       "           -0.01404515, -0.00616937, -0.13400944,  0.05420984,\n",
       "           -0.15849394,  0.01798212],\n",
       "          [-0.09546657, -0.00642325,  0.01698399,  0.01165512,\n",
       "           -0.1624189 , -0.06639475,  0.17512451, -0.13609318,\n",
       "           -0.10183471, -0.13549955],\n",
       "          [ 0.07150143, -0.04271057, -0.05001621, -0.15579191,\n",
       "           -0.02295755, -0.08440073,  0.16827273, -0.05433117,\n",
       "           -0.01002587, -0.19261776]]],\n",
       " \n",
       " \n",
       "        [[[-0.20251998,  0.06005263, -0.04277775, -0.17661318,\n",
       "            0.06117032,  0.04669086,  0.03107794,  0.1361526 ,\n",
       "            0.01230303, -0.01073117],\n",
       "          [-0.07932968,  0.03389952,  0.03450581,  0.11374214,\n",
       "           -0.05691527,  0.13838384,  0.04805163,  0.11313576,\n",
       "            0.00486441, -0.21703206],\n",
       "          [ 0.07742762,  0.10740251, -0.06258997,  0.01293524,\n",
       "           -0.10563585, -0.12704892,  0.12839483, -0.04431823,\n",
       "            0.08342117, -0.02499664],\n",
       "          [-0.09744524, -0.11671849, -0.081049  ,  0.0633539 ,\n",
       "           -0.11850443, -0.04462986,  0.07294473,  0.12783615,\n",
       "           -0.10020887,  0.08576722],\n",
       "          [-0.16309842, -0.04522684, -0.06323169, -0.02649159,\n",
       "            0.05199269, -0.01176355, -0.18834168, -0.04120415,\n",
       "           -0.0309468 , -0.02431837],\n",
       "          [-0.17008474,  0.03527879,  0.2277572 ,  0.0088373 ,\n",
       "            0.0708084 ,  0.05941654, -0.10174911, -0.06945047,\n",
       "           -0.068343  ,  0.06255998],\n",
       "          [ 0.0323961 ,  0.10770498,  0.05446687,  0.01792963,\n",
       "            0.10994683,  0.08290577, -0.03532308, -0.06596331,\n",
       "            0.17329252,  0.03543949],\n",
       "          [-0.17184028,  0.15156507, -0.21384974,  0.15247706,\n",
       "            0.0694531 ,  0.04920005, -0.17181833,  0.18541402,\n",
       "            0.05097588, -0.17945701],\n",
       "          [ 0.20021752, -0.1243031 ,  0.02150231,  0.05106107,\n",
       "            0.08970558,  0.06849953, -0.18999201,  0.12235851,\n",
       "            0.12034073, -0.19433671],\n",
       "          [-0.04437713,  0.03170646,  0.07078171, -0.03053322,\n",
       "            0.10315339, -0.22447115, -0.13915972,  0.15811579,\n",
       "           -0.00648672, -0.01367181]],\n",
       " \n",
       "         [[ 0.16140978,  0.11316536, -0.20823586,  0.01832259,\n",
       "           -0.05768735,  0.09112018, -0.03280644,  0.05539392,\n",
       "           -0.17670569, -0.13981041],\n",
       "          [ 0.03498236,  0.11269991,  0.06907249,  0.00681815,\n",
       "           -0.03897691, -0.14707784,  0.01052724, -0.1433602 ,\n",
       "           -0.10020322, -0.0380563 ],\n",
       "          [ 0.02061832, -0.07458472,  0.03065838,  0.03084412,\n",
       "           -0.06744804,  0.1126416 , -0.08785947,  0.08583914,\n",
       "            0.04491587,  0.02384045],\n",
       "          [-0.07670262, -0.10945703, -0.03515705, -0.0132307 ,\n",
       "           -0.10082775,  0.19376473, -0.08196668,  0.11700287,\n",
       "           -0.15478927,  0.06883012],\n",
       "          [ 0.19329807, -0.07721043, -0.05044011, -0.07425848,\n",
       "            0.01303517,  0.00928462,  0.16434738,  0.02432305,\n",
       "            0.19915117, -0.1370751 ],\n",
       "          [ 0.11291897,  0.03270883, -0.0741464 , -0.22647488,\n",
       "            0.10608979,  0.18271068,  0.07438621, -0.01800349,\n",
       "           -0.16483486,  0.06453727],\n",
       "          [-0.06176921, -0.08688622,  0.20038864,  0.12067015,\n",
       "            0.10269438, -0.2071071 ,  0.0278663 ,  0.20764521,\n",
       "            0.07976376,  0.06854606],\n",
       "          [ 0.0587667 ,  0.05752633, -0.06581632, -0.23164083,\n",
       "           -0.07636323, -0.06333343, -0.10733242, -0.11426241,\n",
       "           -0.16433886, -0.04515222],\n",
       "          [ 0.01505341,  0.06945779,  0.06915069, -0.00130332,\n",
       "            0.17749286,  0.02595201, -0.16322969,  0.07412928,\n",
       "           -0.0337011 ,  0.19729573],\n",
       "          [-0.05369836, -0.10434227, -0.00446344, -0.05092714,\n",
       "           -0.11501723, -0.09971204, -0.1293128 ,  0.0031383 ,\n",
       "           -0.11612388,  0.13635582]],\n",
       " \n",
       "         [[ 0.02212952,  0.14714009, -0.23290543,  0.14697589,\n",
       "           -0.15616232, -0.13103327, -0.03035648,  0.05661544,\n",
       "           -0.16774169, -0.05829484],\n",
       "          [-0.08871987,  0.05801985, -0.19764267, -0.18147   ,\n",
       "           -0.0206146 ,  0.01274008, -0.09907413,  0.01654987,\n",
       "           -0.11404741, -0.10752384],\n",
       "          [-0.06638953,  0.04510949, -0.00762828,  0.06289761,\n",
       "            0.07442535, -0.161487  ,  0.13958897,  0.06849246,\n",
       "            0.08515238,  0.000786  ],\n",
       "          [-0.12195646, -0.01146095,  0.03910394,  0.0471328 ,\n",
       "            0.02098483,  0.03838791,  0.00285232, -0.13144788,\n",
       "           -0.05738182,  0.03315028],\n",
       "          [-0.07006919, -0.10729159, -0.0334678 ,  0.10826814,\n",
       "            0.13783583,  0.19463652,  0.09013799, -0.05372679,\n",
       "            0.05105552,  0.06432493],\n",
       "          [-0.00549034, -0.13485856,  0.03763136,  0.01879119,\n",
       "           -0.03949698,  0.0134861 , -0.04179855, -0.21516779,\n",
       "            0.07669239,  0.03330083],\n",
       "          [ 0.05061371,  0.14030577,  0.03123054, -0.15735008,\n",
       "           -0.16376708,  0.0801961 , -0.05549176, -0.031226  ,\n",
       "            0.04177024, -0.13219434],\n",
       "          [-0.0754587 , -0.03180633,  0.02867719, -0.13401198,\n",
       "            0.11331809, -0.07974488,  0.13790596,  0.1288196 ,\n",
       "           -0.04573924,  0.00581217],\n",
       "          [-0.11379898, -0.10041856, -0.12411294, -0.20843421,\n",
       "            0.02338842,  0.08861326,  0.04386873, -0.09510503,\n",
       "            0.05781312, -0.00976666],\n",
       "          [-0.12648784,  0.16420619,  0.04192559, -0.10347714,\n",
       "           -0.1059532 ,  0.01498076,  0.08983373,  0.00621853,\n",
       "            0.09020483, -0.05104116]]]], dtype=float32)>,\n",
       " <tf.Variable 'conv2d_220_1/bias:0' shape=(10,) dtype=float32, numpy=\n",
       " array([ 0.00197952,  0.000255  , -0.00790314,  0.00083751,  0.00138084,\n",
       "         0.00356111, -0.01154541,  0.00106886, -0.00576153, -0.00365566],\n",
       "       dtype=float32)>,\n",
       " <tf.Variable 'batch_normalization_250_1/gamma:0' shape=(10,) dtype=float32, numpy=\n",
       " array([1.0093904, 0.9823292, 1.0047513, 0.9891817, 0.9999991, 1.0094211,\n",
       "        1.013434 , 0.9825863, 1.0096635, 1.0008535], dtype=float32)>,\n",
       " <tf.Variable 'batch_normalization_250_1/beta:0' shape=(10,) dtype=float32, numpy=\n",
       " array([ 3.70635977e-03,  3.70083237e-03, -7.12216645e-03,  1.43356575e-02,\n",
       "        -5.76976733e-03, -5.98404149e-04, -1.57424845e-02,  3.97230469e-05,\n",
       "         2.08421401e-03,  1.47233149e-02], dtype=float32)>,\n",
       " <tf.Variable 'conv2d_221_1/kernel:0' shape=(3, 3, 10, 10) dtype=float32, numpy=\n",
       " array([[[[-1.49680197e-01, -2.81311218e-02, -1.06407747e-01,\n",
       "            1.96579665e-01,  5.35590202e-02,  1.30665660e-01,\n",
       "            1.75376132e-01,  3.73159833e-02,  1.11317396e-01,\n",
       "           -9.37561225e-03],\n",
       "          [-1.34816870e-01,  6.31669834e-02, -7.94403628e-02,\n",
       "            1.39063090e-01,  9.76344347e-02, -9.13428068e-02,\n",
       "            8.73562545e-02,  8.75391345e-03,  6.88722283e-02,\n",
       "           -7.06084296e-02],\n",
       "          [-1.18113984e-03, -1.40973374e-01,  2.03572884e-01,\n",
       "           -1.18924133e-01, -1.08975284e-01,  1.03204131e-01,\n",
       "           -6.72276970e-03, -1.48482472e-01,  8.27709287e-02,\n",
       "           -2.12832680e-03],\n",
       "          [ 1.38651505e-01,  1.08192943e-01, -1.23593986e-01,\n",
       "           -7.05923587e-02, -8.71801153e-02,  1.84071288e-01,\n",
       "            1.10211700e-01, -8.07639658e-02, -1.95592642e-01,\n",
       "           -8.26702192e-02],\n",
       "          [-9.94699299e-02, -1.07054763e-01, -5.23898639e-02,\n",
       "           -5.42491712e-02,  2.01344024e-02,  2.42214873e-02,\n",
       "            1.64175797e-02,  1.20985433e-01,  4.86409925e-02,\n",
       "            7.69801512e-02],\n",
       "          [-8.52369219e-02, -1.21187329e-01,  7.64299259e-02,\n",
       "           -1.98289320e-01,  1.54101193e-01,  1.18926547e-01,\n",
       "            1.53621465e-01, -5.86784258e-02,  2.36623764e-01,\n",
       "            1.29226185e-02],\n",
       "          [ 2.71709375e-02, -6.13340661e-02, -1.78482607e-02,\n",
       "            1.49160907e-01, -1.73055218e-04,  1.16560422e-01,\n",
       "            3.77960168e-02,  1.83996543e-01,  1.24050363e-03,\n",
       "           -1.00240223e-01],\n",
       "          [-4.57102209e-02, -3.39884944e-02, -1.86633304e-01,\n",
       "           -2.46123910e-01,  1.38793886e-01, -9.83169824e-02,\n",
       "            2.17247140e-02,  3.69331837e-02,  4.76752557e-02,\n",
       "           -5.47556542e-02],\n",
       "          [-2.06578359e-01, -1.85784340e-01, -3.48497927e-02,\n",
       "           -4.31498466e-03,  7.64293410e-03, -3.27919461e-02,\n",
       "           -1.00645043e-01,  4.84678745e-02, -2.19642967e-01,\n",
       "            2.07518041e-02],\n",
       "          [-1.15115782e-02, -1.77961327e-02,  2.01726794e-01,\n",
       "           -8.66440833e-02, -6.40381053e-02, -9.26993322e-03,\n",
       "           -1.11482456e-01, -1.94860343e-02,  6.11087568e-02,\n",
       "           -7.73886815e-02]],\n",
       " \n",
       "         [[-7.47056082e-02, -8.84091016e-03, -2.33575076e-01,\n",
       "            1.36886895e-01,  2.15550974e-01,  4.39960547e-02,\n",
       "            9.35931876e-02,  3.34682949e-02, -2.39152405e-02,\n",
       "            7.93463886e-02],\n",
       "          [-1.45578876e-01, -1.53541192e-01, -2.15596519e-03,\n",
       "            5.89195490e-02, -1.66202962e-01,  1.15048490e-01,\n",
       "            1.14936426e-01, -1.75685678e-02, -2.00182021e-01,\n",
       "            9.13383812e-03],\n",
       "          [-4.28714789e-02, -1.20123802e-02,  2.33108759e-01,\n",
       "           -8.22080225e-02,  6.41822144e-02, -4.79891375e-02,\n",
       "            5.40179424e-02, -4.09348309e-03,  1.22995190e-01,\n",
       "            5.70756458e-02],\n",
       "          [ 7.96533078e-02, -1.00307629e-01, -1.36671901e-01,\n",
       "            5.06461971e-02,  1.36794478e-01,  1.20585058e-02,\n",
       "            5.70732206e-02, -1.91772640e-01, -1.69597894e-01,\n",
       "           -2.57042013e-02],\n",
       "          [-1.87047701e-02,  7.57228732e-02, -9.47461128e-02,\n",
       "            1.00886479e-01, -1.62676588e-01,  2.46307980e-02,\n",
       "            1.11355193e-01,  2.35538393e-01, -1.18823163e-01,\n",
       "            6.09402219e-03],\n",
       "          [ 5.68431914e-02,  4.14320081e-03,  2.79352218e-02,\n",
       "           -5.27897850e-02, -8.07667524e-02,  1.41782582e-01,\n",
       "           -7.45272487e-02, -3.07364929e-02,  6.95183799e-02,\n",
       "            1.81575507e-01],\n",
       "          [-1.02249615e-01, -1.02215834e-01, -1.24335382e-02,\n",
       "           -1.18095484e-02, -1.14101274e-02, -3.90266106e-02,\n",
       "           -6.24566488e-02,  2.20102087e-01,  2.75235679e-02,\n",
       "            6.45985734e-03],\n",
       "          [ 1.38521582e-01, -7.55202174e-02, -1.61772326e-01,\n",
       "            4.52878959e-02, -2.49117147e-02, -7.83284893e-04,\n",
       "           -1.19661674e-01, -8.43393952e-02,  7.61132687e-02,\n",
       "           -1.61605820e-01],\n",
       "          [-3.29880267e-02,  1.00795135e-01, -7.58266747e-02,\n",
       "            7.65054002e-02, -1.62856057e-01, -9.46197659e-02,\n",
       "            9.10825431e-02, -1.36923650e-02, -1.24775171e-01,\n",
       "            2.34196529e-01],\n",
       "          [-2.21790057e-02, -2.88499668e-02,  1.06571302e-01,\n",
       "           -2.22299710e-01, -3.22095379e-02, -5.36386147e-02,\n",
       "           -1.00468509e-01,  2.91865859e-02, -3.53289843e-02,\n",
       "            1.30740225e-01]],\n",
       " \n",
       "         [[-4.94829454e-02,  9.59822685e-02,  1.34878367e-01,\n",
       "           -1.05875529e-01,  1.25939637e-01, -7.20457658e-02,\n",
       "           -6.71485439e-02, -1.47675037e-01,  3.79866958e-02,\n",
       "           -2.54017543e-02],\n",
       "          [ 1.22674249e-01, -5.52300476e-02,  2.04978153e-01,\n",
       "            1.23918727e-02, -9.47810560e-02, -1.37793375e-02,\n",
       "            7.80836791e-02, -1.02386028e-01,  1.29711311e-02,\n",
       "           -7.56295845e-02],\n",
       "          [ 6.70239031e-02, -8.30368418e-03, -1.09383456e-01,\n",
       "           -1.12906948e-01,  1.16848655e-01,  1.00945115e-01,\n",
       "           -1.09600723e-01,  2.17050701e-01, -5.94233200e-02,\n",
       "            2.28721678e-01],\n",
       "          [ 2.59420672e-03,  1.68077871e-01, -7.18516037e-02,\n",
       "           -4.95075509e-02, -7.00223222e-02, -1.16094835e-01,\n",
       "           -1.88895375e-01, -1.74512297e-01,  1.03838429e-01,\n",
       "           -1.63749263e-01],\n",
       "          [ 4.80861440e-02, -1.77174062e-02,  4.24010567e-02,\n",
       "           -3.42526771e-02, -1.68942541e-01,  8.75190794e-02,\n",
       "           -2.44816989e-01,  6.94306120e-02,  6.21153079e-02,\n",
       "            1.37290135e-01],\n",
       "          [-2.08797172e-01, -4.05532308e-02, -4.29239459e-02,\n",
       "            8.96525458e-02, -1.89520866e-02,  1.84562236e-01,\n",
       "            1.36201784e-01, -3.01860981e-02,  1.25725225e-01,\n",
       "            2.49877460e-02],\n",
       "          [ 1.39584485e-02, -9.47535262e-02, -2.21405141e-02,\n",
       "            9.24755782e-02,  8.36518928e-02, -6.96720649e-03,\n",
       "           -7.20776990e-02, -3.25784758e-02, -3.67111228e-02,\n",
       "            2.31812999e-01],\n",
       "          [-6.13574795e-02, -6.37081414e-02,  2.74964310e-02,\n",
       "            2.70515457e-02, -1.56575561e-01,  2.83582998e-03,\n",
       "            8.35375488e-03,  6.32457212e-02, -1.57961667e-01,\n",
       "            2.86451802e-02],\n",
       "          [ 3.34097818e-02,  1.91885214e-02,  9.11539048e-02,\n",
       "            1.37058824e-01,  8.44052881e-02, -3.01868096e-02,\n",
       "            1.27720889e-02, -2.01866459e-02,  9.49821472e-02,\n",
       "            1.30769491e-01],\n",
       "          [ 6.05794303e-02,  2.67430171e-02,  1.07398639e-02,\n",
       "           -1.08180352e-01, -8.17065686e-02, -4.98952456e-02,\n",
       "           -1.40352711e-01, -1.61531389e-01, -1.97286233e-02,\n",
       "           -2.57325303e-02]]],\n",
       " \n",
       " \n",
       "        [[[-5.39280064e-02,  1.67594641e-01, -9.17426944e-02,\n",
       "           -1.02081418e-01,  3.45276445e-02, -7.74610788e-02,\n",
       "           -1.24233454e-01,  1.17162026e-01,  3.20462473e-02,\n",
       "            1.68169081e-01],\n",
       "          [ 5.10880463e-02, -8.38677064e-02,  1.40391767e-01,\n",
       "           -7.46513531e-02,  1.10769182e-01,  9.88962576e-02,\n",
       "            1.25173047e-01, -7.63666332e-02, -9.18156505e-02,\n",
       "            9.08718109e-02],\n",
       "          [-6.14559278e-03,  6.58683851e-02, -3.04469932e-02,\n",
       "           -1.17772453e-01,  1.07677348e-01,  1.31930094e-02,\n",
       "            1.33603722e-01,  8.21027085e-02,  4.84314524e-02,\n",
       "            3.54344882e-02],\n",
       "          [ 3.58222798e-02,  1.16257384e-01,  6.22684211e-02,\n",
       "           -1.76238403e-01, -1.74277253e-03,  8.58085230e-02,\n",
       "            7.66934007e-02, -2.01055333e-01,  1.20316833e-01,\n",
       "            1.64935235e-02],\n",
       "          [ 1.76936816e-02, -1.55859562e-02, -6.96527287e-02,\n",
       "            6.57303333e-02,  5.83223104e-02, -2.25793980e-02,\n",
       "           -1.19101703e-02, -1.28977269e-01, -1.33931696e-01,\n",
       "           -1.92136049e-01],\n",
       "          [ 7.72732822e-03, -6.29518330e-02,  1.42663531e-03,\n",
       "           -2.27829423e-02,  1.13558425e-02, -1.86278984e-01,\n",
       "            3.35377385e-03,  7.06866756e-02,  7.20796222e-03,\n",
       "            1.63015053e-01],\n",
       "          [-7.32125267e-02,  6.63840249e-02, -1.14432603e-01,\n",
       "           -5.58559708e-02, -1.91648737e-01, -3.74244675e-02,\n",
       "            5.54271750e-02, -1.57391354e-01,  1.93031117e-01,\n",
       "           -4.39109094e-02],\n",
       "          [-6.34488687e-02,  4.90583405e-02, -7.45063052e-02,\n",
       "           -8.99688378e-02,  1.07905623e-02,  7.75345191e-02,\n",
       "            1.54253751e-01, -6.86039636e-03,  3.26282568e-02,\n",
       "           -3.87118477e-03],\n",
       "          [ 5.53019978e-02,  7.22846463e-02,  1.06936917e-01,\n",
       "            1.16029933e-01,  2.21494352e-03,  9.31462646e-03,\n",
       "            3.24176922e-02,  1.25462832e-02,  1.36775136e-01,\n",
       "           -6.49383618e-03],\n",
       "          [ 7.27056293e-03, -1.55529752e-01, -6.07924387e-02,\n",
       "           -1.01503558e-01, -1.65364444e-01, -1.15813792e-01,\n",
       "           -8.70182067e-02, -1.33265793e-01, -5.25717773e-02,\n",
       "           -5.58144674e-02]],\n",
       " \n",
       "         [[-2.60091363e-03, -3.93324606e-02,  9.52863842e-02,\n",
       "            1.97784916e-01,  1.95907857e-02,  1.26940668e-01,\n",
       "           -1.04159534e-01,  5.43266274e-02, -4.64087948e-02,\n",
       "            9.67282522e-03],\n",
       "          [ 1.54243529e-01,  1.98608935e-01,  1.91035364e-02,\n",
       "            1.10068418e-01, -3.28944027e-02,  6.63913414e-02,\n",
       "           -6.70764968e-03,  1.86666511e-02,  6.22768104e-02,\n",
       "           -1.19018868e-01],\n",
       "          [-1.39641896e-01,  1.05240000e-02,  9.22252014e-02,\n",
       "           -9.33112204e-02, -1.84091642e-01,  2.02633157e-01,\n",
       "           -1.04232915e-01, -7.39971409e-03, -1.02201596e-01,\n",
       "           -2.01537296e-01],\n",
       "          [ 5.91648370e-02, -6.77326927e-03, -6.46430627e-02,\n",
       "           -1.37257531e-01, -4.14509289e-02, -4.60041054e-02,\n",
       "           -2.31061522e-02,  4.07638066e-02, -1.75786361e-01,\n",
       "           -5.33430502e-02],\n",
       "          [ 3.54920588e-02,  1.42556027e-01,  8.63464251e-02,\n",
       "            1.40445353e-02,  1.47527328e-03,  1.50682479e-01,\n",
       "            2.22244784e-01, -2.16357484e-01, -1.07420444e-01,\n",
       "            4.31216024e-02],\n",
       "          [-6.11734651e-02,  9.21869949e-02, -5.65241426e-02,\n",
       "           -2.96385884e-02, -1.75565314e-02,  3.25573012e-02,\n",
       "            1.06514804e-01, -1.41993925e-01, -3.88675258e-02,\n",
       "            7.25356117e-02],\n",
       "          [-1.96413562e-01, -7.09757134e-02,  2.45310161e-02,\n",
       "           -2.36587916e-02,  5.90995178e-02,  9.04940590e-02,\n",
       "            1.41443327e-01,  2.56511550e-02,  5.03312424e-02,\n",
       "            7.23049268e-02],\n",
       "          [ 6.33128062e-02,  2.36100666e-02,  4.06751037e-02,\n",
       "           -8.12039077e-02,  8.23835954e-02,  6.55425861e-02,\n",
       "           -5.58932759e-02, -1.19248129e-01, -8.08746088e-03,\n",
       "            1.83601037e-01],\n",
       "          [ 2.02790454e-01,  1.83140442e-01, -1.68053228e-02,\n",
       "            3.10031809e-02, -7.07050636e-02, -3.20337601e-02,\n",
       "            1.71345830e-01, -4.90078591e-02,  1.77566901e-01,\n",
       "            6.13179021e-02],\n",
       "          [ 6.95571080e-02,  1.19671166e-01,  1.34246409e-01,\n",
       "            7.16535076e-02, -5.09294458e-02, -1.27774417e-01,\n",
       "           -1.99407250e-01,  2.29503989e-01, -1.83411129e-02,\n",
       "            4.22849059e-02]],\n",
       " \n",
       "         [[ 4.04368304e-02, -2.88768020e-02, -1.06188422e-02,\n",
       "           -3.12712900e-02,  1.61021411e-01, -2.22817250e-02,\n",
       "            8.57803151e-02, -5.86468168e-02, -1.67912379e-01,\n",
       "           -1.13414377e-01],\n",
       "          [ 2.36013848e-02,  7.50278309e-02, -2.62412038e-02,\n",
       "           -9.15828273e-02,  9.23719853e-02,  1.04178198e-01,\n",
       "            2.83821113e-02, -7.69179314e-02, -8.28887895e-02,\n",
       "            7.48398975e-02],\n",
       "          [-1.65404305e-02, -9.48879421e-02, -8.04012828e-03,\n",
       "           -8.05114396e-03,  4.29966785e-02,  4.92052920e-02,\n",
       "            1.01576276e-01, -5.66836223e-02,  1.73120216e-01,\n",
       "            1.40896976e-01],\n",
       "          [-1.98316067e-01, -9.32448059e-02, -5.05019724e-02,\n",
       "           -7.47036934e-02, -1.68954998e-01, -1.94620445e-01,\n",
       "            7.69003853e-02,  1.94854308e-02,  1.85638860e-01,\n",
       "           -1.22129671e-01],\n",
       "          [-1.10229202e-01, -4.19616513e-02, -2.87538581e-02,\n",
       "           -1.43047631e-01,  6.77324980e-02, -1.20855018e-03,\n",
       "           -4.61208075e-02,  9.39248279e-02,  7.57611096e-02,\n",
       "            7.49965012e-02],\n",
       "          [ 6.50469363e-02, -1.11282118e-01,  6.93727657e-02,\n",
       "           -5.99698052e-02, -1.60088297e-02, -2.38666162e-02,\n",
       "           -5.72486334e-02, -9.65825990e-02, -6.67491183e-03,\n",
       "            1.29542992e-01],\n",
       "          [ 1.04570135e-01, -7.75957629e-02, -2.40392778e-02,\n",
       "            1.97381988e-01,  3.20416540e-02,  1.84741259e-01,\n",
       "           -2.32209209e-02, -3.04796081e-03,  1.72492445e-01,\n",
       "           -1.25666201e-01],\n",
       "          [ 9.71554406e-03,  5.97857758e-02, -7.69614801e-02,\n",
       "            1.35467604e-01, -1.24438420e-01,  5.70588410e-02,\n",
       "            1.20569788e-01, -4.38539460e-02,  6.12866227e-03,\n",
       "            2.16522694e-01],\n",
       "          [-2.20400438e-01,  5.77064604e-02, -1.29826680e-01,\n",
       "            5.30939922e-02, -2.98254844e-03,  1.35260567e-01,\n",
       "           -4.64693122e-02, -1.35895371e-01, -8.52931663e-02,\n",
       "           -2.03872427e-01],\n",
       "          [ 5.97086586e-02,  9.83923599e-02,  7.80172125e-02,\n",
       "            1.26716301e-01,  1.15544632e-01,  1.03848159e-01,\n",
       "            4.37828666e-03,  1.00937456e-01, -2.42823195e-02,\n",
       "           -7.42272884e-02]]],\n",
       " \n",
       " \n",
       "        [[[ 1.92128289e-02,  1.77934226e-02, -5.53577673e-03,\n",
       "           -6.63697571e-02, -1.40888870e-01, -7.45937154e-02,\n",
       "           -4.82417233e-02, -7.10275546e-02, -3.94047052e-02,\n",
       "            8.05705488e-02],\n",
       "          [ 8.62730071e-02,  9.94455535e-03,  3.47991996e-02,\n",
       "            7.12873489e-02, -9.57207233e-02, -6.55744150e-02,\n",
       "           -3.02342251e-02, -3.15203555e-02,  1.64673761e-01,\n",
       "            7.17963874e-02],\n",
       "          [ 2.60472409e-02, -7.02779740e-03, -2.30330274e-01,\n",
       "            7.49110505e-02,  1.10953651e-01, -1.33140087e-01,\n",
       "            1.55009940e-01, -3.11664622e-02, -4.24729697e-02,\n",
       "            1.03534475e-01],\n",
       "          [ 6.04713373e-02,  2.05682561e-01, -9.54778269e-02,\n",
       "           -1.62574306e-01,  7.96765089e-02,  2.90449639e-03,\n",
       "            1.55522078e-01, -6.13388717e-02, -1.09134197e-01,\n",
       "           -3.42827961e-02],\n",
       "          [-1.71907708e-01,  9.78100225e-02, -4.37815711e-02,\n",
       "           -1.76050887e-01,  9.80441868e-02, -8.18742588e-02,\n",
       "            1.43085644e-01,  2.07248360e-01,  5.44944704e-02,\n",
       "           -3.73346321e-02],\n",
       "          [-2.67435536e-02,  6.55322075e-02, -7.86309466e-02,\n",
       "            6.49956614e-02, -1.78760543e-01, -2.54943371e-02,\n",
       "           -1.55982301e-01,  1.38953626e-01,  9.11766738e-02,\n",
       "           -1.86520010e-01],\n",
       "          [-1.03331774e-01,  4.88946997e-02,  9.34663862e-02,\n",
       "           -6.95407018e-02,  1.66091416e-02, -9.96493772e-02,\n",
       "           -2.13903427e-01, -2.01650411e-01, -2.34020799e-01,\n",
       "           -5.94956391e-02],\n",
       "          [-6.07179254e-02,  1.23180442e-01, -1.21208891e-01,\n",
       "            1.52541086e-01,  1.18657365e-01, -1.48040010e-02,\n",
       "           -1.90515950e-01, -2.63537420e-03, -1.22922231e-02,\n",
       "           -7.93105811e-02],\n",
       "          [ 1.46470889e-01,  1.21206902e-01, -1.37490988e-01,\n",
       "            6.40377849e-02, -9.16145220e-02, -1.50441200e-01,\n",
       "           -7.55041763e-02,  5.98738752e-02, -2.31982842e-02,\n",
       "            2.20536336e-01],\n",
       "          [ 1.05487369e-01,  1.28338754e-01, -1.52771801e-01,\n",
       "           -1.88242853e-01, -1.10845469e-01, -1.40596122e-01,\n",
       "           -1.20144151e-01,  2.45242100e-02,  1.51708767e-01,\n",
       "           -1.01708561e-01]],\n",
       " \n",
       "         [[ 1.72845334e-01, -1.25604019e-01,  1.33641317e-01,\n",
       "            4.04579490e-02,  8.35966021e-02,  1.39085367e-01,\n",
       "           -2.91587343e-03,  5.89791313e-02, -1.10722467e-01,\n",
       "           -8.25300533e-03],\n",
       "          [-3.85943651e-02, -5.52834244e-03, -6.60579130e-02,\n",
       "           -4.00725901e-02, -7.33786449e-02, -5.57579473e-02,\n",
       "            5.49288765e-02, -7.71716461e-02,  9.67678949e-02,\n",
       "           -5.36358766e-02],\n",
       "          [ 6.31357506e-02, -7.95869902e-02,  2.30668597e-02,\n",
       "           -1.25354260e-01,  4.57943007e-02, -1.31031916e-01,\n",
       "           -2.23545469e-02, -1.45911336e-01, -2.58782338e-02,\n",
       "            1.26057863e-01],\n",
       "          [ 1.42065957e-01, -6.37072995e-02,  1.58923030e-01,\n",
       "            1.36071444e-01,  2.32386366e-01,  2.99505480e-02,\n",
       "            1.59596339e-01,  1.16529599e-01,  8.29188302e-02,\n",
       "            4.13727425e-02],\n",
       "          [-8.39753449e-02, -1.22638559e-03, -1.22768596e-01,\n",
       "            1.76768869e-01, -4.71950434e-02, -4.97799888e-02,\n",
       "           -9.00771767e-02,  4.85064164e-02,  6.78406358e-02,\n",
       "           -8.99808332e-02],\n",
       "          [-2.27302998e-01,  1.02155045e-01, -2.28036195e-01,\n",
       "            9.61558223e-02,  1.34382434e-02, -1.59893651e-02,\n",
       "           -4.69922610e-02,  3.54808718e-02,  1.02006935e-01,\n",
       "            7.01127350e-02],\n",
       "          [ 5.33290021e-02,  3.64869498e-02,  6.65657222e-02,\n",
       "            1.25354365e-01,  1.80090964e-02, -4.92076427e-02,\n",
       "           -9.24024284e-02, -3.73851992e-02, -1.61868349e-01,\n",
       "           -6.48851842e-02],\n",
       "          [-7.01166913e-02,  1.58480749e-01,  2.07382396e-01,\n",
       "           -6.58719540e-02,  4.88419533e-02, -2.45018918e-02,\n",
       "           -1.10430539e-01,  7.38897473e-02,  6.01079352e-02,\n",
       "            1.10069431e-01],\n",
       "          [ 9.77990776e-02,  2.81934198e-02, -1.06900953e-01,\n",
       "            6.20236620e-02, -1.51472734e-02, -5.14046215e-02,\n",
       "            2.27638502e-02,  2.13117838e-01, -1.41426787e-01,\n",
       "            1.06300861e-01],\n",
       "          [-1.81076095e-01, -5.51315770e-02, -3.71913537e-02,\n",
       "            8.13364461e-02,  9.51878130e-02,  2.31749862e-02,\n",
       "            5.63872904e-02, -1.39083579e-01, -1.43878609e-02,\n",
       "            9.25984606e-02]],\n",
       " \n",
       "         [[ 1.36738136e-01, -6.34746477e-02,  1.66212648e-01,\n",
       "            9.85228047e-02,  1.38977438e-01, -1.03970706e-01,\n",
       "            1.40005937e-02,  1.53721021e-02, -4.81428504e-02,\n",
       "            5.00008352e-02],\n",
       "          [ 1.15253128e-01,  8.35740566e-03, -1.12546887e-02,\n",
       "            4.10504714e-02,  2.47760210e-02,  1.58003084e-02,\n",
       "            2.12162256e-01,  4.83559333e-02,  3.87155600e-02,\n",
       "            1.36043280e-01],\n",
       "          [-1.48874342e-01, -4.93719354e-02, -8.75356123e-02,\n",
       "            9.88865942e-02, -6.89833462e-02,  1.58193648e-01,\n",
       "            5.75094819e-02, -1.07686117e-03, -5.68900518e-02,\n",
       "           -2.07435936e-02],\n",
       "          [ 5.35938516e-02,  1.06049046e-01, -5.05688675e-02,\n",
       "           -1.31206550e-02,  7.94260055e-02, -2.19230905e-01,\n",
       "           -2.94644907e-02, -1.60624027e-01, -1.47828788e-01,\n",
       "            1.84335597e-02],\n",
       "          [ 3.48137431e-02, -5.34157865e-02, -2.16885135e-02,\n",
       "           -6.33188710e-02, -9.05745625e-02, -7.91164264e-02,\n",
       "           -2.10458964e-01, -6.76964000e-02, -1.80010900e-01,\n",
       "           -9.71905049e-03],\n",
       "          [ 6.61928579e-02,  1.20223477e-01, -1.90484464e-01,\n",
       "           -7.68819973e-02, -1.28065795e-01,  5.69903776e-02,\n",
       "           -7.33211190e-02, -1.85040031e-02,  1.05476305e-01,\n",
       "           -7.10267350e-02],\n",
       "          [ 1.24808298e-02, -4.60802019e-02, -1.11355275e-01,\n",
       "            6.49864301e-02,  4.54837084e-02,  1.05324537e-01,\n",
       "            2.29376461e-02, -1.19073108e-01, -1.38661247e-02,\n",
       "           -1.79929640e-02],\n",
       "          [ 9.60325450e-02,  1.83594823e-01, -1.58979416e-01,\n",
       "            1.00522146e-01,  1.11762155e-02,  2.41647605e-02,\n",
       "           -2.18177699e-02,  1.85354859e-01, -1.09150745e-02,\n",
       "            1.15122646e-01],\n",
       "          [ 4.16480787e-02,  4.95696254e-02, -8.30421448e-02,\n",
       "            7.16779083e-02, -1.18645743e-01,  1.66722015e-01,\n",
       "            1.91439301e-01, -1.49367750e-02, -5.70707396e-03,\n",
       "           -1.21999301e-01],\n",
       "          [-1.46046758e-01,  1.34193942e-01, -1.04773799e-02,\n",
       "            1.38629824e-01, -6.06858805e-02, -2.07495406e-01,\n",
       "           -1.32488281e-01, -2.08368316e-01,  6.05427995e-02,\n",
       "            1.39334664e-01]]]], dtype=float32)>,\n",
       " <tf.Variable 'conv2d_221_1/bias:0' shape=(10,) dtype=float32, numpy=\n",
       " array([-0.0062598 ,  0.01044028,  0.00826102,  0.00102786,  0.0070779 ,\n",
       "        -0.01753479,  0.00921691,  0.00088934, -0.0003165 ,  0.00144496],\n",
       "       dtype=float32)>,\n",
       " <tf.Variable 'batch_normalization_251_1/gamma:0' shape=(10,) dtype=float32, numpy=\n",
       " array([1.0206   , 1.012373 , 0.9850669, 1.0041416, 1.0102712, 0.9787165,\n",
       "        0.9942517, 1.0116374, 1.0117885, 0.9851484], dtype=float32)>,\n",
       " <tf.Variable 'batch_normalization_251_1/beta:0' shape=(10,) dtype=float32, numpy=\n",
       " array([ 0.00484862,  0.01330577,  0.02101878,  0.00546109, -0.00357618,\n",
       "        -0.00865569,  0.01535316,  0.01735472, -0.00729665, -0.00832963],\n",
       "       dtype=float32)>,\n",
       " <tf.Variable 'conv2d_222_1/kernel:0' shape=(3, 3, 10, 10) dtype=float32, numpy=\n",
       " array([[[[ 6.09884225e-02, -1.55911520e-01,  9.18808728e-02,\n",
       "            1.53676269e-03,  5.16833141e-02, -1.08217997e-02,\n",
       "           -4.36521694e-02,  1.48633197e-01, -9.18598175e-02,\n",
       "            2.58852411e-02],\n",
       "          [ 5.19879609e-02, -4.10362408e-02,  6.83984607e-02,\n",
       "            2.52555124e-03, -2.04759493e-01,  1.38535276e-01,\n",
       "           -5.86690428e-03, -4.08499874e-02,  5.56018203e-02,\n",
       "            1.61608949e-01],\n",
       "          [ 2.09904820e-01, -5.13222218e-02,  8.10022503e-02,\n",
       "            7.11881071e-02,  1.56515595e-02, -1.68078229e-01,\n",
       "            3.81507240e-02, -6.74333831e-04, -1.31781250e-01,\n",
       "            6.29176944e-02],\n",
       "          [ 1.30435228e-02, -7.94094875e-02, -6.18874375e-03,\n",
       "           -4.98803221e-02, -1.24663994e-01, -1.15591541e-01,\n",
       "           -8.88058171e-02, -6.01012036e-02, -1.94526270e-01,\n",
       "            4.12803255e-02],\n",
       "          [ 1.29509240e-01,  6.09131344e-02,  2.03955546e-02,\n",
       "           -4.90079857e-02,  3.84364091e-02,  6.47365525e-02,\n",
       "           -1.61823064e-01,  2.28582054e-01, -3.57664153e-02,\n",
       "            1.01007037e-01],\n",
       "          [-1.62467107e-01,  1.56917498e-01, -8.92669521e-03,\n",
       "            6.24041520e-02,  1.74723551e-01,  9.41444114e-02,\n",
       "            6.63095266e-02,  9.04447660e-02,  8.76063183e-02,\n",
       "            4.88353074e-02],\n",
       "          [ 1.15862727e-01,  1.59298424e-02, -1.27125708e-02,\n",
       "           -1.20021395e-01, -1.42834798e-01, -3.79167460e-02,\n",
       "            3.38648930e-02, -1.79873988e-01,  5.13253771e-02,\n",
       "            1.32107303e-01],\n",
       "          [ 1.40298158e-01, -1.27087101e-01,  6.43701181e-02,\n",
       "           -1.84707731e-01,  1.49094462e-01, -4.23004180e-02,\n",
       "            1.75103962e-01, -1.12789813e-02, -1.08008876e-01,\n",
       "            2.07520336e-01],\n",
       "          [-1.02187358e-01, -6.60104975e-02,  2.85069495e-02,\n",
       "           -2.51403470e-02,  1.21435270e-01, -7.89074227e-02,\n",
       "           -3.65192927e-02, -1.22461677e-01, -6.09741546e-02,\n",
       "           -7.44605884e-02],\n",
       "          [-6.00945465e-02,  9.92450938e-02, -3.98885868e-02,\n",
       "            7.21316114e-02, -1.90605402e-01, -1.14099510e-01,\n",
       "            1.39880344e-01, -1.06888019e-01,  2.62934994e-02,\n",
       "           -1.49898633e-01]],\n",
       " \n",
       "         [[-2.13918164e-01, -6.71755224e-02,  1.44938603e-01,\n",
       "           -6.01470843e-02,  2.52604723e-01,  5.40407710e-02,\n",
       "            4.49910946e-02,  1.23251565e-01,  1.24979794e-01,\n",
       "           -1.10018790e-01],\n",
       "          [ 2.21881270e-01,  9.24755633e-02,  1.22763634e-01,\n",
       "           -1.91852927e-01, -3.44604664e-02,  3.72907557e-02,\n",
       "           -6.26171678e-02,  3.17012891e-03,  1.04743809e-01,\n",
       "           -1.60263237e-02],\n",
       "          [ 4.77342531e-02,  9.81134623e-02,  7.02554658e-02,\n",
       "            5.67419939e-02,  8.02766755e-02, -2.28695720e-02,\n",
       "           -5.28047187e-03, -1.27478866e-02,  6.05510399e-02,\n",
       "           -4.73402105e-02],\n",
       "          [-1.26675561e-01,  1.52781978e-01, -3.76594514e-02,\n",
       "           -3.94495241e-02, -1.91794947e-01, -1.12947136e-01,\n",
       "            5.87089024e-02, -1.02343678e-01,  1.70376480e-01,\n",
       "            8.24336428e-03],\n",
       "          [ 7.01066405e-02, -2.42821947e-02, -1.25176013e-01,\n",
       "           -1.58636257e-01, -1.77911818e-01, -7.83485472e-02,\n",
       "           -8.41975063e-02, -7.50746112e-03, -1.40094519e-01,\n",
       "            4.63505127e-02],\n",
       "          [-3.87127027e-02, -6.61593825e-02, -8.03589448e-02,\n",
       "            2.19131052e-03, -1.34593681e-01, -2.33924761e-01,\n",
       "           -2.90767523e-03,  5.28427996e-02, -3.24732363e-02,\n",
       "            8.84874240e-02],\n",
       "          [ 1.03716865e-01,  5.59312031e-02, -7.19450861e-02,\n",
       "           -2.10272506e-01,  1.02196261e-01, -7.70337582e-02,\n",
       "            9.20567289e-02,  8.42153504e-02,  1.13757484e-01,\n",
       "           -1.93421707e-01],\n",
       "          [-2.03814059e-02,  1.43949986e-01,  9.89322588e-02,\n",
       "            2.11724713e-01, -4.90998887e-02, -1.50525704e-01,\n",
       "            1.85306519e-01,  1.55779928e-01,  7.76550323e-02,\n",
       "            1.06205093e-02],\n",
       "          [ 3.06025520e-02,  4.15136032e-02, -1.44123763e-01,\n",
       "            2.44146436e-02,  6.59703612e-02,  9.15602073e-02,\n",
       "            1.16755143e-02,  1.49887167e-02, -1.94965333e-01,\n",
       "           -1.89973131e-01],\n",
       "          [-1.01540357e-01,  1.64853185e-01,  6.01237193e-02,\n",
       "           -1.04176784e-02,  1.61645994e-01, -1.07164793e-01,\n",
       "           -6.43010512e-02,  1.56997576e-01, -1.63661003e-01,\n",
       "            1.41821012e-01]],\n",
       " \n",
       "         [[-6.58762604e-02, -6.92506805e-02, -1.54798096e-02,\n",
       "            1.99335925e-02,  1.70848772e-01,  9.04776379e-02,\n",
       "            2.74351034e-02,  1.74217336e-02, -9.63249207e-02,\n",
       "           -5.43724895e-02],\n",
       "          [-3.66811752e-02, -1.39256582e-01,  8.70228000e-03,\n",
       "           -7.19759241e-02, -5.08343130e-02,  1.82408273e-01,\n",
       "           -5.78069799e-02,  5.10350056e-02,  6.81904852e-02,\n",
       "           -1.56853408e-01],\n",
       "          [ 4.68716724e-03, -4.16644625e-02,  1.09844301e-02,\n",
       "            1.24518527e-02,  8.31575245e-02,  1.90592647e-01,\n",
       "           -5.79037368e-02,  1.44653860e-02, -7.86593556e-02,\n",
       "           -1.17759164e-02],\n",
       "          [ 8.98692980e-02,  1.36323154e-01, -2.01560222e-02,\n",
       "            1.16608866e-01, -2.89750900e-02, -4.50599566e-02,\n",
       "           -3.94578837e-02,  2.47135460e-01,  2.57095005e-02,\n",
       "            5.06814942e-02],\n",
       "          [ 2.80491170e-02,  2.14132220e-02,  1.64653063e-01,\n",
       "            3.87462378e-02,  3.29150259e-02, -2.28710040e-01,\n",
       "            6.44334629e-02, -1.75837234e-01, -6.53662160e-02,\n",
       "           -2.34033223e-02],\n",
       "          [ 2.94972993e-02,  8.22214708e-02,  3.71754505e-02,\n",
       "           -1.21380508e-01,  1.43267006e-01,  4.30392437e-02,\n",
       "            2.07553372e-01, -1.13371797e-01, -7.29790255e-02,\n",
       "            8.57114196e-02],\n",
       "          [ 1.61846094e-02,  6.82179332e-02, -8.19583982e-03,\n",
       "           -5.14749996e-02, -3.29122171e-02, -1.31908860e-02,\n",
       "           -5.33885173e-02, -3.28062065e-02,  2.66196597e-02,\n",
       "            1.33457080e-01],\n",
       "          [ 8.15095231e-02, -1.73285097e-01,  1.04799554e-01,\n",
       "           -1.29630845e-02,  1.81698352e-02,  2.06590831e-01,\n",
       "           -9.35521051e-02, -1.79075032e-01, -3.47759924e-03,\n",
       "           -1.09985515e-01],\n",
       "          [-3.85013558e-02, -4.63656336e-02,  2.35404130e-02,\n",
       "            6.86504319e-02,  1.68991566e-01,  3.28610800e-02,\n",
       "            1.38017133e-01, -4.32992838e-02, -1.98117360e-01,\n",
       "           -1.67059571e-01],\n",
       "          [-1.26659870e-01, -6.14094362e-02, -3.14549319e-02,\n",
       "            4.73948866e-02, -5.30263782e-02,  2.63188640e-03,\n",
       "           -3.49291936e-02, -6.14689887e-02,  7.07498044e-02,\n",
       "           -1.28549233e-01]]],\n",
       " \n",
       " \n",
       "        [[[-1.82318920e-03,  1.58854146e-02, -1.94047481e-01,\n",
       "           -3.24399509e-02, -6.61604013e-03,  5.57613969e-02,\n",
       "           -1.81691106e-02,  4.67351116e-02,  1.15099840e-01,\n",
       "            5.16282879e-02],\n",
       "          [-1.85927272e-01,  1.75824184e-02, -1.73490137e-01,\n",
       "           -1.28140420e-01,  2.13512644e-01,  8.41040537e-02,\n",
       "            7.88671747e-02,  5.54748476e-02,  5.77501208e-02,\n",
       "           -2.88808113e-03],\n",
       "          [-4.81532030e-02,  5.08492403e-02, -1.45248502e-01,\n",
       "           -3.25083844e-02, -4.92570139e-02, -1.58101916e-02,\n",
       "            1.00662462e-01, -7.95356855e-02,  5.03704175e-02,\n",
       "            2.03764960e-02],\n",
       "          [-4.22944762e-02, -4.58547100e-02,  6.97943866e-02,\n",
       "           -7.30946362e-02, -1.45391181e-01,  7.38340802e-03,\n",
       "           -2.03544274e-03,  1.98411345e-01,  4.26901914e-02,\n",
       "           -1.31678283e-01],\n",
       "          [-7.15149343e-02, -1.12513982e-01,  1.02087006e-01,\n",
       "           -1.06376559e-01,  1.53624833e-01,  1.52645767e-01,\n",
       "           -3.31143057e-03,  1.18051559e-01, -8.44356567e-02,\n",
       "            2.98468154e-02],\n",
       "          [ 9.04143453e-02, -1.28598571e-01,  3.59382480e-02,\n",
       "           -1.61953479e-01, -9.37252268e-02, -1.16764553e-01,\n",
       "           -1.53854921e-01,  2.68188938e-02, -7.91809037e-02,\n",
       "            1.53061986e-01],\n",
       "          [ 4.44421284e-02,  6.51991963e-02, -1.85128093e-01,\n",
       "            1.44432679e-01, -1.36698723e-01, -9.50672552e-02,\n",
       "            1.74300186e-02,  1.00136586e-01,  3.29212137e-02,\n",
       "           -1.45648435e-01],\n",
       "          [-1.36186898e-01, -1.50406301e-01,  1.90345217e-02,\n",
       "            5.48743531e-02,  2.73896828e-02, -4.92608882e-02,\n",
       "            9.10690147e-03, -1.38297863e-02,  8.93360525e-02,\n",
       "            9.89797935e-02],\n",
       "          [-1.90612599e-02, -3.74721438e-02,  2.32740149e-01,\n",
       "           -1.59774810e-01,  1.01502709e-01,  2.25890294e-01,\n",
       "           -1.21507108e-01,  5.68931960e-02,  6.63412139e-02,\n",
       "            2.05370076e-02],\n",
       "          [ 8.72998834e-02,  3.61935496e-02,  1.79456430e-04,\n",
       "           -1.81920633e-01, -2.79958416e-02, -1.39296325e-02,\n",
       "           -1.79953769e-01,  7.07977265e-02,  2.19890438e-02,\n",
       "           -2.84942761e-02]],\n",
       " \n",
       "         [[ 1.01596676e-01,  3.93851986e-03, -1.43210009e-01,\n",
       "            6.30257353e-02, -2.44287178e-02, -1.38574108e-01,\n",
       "            6.91541210e-02,  4.49186899e-02, -1.91653818e-01,\n",
       "            8.36449191e-02],\n",
       "          [ 1.24641120e-01,  1.01457663e-01, -1.05865046e-01,\n",
       "            2.62778662e-02, -9.23217908e-02,  2.72993334e-02,\n",
       "           -6.63280440e-03,  1.09415159e-01,  6.44654930e-02,\n",
       "           -5.95266782e-02],\n",
       "          [-1.57383785e-01, -7.53770862e-03,  1.20534502e-01,\n",
       "            3.56074460e-02, -1.19481571e-01,  8.19243565e-02,\n",
       "            1.72155857e-01,  1.70612521e-02,  6.02865927e-02,\n",
       "           -2.84028184e-02],\n",
       "          [ 1.81452166e-02, -7.89756700e-02, -6.85189515e-02,\n",
       "            2.98589021e-02,  1.27622470e-01, -2.96098031e-02,\n",
       "            1.49903595e-01, -1.49620384e-01,  5.24331145e-02,\n",
       "            1.77456066e-01],\n",
       "          [-1.00342497e-01,  1.54135093e-01,  2.18511477e-01,\n",
       "            5.91389015e-02,  1.73015326e-01, -1.15863383e-01,\n",
       "           -2.15712711e-01, -8.87726620e-02, -2.15742573e-01,\n",
       "           -6.22709394e-02],\n",
       "          [-8.80780965e-02, -1.48520932e-01, -3.39175873e-02,\n",
       "           -1.47602439e-01, -5.26952147e-02, -1.72419082e-02,\n",
       "           -1.77932322e-01, -1.45525590e-01,  1.47714972e-01,\n",
       "            6.36498109e-02],\n",
       "          [-2.40673404e-02,  1.79054625e-02, -1.14512183e-01,\n",
       "            9.48490016e-03,  6.46733586e-03, -3.94448042e-02,\n",
       "            5.88647686e-02,  2.04063609e-01, -3.25348675e-02,\n",
       "            1.22611679e-01],\n",
       "          [ 7.30698556e-02,  6.16005957e-02,  1.43148992e-02,\n",
       "           -1.83108856e-03, -1.04686283e-01, -9.93366241e-02,\n",
       "           -1.70938119e-01, -1.18930995e-01,  8.71630386e-02,\n",
       "            1.45819291e-01],\n",
       "          [ 4.42778692e-02,  9.28258970e-02,  1.42958211e-02,\n",
       "           -4.03711852e-03,  1.97738279e-02,  2.22641453e-01,\n",
       "           -7.89072886e-02,  1.60942331e-01, -5.63588925e-03,\n",
       "            5.40717579e-02],\n",
       "          [-8.09699073e-02,  2.33436357e-02, -6.38746619e-02,\n",
       "           -1.26720175e-01, -1.62958235e-01,  1.23562165e-01,\n",
       "            9.83533114e-02, -5.56362309e-02, -3.47247906e-02,\n",
       "           -5.98955452e-02]],\n",
       " \n",
       "         [[-8.37513953e-02, -9.82432589e-02,  6.98158219e-02,\n",
       "           -2.25838348e-01,  1.59346953e-01,  3.08765983e-03,\n",
       "            3.75548303e-02, -3.88259068e-02, -1.00911453e-01,\n",
       "           -1.03307657e-01],\n",
       "          [ 1.97575584e-01,  1.60355896e-01,  6.95593730e-02,\n",
       "           -4.14747894e-02, -3.35847326e-02,  3.82494517e-02,\n",
       "            2.49884147e-02, -2.25087963e-02, -2.31039189e-02,\n",
       "           -6.34907633e-02],\n",
       "          [-6.77883029e-02,  1.40978489e-02,  8.33611339e-02,\n",
       "           -8.49131793e-02,  5.40235154e-02,  2.21297175e-01,\n",
       "           -2.03892179e-02,  3.55404876e-02,  6.72701597e-02,\n",
       "            3.10638621e-02],\n",
       "          [ 1.23960897e-01,  8.72278214e-03, -5.74046839e-03,\n",
       "           -7.40369558e-02, -8.00102670e-03, -6.55037165e-02,\n",
       "           -1.79585844e-01,  1.39023392e-02,  2.24092484e-01,\n",
       "           -2.15746667e-02],\n",
       "          [-5.40797412e-02,  5.57266101e-02, -1.52766556e-01,\n",
       "           -1.43109813e-01, -1.50904417e-01,  1.52364790e-01,\n",
       "           -3.03326491e-02,  1.63265839e-02, -9.15180147e-02,\n",
       "            4.28685732e-02],\n",
       "          [-3.24619114e-02, -5.75355161e-03,  5.33844680e-02,\n",
       "           -2.72240937e-02,  1.15254737e-01, -1.07539475e-01,\n",
       "            1.05242670e-01,  2.19424456e-01, -2.09927540e-02,\n",
       "           -1.11736253e-01],\n",
       "          [ 7.27966204e-02, -7.76667073e-02, -8.13511088e-02,\n",
       "            1.46623403e-01, -1.23419903e-01, -1.91197414e-02,\n",
       "            3.72774303e-02, -1.70879792e-02, -1.75712302e-01,\n",
       "           -1.28107116e-01],\n",
       "          [-5.81778549e-02,  7.52703426e-03, -5.36224395e-02,\n",
       "           -1.53226420e-01, -1.76591828e-01,  7.51886219e-02,\n",
       "            1.83194727e-02, -1.66053548e-02,  1.25225857e-01,\n",
       "            7.61967450e-02],\n",
       "          [-6.47076368e-02,  7.81276226e-02,  5.90285659e-02,\n",
       "           -1.03489295e-01,  1.26920745e-01,  3.21165174e-02,\n",
       "            6.91256151e-02,  1.33773923e-01,  1.62373170e-01,\n",
       "           -1.52625337e-01],\n",
       "          [-3.38198338e-03,  9.60136950e-02, -1.03340603e-01,\n",
       "            2.15928163e-02, -1.72715947e-01, -6.54332489e-02,\n",
       "            1.34748304e-02,  1.84389338e-01,  2.14733124e-01,\n",
       "           -1.27636895e-01]]],\n",
       " \n",
       " \n",
       "        [[[-5.72487153e-02, -8.48052278e-02,  3.22265662e-02,\n",
       "           -5.78179918e-02,  1.28101677e-01,  1.58473939e-01,\n",
       "           -7.54220560e-02,  1.32093668e-01,  2.33383805e-01,\n",
       "           -1.15722366e-01],\n",
       "          [-1.02224788e-02, -1.60959110e-01, -2.02564567e-01,\n",
       "            1.11231148e-01,  8.12135357e-03,  4.37409021e-02,\n",
       "           -1.63944885e-02,  3.87283005e-02, -1.03581138e-01,\n",
       "            9.66289192e-02],\n",
       "          [ 1.70913041e-01, -9.86335799e-02, -2.22286731e-01,\n",
       "           -1.93155110e-01,  7.00056106e-02,  9.87530276e-02,\n",
       "           -3.02797779e-02, -3.65099832e-02, -3.35889868e-02,\n",
       "           -1.37804057e-02],\n",
       "          [-1.14387140e-01, -6.47734329e-02, -4.94480133e-03,\n",
       "           -1.34902552e-01,  1.18751340e-01,  9.33219790e-02,\n",
       "           -1.67215541e-01,  2.52969265e-02, -1.24690764e-01,\n",
       "            1.08907409e-01],\n",
       "          [-6.02494143e-02,  4.37940424e-03, -1.16560765e-01,\n",
       "            7.19733490e-03,  2.02703089e-01, -7.45186508e-02,\n",
       "           -1.44066596e-02, -3.16836722e-02,  1.24907687e-01,\n",
       "           -7.63059333e-02],\n",
       "          [-3.83481383e-02, -2.13824064e-02, -2.75953449e-02,\n",
       "            6.84097409e-02, -1.69583216e-01,  5.95436199e-03,\n",
       "           -1.00538835e-01, -1.17597524e-02, -8.25946555e-02,\n",
       "           -1.04783960e-01],\n",
       "          [ 1.03725612e-01,  1.56569656e-03, -8.72428864e-02,\n",
       "           -1.35807589e-01,  3.58727798e-02,  5.36341369e-02,\n",
       "           -5.95963299e-02, -1.02669545e-01, -1.92129537e-02,\n",
       "           -5.29571250e-03],\n",
       "          [ 1.66431412e-01, -8.50443915e-02, -3.68361216e-04,\n",
       "           -1.22271016e-01, -6.22071549e-02,  1.84024990e-01,\n",
       "           -2.06981655e-02,  5.10773808e-02, -1.64195761e-01,\n",
       "           -7.47016212e-03],\n",
       "          [-1.33918226e-01,  8.10595378e-02,  1.91381991e-01,\n",
       "            5.97120337e-02, -1.25407830e-01, -3.39757986e-02,\n",
       "            1.42521173e-01, -2.16524497e-01, -1.72460034e-01,\n",
       "            3.27481590e-02],\n",
       "          [-9.17129964e-02, -1.33029699e-01,  1.40553817e-01,\n",
       "            2.17585359e-02, -2.09670052e-01, -1.32320493e-01,\n",
       "            5.36734015e-02, -6.50676936e-02, -1.29806519e-01,\n",
       "            1.33882836e-01]],\n",
       " \n",
       "         [[ 1.77583709e-01,  7.08102807e-02, -3.56910601e-02,\n",
       "            2.93368306e-02,  1.22254997e-01,  1.12285845e-01,\n",
       "            1.02203555e-01, -9.37266573e-02,  4.18816768e-02,\n",
       "            8.75405744e-02],\n",
       "          [-5.05812056e-02, -4.99347895e-02, -1.37910038e-01,\n",
       "           -1.20067693e-01, -2.29363680e-01,  2.72239074e-02,\n",
       "            1.82496414e-01, -1.11918829e-01,  1.24363698e-01,\n",
       "           -2.42734049e-02],\n",
       "          [-1.51678041e-01, -3.48596200e-02, -8.25854391e-02,\n",
       "            5.61607741e-02,  3.47820632e-02,  1.52223960e-01,\n",
       "            1.13834448e-01,  7.94520974e-03,  1.10888965e-02,\n",
       "           -1.45252571e-01],\n",
       "          [ 1.35262772e-01, -1.50139228e-01,  2.75228396e-02,\n",
       "           -9.38156992e-03, -1.38097093e-01, -3.54256555e-02,\n",
       "            2.03926843e-02,  1.46144882e-01,  3.45589779e-02,\n",
       "            1.09135181e-01],\n",
       "          [-1.84741840e-02,  8.79635215e-02,  2.00954959e-01,\n",
       "           -6.98660091e-02, -1.77546404e-02,  7.23016262e-02,\n",
       "            4.78423573e-02,  4.67819646e-02, -1.49977699e-01,\n",
       "           -1.52793974e-02],\n",
       "          [-1.18439220e-01, -3.81074138e-02, -1.26110688e-01,\n",
       "           -8.90176464e-03, -1.29179403e-01, -2.09559157e-01,\n",
       "           -6.02095723e-02, -6.51390254e-02,  1.00703910e-01,\n",
       "            5.50648421e-02],\n",
       "          [-4.35692780e-02,  1.21340938e-01, -1.78980842e-01,\n",
       "            5.71243614e-02,  1.54734597e-01, -5.60330376e-02,\n",
       "           -2.92591006e-02,  6.46137297e-02,  4.10101824e-02,\n",
       "            1.21786036e-01],\n",
       "          [ 3.55715714e-02, -4.23233472e-02,  7.59725496e-02,\n",
       "            5.39392084e-02, -1.43719777e-01,  1.69608351e-02,\n",
       "           -2.08356440e-01,  1.04784913e-01,  1.04015715e-01,\n",
       "            6.46675229e-02],\n",
       "          [-2.75656898e-02,  1.00417562e-01, -9.97113660e-02,\n",
       "           -9.30047408e-02, -8.76960009e-02,  9.78812352e-02,\n",
       "           -1.82487652e-01,  9.78306960e-03,  6.39280584e-03,\n",
       "           -1.72535109e-03],\n",
       "          [-1.92937572e-02,  7.16361851e-02, -5.94575889e-02,\n",
       "            3.82559486e-02, -1.42993182e-01, -1.46934018e-01,\n",
       "            1.94515109e-01, -1.71059519e-01,  2.74748113e-02,\n",
       "            1.81733117e-01]],\n",
       " \n",
       "         [[ 8.81209504e-04,  4.02738042e-02, -8.63057077e-02,\n",
       "            1.08388431e-01,  1.55251175e-01, -1.16841279e-01,\n",
       "           -1.67092800e-01, -2.98405020e-03, -7.43149966e-02,\n",
       "           -2.19333440e-01],\n",
       "          [-1.61251381e-01,  5.02078272e-02, -8.63676593e-02,\n",
       "           -9.01486054e-02, -6.28710538e-02, -1.87068488e-02,\n",
       "           -4.93947789e-02, -2.34395582e-02, -9.83483046e-02,\n",
       "            1.65921822e-02],\n",
       "          [-7.74485320e-02, -4.57736403e-02, -1.02327608e-01,\n",
       "           -2.31521819e-02, -1.10409766e-01, -1.46637470e-01,\n",
       "           -1.04474314e-02,  2.04918012e-02, -5.89139052e-02,\n",
       "           -3.72169986e-02],\n",
       "          [ 6.97484463e-02,  9.64208320e-02, -1.37324080e-01,\n",
       "           -6.01743609e-02, -1.52207851e-01,  6.03301264e-02,\n",
       "            1.16438670e-02,  1.07263736e-01,  1.98158398e-01,\n",
       "           -3.21472064e-02],\n",
       "          [ 8.53320137e-02, -4.13481966e-02,  2.71352306e-02,\n",
       "           -1.31538287e-01, -7.74883777e-02,  2.63664778e-03,\n",
       "            2.34595202e-02, -1.05661817e-01,  1.67327136e-01,\n",
       "           -1.93270594e-01],\n",
       "          [-4.43720780e-02, -7.14359456e-04, -1.13732196e-01,\n",
       "            8.67425874e-02,  3.29676233e-02,  1.02667540e-01,\n",
       "           -1.29162585e-02, -4.07149680e-02, -1.27355844e-01,\n",
       "           -1.44696552e-02],\n",
       "          [-2.18618557e-01,  1.37471005e-01,  1.70343012e-01,\n",
       "           -1.51689779e-02,  1.86409056e-01,  1.71448976e-01,\n",
       "           -1.54014798e-02, -7.06073865e-02,  1.34535804e-01,\n",
       "            1.65892318e-01],\n",
       "          [ 2.73819696e-02,  1.61369368e-01,  9.99595001e-02,\n",
       "            2.73034461e-02,  7.36600719e-03, -5.33396639e-02,\n",
       "           -3.27817611e-02,  1.06058352e-01, -3.62744331e-02,\n",
       "            6.09628074e-02],\n",
       "          [-8.53351429e-02,  3.21087800e-02,  1.34820491e-01,\n",
       "            4.01169509e-02,  1.72940999e-01, -1.55232772e-01,\n",
       "            2.08530471e-01, -4.83142287e-02,  1.01973787e-01,\n",
       "           -6.35484159e-02],\n",
       "          [ 5.69912456e-02,  2.40827650e-01,  2.71893349e-02,\n",
       "            6.79329410e-02,  6.79738820e-02,  1.00607254e-01,\n",
       "            1.88058652e-02, -1.38892308e-01, -1.58151060e-01,\n",
       "            8.87979269e-02]]]], dtype=float32)>,\n",
       " <tf.Variable 'conv2d_222_1/bias:0' shape=(10,) dtype=float32, numpy=\n",
       " array([ 0.01038157,  0.01591234, -0.01501723, -0.00158185, -0.00270523,\n",
       "         0.00681001,  0.00779648,  0.01508327, -0.00130156,  0.01191309],\n",
       "       dtype=float32)>,\n",
       " <tf.Variable 'batch_normalization_252_1/gamma:0' shape=(10,) dtype=float32, numpy=\n",
       " array([1.0190768 , 1.0104896 , 0.9892408 , 0.9941015 , 0.9771624 ,\n",
       "        0.99164206, 1.0099045 , 0.9773337 , 1.0104694 , 1.0175258 ],\n",
       "       dtype=float32)>,\n",
       " <tf.Variable 'batch_normalization_252_1/beta:0' shape=(10,) dtype=float32, numpy=\n",
       " array([-0.01144655, -0.00905783,  0.02508394, -0.02210938, -0.01558204,\n",
       "        -0.0015242 ,  0.00679631,  0.00771992, -0.02037501,  0.01387537],\n",
       "       dtype=float32)>,\n",
       " <tf.Variable 'dense_91_1/kernel:0' shape=(490, 20) dtype=float32, numpy=\n",
       " array([[-0.01174289, -0.06218738, -0.00271733, ...,  0.01561758,\n",
       "         -0.02404902,  0.00484791],\n",
       "        [ 0.00962214,  0.05815508,  0.02876431, ..., -0.01578763,\n",
       "         -0.01768532, -0.01353296],\n",
       "        [ 0.04160998,  0.05566163, -0.06045888, ..., -0.04044367,\n",
       "          0.02588542,  0.05591514],\n",
       "        ...,\n",
       "        [ 0.09206723, -0.00655904, -0.00282526, ..., -0.06398024,\n",
       "         -0.05080422,  0.06562621],\n",
       "        [ 0.00476659, -0.01276326, -0.06191447, ...,  0.00530164,\n",
       "         -0.03213973, -0.02985386],\n",
       "        [ 0.0200663 ,  0.01595821, -0.06984008, ..., -0.03426492,\n",
       "          0.05033324,  0.0783935 ]], dtype=float32)>,\n",
       " <tf.Variable 'dense_91_1/bias:0' shape=(20,) dtype=float32, numpy=\n",
       " array([ 0.005835  , -0.00596507,  0.00147384, -0.00054252, -0.02727936,\n",
       "         0.00847889, -0.00048217, -0.01216589, -0.00139977, -0.00922385,\n",
       "         0.00526728,  0.02018436,  0.00739786,  0.00179655, -0.00392976,\n",
       "        -0.00871166,  0.01200207,  0.01996418, -0.01012331, -0.00982745],\n",
       "       dtype=float32)>,\n",
       " <tf.Variable 'batch_normalization_253_1/gamma:0' shape=(20,) dtype=float32, numpy=\n",
       " array([1.0260072 , 1.0223162 , 1.0300529 , 1.0317101 , 1.0334682 ,\n",
       "        1.020658  , 0.9955607 , 0.98868316, 0.99532115, 1.0237337 ,\n",
       "        1.0348066 , 1.0262543 , 1.0313947 , 1.0356615 , 0.99503124,\n",
       "        1.0198082 , 1.0200044 , 1.0432831 , 1.0317796 , 1.0112538 ],\n",
       "       dtype=float32)>,\n",
       " <tf.Variable 'batch_normalization_253_1/beta:0' shape=(20,) dtype=float32, numpy=\n",
       " array([-0.04179243, -0.04216946, -0.03859504, -0.04131984, -0.04142517,\n",
       "        -0.04254702, -0.04139746,  0.04165493, -0.040511  ,  0.0378389 ,\n",
       "         0.03711302, -0.04178489,  0.04159493,  0.04190646,  0.0401951 ,\n",
       "        -0.04103273, -0.04173254,  0.04161543, -0.04261713, -0.04052645],\n",
       "       dtype=float32)>,\n",
       " <tf.Variable 'dense_92_1/kernel:0' shape=(20, 20) dtype=float32, numpy=\n",
       " array([[ 2.99599916e-01, -2.73134381e-01, -4.06697780e-01,\n",
       "          4.72388417e-01, -1.46952599e-01, -1.91827238e-01,\n",
       "          1.74954385e-01, -4.26740080e-01,  6.10167794e-02,\n",
       "         -2.56492108e-01, -4.33208235e-02, -2.69241661e-01,\n",
       "          6.27774596e-02, -4.28875275e-02,  2.99391866e-01,\n",
       "         -2.05162778e-01, -9.27182008e-03, -2.49004960e-01,\n",
       "          4.66867805e-01,  2.16461703e-01],\n",
       "        [ 1.33018151e-01, -3.75863642e-01, -1.80779815e-01,\n",
       "         -3.08480769e-01,  3.36188257e-01,  4.84300591e-02,\n",
       "          2.21055061e-01, -1.21558970e-02,  1.35197369e-02,\n",
       "          2.89225936e-01, -4.06939924e-01,  2.36521006e-01,\n",
       "          4.27917421e-01,  1.02636382e-01,  2.14320898e-01,\n",
       "          3.64725408e-03,  2.90411681e-01,  2.90761665e-02,\n",
       "         -8.30228031e-02,  1.00582838e-03],\n",
       "        [ 3.02828610e-01, -1.06324337e-01,  3.40818077e-01,\n",
       "          2.12999940e-01, -4.16552238e-02,  5.70539124e-02,\n",
       "          6.14924058e-02,  1.05800681e-01, -4.92577255e-02,\n",
       "         -3.12945247e-01, -2.70478964e-01, -2.07692042e-01,\n",
       "          4.00464892e-01, -1.68190166e-01, -6.27418533e-02,\n",
       "         -1.68293640e-01, -4.20632899e-01, -4.07618999e-01,\n",
       "          1.01577364e-01, -8.14381894e-03],\n",
       "        [-3.30091894e-01, -4.51181620e-01, -3.49830091e-01,\n",
       "          1.96558580e-01, -3.66561383e-01, -9.77515951e-02,\n",
       "          3.35935384e-01, -1.63142920e-01,  7.07920864e-02,\n",
       "         -2.03003660e-01, -2.82687843e-01,  1.93242520e-01,\n",
       "          3.32783610e-01, -4.83936481e-02, -3.56034219e-01,\n",
       "          7.43166804e-02, -2.80525178e-01, -3.08047235e-02,\n",
       "          1.14341654e-01,  4.07730550e-01],\n",
       "        [-3.23909432e-01,  1.77694649e-01, -1.35667861e-01,\n",
       "          7.87703544e-02, -2.95743942e-01, -9.72761512e-02,\n",
       "          1.99221820e-01, -6.43795952e-02,  3.69074456e-02,\n",
       "         -3.15872192e-01, -8.80443901e-02, -1.54430106e-01,\n",
       "         -2.26384532e-02, -1.40667766e-01, -2.42293719e-02,\n",
       "         -2.70222217e-01, -2.10120469e-01,  1.45511791e-01,\n",
       "         -1.83593899e-01,  1.34862006e-01],\n",
       "        [ 1.58552695e-02, -1.34397164e-01,  1.86190963e-01,\n",
       "          7.46620819e-02,  4.12022591e-01, -3.22353870e-01,\n",
       "          1.10812582e-01, -3.41550112e-01,  2.06524059e-01,\n",
       "         -6.44949973e-02,  8.51373374e-02, -1.01333268e-01,\n",
       "          1.37194440e-01, -6.66373298e-02, -1.58831552e-01,\n",
       "          4.60253693e-02, -3.79694551e-01, -1.09755762e-01,\n",
       "         -1.83818804e-03,  3.24807823e-01],\n",
       "        [-1.14777729e-01, -1.02372281e-01, -2.57812440e-01,\n",
       "          2.67992914e-01, -3.76671582e-01,  6.18517697e-02,\n",
       "         -2.74833411e-01, -3.92653614e-01, -7.66408145e-02,\n",
       "          1.75409332e-01,  5.64987026e-03,  1.22065477e-01,\n",
       "          1.06465518e-01,  4.64581996e-02, -9.66835991e-02,\n",
       "          1.19479358e-01,  2.77958542e-01, -3.27194333e-01,\n",
       "         -8.12570751e-02, -4.36380366e-03],\n",
       "        [-4.23274487e-01,  3.34854096e-01, -1.51443794e-01,\n",
       "         -2.12288961e-01,  1.90088093e-01,  3.13372426e-02,\n",
       "         -4.60065067e-01,  7.01170489e-02, -4.69130546e-01,\n",
       "         -7.13685900e-02,  1.62651703e-01, -1.13034986e-01,\n",
       "         -2.94327736e-01, -2.41295691e-03,  1.44040622e-02,\n",
       "          2.67078757e-01,  2.09789854e-02,  1.24401778e-01,\n",
       "          2.30498388e-01,  1.88573942e-01],\n",
       "        [-4.62124534e-02,  6.50997981e-02,  1.78328887e-01,\n",
       "          4.91204828e-01, -3.17882717e-01,  4.37086940e-01,\n",
       "          2.74932444e-01,  2.08560318e-01, -2.51891892e-02,\n",
       "         -1.19307354e-01, -2.91085869e-01, -3.66291165e-01,\n",
       "         -2.00023472e-01, -1.58861816e-01,  1.76564101e-02,\n",
       "         -2.70787664e-02,  2.21220292e-02,  3.84220332e-01,\n",
       "          3.32995862e-01,  3.87204856e-01],\n",
       "        [-4.22428280e-01,  7.77456611e-02, -2.48735934e-01,\n",
       "          6.48518503e-02,  1.29164442e-01, -1.08281210e-01,\n",
       "         -1.58385739e-01, -1.35851741e-01, -1.50341354e-02,\n",
       "          1.48870692e-01,  3.28828722e-01,  1.70669094e-01,\n",
       "         -2.90140174e-02, -1.18238561e-01,  6.30835891e-02,\n",
       "          8.50695651e-03,  5.07044941e-02,  4.70533445e-02,\n",
       "         -1.15768760e-01, -3.04502249e-01],\n",
       "        [-1.26281947e-01, -3.57319593e-01,  3.99587929e-01,\n",
       "          3.21240515e-01, -2.32160673e-01,  3.17092389e-01,\n",
       "          1.64929554e-01, -1.68221354e-01, -1.29195139e-01,\n",
       "          4.08921055e-02, -1.02015920e-01,  6.65706694e-02,\n",
       "         -1.45824449e-02,  4.92834091e-01, -8.89570042e-02,\n",
       "         -2.48078108e-01,  2.98042059e-01,  1.06556058e-01,\n",
       "          4.46292102e-01,  6.21193573e-02],\n",
       "        [ 3.23827684e-01, -2.51074910e-01,  3.21160346e-01,\n",
       "         -1.25917301e-01, -1.72094882e-01, -1.34836659e-01,\n",
       "         -9.23023745e-03,  2.66747177e-01,  1.43714786e-01,\n",
       "          1.61287021e-02, -3.66878770e-02,  2.21647248e-01,\n",
       "          1.12739526e-01, -3.12222838e-01,  4.65259671e-01,\n",
       "          3.65342110e-01,  8.79125744e-02,  2.34022960e-01,\n",
       "          1.59147382e-02,  8.87308717e-02],\n",
       "        [ 3.10066760e-01,  1.25016585e-01,  2.43060187e-01,\n",
       "         -1.06794141e-01,  1.11823864e-01,  4.19204295e-01,\n",
       "         -3.95935029e-02,  1.74473196e-01, -1.37178645e-01,\n",
       "         -9.45263579e-02,  3.04615289e-01, -9.39097777e-02,\n",
       "          4.30877507e-02,  2.84977168e-01, -4.90389317e-01,\n",
       "          1.78002879e-01, -2.60390401e-01, -4.36757207e-01,\n",
       "          3.10625851e-01,  3.50128114e-03],\n",
       "        [-2.24359930e-02,  1.97580159e-01, -4.01191145e-01,\n",
       "         -1.60192415e-01, -3.39163363e-01,  5.21971360e-02,\n",
       "         -4.28944200e-01, -4.94456701e-02,  7.81828165e-02,\n",
       "         -7.92436302e-02,  1.52372733e-01, -3.75381321e-01,\n",
       "          4.24793325e-02,  4.05663788e-01, -2.17345104e-01,\n",
       "         -1.64412156e-01,  1.78612843e-01,  2.21970960e-01,\n",
       "          5.77402487e-02,  4.89510037e-02],\n",
       "        [ 8.36248770e-02,  3.12338173e-01,  7.18631595e-02,\n",
       "         -4.62243915e-01, -2.29932904e-01,  1.28200531e-01,\n",
       "         -6.61311448e-02, -2.00959429e-01,  3.77116054e-01,\n",
       "         -2.66955376e-01, -4.90259677e-01,  1.17273875e-01,\n",
       "          1.14477262e-01,  1.26999661e-01, -3.23949844e-01,\n",
       "         -2.26480231e-01, -3.58969420e-01,  1.29982218e-01,\n",
       "          2.38661408e-01,  1.36646762e-01],\n",
       "        [ 2.93212384e-01, -1.81641251e-01, -4.02715832e-01,\n",
       "          3.39606702e-01, -3.31700593e-02, -6.24993667e-02,\n",
       "         -2.77108878e-01,  7.62630533e-03,  1.17902957e-01,\n",
       "          4.93169576e-01, -3.67985547e-01, -2.38725245e-01,\n",
       "          2.68760234e-01, -1.92781344e-01,  8.88125878e-03,\n",
       "          2.40026802e-01,  2.75674164e-01, -7.12501183e-02,\n",
       "          3.18734616e-01,  4.27052081e-01],\n",
       "        [ 1.83172241e-01,  1.79302588e-01, -1.41301066e-01,\n",
       "          1.98317185e-01, -2.70613372e-01, -1.82698503e-01,\n",
       "          3.24099511e-01,  8.61837938e-02,  5.39500453e-03,\n",
       "          3.67110163e-01,  1.38997704e-01, -4.60291088e-01,\n",
       "         -4.63579595e-03,  2.12024242e-01, -2.57564157e-01,\n",
       "          1.03451009e-03,  1.14634059e-01, -1.46756366e-01,\n",
       "          2.10598081e-01, -2.12853588e-03],\n",
       "        [ 2.81699687e-01,  4.44486290e-02,  5.38320243e-01,\n",
       "          8.70354325e-02,  1.61969274e-01,  2.38970667e-01,\n",
       "         -8.01527053e-02,  1.63299501e-01, -2.78860837e-01,\n",
       "          1.24063306e-01,  3.05478007e-01, -1.12916537e-01,\n",
       "         -1.41824126e-01, -2.00520039e-01, -1.82932708e-04,\n",
       "         -2.20972791e-01, -2.47263640e-01, -1.63838327e-01,\n",
       "         -1.18211873e-01, -1.28540605e-01],\n",
       "        [ 1.47211641e-01, -2.96687037e-01,  8.34182650e-02,\n",
       "          3.98750193e-02,  1.60176709e-01, -1.66421175e-01,\n",
       "          1.65672526e-01, -2.36915290e-01, -7.81371370e-02,\n",
       "         -4.31378074e-02,  2.44673520e-01, -3.89729500e-01,\n",
       "          3.65015745e-01, -2.21627980e-01, -3.04115172e-02,\n",
       "         -4.53926474e-02,  1.85000122e-01,  8.84144660e-03,\n",
       "         -1.23869099e-01,  2.94539961e-03],\n",
       "        [-1.68392196e-01,  9.21700597e-02, -1.69607639e-01,\n",
       "          3.28257263e-01, -6.13275319e-02,  2.51490176e-01,\n",
       "          3.49042505e-01,  5.84801193e-03,  1.31148696e-01,\n",
       "          9.43008065e-02,  2.11534873e-01, -1.18140668e-01,\n",
       "         -2.21414223e-01, -3.67708355e-01, -1.90030560e-01,\n",
       "          3.26367654e-03,  2.06575245e-02,  1.57077566e-01,\n",
       "         -7.29260445e-02, -3.35043631e-02]], dtype=float32)>,\n",
       " <tf.Variable 'dense_92_1/bias:0' shape=(20,) dtype=float32, numpy=\n",
       " array([ 0.04163145,  0.0424313 ,  0.03932239, -0.04048116,  0.04493926,\n",
       "         0.04437481, -0.04125374, -0.03716297, -0.04571662,  0.03248229,\n",
       "         0.04089603,  0.04193347, -0.04243973,  0.04086624, -0.01354346,\n",
       "        -0.04182826, -0.03560619, -0.03794827,  0.02598363, -0.04027464],\n",
       "       dtype=float32)>,\n",
       " <tf.Variable 'dense_93_1/kernel:0' shape=(20, 1) dtype=float32, numpy=\n",
       " array([[ 0.06549514],\n",
       "        [ 0.06548209],\n",
       "        [ 0.19249259],\n",
       "        [-0.13684149],\n",
       "        [ 0.02536809],\n",
       "        [ 0.02907612],\n",
       "        [-0.09040824],\n",
       "        [-0.02064584],\n",
       "        [-0.03878889],\n",
       "        [-0.00501134],\n",
       "        [ 0.07914264],\n",
       "        [ 0.03651865],\n",
       "        [-0.05365163],\n",
       "        [ 0.08613516],\n",
       "        [-0.01720366],\n",
       "        [-0.0727512 ],\n",
       "        [-0.01283225],\n",
       "        [-0.01932631],\n",
       "        [-0.0084159 ],\n",
       "        [-0.13415384]], dtype=float32)>,\n",
       " <tf.Variable 'dense_93_1/bias:0' shape=(1,) dtype=float32, numpy=array([0.03977264], dtype=float32)>,\n",
       " <tf.Variable 'batch_normalization_246_1/moving_mean:0' shape=(10,) dtype=float32, numpy=\n",
       " array([-1.9394011e-03,  6.5676626e-03, -3.0744933e-03,  1.6120675e-03,\n",
       "         2.7616338e-03, -4.5280424e-03, -7.2699571e-03,  7.1180140e-05,\n",
       "         2.9944615e-03,  4.5653118e-04], dtype=float32)>,\n",
       " <tf.Variable 'batch_normalization_246_1/moving_variance:0' shape=(10,) dtype=float32, numpy=\n",
       " array([0.66981816, 0.66968334, 0.66985476, 0.66994005, 0.6692672 ,\n",
       "        0.6695408 , 0.6694257 , 0.67015505, 0.66956013, 0.67049336],\n",
       "       dtype=float32)>,\n",
       " <tf.Variable 'batch_normalization_247_1/moving_mean:0' shape=(20,) dtype=float32, numpy=\n",
       " array([ 0.02320888,  0.00119234,  0.01103519, -0.02204275,  0.00114479,\n",
       "         0.01997797, -0.03331695,  0.00912547, -0.01028076,  0.00296711,\n",
       "         0.04372228,  0.04560302, -0.01411749,  0.02751859,  0.00152295,\n",
       "         0.0056313 ,  0.00965947, -0.00943227,  0.04061076,  0.03658664],\n",
       "       dtype=float32)>,\n",
       " <tf.Variable 'batch_normalization_247_1/moving_variance:0' shape=(20,) dtype=float32, numpy=\n",
       " array([0.7941357 , 0.80539536, 0.8162422 , 1.0574975 , 0.7305754 ,\n",
       "        0.8919972 , 0.89699066, 0.8758905 , 0.7930839 , 0.8168984 ,\n",
       "        0.8905547 , 0.92217135, 0.83876693, 0.75957054, 0.7470627 ,\n",
       "        0.81294066, 0.80272686, 0.915586  , 0.8367836 , 0.76386935],\n",
       "       dtype=float32)>,\n",
       " <tf.Variable 'batch_normalization_248_1/moving_mean:0' shape=(10,) dtype=float32, numpy=\n",
       " array([ 0.04688541, -0.00420344,  0.08072429,  0.05427508,  0.07498758,\n",
       "        -0.00758899, -0.0193714 , -0.00884765,  0.03826338,  0.01565486],\n",
       "       dtype=float32)>,\n",
       " <tf.Variable 'batch_normalization_248_1/moving_variance:0' shape=(10,) dtype=float32, numpy=\n",
       " array([0.98662394, 1.0902193 , 1.036755  , 0.96188647, 1.075315  ,\n",
       "        1.5192846 , 1.324617  , 0.9831705 , 0.86818933, 1.1354446 ],\n",
       "       dtype=float32)>,\n",
       " <tf.Variable 'batch_normalization_249_1/moving_mean:0' shape=(10,) dtype=float32, numpy=\n",
       " array([ 0.04117984,  0.06483054,  0.01762968,  0.01609576, -0.00474573,\n",
       "         0.04773308,  0.04909327,  0.04584774,  0.03018713,  0.02709959],\n",
       "       dtype=float32)>,\n",
       " <tf.Variable 'batch_normalization_249_1/moving_variance:0' shape=(10,) dtype=float32, numpy=\n",
       " array([0.8711536 , 0.8559602 , 0.8900384 , 2.071681  , 1.5578189 ,\n",
       "        0.8050348 , 0.85339046, 0.8622375 , 0.8862746 , 1.4472008 ],\n",
       "       dtype=float32)>,\n",
       " <tf.Variable 'batch_normalization_250_1/moving_mean:0' shape=(10,) dtype=float32, numpy=\n",
       " array([0.0657252 , 0.04822057, 0.04766611, 0.06729601, 0.04711656,\n",
       "        0.10427711, 0.03294663, 0.05745428, 0.04116364, 0.04767036],\n",
       "       dtype=float32)>,\n",
       " <tf.Variable 'batch_normalization_250_1/moving_variance:0' shape=(10,) dtype=float32, numpy=\n",
       " array([0.85310245, 0.7784063 , 0.753104  , 0.84719384, 0.9923937 ,\n",
       "        1.1230441 , 1.2785518 , 0.84159225, 0.89087677, 0.8092762 ],\n",
       "       dtype=float32)>,\n",
       " <tf.Variable 'batch_normalization_251_1/moving_mean:0' shape=(10,) dtype=float32, numpy=\n",
       " array([0.10666236, 0.10472652, 0.12745905, 0.15674022, 0.12762395,\n",
       "        0.11009295, 0.1082961 , 0.1370659 , 0.10480997, 0.10489315],\n",
       "       dtype=float32)>,\n",
       " <tf.Variable 'batch_normalization_251_1/moving_variance:0' shape=(10,) dtype=float32, numpy=\n",
       " array([0.8387572 , 0.8790204 , 0.935405  , 1.6025075 , 0.8837194 ,\n",
       "        1.0779358 , 0.9261465 , 1.3497232 , 0.96305126, 0.9070433 ],\n",
       "       dtype=float32)>,\n",
       " <tf.Variable 'batch_normalization_252_1/moving_mean:0' shape=(10,) dtype=float32, numpy=\n",
       " array([0.19127442, 0.2076104 , 0.20707585, 0.18812068, 0.32215562,\n",
       "        0.24842939, 0.18262662, 0.2079789 , 0.2627139 , 0.24776393],\n",
       "       dtype=float32)>,\n",
       " <tf.Variable 'batch_normalization_252_1/moving_variance:0' shape=(10,) dtype=float32, numpy=\n",
       " array([0.8686013 , 1.0528005 , 0.90167344, 0.8127699 , 1.1835014 ,\n",
       "        1.1651387 , 0.8897972 , 0.9255455 , 1.2347124 , 1.5186111 ],\n",
       "       dtype=float32)>,\n",
       " <tf.Variable 'batch_normalization_253_1/moving_mean:0' shape=(20,) dtype=float32, numpy=\n",
       " array([ 0.03885742, -0.05981798, -0.02331911, -0.04693062,  0.00581599,\n",
       "         0.02036424, -0.00773874,  0.05292863, -0.01915769, -0.017059  ,\n",
       "         0.03897826, -0.02728937, -0.03147633,  0.00087034,  0.00684528,\n",
       "        -0.00420183,  0.05794048,  0.01339531,  0.05333511,  0.03484641],\n",
       "       dtype=float32)>,\n",
       " <tf.Variable 'batch_normalization_253_1/moving_variance:0' shape=(20,) dtype=float32, numpy=\n",
       " array([1.1118042 , 1.0466352 , 1.066351  , 1.082121  , 1.0597026 ,\n",
       "        1.1042202 , 1.0175657 , 1.1365969 , 0.96844643, 1.0403498 ,\n",
       "        1.1054975 , 1.1156838 , 1.0050486 , 1.131593  , 1.0983351 ,\n",
       "        1.1673137 , 1.132628  , 1.041215  , 1.0566562 , 1.1330907 ],\n",
       "       dtype=float32)>]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnns[1].weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/45693 [00:00<?, ?it/s]\n",
      "  0%|          | 0/43033 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prev:  0 , now:  2000 , size 2000\n",
      "[383, 383, 383, 383, 383, 85]\n",
      "Lambda: 0\n",
      "0 383\n",
      "0 0\n",
      "383 766\n",
      "0 0\n",
      "766 1149\n",
      "0 0\n",
      "1149 1532\n",
      "0 0\n",
      "1532 1915\n",
      "0 0\n",
      "1915 2000\n",
      "0 85\n",
      "validating\n",
      "2000 2660\n",
      "Lambda: 0.001\n",
      "0 383\n",
      "0 0\n",
      "383 766\n",
      "0 0\n",
      "766 1149\n",
      "0 0\n",
      "1149 1532\n",
      "0 0\n",
      "1532 1915\n",
      "0 0\n",
      "1915 2000\n",
      "0 85\n",
      "validating\n",
      "2000 2660\n",
      "Test\n",
      "2660 48353\n",
      "prev:  2000 , now:  4000 , size 2000\n",
      "[383, 383, 383, 383, 383, 85]\n",
      "Lambda: 0\n",
      "2000 2383\n",
      "0 0\n",
      "2383 2766\n",
      "0 0\n",
      "2766 3149\n",
      "0 0\n",
      "3149 3532\n",
      "0 0\n",
      "3532 3915\n",
      "0 0\n",
      "3915 4000\n",
      "0 85\n",
      "validating\n",
      "4000 5320\n",
      "Lambda: 0.001\n",
      "2000 2383\n",
      "0 0\n",
      "2383 2766\n",
      "0 0\n",
      "2766 3149\n",
      "0 0\n",
      "3149 3532\n",
      "0 0\n",
      "3532 3915\n",
      "0 0\n",
      "3915 4000\n",
      "0 85\n",
      "validating\n",
      "4000 5320\n",
      "Test\n",
      "5320 48353\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# CNN: support batching\n",
    "prev_sample = 0\n",
    "# number_samples = [120, 200, 700]\n",
    "lambda_vec = [0, 0.001]  #0.003, 0.01, 0.03, 0.1, 0.3, 1, 3, 10\n",
    "average_diff_power, fp_mean_power = [], []\n",
    "cnns = [cnn_model(10, lamb, 0) for lamb in lambda_vec]\n",
    "for cnn in cnns:\n",
    "    cnn.compile(loss='mean_squared_error', optimizer='adam', metrics=['mse','mae'])\n",
    "for number_sample in number_samples:\n",
    "    current_sample = number_sample - prev_sample\n",
    "    print(\"prev: \", prev_sample, \", now: \", number_sample, \", size\", current_sample) \n",
    "    train_samples = [batch_size] * (current_sample//batch_size) + ([current_sample%batch_size] if \n",
    "                                                                    current_sample%batch_size else [])\n",
    "    print(train_samples)\n",
    "    \n",
    "    min_error = float('inf')\n",
    "    best_model, best_lam = None, None\n",
    "    for lamb_idx, lamb in enumerate(lambda_vec):\n",
    "        print(\"Lambda:\", lamb)\n",
    "#         cnn = cnn_model(10, lamb, 0)\n",
    "#         cnn.compile(loss='mean_squared_error', optimizer='adam', metrics=['mse','mae'])\n",
    "        \n",
    "        # training on all batches\n",
    "                                    \n",
    "        for i, train_sample in enumerate(train_samples):\n",
    "            for image_num in range(prev_sample + i * batch_size, prev_sample + i * batch_size + train_sample):\n",
    "                print(prev_sample + i * batch_size, prev_sample + i * batch_size + train_sample)\n",
    "                print((prev_sample + i * batch_size - prev_sample) % batch_size, \n",
    "                      (prev_sample + i * batch_size + train_sample - prev_sample)% batch_size)\n",
    "                break\n",
    "\n",
    "        \n",
    "        # validating\n",
    "        print(\"validating\")\n",
    "        val_size = math.ceil(number_sample * validation_size)\n",
    "        for image_num in range(val_size):\n",
    "            print(number_sample, val_size + number_sample)\n",
    "            break\n",
    "     \n",
    "    print(\"Test\") \n",
    "    \n",
    "    # evaluating test images\n",
    "\n",
    "    \n",
    "    for test_num in tqdm.tqdm(range(number_sample + val_size, data_reg.shape[0])):\n",
    "        print(number_sample + val_size, data_reg.shape[0])\n",
    "        break\n",
    "    prev_sample = number_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_f = open('/'.join(image_dir.split('/')[:-1]) +  '/' + 'best_cnn_4000samples' + intensity_degradation + '_' + str(slope) + '_' + \n",
    "                 dtime + \".dat\", \"wb\") # file for saving results\n",
    "pickle.dump(best_model, file=var_f)\n",
    "var_f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/6 [00:00<?, ?it/s]\n",
      "0it [00:00, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lambda: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1it [00:57, 57.66s/it]\u001b[A\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "1it [02:23, 143.92s/it]\u001b[A\n",
      "2it [04:46, 143.57s/it]\u001b[A\n",
      "3it [07:09, 143.27s/it]\u001b[A\n",
      "4it [09:32, 143.17s/it]\u001b[A\n",
      "5it [10:23, 124.68s/it]\u001b[A\n",
      " 17%|█▋        | 1/6 [11:23<56:58, 683.79s/it]\n",
      "0it [00:00, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.211368327594533\n",
      "Lambda: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1it [00:56, 56.09s/it]\u001b[A\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "1it [02:19, 139.20s/it]\u001b[A\n",
      "2it [04:38, 139.22s/it]\u001b[A\n",
      "3it [06:57, 139.21s/it]\u001b[A\n",
      "4it [09:18, 139.72s/it]\u001b[A\n",
      "5it [10:08, 121.72s/it]\u001b[A\n",
      " 33%|███▎      | 2/6 [22:31<45:15, 678.85s/it]\n",
      "0it [00:00, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.379879925942782\n",
      "Lambda: 0.003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1it [00:58, 58.13s/it]\u001b[A\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "1it [02:24, 144.03s/it]\u001b[A\n",
      "2it [04:48, 144.30s/it]\u001b[A\n",
      "3it [07:14, 144.74s/it]\u001b[A\n",
      "4it [09:39, 144.71s/it]\u001b[A\n",
      "5it [10:31, 126.24s/it]\u001b[A\n",
      " 50%|█████     | 3/6 [34:03<34:08, 682.78s/it]\n",
      "0it [00:00, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.986079897483191\n",
      "Lambda: 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1it [00:56, 56.09s/it]\u001b[A\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "1it [02:18, 138.72s/it]\u001b[A\n",
      "2it [04:37, 138.66s/it]\u001b[A\n",
      "3it [06:56, 138.75s/it]\u001b[A\n",
      "4it [09:14, 138.70s/it]\u001b[A\n",
      "5it [10:04, 120.90s/it]\u001b[A\n",
      " 67%|██████▋   | 4/6 [45:06<22:33, 676.92s/it]\n",
      "0it [00:00, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.314812567979636\n",
      "Lambda: 0.03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1it [00:55, 55.93s/it]\u001b[A\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "1it [02:18, 138.30s/it]\u001b[A\n",
      "2it [04:36, 138.31s/it]\u001b[A\n",
      "3it [06:54, 138.26s/it]\u001b[A\n",
      "4it [09:12, 138.16s/it]\u001b[A\n",
      "5it [10:02, 120.43s/it]\u001b[A\n",
      " 83%|████████▎ | 5/6 [56:06<11:12, 672.05s/it]\n",
      "0it [00:00, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.257873288155691\n",
      "Lambda: 0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1it [00:56, 56.58s/it]\u001b[A\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "1it [02:20, 140.02s/it]\u001b[A\n",
      "2it [04:40, 140.21s/it]\u001b[A\n",
      "3it [07:00, 140.04s/it]\u001b[A\n",
      "4it [09:20, 139.96s/it]\u001b[A\n",
      "5it [10:10, 122.09s/it]\u001b[A\n",
      "100%|██████████| 6/6 [1:07:16<00:00, 672.77s/it]\n",
      "  0%|          | 27/41779 [00:00<02:35, 269.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.043452635827206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41779/41779 [02:38<00:00, 263.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number_samples:  2000 , average_error:  6.545  fp_average_error:  3.067\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "## use self-training\n",
    "unlabeled_train_samples = [batch_size] * (len(y_test_p)//batch_size) + ([len(y_test_p)%batch_size] if len(y_test_p)%batch_size else [])\n",
    "labeled_train_samples = [batch_size] * (number_sample//batch_size) + ([number_sample%batch_size] if number_sample%batch_size else [])   \n",
    "min_min_error = float('inf')\n",
    "best_best_model, best_best_lam = None, None\n",
    "for lamb in tqdm.tqdm(lambda_vec):\n",
    "    print(\"Lambda:\", lamb)\n",
    "    cnn = cnn_model(10, lamb, 0)\n",
    "    cnn.compile(loss='mean_squared_error', optimizer='adam', metrics=['mse','mae'])\n",
    "#     cnn.compile(loss='mean_squared_error', optimizer='adam', metrics=['mse','mae'])\n",
    "        \n",
    "    # training on all batches\n",
    "    # training on all batches\n",
    "    for i, train_sample in tqdm.tqdm(enumerate(labeled_train_samples)):\n",
    "        x_train = np.empty((train_sample, number_image_channels, max_x, max_y), dtype=float_memory_used)\n",
    "        y_train = np.empty((train_sample), dtype=float_memory_used)\n",
    "        for image_num in range(i * batch_size, i * batch_size + train_sample):\n",
    "            x_train[image_num % batch_size] = read_image(image_num)\n",
    "            y_train[image_num % batch_size] = np.asarray(data_reg[image_num][-1], dtype=float_memory_used)\n",
    "        cnn.fit(x_train, y_train, epochs=6, verbose=0, batch_size=1, validation_split=0.0)\n",
    "        del x_train, y_train\n",
    "            \n",
    "    for i, train_sample in tqdm.tqdm(enumerate(unlabeled_train_samples)):\n",
    "        x_train = np.empty((train_sample, number_image_channels, max_x, max_y), dtype=float_memory_used)\n",
    "        y_train = np.empty((train_sample), dtype=float_memory_used)\n",
    "        for image_num in range(i * batch_size + number_sample + val_size, i * batch_size + number_sample + val_size + train_sample):\n",
    "            x_train[(image_num-number_sample - val_size) % batch_size] = read_image(image_num)\n",
    "            y_train[(image_num-number_sample - val_size) % batch_size] = np.asarray(y_test_p[image_num-(number_sample + val_size)], dtype=float_memory_used)\n",
    "        cnn.fit(x_train, y_train, epochs=3, verbose=0, batch_size=1, validation_split=0.0)\n",
    "        del x_train, y_train\n",
    "        \n",
    "    # validating\n",
    "    val_size = math.ceil(number_sample * validation_size)\n",
    "    val_mae, val_fp_mae = 0.0, 0.0\n",
    "    for image_num in range(val_size):\n",
    "        val_y = data_reg[image_num + number_sample][-1]\n",
    "        image = read_image(image_num + number_sample)\n",
    "        val_yp = cnn.predict(image)[0][0]\n",
    "        val_mae += abs(val_y - val_yp)\n",
    "        if val_yp > val_y:\n",
    "            val_fp_mae += abs(val_yp - val_y)\n",
    "    val_mae /= val_size\n",
    "    val_fp_mae /= val_size\n",
    "    print(val_mae)\n",
    "    if val_mae < min_min_error:\n",
    "        min_min_error = val_mae\n",
    "        best_best_model = cnn\n",
    "        best_best_lam = lamb\n",
    "    sum_mae, sum_fp_mae = 0, 0\n",
    "    test_size = 0\n",
    "    \n",
    "for test_num in tqdm.tqdm(range(number_sample + val_size, data_reg.shape[0])):\n",
    "    test_size += 1\n",
    "    test_image = read_image(test_num)\n",
    "    test_y = data_reg[test_num][-1]\n",
    "    test_yp = best_best_model.predict(test_image)[0][0]\n",
    "#     y_test_p[test_num - (number_sample + val_size)] = test_yp\n",
    "    sum_mae += abs(test_yp - test_y)\n",
    "    if test_yp > test_y:\n",
    "        sum_fp_mae += abs(test_yp - test_y)\n",
    "fp_mean_power.append(round(sum_fp_mae/ test_size, 3))\n",
    "average_diff_power.append(round(sum_mae / test_size, 3))\n",
    "print('number_samples: ', number_sample, ', average_error: ', average_diff_power[-1], ' fp_average_error: ', \n",
    "      fp_mean_power[-1])\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "6.285, 6.366, 6.45, 6.454, 6.382, 6.26, 6.49, 6.224, 6.052, 5.87, 4.915, 4.836"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN\n",
    "prev_sample = 0\n",
    "lambda_vec = [0, 0.001, 0.003, 0.01, 0.03, 0.1]\n",
    "max_train_samples = math.ceil(number_samples[-1] * (1 + validation_size))\n",
    "x_train = np.empty((max_train_samples, number_image_channels, max_x, max_y), dtype=float_memory_used)\n",
    "# x_train1 = np.empty((max_train_samples, 1, max_x, max_y), dtype=float_memory_used)\n",
    "# x_train2 = np.empty((max_train_samples, 1, max_x, max_y), dtype=float_memory_used)\n",
    "y_train = np.empty((max_train_samples), dtype=float_memory_used)\n",
    "average_diff_power, fp_mean_power = [], []\n",
    "for number_sample in number_samples:\n",
    "    sample = math.ceil(number_sample * (1 + validation_size))\n",
    "    for image_num in range(prev_sample, sample):\n",
    "        prev_sample = sample\n",
    "        if style == \"image_intensity\":\n",
    "            image = plt.imread(image_dir + '/image' + str(image_num)+'.png')\n",
    "            image = np.swapaxes(image, 0, 2)\n",
    "            x_train[image_num] = np.array(image[:number_image_channels], dtype=float_memory_used).reshape(1, number_image_channels, max_x, max_y)\n",
    "            del image\n",
    "        elif  style == \"raw_power_min_max_norm\" or style == \"raw_power_zscore_norm\":\n",
    "            x_train[image_num] = np.load(image_dir + '/image' + str(image_num)+'.npy')\n",
    "#             image = np.load(image_dir + '/image' + str(image_num)+'.npy')\n",
    "#             x_train1[image_num][0] = image[0][0]\n",
    "#             x_train2[image_num][0] = image[0][1]\n",
    "        y_train[image_num] = np.asarray(data_reg[image_num][-1], dtype=float_memory_used)\n",
    "        if image_num + 1 % 100 == 0:\n",
    "            print(image_num)\n",
    "#     cnn = cnn_model(7, 0, 0)\n",
    "#     cnn.compile(loss='mean_squared_error', optimizer='adam', metrics=['mse','mae'])\n",
    "#     cnn.fit(x_train[:sample], y_train[:sample], epochs=5, verbose=1, batch_size=1, validation_split=validation_size/\n",
    "#             (validation_size + 1))\n",
    "    \n",
    "    min_error = float('inf')\n",
    "    best_model, best_lam = None, None\n",
    "    for lamb in lambda_vec:\n",
    "        print(\"Lambda:\", lamb)\n",
    "        cnn = cnn_model(10, lamb, 0)\n",
    "        cnn.compile(loss='mean_squared_error', optimizer='adam', metrics=['mse','mae'])\n",
    "#         cnn.fit([x_train1[:sample], x_train2[:sample]], y_train[:sample], epochs=6, verbose=1, batch_size=1, validation_split=validation_size/\n",
    "#                 (validation_size + 1))\n",
    "        cnn.fit(x_train[:sample], y_train[:sample], epochs=6, verbose=0, batch_size=1, validation_split=validation_size/\n",
    "                (validation_size + 1))\n",
    "        if cnn.history.history['val_mean_absolute_error'][-1] < min_error:\n",
    "            min_error = cnn.history.history['val_mean_absolute_error'][-1]\n",
    "            best_model = cnn\n",
    "            best_lam = lamb\n",
    "    print(\"best_lambda, \", best_lam, \"min_error\", min_error)    \n",
    "    # evaluating test images\n",
    "    sum_mae, sum_fp_mae = 0, 0\n",
    "    test_size = 0\n",
    "#     for test_num in range(max_train_samples, data_reg.shape[0]):\n",
    "    for test_num in range(sample, data_reg.shape[0]):\n",
    "        test_size += 1\n",
    "        if style == \"image_intensity\":\n",
    "            test_image = plt.imread(image_dir + '/image' + str(test_num) + '.png')\n",
    "            test_image = np.swapaxes(test_image, 0, 2)\n",
    "            test_image = np.array(test_image[:number_image_channels]).reshape(1, number_image_channels, max_x, max_y)\n",
    "        elif  style == \"raw_power_min_max_norm\" or style == \"raw_power_zscore_norm\":\n",
    "            test_image = np.load(image_dir + '/image' + str(test_num)+'.npy')\n",
    "        test_y = data_reg[test_num][-1]\n",
    "        test_yp = best_model.predict(test_image)[0][0]\n",
    "        sum_mae += abs(test_yp - test_y)\n",
    "        if test_yp > test_y:\n",
    "            sum_fp_mae += abs(test_yp - test_y)\n",
    "        if test_num % 500 == 0:\n",
    "            print('test: ', test_num)\n",
    "    fp_mean_power.append(round(sum_fp_mae/ test_size, 3))\n",
    "    average_diff_power.append(round(sum_mae / test_size, 3))\n",
    "    print('number_samples: ', number_sample, ', average_error: ', average_diff_power[-1], ' fp_average_error: ', fp_mean_power[-1])\n",
    "    print(\"\\n\")\n",
    "    var_f = open('/'.join(image_dir.split('/')[:-1]) +  '/' + intensity_degradation + '_' + str(slope) + '_' + dtime + \".dat\", \"wb\") # file for saving results\n",
    "    pickle.dump([average_diff_power, fp_mean_power, number_samples], file=var_f)\n",
    "    var_f.close()       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_diff_power[8], average_diff_power[9] = average_diff_power[9], average_diff_power[8]\n",
    "# fp_mean_power = fp_mean_power[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shapee = Input(shape=(number_image_channels, max_x, max_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shapee[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = cnn_model(1, 0)\n",
    "cnn.compile(loss='mean_squared_error', optimizer='adam', metrics=['mse','mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cnn.history.history['val_mean_absolute_error'][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = cnn_model(10, 0, 0)\n",
    "cnn.compile(loss='mean_squared_error', optimizer='adam', metrics=['mse','mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cnn.fit(x_train[:sample], y_train[:sample], epochs=5, verbose=1, batch_size=1, validation_split=validation_size/\n",
    "            (validation_size + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_vec = [0, 0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1, 3]\n",
    "min_error = float('inf')\n",
    "best_model, best_lam = None, None\n",
    "for lamb in lambda_vec:\n",
    "    print(\"Lambda:\", lamb)\n",
    "    cnn = cnn_model(15, lamb, 0)\n",
    "    cnn.compile(loss='mean_squared_error', optimizer='adam', metrics=['mse','mae'])\n",
    "    cnn.fit(x_train[:sample], y_train[:sample], epochs=5, verbose=1, batch_size=1, validation_split=validation_size/\n",
    "            (validation_size + 1))\n",
    "    if cnn.history.history['val_mean_absolute_error'][-1] < min_error:\n",
    "        min_error = cnn.history.history['val_mean_absolute_error'][-1]\n",
    "        best_model = cnn\n",
    "        best_lam = lamb\n",
    "    print(\"\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_lam)\n",
    "print(best_model.history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# just run to dispaly the image. First change return line from create_image\n",
    "aa = np.swapaxes(np.append(np.array(x_train[50]), np.zeros((2,max_x, max_y), dtype=float_memory_used), axis=0), 0, 2)\n",
    "plt.imshow(aa)\n",
    "# plt.imsave('image.png', aa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_diff_power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this cell to read saved variables\n",
    "var_ff = open('ML/data/pictures_1000_1000/log_201912_0705_37.txt', 'rb')\n",
    "[average_diff_power_1, fp_mean_power_1, number_samples_1] = pickle.load(var_ff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp_mean_power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_diff_power[-1]*(data_reg.shape[0] - max_train_samples)/(300-sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_fp_mae/200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp_mean_power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import pydotplus\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "keras.utils.vis_utils.pydot = pydot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARALLEL CNN\n",
    "def cnn_model(num_filters, kernel_lam, bias_lam):\n",
    "#     num_filters, lam = 5, 5\n",
    "    data_format = 'channels_first'\n",
    "    convolution_filter, dense_filter = 'selu', 'linear' #softsign, sigmoid; relu, linear\n",
    "    filter_shape, pool_size = (3, 3), (2,2)\n",
    "    # CNN for PU image\n",
    "    input1  = layers.Input(shape=(number_image_channels - 1, max_x, max_y), name='pus_input')\n",
    "    x1 = layers.Conv2D(num_filters, filter_shape, padding='same', activation=convolution_filter, \n",
    "                          input_shape=(number_image_channels - 1, max_x, max_y), data_format=data_format,\n",
    "                          kernel_regularizer=regularizers.l2(kernel_lam), bias_regularizer=regularizers.l2(bias_lam),\n",
    "                          kernel_initializer='lecun_normal')(input1)\n",
    "    x1 = layers.MaxPooling2D(pool_size=pool_size, data_format=data_format)(x1)\n",
    "    \n",
    "    x1 = layers.Conv2D(num_filters, filter_shape, padding='same', activation=convolution_filter, \n",
    "                          input_shape=(number_image_channels - 1, max_x, max_y), data_format=data_format,\n",
    "                          kernel_regularizer=regularizers.l2(kernel_lam), bias_regularizer=regularizers.l2(bias_lam),\n",
    "                          kernel_initializer='lecun_normal')(x1)\n",
    "    x1 = layers.MaxPooling2D(pool_size=pool_size, data_format=data_format)(x1)\n",
    "    \n",
    "    x1 = layers.Conv2D(num_filters, filter_shape, padding='same', activation=convolution_filter, \n",
    "                          input_shape=(number_image_channels - 1, max_x, max_y), data_format=data_format,\n",
    "                          kernel_regularizer=regularizers.l2(kernel_lam), bias_regularizer=regularizers.l2(bias_lam),\n",
    "                          kernel_initializer='lecun_normal')(x1)\n",
    "    x1 = layers.MaxPooling2D(pool_size=pool_size, data_format=data_format)(x1)\n",
    "    \n",
    "    x1 = layers.Conv2D(num_filters, filter_shape, padding='same', activation=convolution_filter, \n",
    "                          input_shape=(number_image_channels - 1, max_x, max_y), data_format=data_format,\n",
    "                          kernel_regularizer=regularizers.l2(kernel_lam), bias_regularizer=regularizers.l2(bias_lam),\n",
    "                          kernel_initializer='lecun_normal')(x1)\n",
    "    x1 = layers.MaxPooling2D(pool_size=pool_size, data_format=data_format)(x1)\n",
    "    \n",
    "    x1 = layers.Conv2D(num_filters, filter_shape, padding='same', activation=convolution_filter, \n",
    "                          input_shape=(number_image_channels - 1, max_x, max_y), data_format=data_format,\n",
    "                          kernel_regularizer=regularizers.l2(kernel_lam), bias_regularizer=regularizers.l2(bias_lam),\n",
    "                          kernel_initializer='lecun_normal')(x1)\n",
    "    x1 = layers.MaxPooling2D(pool_size=pool_size, data_format=data_format)(x1)\n",
    "    \n",
    "    \n",
    "    # CNN for SU\n",
    "    input2  = layers.Input(shape=(1, max_x, max_y), name='su_input')\n",
    "    x2 = layers.Conv2D(num_filters, filter_shape, padding='same', activation=convolution_filter, \n",
    "                          input_shape=(1, max_x, max_y), data_format=data_format,\n",
    "                          kernel_regularizer=regularizers.l2(kernel_lam), bias_regularizer=regularizers.l2(bias_lam),\n",
    "                          kernel_initializer='lecun_normal')(input2)\n",
    "    x2 = layers.MaxPooling2D(pool_size=pool_size, data_format=data_format)(x2)\n",
    "    \n",
    "    x2 = layers.Conv2D(num_filters, filter_shape, padding='same', activation=convolution_filter, \n",
    "                          input_shape=(1, max_x, max_y), data_format=data_format,\n",
    "                          kernel_regularizer=regularizers.l2(kernel_lam), bias_regularizer=regularizers.l2(bias_lam),\n",
    "                          kernel_initializer='lecun_normal')(x2)\n",
    "    x2 = layers.MaxPooling2D(pool_size=pool_size, data_format=data_format)(x2)\n",
    "    \n",
    "    x2 = layers.Conv2D(num_filters, filter_shape, padding='same', activation=convolution_filter, \n",
    "                          input_shape=(1, max_x, max_y), data_format=data_format,\n",
    "                          kernel_regularizer=regularizers.l2(kernel_lam), bias_regularizer=regularizers.l2(bias_lam),\n",
    "                          kernel_initializer='lecun_normal')(x2)\n",
    "    x2 = layers.MaxPooling2D(pool_size=pool_size, data_format=data_format)(x2)\n",
    "    \n",
    "    x2 = layers.Conv2D(num_filters, filter_shape, padding='same', activation=convolution_filter, \n",
    "                          input_shape=(1, max_x, max_y), data_format=data_format,\n",
    "                          kernel_regularizer=regularizers.l2(kernel_lam), bias_regularizer=regularizers.l2(bias_lam),\n",
    "                          kernel_initializer='lecun_normal')(x2)\n",
    "    x2 = layers.MaxPooling2D(pool_size=pool_size, data_format=data_format)(x2)\n",
    "    \n",
    "    x2 = layers.Conv2D(num_filters, filter_shape, padding='same', activation=convolution_filter, \n",
    "                          input_shape=(1, max_x, max_y), data_format=data_format,\n",
    "                          kernel_regularizer=regularizers.l2(kernel_lam), bias_regularizer=regularizers.l2(bias_lam),\n",
    "                          kernel_initializer='lecun_normal')(x2)\n",
    "    x2 = layers.MaxPooling2D(pool_size=pool_size, data_format=data_format)(x2)\n",
    "    \n",
    "    \n",
    "    # concatanate two CNN outputs\n",
    "    x = layers.concatenate([x1, x2])\n",
    "    x = layers.Flatten()(x)\n",
    "    \n",
    "    x = layers.Dense(20, activation=convolution_filter, kernel_regularizer=regularizers.l2(kernel_lam),\n",
    "                         bias_regularizer=regularizers.l2(bias_lam), kernel_initializer='lecun_normal')(x)\n",
    "    x = layers.Dense(20, activation=convolution_filter, kernel_regularizer=regularizers.l2(kernel_lam),\n",
    "                         bias_regularizer=regularizers.l2(bias_lam), kernel_initializer='lecun_normal')(x)\n",
    "    out = layers.Dense(1, activation=dense_filter, kernel_regularizer=regularizers.l2(kernel_lam),\n",
    "                         bias_regularizer=regularizers.l2(bias_lam), kernel_initializer='lecun_normal')(x)\n",
    "    \n",
    "    model = models.Model(inputs=[input1, input2], outputs=out)\n",
    "#     plot_model(model, to_file='model.png')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number_samples = [5] + list(range(10, 101, 10)) + [120, 150, 200, 250, 300, 400, 500, 700] + list(range(1000, 1001, 1000))\n",
    "\n",
    "# validation_size, noise_floor = 0.33, -90.0\n",
    "# su_power = 0 # this is not actually su power just a number to show there is an SU in its image\n",
    "# max_x, max_y, number_image_channels, su_szie = 1000, 1000, 2, 10\n",
    "# pu_shape, su_shape = 'circle', 'square'\n",
    "# style = \"image_intensity\"  # {\"raw_power_zscore_norm\", \"image_intensity\", \"raw_power_min_max_norm\"}\n",
    "# pus_num, intensity_degradation, slope = 15, 'log', 4\n",
    "# average_diff_power = [9.110476626067186, 21.070721128267266, 9.389938883165568, 10.886098907990405,\n",
    "#                                        7.697396928362106, 7.522477509027216, 9.493729427772132, 8.198866980620753,\n",
    "#                                        7.781910785203122, 9.41743984825801, 8.499455442627129, 9.86776958065812,\n",
    "#                                        9.033719411254367, 8.150143941293027, 8.963829050517273, 8.708150642874065,\n",
    "#                                        7.468060397898071, 8.233182799553932,8.206, 7.768]\n",
    "# fp_mean_power =  [8.174990557021465, 0.18043087058937837, 1.5141939559853392, 10.273307557711494,\n",
    "#                                    3.2306742061521443, 4.423113329284006, 8.674172526579392, 2.38235061342411,\n",
    "#                                    5.014172646429496, 6.884079514994618, 3.4544130456368367, 7.81721202679044,\n",
    "#                                    6.438635364829745, 4.069245107144559, 5.202978504937615, 3.405858414831347,\n",
    "#                                    4.117573271657338, 2.8100743146184377, 3.951, 3.502]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MAX_POWER ANAlysis\n",
    "# number_samples = [5] + list(range(10, 101, 10)) + [120, 150, 200, 250, 300, 400, 500, 700] + list(range(1000, 1001, 1000))\n",
    "\n",
    "# validation_size, noise_floor = 0.33, -90.0\n",
    "# su_power = 0 # this is not actually su power just a number to show there is an SU in its image\n",
    "# max_x, max_y, number_image_channels, su_szie = 1000, 1000, 2, 10\n",
    "# pus_num, intensity_degradation, slope = 15, 'log', 4\n",
    "# pu_shape, su_shape = 'circle', 'circle'\n",
    "# style = \"image_intensity\"  # {\"raw_power_zscore_norm\", \"image_intensity\", \"raw_power_min_max_norm\"}\n",
    "# test_size = 3670\n",
    "# average_diff_power = [7.811849328268183, 9.178415418536536, 8.11891504382307, 7.881934146750136, 7.918868224324312,\n",
    "#                       7.709452054502398, 7.471729821563216, 8.63783455122861, 7.7635068514166345, 8.557134470036884,\n",
    "#                       8.103793715416188, 9.189284948409279, 11.977416480154307, 8.291134394492891, 8.960065032512803,\n",
    "#                       9.992745143323642, 8.475335283779392, 8.051642160173987, 7.322538645284376, 7.768582958795206]\n",
    "# fp_mean_power = [6.1844398077234635, 1.6157812496465958, 6.5620574110067595, 2.898169187355567, 6.262096880097353,\n",
    "#                  2.5478307871639267, 3.5784209073932067, 7.416731632966506, 5.5822838290638135, 5.800529848947965,\n",
    "#                  4.6984887763519785, 2.337296353076653, 9.85739104089764, 3.710259461284922, 5.323224159423669, \n",
    "#                  6.198328912769283, 2.302462751745074, 4.023802978234984, 3.781413967880959, 3.2793608103510508]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INIT\n",
    "# number_samples = [5] + list(range(10, 101, 10)) + [120, 150, 200, 250, 300, 400, 500, 700] + list(range(1000, 1001, 1000))\n",
    "\n",
    "# validation_size, noise_floor = 0.33, -90.0\n",
    "# su_power = 0 # this is not actually su power just a number to show there is an SU in its image\n",
    "# max_x, max_y, number_image_channels, su_szie = 1000, 1000, 2, 10\n",
    "# pu_shape, su_shape = 'circle', 'circle'\n",
    "# style = \"raw_power_min_max_norm\"  # {\"raw_power_zscore_norm\", \"image_intensity\", \"raw_power_min_max_norm\"}\n",
    "# pus_num, intensity_degradation, slope = 15, 'log', 4\n",
    "# if su_shape == 'circle':\n",
    "#     su_param = Circle(su_szie)\n",
    "# elif su_shape == 'square':\n",
    "#     su_param = Square(su_szie)\n",
    "# num_pus = 15\n",
    "average_diff_power = [9.711, 7.867, 8.958, 7.571, 7.509, 7.891, 8.272, 7.118, 7.696, 7.689, 8.026, 9.674, 7.51, 7.771, 8.17,\n",
    "                      7.938, 7.869, 7.833, 9.434, 8.501]\n",
    "fp_mean_power = [9.229, 5.101, 8.037, 3.993, 5.095, 2.491, 2.298, 4.654, 3.787, 2.685, 5.676, 8.033, 3.911, 4.235, 3.278,\n",
    "                 5.809, 3.586, 4.257, 4.377, 5.015]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INIT\n",
    "\n",
    "# number_samples = [120, 150, 200, 250, 300, 400, 500, 700] + list(range(1000, 8001, 1000))\n",
    "\n",
    "\n",
    "# cnn_type = \"classification\"  # {\"classification\", \"regression\"}\n",
    "# validation_size, noise_floor = 0.33, -90.0\n",
    "# su_power = 0 # this is not actually su power just a number to show there is an SU in its image\n",
    "# max_x, max_y, number_image_channels, su_szie = 200, 200, 2, 10\n",
    "# pu_shape, su_shape = 'circle', 'circle' # shape = {'circle', 'square', 'point'}\n",
    "# style = \"raw_power_min_max_norm\"  # {\"raw_power_zscore_norm\", \"image_intensity\", \"raw_power_min_max_norm\"}\n",
    "# pus_num, intensity_degradation, slope = 15, 'log', 4\n",
    "# if su_shape == 'circle':\n",
    "#     su_param = Circle(su_szie)\n",
    "# elif su_shape == 'square':\n",
    "#     su_param = Square(su_szie)\n",
    "# else:\n",
    "#     su_param = None\n",
    "# sensors = False\n",
    "# if sensors:\n",
    "#     sensors_num = 50\n",
    "#     sensors_file_path = \"rsc/\" + str(sensors_num) + \"/sensors\"\n",
    "    \n",
    "average_diff_power = [6.779, 5.645, 5.473, 4.982, 4.481, 4.071, 4.05, 3.639, 2.813, 2.343, 2.21, 2.372, 2.005, 1.997,\n",
    "                      1.937, 1.901]\n",
    "\n",
    "fp_mean_power = [4.073, 2.409, 3.424, 3.163, 2.833, 2.663, 2.857, 2.744, 1.744, 1.33, 1.184, 1.55, 0.579, 1.216, 1.492, 1.266]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INIT\n",
    "\n",
    "number_samples = [120, 150, 200, 250, 300, 400, 500, 700] + list(range(1000, 6001, 1000)) + [8000]\n",
    "# dataframe = pd.read_csv('ML/data/dynamic_pus_using_pus50000_15PUs_201912_3000_40_200.txt', delimiter=',', header=None)\n",
    "# dataframe_max = pd.read_csv('ML/data/dynamic_pus_max_power50000_15PUs_201912_3000_40_200.txt', delimiter=',', header=None)\n",
    "\n",
    "\n",
    "# validation_size, noise_floor = 0.33, -90.0\n",
    "# su_power = 0 # this is not actually su power just a number to show there is an SU in its image\n",
    "# max_x, max_y, number_image_channels, su_szie = 200, 200, 4, 10\n",
    "# pu_shape, su_shape = 'circle', 'circle' # shape = {'circle', 'square', 'point'}\n",
    "# style = \"raw_power_min_max_norm\"  # {\"raw_power_zscore_norm\", \"image_intensity\", \"raw_power_min_max_norm\"}\n",
    "# pus_num, intensity_degradation, slope = 15, 'log', 5\n",
    "# if su_shape == 'circle':\n",
    "#     su_param = Circle(su_szie)\n",
    "# elif su_shape == 'square':\n",
    "#     su_param = Square(su_szie)\n",
    "# else:\n",
    "#     su_param = None\n",
    "# sensors = False\n",
    "    \n",
    "average_diff_power = [12.742, 12.906, 12.731, 12.595, 12.859, 13.272, 12.632, 12.647, 11.309, 7.455, 7.131, 5.677,\n",
    "                      5.645, 5.292, 4.445]\n",
    "\n",
    "fp_mean_power = [5.963, 5.861, 8.957, 8.821, 8.215, 9.518, 8.633, 6.644, 6.605, 3.919, 2.539, 3.866, 1.96, 2.717, 1.671]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(average_diff_power)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line_width = 5\n",
    "marker_size = 12\n",
    "reg_style = 'solid'\n",
    "class_reg = 'dashed'\n",
    "fig = plt.figure(figsize=(15,8))\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "plt.plot(number_samples, average_diff_power, color='r', linewidth=line_width, markersize=marker_size, linestyle=class_reg)\n",
    "plt.plot(number_samples, fp_mean_power, color='midnightblue', linewidth=line_width, markersize=marker_size, linestyle=class_reg)\n",
    "plt.xlabel('# of Training Samples', fontsize=47)\n",
    "plt.ylabel('Avg. Diff. wrt Opt. (dB)', fontsize=45)\n",
    "plt.title('Dynamic PUs(200m*200m)')\n",
    "plt.grid(True)\n",
    "\n",
    "ax.set_yticks(np.arange(0,14, 2))\n",
    "# ax.set_xticks(np.arange(100,7000, 1500))\n",
    "plt.rcParams.update({'font.size': 42})\n",
    "ax.tick_params(axis='x', labelsize=46)\n",
    "ax.tick_params(axis='y', labelsize=45)\n",
    "\n",
    "# matplotlib.rcParams.update({'font.size': 22})\n",
    "\n",
    "ax.set_ylim([0, 14])\n",
    "ax.set_xlim([0, 8000])\n",
    "plt.legend(['Total', 'False-Positive'], ncol=2, loc='best', handletextpad=0.1,borderpad=0, columnspacing=0.2, borderaxespad=0.2)\n",
    "# plt.legend(handletextpad=0.1)\n",
    "plt.savefig('/'.join(image_dir.split('/')[:-1]) +  '/' + intensity_degradation + '_' + str(slope) + '_' + dtime + \".png\", \n",
    "            bbox_inches = 'tight', pad_inches = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.load(image_dir + '/image10.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('/'.join(image_dir.split('/')[:-1]) + '/log_5__202001_1519_01.dat', 'rb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('/home/shahrokh/projects/research/MLSpectrumAllocation/ML/data/pictures_1000_1000/log/noisy_std_1/' +\n",
    "            'pu_circle_su_circle_30/raw_power_min_max_norm/color/log_4/pus/log_4__202005_0512_10.dat', 'rb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "[average_diff_power, fp_mean_power, number_samples, best_lambda, \n",
    " dataset_name, max_dataset_name, average_power_conserve, fp_mean_power_conserve] = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[256, 512, 1024, 2048, 4096, 8192]\n",
      "[7.376, 7.312, 7.505, 5.391]\n",
      "[5.207, 4.595, 5.154, 2.383]\n",
      "[1, 1, 0.1, 0.001]\n",
      "[7.237, 7.308, 7.389, 5.607]\n",
      "[4.509, 3.964, 4.467, 1.862]\n"
     ]
    }
   ],
   "source": [
    "print(number_samples)\n",
    "print(average_diff_power)\n",
    "print(fp_mean_power)\n",
    "print(best_lambda)\n",
    "print(average_power_conserve)\n",
    "print(fp_mean_power_conserve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fp1, fp2, fp3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(samples1, samples2, samples3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(samples3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
