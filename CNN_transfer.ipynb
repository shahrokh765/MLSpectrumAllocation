{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, Input,optimizers\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import regularizers, Model\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras import applications\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from collections import namedtuple\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import datetime, time\n",
    "import os, sys\n",
    "import tqdm\n",
    "import gc\n",
    "from multiprocessing import Process\n",
    "Point = namedtuple('Point', ('x', 'y'))\n",
    "Circle = namedtuple('Circle', ('r'))\n",
    "Square = namedtuple('Square', ('side'))\n",
    "Rectangle = namedtuple('Rectangle', ('length', 'width'))\n",
    "PointWithDistance = namedtuple('PointWithDistance', ('p', 'dist'))\n",
    "float_memory_used = 'float32'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INIT\n",
    "# PART 1\n",
    "number_samples = [128, 256, 512, 1024, 4096] \n",
    "validation_size, noise_floor = 0.2, -110.0\n",
    "su_power = 0 # this is not actually su power just a number to show there is an SU in its image\n",
    "max_x, max_y, number_image_channels, su_szie = 299, 299, 3, 60  # su_size:30 for 1000, 10 for 100\n",
    "cell_size, pixel_expansion = 1000 / max_x, max_x / 100\n",
    "pu_shape, su_shape = 'circle', 'circle' # shape = {'circle', 'square', 'point'}\n",
    "style = \"raw_power_min_max_norm\"  # {\"raw_power_zscore_norm\", \"image_intensity\", \"raw_power_min_max_norm\"}\n",
    "intensity_degradation, slope, su_slope = 'log', 5, 5  # 'log', 'linear', slope 3 for 1000, 5 for 100\n",
    "max_pus_num, max_sus_num = 20, 5\n",
    "propagation_model = 'splat' # 'splat', 'log', 'testbed'\n",
    "noise, std = False, 1 # False for splat\n",
    "if su_shape == 'circle':\n",
    "    su_param = Circle(su_szie)\n",
    "elif su_shape == 'square':\n",
    "    su_param = Square(su_szie)\n",
    "else:\n",
    "    su_param = None\n",
    "    \n",
    "sensors = False\n",
    "if sensors:\n",
    "    sensors_num = 225\n",
    "    sensors_file_path = f\"data/sensors/square{100}/{sensors_num}/sensors.txt\"\n",
    "# num_pus = (data_reg.shape[1] - 3)//3\n",
    "\n",
    "# PART 2\n",
    "number_of_proccessors = 12\n",
    "memory_size_allowed = 4 # in Gigabyte\n",
    "float_size = 0\n",
    "if float_memory_used == \"float16\":\n",
    "    float_size = 16\n",
    "elif float_memory_used == \"float\" or \"float32\":\n",
    "    float_size = 32\n",
    "elif float_memory_used == \"float8\":\n",
    "    float_size = 8\n",
    "\n",
    "\n",
    "batch_size = int(memory_size_allowed / (max_x * max_y * number_image_channels * float_size/(8 * 1024 ** 3)))\n",
    "\n",
    "\n",
    "dtime = datetime.datetime.now().strftime('_%Y%m_%d%H_%M')\n",
    "color = \"color\" if number_image_channels > 1 else \"gray\"\n",
    "image_dir = 'ML/data/pictures_' + str(max_x) + '_' + str(max_y) + '_transfer/' + propagation_model + (\n",
    "    \"/noisy_std_\" + str(std) if noise else \"\") + '/pu_' + pu_shape + '_su_' + su_shape + '_' + (\n",
    "    \"\" if su_shape == 'point' else str(su_szie)) + \"/\" + style + \"/\" + color +'/' + (\n",
    "    \"\" if pu_shape == 'point' and su_shape == 'point' else (f\"{intensity_degradation}_pu{slope}_su{su_slope}\")) + (\n",
    "    \"/\" + str(sensors_num) + \"sensors\" if sensors else f\"/{max_pus_num}pus\") + \\\n",
    "        f\"_{max_sus_num}sus_{number_image_channels}channels\" + \"/images\"\n",
    "\n",
    "if not os.path.exists(image_dir):\n",
    "        os.makedirs(image_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dir "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_dir = \"/home/shahrokh/projects/MLSpectrumAllocation/ML/data/pictures_299_299_transfer/log/\" +\\\n",
    "#             \"noisy_std_1/pu_circle_su_circle_60/raw_power_min_max_norm/color/log_pu5_su6/\" +\\\n",
    "#             \"variable_sensors_10_20_pus_5_sus_8_channels/images\"\n",
    "sensors_location = {}\n",
    "for sensor_num in [49, 100, 225, 400, 625]:\n",
    "    sensors_location[sensor_num] = []\n",
    "    with open(f\"data/sensors/square{100}/{sensor_num}/sensors.txt\", 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            line = line.split(',')\n",
    "            sensors_location[sensor_num].append(Point(int(float(line[0])), int(float(line[1]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.hist(data_reg[0:100:1, :100], bins='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD DATA\n",
    "MAX_SU_TOTAL = True\n",
    "num_columns = (sensors_num + 1 if sensors else max_pus_num * 3 + 1) + max_sus_num * 3 + 2\n",
    "# num_columns = max_pus_num * 3 + 1 + max_sus_num * 3 + 2\n",
    "cols = [i for i in range(num_columns)]\n",
    "dataset_name = \"dynamic_pus_using_pus_50000_min10_max20PUs_5SUs_square100grid_splat_2022_06_09_13_24.txt\"\n",
    "max_dataset_name = \"dynamic_pus_max_power_50000_min10_max20PUs_5SUs_square100grid_splat_2022_06_09_13_24.txt\"\n",
    "if MAX_SU_TOTAL:\n",
    "    max_su_total_dataset_name = \"dynamic_pus_maximum_total_sus50000_min10_max20PUs_5SUs_square100grid_splat_2022_06_09_13_24.txt\"\n",
    "with open('/'.join(image_dir.split('/')[:-1]) + '/datasets' + dtime + '.txt', 'w') as set_file:\n",
    "    set_file.write(dataset_name + \"\\n\")\n",
    "    set_file.write(max_dataset_name)\n",
    "    if MAX_SU_TOTAL:\n",
    "        set_file.write(max_su_total_dataset_name)\n",
    "\n",
    "dataframe = pd.read_csv('data/' \n",
    "                        + dataset_name, delimiter=',', header=None, names=cols)\n",
    "dataframe_max = pd.read_csv('data/' \n",
    "                            + max_dataset_name, delimiter=',', header=None)\n",
    "if MAX_SU_TOTAL:\n",
    "    dataframe_max_su_total = pd.read_csv('data/' + max_su_total_dataset_name, delimiter=\",\", header=None,\n",
    "                                        names=[i for i in range(max_sus_num * 3 + 1)])\n",
    "\n",
    "dataframe.reset_index(drop=True, inplace=True)\n",
    "dataframe_max.reset_index(drop=True, inplace=True)\n",
    "if MAX_SU_TOTAL:\n",
    "    dataframe_max_su_total.reset_index(drop=True, inplace=True)\n",
    "dataframe_max[dataframe_max.shape[1] - 1] = dataframe_max[dataframe_max.shape[1] - 1].astype(float)\n",
    "\n",
    "dataframe_tot = pd.concat([dataframe, dataframe_max.iloc[:, dataframe_max.columns.values[-1:]]], axis=1,\n",
    "                        ignore_index=True)\n",
    "\n",
    "idx = dataframe_tot[dataframe_tot[dataframe_tot.columns[-1]] == -float('inf')].index\n",
    "dataframe_tot.drop(idx, inplace=True)\n",
    "if MAX_SU_TOTAL:\n",
    "    dataframe_max_su_total.drop(idx, inplace=True)\n",
    "\n",
    "data_reg = dataframe_tot.values\n",
    "data_reg[data_reg < noise_floor] = noise_floor\n",
    "if MAX_SU_TOTAL:\n",
    "    data_max_su_tot = dataframe_max_su_total.values\n",
    "\n",
    "if False and sensors:\n",
    "    sensors_location = []\n",
    "    with open(sensors_file_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            line = line.split(',')\n",
    "            sensors_location.append(Point(int(float(line[0])), int(float(line[1]))))\n",
    "del dataframe, dataframe_tot, dataframe_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([225.   , -87.793, -84.376, -83.528, -81.822, -75.905, -66.582,\n",
       "       -74.179, -73.021, -69.737, -57.872, -67.844, -70.076, -70.457,\n",
       "       -73.44 , -77.393, -85.975, -82.953, -74.184, -79.187, -77.676,\n",
       "       -76.026, -73.896, -73.696, -70.641, -65.995, -68.487, -69.938,\n",
       "       -69.732, -69.752, -72.175, -86.43 , -83.924, -78.311, -77.882,\n",
       "       -75.753, -72.736, -69.31 , -72.285, -71.614, -70.488, -65.709,\n",
       "       -64.755, -64.035, -65.505, -70.898, -87.179, -85.966, -80.402,\n",
       "       -80.979, -77.596, -72.971, -68.651, -72.127, -71.146, -68.837,\n",
       "       -67.846, -64.873, -45.862, -66.383, -69.093, -87.078, -85.946,\n",
       "       -84.992, -80.244, -79.394, -76.972, -76.694, -77.052, -70.44 ,\n",
       "       -71.027, -68.209, -63.084, -62.659, -67.373, -67.48 , -87.243,\n",
       "       -87.241, -86.551, -80.209, -78.736, -77.566, -74.604, -74.698,\n",
       "       -71.771, -70.846, -67.554, -60.771, -60.797, -67.122, -67.29 ,\n",
       "       -88.056, -84.096, -86.428, -85.156, -83.467, -81.174, -80.467,\n",
       "       -78.297, -71.936, -68.293, -70.53 , -66.053, -65.932, -68.541,\n",
       "       -70.025, -84.677, -78.195, -81.945, -82.068, -82.39 , -85.014,\n",
       "       -81.664, -78.841, -77.831, -67.678, -70.942, -69.837, -68.264,\n",
       "       -69.62 , -68.98 , -78.345, -78.29 , -77.721, -78.132, -83.258,\n",
       "       -82.132, -81.343, -82.106, -81.984, -75.755, -75.314, -76.444,\n",
       "       -76.062, -79.373, -79.554, -71.326, -71.278, -71.376, -72.143,\n",
       "       -79.739, -79.912, -81.152, -81.207, -79.252, -79.425, -79.278,\n",
       "       -80.805, -80.783, -83.097, -83.002, -68.73 , -67.88 , -71.206,\n",
       "       -73.005, -73.188, -79.479, -81.783, -79.901, -79.246, -81.386,\n",
       "       -82.424, -82.067, -83.273, -83.307, -83.621, -62.642, -59.683,\n",
       "       -68.902, -73.457, -73.56 , -79.847, -78.544, -75.762, -72.591,\n",
       "       -79.643, -82.243, -82.596, -83.624, -83.468, -86.624, -65.589,\n",
       "       -64.938, -69.609, -73.977, -73.435, -79.186, -77.75 , -77.441,\n",
       "       -75.175, -80.391, -82.766, -84.377, -85.999, -86.419, -89.037,\n",
       "       -71.871, -71.128, -68.329, -68.942, -79.485, -78.764, -72.52 ,\n",
       "       -80.078, -82.047, -82.88 , -83.454, -84.624, -87.134, -89.314,\n",
       "       -90.114, -71.249, -71.182, -71.696, -78.546, -78.982, -83.174,\n",
       "       -81.237, -81.857, -82.319, -82.554, -83.109, -87.486, -87.946,\n",
       "       -90.222, -90.552,   1.   ,  97.   ,  82.   , -33.251,   1.   ,\n",
       "        19.261])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_reg[56,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_reg = data_reg[:30000][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_reg = data_reg[:40000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_reg_train = data_reg[12000][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32162, 79)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_reg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5.   , 30.   , 80.   , 12.773, 68.   , 91.   , 18.252, 66.   ,\n",
       "       56.   ,  9.604, 30.   , 82.   , 10.786, 28.   ,  8.   , 11.592])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_max_su_tot[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 19.   ,  38.   ,  65.   , -19.658,  69.   ,  49.   ,  -7.351,\n",
       "        88.   ,  42.   ,  -6.322,  28.   ,  17.   ,  -5.694,  61.   ,\n",
       "        45.   , -17.43 ,   2.   ,  55.   , -24.392,  57.   ,  95.   ,\n",
       "       -20.778,  52.   ,  79.   , -21.744,  52.   ,   5.   , -13.93 ,\n",
       "         6.   ,  39.   ,  -6.269,  87.   ,   8.   , -21.177,  26.   ,\n",
       "        80.   , -20.406,  19.   ,  63.   , -15.858,  30.   ,  19.   ,\n",
       "       -13.793,  27.   ,  50.   , -17.062,  95.   ,  95.   , -29.368,\n",
       "        39.   ,  71.   ,  -3.309,  52.   ,  59.   , -11.98 ,  38.   ,\n",
       "        16.   , -22.553,   1.   ,  95.   ,  30.   ,  -7.839,   1.   ,\n",
       "           nan,     nan,     nan,  11.077])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_reg[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(data_reg[:, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidian_distance(p1: Point, p2: Point):\n",
    "    return ((p1.x - p2.x) ** 2 + (p1.y - p2.y) ** 2) ** 0.5 * cell_size\n",
    "\n",
    "def calculate_mu_sigma(data, num_pus):\n",
    "    sum_non_noise = 0\n",
    "    for pu_n in range(num_pus): # calculate mu\n",
    "        sum_non_noise += data[pu_n*3+2]\n",
    "    mu = ((max_x * max_y - num_pus) * noise_floor + sum_non_noise)/(max_x * max_y)\n",
    "    sum_square = 0\n",
    "    for pu_n in range(num_pus): # calculate sigma\n",
    "        sum_square += (data[pu_n*3+2]-mu)**2\n",
    "    sum_square += (max_x * max_y - num_pus) * (noise_floor - mu)**2\n",
    "    sigma = math.sqrt(sum_square/(max_x * max_y))\n",
    "    return mu, sigma\n",
    "\n",
    "def get_pu_param(pu_shape: str, intensity_degradation: str, pu_p: float, noise_floor: float, slope: float):\n",
    "    pu_param = None\n",
    "    if pu_shape == 'circle':\n",
    "        if intensity_degradation == \"linear\":\n",
    "            pu_param = Circle(int((pu_p - noise_floor) / slope)) # linear\n",
    "        elif intensity_degradation == \"log\":\n",
    "            pu_param = Circle(int(10 ** ((pu_p - noise_floor) / (10 *slope)))) # log_based\n",
    "    elif pu_shape == 'square':\n",
    "        if intensity_degradation == \"linear\":\n",
    "            pu_param = Square(int(2 ** 0.5 * (pu_p - noise_floor) / slope)) # linear\n",
    "        elif intensity_degradation == \"log\":\n",
    "            pu_param = Square(int(2 ** 0.5 * 10 ** ((pu_p - noise_floor) / (10 *slope)))) # log_based\n",
    "    elif pu_shape == 'point':\n",
    "        pu_param = None\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported PU shape(create_image)! \", pu_shape)\n",
    "    return pu_param\n",
    "\n",
    "def create_image(data, slope, sensors_num, style=\"raw_power_z_score\", noise_floor=-90, pu_shape= 'circle',\n",
    "                 pu_param=None, su_shape='circle', su_param=None, intensity_degradation=\"log\", \n",
    "                 max_pu_power: float=0, max_su_power: float=0):  \n",
    "    # style = {\"raw_power_zscore_norm\", \"image_intensity\", \"raw_power_min_max_norm\"}\n",
    "    # intensity_degradation= {\"log\", \"linear\"}\n",
    "    # if param is None, it's automatically calculated. Highest brightness(or power value) (255 or 1.) would\n",
    "    # assigned to the center(PU location) and radius(side) would be calculated based on its power, slope, and noise floor.\n",
    "    # If it is given, intensity(power) of pixel beside center would be calculated in the same fashin with an exception that \n",
    "    # intensity below zero(noise_floor) would be replaced by zero(noise_floor)\n",
    "    if style == \"raw_power_min_max_norm\":\n",
    "        # In this way, PUs' location are replaced with their power(dBm) and the power would fade with \n",
    "        # slope till gets noise_floor(in circle shape)\n",
    "        \n",
    "        # creating pu matrix\n",
    "        image = np.zeros((max_x, max_y, number_image_channels), dtype=float_memory_used)\n",
    "        if not sensors:\n",
    "            pus_num = int(data[0])\n",
    "#             print(pus_num)\n",
    "            for pu_i in range(pus_num):\n",
    "                pu_x = max(0, min(max_x-1, int(data[pu_i * 3 + 1] * pixel_expansion))) \n",
    "                pu_y = max(0, min(max_x-1, int(data[pu_i * 3 + 2] * pixel_expansion)))\n",
    "                pu_p = data[pu_i * 3 + 3]\n",
    "                pu_channel = int(abs(pu_p)//5) if number_image_channels > 3 else 0\n",
    "#                 print(pu_x, pu_y, pu_p)\n",
    "                if pu_param is None:\n",
    "                    pu_param_p = get_pu_param(pu_shape, intensity_degradation, pu_p, noise_floor, slope)\n",
    "                else:\n",
    "                    pu_param_p = pu_param\n",
    "                points = points_inside_shape(center=Point(pu_x, pu_y),\n",
    "                                             shape=pu_shape, param=pu_param_p)\n",
    "                for point in points:\n",
    "                    if 0 <= point.p.x < max_x and 0 <= point.p.y < max_y: # TODO should pass image size\n",
    "                        if intensity_degradation == \"linear\":\n",
    "                            image[int(abs(pu_p))//10][point.p.x][point.p.y] += (pu_p - slope * point.dist - noise_floor)/(\n",
    "                                max_pu_power - noise_floor)\n",
    "                        elif intensity_degradation == \"log\":\n",
    "                            if point.dist < 1:\n",
    "                                image[point.p.x][point.p.y][pu_channel] += (pu_p - noise_floor) / (max_pu_power - noise_floor)\n",
    "                            else:\n",
    "                                image[point.p.x][point.p.y][pu_channel] += (pu_p - slope * 10*math.log10(point.dist) - noise_floor)/(\n",
    "                                    max_pu_power - noise_floor)\n",
    "        else:\n",
    "            ss_channels_num = number_image_channels - 2\n",
    "            ss_param, ss_shape = pu_param, pu_shape\n",
    "            sensors_num = int(data[0])\n",
    "            sensors_num_at_each_row = int(sensors_num ** 0.5)\n",
    "            for ss_i in range(sensors_num):\n",
    "                ss_x = max(0, min(max_x-1, int(sensors_location[sensors_num][ss_i].x * pixel_expansion)))\n",
    "                ss_y =  max(0, min(max_x-1, int(sensors_location[sensors_num][ss_i].y * pixel_expansion)))\n",
    "                ss_p = max(noise_floor, data[ss_i+1])\n",
    "#                 ss_x, ss_y, ss_p = max(0, min(max_x-1, int(sensors_location[ss_i].x * pixel_expansion))), max(\n",
    "#                     0, min(max_x-1, int(sensors_location[ss_i].y * pixel_expansion))), max(noise_floor, data[ss_i])\n",
    "#                 ss_channel = 0 \n",
    "# #                 if -62.5 <= ss_p < -50.0:\n",
    "# #                     ss_channel = 1\n",
    "# #                 elif -70.0 <= ss_p < -62.6:\n",
    "# #                     ss_channel = 2\n",
    "# #                 elif -77.5 <= ss_p < -70.0:\n",
    "# #                     ss_channel = 3\n",
    "# #                 elif -85.0 <= ss_p < -77.5:\n",
    "# #                     ss_channel = 4\n",
    "# # #                 elif -70.0 <= ss_p < -65.0:\n",
    "# # #                     ss_channel = 5\n",
    "# #                 elif ss_p < -85.0:\n",
    "# #                     ss_channel = 5\n",
    "#                 if -70 <= ss_p < -60.0:\n",
    "#                     ss_channel = 1\n",
    "#                 elif -80.0 <= ss_p < -70:\n",
    "#                     ss_channel = 2\n",
    "#                 elif -90.0 <= ss_p < -80.0:\n",
    "#                     ss_channel = 3\n",
    "#                 elif -100.0 <= ss_p < -90.0:\n",
    "#                     ss_channel = 4\n",
    "#                 elif ss_p < -100.0:\n",
    "#                     ss_channel = 5\n",
    "#                 ss_channel = 0\n",
    "                ss_row, ss_cols = ss_i // sensors_num_at_each_row, ss_i % sensors_num_at_each_row\n",
    "                ss_channel = ss_cols % ss_channels_num\n",
    "                if ss_row % 2:\n",
    "                    ss_channel = (ss_channel + ss_channels_num // 2) % ss_channels_num\n",
    "                if ss_param is None:\n",
    "                    ss_param_p = get_pu_param(ss_shape, intensity_degradation, ss_p, noise_floor, slope)\n",
    "                else:\n",
    "                    ss_param_p = ss_param\n",
    "                points = points_inside_shape(center=Point(ss_x, ss_y), shape=ss_shape, param=ss_param_p)\n",
    "                for point in points:\n",
    "                    if 0 <= point.p.x < max_x and 0 <= point.p.y < max_y: # TODO should pass image size\n",
    "                        if intensity_degradation == \"linear\":\n",
    "                            image[point.p.x][point.p.y][ss_channel] += (ss_p - slope * point.dist - noise_floor)/(\n",
    "                                max_pu_power - noise_floor)\n",
    "                        elif intensity_degradation == \"log\":\n",
    "                            if point.dist < 1:\n",
    "                                image[point.p.x][point.p.y][ss_channel] += (ss_p - noise_floor) / (max_pu_power - noise_floor)\n",
    "                            else:\n",
    "                                image[point.p.x][point.p.y][ss_channel] += (ss_p - slope * 10*math.log10(point.dist) - noise_floor)/(\n",
    "                                    max_pu_power - noise_floor)\n",
    "        del points\n",
    "        # creating su matrix\n",
    "        su_num_idx = sensors_num + 1 if sensors else (pus_num * 3 + 1)\n",
    "        su_num = int(data[su_num_idx])\n",
    "#         print(su_num)\n",
    "#         su_num = (len(data) - pus_num * (3 if not sensors else 1)) // 2\n",
    "#         if not (len(data) - pus_num * (3 if not sensors else 1)) % 2:\n",
    "#             raise ValueError(\"Data provided is not correct; can't get SUs' information(create_image)\")\n",
    "        if su_param is None:\n",
    "            # if su_param is unavailable, a circle(square) with radius(side) 1 is created\n",
    "            if su_shape == 'circle':\n",
    "                su_param = Circle(1)\n",
    "            elif su_shape == 'square':\n",
    "                su_param = Square(1)\n",
    "            elif su_shape == 'point':\n",
    "                su_param = None\n",
    "            else:\n",
    "                raise ValueError(\"Unsupported SU shape(create_image)! \", su_shape)\n",
    "        \n",
    "        for su_i in range(su_num - 1):\n",
    "            su_x = max(0, min(max_x-1, int(data[su_num_idx + su_i * 3 + 1] * pixel_expansion)))\n",
    "            su_y = max(0, min(max_x-1, int(data[su_num_idx + su_i * 3 + 2] * pixel_expansion)))\n",
    "            su_p = data[su_num_idx + su_i * 3 + 3]\n",
    "#             su_p = su_intensity\n",
    "            su_param_p = get_pu_param(su_shape, intensity_degradation, su_p, noise_floor, su_slope)\n",
    "            points = points_inside_shape(center=Point(su_x, su_y),\n",
    "                                         param=su_param_p, shape=su_shape)\n",
    "            su_channel = 0 if number_image_channels == 1 else -2\n",
    "            for point in points:\n",
    "                if 0 <= point.p.x < max_x and 0 <= point.p.y < max_y: # TODO should pass image size\n",
    "                    if intensity_degradation == \"linear\":\n",
    "                            su_val = (su_p - su_slope * point.dist - noise_floor)/(max_su_power - noise_floor)\n",
    "                    elif intensity_degradation == \"log\":\n",
    "                        if point.dist < 1:\n",
    "                            su_val = (su_p - noise_floor) / (max_su_power - noise_floor)\n",
    "                        else:\n",
    "                            su_val = (su_p - su_slope * 10*math.log10(point.dist) - noise_floor)/(\n",
    "                                max_su_power - noise_floor)\n",
    "                    image[point.p.x][point.p.y][su_channel] += su_val\n",
    "            del points\n",
    "        # the last and  target SU\n",
    "        su_intensity = 1.\n",
    "        su_x = max(0, min(max_x-1, int(data[su_num_idx + (su_num - 1) * 3 + 1] * pixel_expansion)))\n",
    "        su_y = max(0, min(max_x-1, int(data[su_num_idx + (su_num - 1) * 3 + 2] * pixel_expansion)))\n",
    "#         print(su_x, su_y)\n",
    "#         print(su_shape)\n",
    "#         print(su_param)\n",
    "        points = points_inside_shape(center=Point(su_x, su_y),\n",
    "                                     param=su_param, shape=su_shape)\n",
    "        su_channel = -1\n",
    "        for point in points:\n",
    "            if 0 <= point.p.x < max_x and 0 <= point.p.y < max_y: # TODO should pass image size\n",
    "                image[point.p.x][point.p.y][su_channel] += su_intensity\n",
    "        del points\n",
    "        return image\n",
    "        \n",
    "#         pu_image = [[(noise_floor - mu)/sigma] * max_y for _ in range(max_x)]\n",
    "    elif style == \"image_intensity\":\n",
    "        # creating PU image\n",
    "        image = np.zeros((1,number_image_channels,max_x, max_y), dtype=float_memory_used)\n",
    "        pus_num = int(data[0])\n",
    "        for pu_i in range(pus_num):\n",
    "            pu_x = max(0, min(max_x-1, int(data[pu_i * 3 + 1] * pixel_expansion))) \n",
    "            pu_y = max(0, min(max_x-1, int(data[pu_i * 3 + 2] * pixel_expansion)))\n",
    "            pu_p = data[pu_i * 3 + 3]\n",
    "            \n",
    "            if pu_param is None:\n",
    "                pu_param_p = get_pu_param(pu_shape, intensity_degradation, pu_p, noise_floor, slope)\n",
    "            else:\n",
    "                pu_param_p = pu_param\n",
    "            \n",
    "            points = points_inside_shape(center=Point(pu_x, pu_y), shape=pu_shape, param=pu_param_p)\n",
    "            for point in points:\n",
    "                if 0 <= point.p.x < max_x and 0 <= point.p.y < max_y: # TODO should pass image size\n",
    "                    if intensity_degradation == \"linear\":\n",
    "                        image[0][0][point.p.x][point.p.y] += (pu_p - slope * point.dist - noise_floor)/(\n",
    "                            max_pu_power - noise_floor)\n",
    "                    elif intensity_degradation == \"log\":\n",
    "                        if point.dist < 1:\n",
    "#                             image[0][0][point.p.x][point.p.y] += (pu_p - noise_floor) / (max_pu_power - noise_floor)\n",
    "                            image[0][0][point.p.x][point.p.y] += 0.1\n",
    "                        else:\n",
    "#                             image[0][0][point.p.x][point.p.y] += (pu_p - slope * 10*math.log10(point.dist) - noise_floor)/(\n",
    "#                                 max_pu_power - noise_floor)\n",
    "                            image[0][0][point.p.x][point.p.y] += 0.1\n",
    "                        \n",
    "        # creating SU image\n",
    "        del points\n",
    "        # creating su matrix\n",
    "        su_num_idx = sensors_num if sensors else (pus_num * 3 + 1)\n",
    "        su_num = int(data[su_num_idx])\n",
    "#         print(su_num)\n",
    "#         su_num = (len(data) - pus_num * (3 if not sensors else 1)) // 2\n",
    "#         if not (len(data) - pus_num * (3 if not sensors else 1)) % 2:\n",
    "#             raise ValueError(\"Data provided is not correct; can't get SUs' information(create_image)\")\n",
    "        if su_param is None:\n",
    "            # if su_param is unavailable, a circle(square) with radius(side) 1 is created\n",
    "            if su_shape == 'circle':\n",
    "                su_param = Circle(1)\n",
    "            elif su_shape == 'square':\n",
    "                su_param = Square(1)\n",
    "            elif su_shape == 'point':\n",
    "                su_param = None\n",
    "            else:\n",
    "                raise ValueError(\"Unsupported SU shape(create_image)! \", su_shape)\n",
    "        \n",
    "        for su_i in range(su_num - 1):\n",
    "            su_x = max(0, min(max_x-1, int(data[su_num_idx + su_i * 3 + 1])))\n",
    "            su_y = max(0, min(max_x-1, int(data[su_num_idx + su_i * 3 + 2])))\n",
    "            su_p = data[su_num_idx + su_i * 3 + 3]\n",
    "            \n",
    "#             su_p = su_intensity\n",
    "            points = points_inside_shape(center=Point(su_x, su_y), param=su_param, shape=su_shape)\n",
    "            su_channel = 0 if number_image_channels == 1 else -1\n",
    "            for point in points:\n",
    "                if 0 <= point.p.x < max_x and 0 <= point.p.y < max_y: # TODO should pass image size\n",
    "                    if intensity_degradation == \"linear\":\n",
    "                            su_val = (su_p - slope * point.dist - noise_floor)/(max_pu_power - noise_floor)\n",
    "                    elif intensity_degradation == \"log\":\n",
    "                        if point.dist < 1:\n",
    "                            su_val = (su_p - noise_floor) / (max_pu_power - noise_floor)\n",
    "                        else:\n",
    "                            su_val = (su_p - slope * 10*math.log10(point.dist) - noise_floor)/(\n",
    "                                max_pu_power - noise_floor)\n",
    "                    image[0][su_channel][point.p.x][point.p.y] += su_val\n",
    "            del points\n",
    "        # the last and  target SU\n",
    "        su_intensity = 1.\n",
    "        su_x = max(0, min(max_x-1, int(data[su_num_idx + (su_num - 1) * 3 + 1])))\n",
    "        su_y = max(0, min(max_x-1, int(data[su_num_idx + (su_num - 1) * 3 + 2])))\n",
    "#         print(su_x, su_y)\n",
    "        points = points_inside_shape(center=Point(su_x, su_y), param=su_param, shape=su_shape)\n",
    "        su_channel = 0 if number_image_channels == 1 else -1\n",
    "        for point in points:\n",
    "            if 0 <= point.p.x < max_x and 0 <= point.p.y < max_y: # TODO should pass image size\n",
    "                image[0][su_channel][point.p.x][point.p.y] += su_intensity\n",
    "        del points\n",
    "        return image       \n",
    "            \n",
    "    else:\n",
    "        raise ValueError(\"Unsupported style(create_image)! \", style)\n",
    "        \n",
    "def points_inside_shape(center: Point, shape: str, param)-> list:\n",
    "    # This function returns points+distance around center with defined shape\n",
    "    if shape == 'circle':\n",
    "        # First creates points inside a square(around orgigin) with 2*r side and then remove those with distance > r.\n",
    "        # Shift all remaining around center. O(4r^2)\n",
    "        r, origin = param.r, Point(0, 0)\n",
    "        square_points = set((Point(x, y) for x in range(max(-int(r/cell_size), -max_x), \n",
    "                             min(int(r/cell_size), max_x) + 1) \n",
    "                             for y in range(max(-int(r/cell_size), -max_y), min(int(r/cell_size), max_y) + 1)))\n",
    "        points = []\n",
    "        while square_points:\n",
    "            p = square_points.pop()\n",
    "            dist = euclidian_distance(p, origin)\n",
    "            if dist <= r:\n",
    "                points.append(PointWithDistance(Point(p.x + center.x, p.y + center.y), dist))\n",
    "                if p.x != 0:\n",
    "                    points.append(PointWithDistance(Point(-p.x + center.x, p.y + center.y), dist))\n",
    "                    square_points.remove(Point(-p.x, p.y))\n",
    "                if p.y != 0:\n",
    "                    points.append(PointWithDistance(Point(p.x + center.x, -p.y + center.y), dist))\n",
    "                    square_points.remove(Point(p.x, -p.y))\n",
    "                if p.x != 0 and p.y != 0:\n",
    "                    points.append(PointWithDistance(Point(-p.x + center.x, -p.y + center.y), dist))\n",
    "                    square_points.remove(Point(-p.x, -p.y))\n",
    "        del square_points\n",
    "        return points\n",
    "    elif shape == 'square':\n",
    "        half_side = param.side // 2\n",
    "        return [PointWithDistance(Point(x, y), euclidian_distance(Point(x, y), center)) for x in range(-half_side + center.x,\n",
    "                                                                                               half_side + center.x+1) \n",
    "                         for y in range(-half_side + center.y, half_side + center.y + 1)]\n",
    "    elif shape == 'point':\n",
    "        return [PointWithDistance(center, 0)]\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported shape(points_inside_shape)! \", shape)\n",
    "        \n",
    "def read_image(image_num, image_dir=image_dir):\n",
    "    if False and style == \"image_intensity\":\n",
    "        image = plt.imread(image_dir + '/image' + str(image_num)+'.png')\n",
    "        image = np.swapaxes(image, 0, 2)\n",
    "        image = np.array(image[:number_image_channels], dtype=float_memory_used).reshape(1, number_image_channels, max_x, max_y)\n",
    "    elif  style == \"raw_power_min_max_norm\" or style == \"raw_power_zscore_norm\" or style == \"image_intensity\":\n",
    "        suffix = 'npz'  # npy, npz\n",
    "#         image = np.load(f\"{image_dir}/images{image_num//100000}/image{image_num}.{suffix}\") \n",
    "        image = np.load(f\"{image_dir}/image{image_num}.{suffix}\") \n",
    "        if type(image) == np.lib.npyio.NpzFile:\n",
    "            image = image['a']\n",
    "    \n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 2048\n",
    "data_reg_train = np.repeat(data_reg[:train_size], 4, axis=0)\n",
    "image_state = [\"\", \"rot\", \"lr\", \"ud\"] * train_size\n",
    "p = np.random.permutation(train_size*4)\n",
    "data_reg_train = data_reg_train[p]\n",
    "new_image_state = []\n",
    "for idx in range(train_size*4):\n",
    "    new_image_state.append(image_state[p[idx]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def creating_image(start, end):\n",
    "    # for image_num in range(115, data_reg.shape[0]):\n",
    "    # for image_num in range(1625, 5000):\n",
    "    for image_num in tqdm.tqdm(range(start, end+1)):  #4463, data_reg.shape[0]\n",
    "        image = create_image(data=data_reg_train[image_num], slope=slope, style=style, \n",
    "                             noise_floor=noise_floor,\n",
    "                             pu_shape=pu_shape, su_shape=su_shape, su_param=su_param, \n",
    "                             sensors_num=(sensors_num if sensors else 0), \n",
    "                             intensity_degradation=intensity_degradation, \n",
    "                             max_pu_power=0.0 if not sensors else -60)\n",
    "        if new_image_state[image_num] == \"rot\":\n",
    "            image = np.rot90(image, 2)\n",
    "        elif new_image_state[image_num] == \"lr\":\n",
    "            image = np.fliplr(image)\n",
    "        elif new_image_state[image_num] == \"ud\":\n",
    "            image = np.flipud(image)\n",
    "        np.savez_compressed(image_dir + '/aug/image' + str(image_num), a=np.expand_dims(image,0 ))\n",
    "        \n",
    "        del image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_reg_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2260025/1061041538.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mjobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mproc_sizes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdata_reg_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mnumber_of_proccessors\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnumber_of_proccessors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mproc_sizes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mdata_reg_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0mnumber_of_proccessors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mproc_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproc_sizes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproc_sizes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumber_of_proccessors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data_reg_train' is not defined"
     ]
    }
   ],
   "source": [
    "jobs = []\n",
    "proc_sizes = [data_reg_train.shape[0]//number_of_proccessors] * (number_of_proccessors)\n",
    "proc_sizes[-1] += data_reg_train.shape[0]%number_of_proccessors\n",
    "proc_idx = [(sum(proc_sizes[:i]), sum(proc_sizes[:i+1])-1) for i in range(number_of_proccessors)]\n",
    "\n",
    "for i in range(number_of_proccessors):\n",
    "    p = Process(target=creating_image, args=(proc_idx[i][0], proc_idx[i][1]))\n",
    "    jobs.append(p)\n",
    "    p.start()\n",
    "for i in range(number_of_proccessors):\n",
    "    jobs[i].join()\n",
    "\n",
    "for i in range(number_of_proccessors):\n",
    "    jobs[i].terminate()\n",
    "    jobs[i].close()\n",
    "del jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving images once to save time\n",
    "# run this cell just for creating images\n",
    "def creating_image(start, end):\n",
    "    # for image_num in range(115, data_reg.shape[0]):\n",
    "    # for image_num in range(1625, 5000):\n",
    "    for image_num in tqdm.tqdm(range(start, end+1)):  #4463, data_reg.shape[0]\n",
    "        image = create_image(data=data_reg[image_num], slope=slope, style=style, \n",
    "                             noise_floor=noise_floor,\n",
    "                             pu_shape=pu_shape, su_shape=su_shape, su_param=su_param, \n",
    "                             sensors_num=(sensors_num if sensors else 0), \n",
    "                             intensity_degradation=intensity_degradation, \n",
    "                             max_pu_power=0.0 if not sensors else -60,\n",
    "                             max_su_power=40.0)\n",
    "        if False and style == \"image_intensity\":\n",
    "            if number_image_channels != 3:\n",
    "                image = np.append(np.array(image[0]), np.zeros((3-number_image_channels,max_x, max_y), \n",
    "                                                               dtype=float_memory_used), axis=0)\n",
    "            image_save = np.swapaxes(image, 0, 2)\n",
    "            plt.imsave(image_dir + '/image' + str(image_num)+'.png', image_save)\n",
    "        elif style == \"raw_power_min_max_norm\" or style == \"raw_power_zscore_norm\" or style == \"image_intensity\":\n",
    "    #         np.save(image_dir + '/image' + str(image_num), image)\n",
    "#             np.savez_compressed(f\"{image_dir}{(600000 + image_num)//100000}/image{600000 + image_num}\",\n",
    "#                                 a=np.expand_dims(image,0))\n",
    "            np.savez_compressed(f\"{image_dir}/image{image_num}\",\n",
    "                                a=np.expand_dims(image,0))\n",
    "        del image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|██████████████████████████████████████▎| 3711/3772 [29:41<00:27,  2.21it/s]\n",
      "100%|███████████████████████████████████████| 3774/3774 [29:45<00:00,  2.11it/s]\n",
      "100%|███████████████████████████████████████| 3772/3772 [29:46<00:00,  2.11it/s]\n",
      "100%|███████████████████████████████████████| 3772/3772 [29:53<00:00,  2.10it/s]\n",
      "100%|███████████████████████████████████████| 3772/3772 [29:55<00:00,  2.10it/s]\n",
      " 99%|██████████████████████████████████████▊| 3749/3772 [29:57<00:07,  3.10it/s]\n",
      "100%|███████████████████████████████████████| 3772/3772 [29:57<00:00,  2.10it/s]\n",
      "100%|███████████████████████████████████████| 3772/3772 [29:58<00:00,  2.10it/s]\n",
      "100%|███████████████████████████████████████| 3772/3772 [29:58<00:00,  2.10it/s]\n",
      "100%|███████████████████████████████████████| 3772/3772 [29:59<00:00,  2.10it/s]\n",
      "100%|███████████████████████████████████████| 3772/3772 [30:03<00:00,  2.09it/s]\n",
      "100%|███████████████████████████████████████| 3772/3772 [30:03<00:00,  2.09it/s]\n"
     ]
    }
   ],
   "source": [
    "jobs = []\n",
    "proc_sizes = [data_reg.shape[0]//number_of_proccessors] * (number_of_proccessors)\n",
    "proc_sizes[-1] += data_reg.shape[0]%number_of_proccessors\n",
    "proc_idx = [(sum(proc_sizes[:i]), sum(proc_sizes[:i+1])-1) for i in range(number_of_proccessors)]\n",
    "\n",
    "for i in range(number_of_proccessors):\n",
    "    p = Process(target=creating_image, args=(proc_idx[i][0], proc_idx[i][1]))\n",
    "    jobs.append(p)\n",
    "    p.start()\n",
    "for i in range(number_of_proccessors):\n",
    "    jobs[i].join()\n",
    "\n",
    "for i in range(number_of_proccessors):\n",
    "    jobs[i].terminate()\n",
    "    jobs[i].close()\n",
    "del jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sensors_location' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2290659/2692102101.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msensors_location\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m225\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m225\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'sensors_location' is not defined"
     ]
    }
   ],
   "source": [
    "sensors_location[225][225]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 49.   , -67.973, -50.789, -72.345, -72.867, -60.808, -68.141,\n",
       "       -73.054, -65.912, -66.931, -81.242, -78.197, -68.693, -68.276,\n",
       "       -75.171, -76.76 , -68.616, -79.3  , -76.408, -70.583, -66.942,\n",
       "       -68.405, -74.092, -75.976, -75.671, -76.973, -78.145, -68.874,\n",
       "       -67.967, -74.006, -71.048, -81.523, -77.934, -76.878, -76.477,\n",
       "       -78.388, -55.4  , -73.306, -76.155, -82.771, -78.345, -84.242,\n",
       "       -85.043, -68.135, -85.481, -85.117, -79.689, -85.389, -88.734,\n",
       "       -89.126,   1.   ,  27.   ,  52.   ,  35.852,   0.   ,  22.075])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_reg[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/shahrokh/projects/MLSpectrumAllocation/ML/data/pictures_299_299_transfer/log/noisy_std_1/pu_circle_su_circle_60/raw_power_min_max_norm/color/log_pu5_su6/variable_sensors_10_20_pus_5_sus_8_channels/images'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "imm = read_image(1)\n",
    "new_imm = np.zeros((max_x, max_y), dtype=float_memory_used)\n",
    "for x in range(max_x):\n",
    "    for y in range(max_y):\n",
    "        new_imm[x][y] = imm[0][x][y][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39.059"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(data_reg[:,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imm = read_image(0, image_dir=f\"{image_dir}\")#, image_dir=\"/home/shahrokh/projects/MLSpectrumAllocation/ML/data/pictures_299_299_transfer/splat/pu_circle_su_circle_60/raw_power_min_max_norm/color/log_5/pus_1_sus_3_channels/images\")\n",
    "immm = np.zeros((max_x, max_y, number_image_channels), dtype='float')\n",
    "for x in range(max_x):\n",
    "    for y in range(max_y):\n",
    "        for ch in range(number_image_channels):\n",
    "            immm[x][y][ch] = float(imm[0][x][y][ch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "immm[immm>1] = 1.0\n",
    "plt.imshow(immm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## immm[immm>1] = 1.0\n",
    "plt.imshow(immm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_reg[123853,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rotated_im = np.rot90(immm, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "img_rot = ndimage.rotate(immm, -90, reshape=False)\n",
    "img_rot = np.rot90(immm,2)\n",
    "plt.imshow(img_rot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 12.   ,  83.   ,  80.   ,  -6.513,  57.   ,  14.   , -16.233,\n",
       "        80.   ,   9.   ,  -4.876,  74.   ,  75.   , -26.081,  42.   ,\n",
       "        49.   ,  -4.702,  36.   ,   7.   , -20.297,  23.   ,  73.   ,\n",
       "        -5.415,  87.   ,  80.   ,  -8.679,   2.   ,  14.   ,  -1.331,\n",
       "        15.   ,  20.   , -16.955,  29.   ,  88.   ,  -2.918,  93.   ,\n",
       "        87.   , -12.061,   1.   ,  65.   ,  28.   , -31.732,   1.   ,\n",
       "           nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
       "           nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
       "           nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
       "           nan,     nan,     nan,  14.658])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_reg[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sensors_location[49]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x1080 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "fig, ax = plt.subplots(figsize=(15,15))         # Sample figsize in inches\n",
    "sns.heatmap(imm[0,:,:,2], ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_model(kernel_lam, bias_lam):\n",
    "    inputs = Input(shape=(max_x, max_y, number_image_channels))\n",
    "    convolution_filter, dense_filter = 'relu', 'linear'\n",
    "    convolution_init, dense_init = \"lecun_normal\", \"RandomNormal\"\n",
    "    # We make sure that the base_model is running in inference mode here,\n",
    "    # by passing `training=False`. This is important for fine-tuning, as you will\n",
    "    # learn in a few paragraphs.\n",
    "#     base_model.trainable = False\n",
    "    cnn = base_model(inputs, training=False)\n",
    "#     cnn = base_model(inputs)\n",
    "    \n",
    "#     cnn = layers.Dense(1024, activation=convolution_filter, kernel_regularizer=regularizers.l2(kernel_lam),\n",
    "#                              bias_regularizer=regularizers.l2(bias_lam), kernel_initializer=convolution_init)(cnn)\n",
    "#     cnn = layers.Dropout(0.5)(cnn)\n",
    "    if True:\n",
    "        # Convert features of shape `base_model.output_shape[1:]` to vectors\n",
    "        cnn = layers.GlobalAveragePooling2D()(cnn)\n",
    "        cnn = layers.Dense(2048, activation=convolution_filter, kernel_regularizer=regularizers.l2(kernel_lam),\n",
    "                             bias_regularizer=regularizers.l2(bias_lam), kernel_initializer=convolution_init)(cnn)\n",
    "        cnn = layers.Dropout(0.5)(cnn)\n",
    "        outputs = layers.Dense(1, activation=dense_filter, kernel_regularizer=regularizers.l2(kernel_lam),\n",
    "                             bias_regularizer=regularizers.l2(bias_lam), kernel_initializer=dense_init)(cnn)\n",
    "        return Model(inputs, outputs)\n",
    "    else:\n",
    "        data_format=\"channels_last\"\n",
    "        filter_shape, pool_size = (1, 1), (2,2)\n",
    "        cnn = layers.Conv2D(512, filter_shape, padding='same', \n",
    "                            activation=convolution_filter, data_format=data_format, \n",
    "                            kernel_regularizer=regularizers.l2(kernel_lam), \n",
    "                            bias_regularizer=regularizers.l2(bias_lam),\n",
    "                         kernel_initializer=convolution_init)(cnn)\n",
    "        cnn = layers.Conv2D(512, filter_shape,padding='same', activation=convolution_filter, data_format=data_format, \n",
    "                         kernel_regularizer=regularizers.l2(kernel_lam), bias_regularizer=regularizers.l2(bias_lam),\n",
    "                         kernel_initializer=convolution_init)(cnn)\n",
    "        cnn = layers.Conv2D(512, filter_shape,padding='same', activation=convolution_filter, data_format=data_format, \n",
    "                         kernel_regularizer=regularizers.l2(kernel_lam), bias_regularizer=regularizers.l2(bias_lam),\n",
    "                         kernel_initializer=convolution_init)(cnn)\n",
    "        cnn = layers.MaxPooling2D(pool_size=pool_size, data_format=data_format)(cnn)\n",
    "        \n",
    "        cnn = layers.Conv2D(512, filter_shape,padding='same', activation=convolution_filter, data_format=data_format, \n",
    "                         kernel_regularizer=regularizers.l2(kernel_lam), bias_regularizer=regularizers.l2(bias_lam),\n",
    "                         kernel_initializer=convolution_init)(cnn)\n",
    "        cnn = layers.Conv2D(512, filter_shape,padding='same', activation=convolution_filter,\n",
    "                            data_format=data_format, \n",
    "                         kernel_regularizer=regularizers.l2(kernel_lam), bias_regularizer=regularizers.l2(bias_lam),\n",
    "                         kernel_initializer=convolution_init)(cnn)\n",
    "        cnn = layers.Conv2D(512, filter_shape,padding='same', activation=convolution_filter, data_format=data_format, \n",
    "                         kernel_regularizer=regularizers.l2(kernel_lam), bias_regularizer=regularizers.l2(bias_lam),\n",
    "                         kernel_initializer=convolution_init)(cnn)\n",
    "        cnn = layers.MaxPooling2D(pool_size=pool_size, data_format=data_format)(cnn)\n",
    "        \n",
    "        cnn = layers.Conv2D(512, filter_shape,padding='same', activation=convolution_filter, data_format=data_format, \n",
    "                         kernel_regularizer=regularizers.l2(kernel_lam), bias_regularizer=regularizers.l2(bias_lam),\n",
    "                         kernel_initializer=convolution_init)(cnn)\n",
    "        cnn = layers.Conv2D(512, filter_shape,padding='same', activation=convolution_filter,\n",
    "                            data_format=data_format, \n",
    "                         kernel_regularizer=regularizers.l2(kernel_lam), bias_regularizer=regularizers.l2(bias_lam),\n",
    "                         kernel_initializer=convolution_init)(cnn)\n",
    "        cnn = layers.Conv2D(512, filter_shape,padding='same', activation=convolution_filter, data_format=data_format, \n",
    "                         kernel_regularizer=regularizers.l2(kernel_lam), bias_regularizer=regularizers.l2(bias_lam),\n",
    "                         kernel_initializer=convolution_init)(cnn)\n",
    "        cnn = layers.MaxPooling2D(pool_size=pool_size, data_format=data_format)(cnn)\n",
    "        cnn = layers.Conv2D(512, filter_shape,padding='same', activation=convolution_filter, data_format=data_format, \n",
    "                         kernel_regularizer=regularizers.l2(kernel_lam), bias_regularizer=regularizers.l2(bias_lam),\n",
    "                         kernel_initializer=convolution_init)(cnn)\n",
    "        cnn = layers.Conv2D(128, filter_shape,padding='same', activation=convolution_filter,\n",
    "                            data_format=data_format, \n",
    "                         kernel_regularizer=regularizers.l2(kernel_lam), bias_regularizer=regularizers.l2(bias_lam),\n",
    "                         kernel_initializer=convolution_init)(cnn)\n",
    "        cnn = layers.Conv2D(32, filter_shape,padding='same', activation=convolution_filter, data_format=data_format, \n",
    "                         kernel_regularizer=regularizers.l2(kernel_lam), bias_regularizer=regularizers.l2(bias_lam),\n",
    "                         kernel_initializer=convolution_init)(cnn)\n",
    "        cnn = layers.Conv2D(8, filter_shape,padding='same', activation=convolution_filter, data_format=data_format, \n",
    "                         kernel_regularizer=regularizers.l2(kernel_lam), bias_regularizer=regularizers.l2(bias_lam),\n",
    "                         kernel_initializer=convolution_init)(cnn)\n",
    "        cnn = layers.Conv2D(1, filter_shape,padding='same', activation=convolution_filter, data_format=data_format, \n",
    "                         kernel_regularizer=regularizers.l2(kernel_lam), bias_regularizer=regularizers.l2(bias_lam),\n",
    "                         kernel_initializer=convolution_init)(cnn)\n",
    "        cnn = layers.GlobalAveragePooling2D()(cnn)\n",
    "        return Model(inputs, cnn)\n",
    "    \n",
    "\n",
    "class DataBatchGenerator(Sequence):\n",
    "    def __init__(self, dataset:np.ndarray, batch_size:int, start_idx:int,\n",
    "                 number_image_channels:int,\n",
    "                 max_x, max_y, float_memory_used, image_dir = image_dir, conserve=0):\n",
    "#         print(dataset.shape[0])\n",
    "#         print(\"generator initiated\")\n",
    "        self.dataset, self.batch_size, self.start_idx = dataset, batch_size, start_idx\n",
    "        self.number_image_channels, self.max_x, self.max_y = number_image_channels, max_x, max_y\n",
    "        self.float_memory_used = float_memory_used\n",
    "        self.conserve = conserve\n",
    "        self._image_dir = image_dir\n",
    "    \n",
    "    def __len__(self):\n",
    "        return np.ceil(self.dataset.shape[0] / self.batch_size).astype(int)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "#         print('generator yielded a batch %d' % idx)\n",
    "        size = min(self.dataset.shape[0] - idx * self.batch_size, self.batch_size)\n",
    "        batch_x = np.empty((size, self.max_x, self.max_y, self.number_image_channels), dtype=self.float_memory_used)\n",
    "        batch_y = np.empty((size), dtype=self.float_memory_used)\n",
    "        for i in range(size):\n",
    "            batch_x[i] = read_image(self.start_idx + idx * self.batch_size + i, self._image_dir)\n",
    "            batch_y[i] = self.dataset[idx * self.batch_size + i][- 1 - self.conserve]\n",
    "        return batch_x, batch_y\n",
    "    \n",
    "def custom_loss(fp_penalty_coef, fn_penalty_coef):\n",
    "    # custom loss function that penalize false positive and negative differently\n",
    "    def loss(y_true, y_pred):\n",
    "        res = y_pred - y_true\n",
    "        res = tf.where(res > 0, res * fp_penalty_coef, res * fn_penalty_coef)\n",
    "        return K.mean(K.square(res))\n",
    "    return loss\n",
    "\n",
    "def fp_mae(y_true, y_pred):\n",
    "    # custom metric that replace false negative with zero and return the mean of new vector\n",
    "    res = y_pred - y_true\n",
    "    res = tf.nn.relu(res)\n",
    "#     res = tf.where(res <= 0, 0, res)\n",
    "    return K.mean(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shahrokh/miniconda3/envs/research/lib/python3.9/site-packages/keras/applications/imagenet_utils.py:331: UserWarning: This model usually expects 1 or 3 input channels. However, it was passed an input_shape with 8 input channels.\n",
      "  warnings.warn('This model usually expects 1 or 3 input channels. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"resnet101\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_17 (InputLayer)           [(None, 300, 300, 8) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1_pad (ZeroPadding2D)       (None, 306, 306, 8)  0           input_17[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1_conv (Conv2D)             (None, 150, 150, 64) 25152       conv1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1_bn (BatchNormalization)   (None, 150, 150, 64) 256         conv1_conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1_relu (Activation)         (None, 150, 150, 64) 0           conv1_bn[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pad (ZeroPadding2D)       (None, 152, 152, 64) 0           conv1_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pool (MaxPooling2D)       (None, 75, 75, 64)   0           pool1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_conv (Conv2D)    (None, 75, 75, 64)   4160        pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_bn (BatchNormali (None, 75, 75, 64)   256         conv2_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_relu (Activation (None, 75, 75, 64)   0           conv2_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_conv (Conv2D)    (None, 75, 75, 64)   36928       conv2_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_bn (BatchNormali (None, 75, 75, 64)   256         conv2_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_relu (Activation (None, 75, 75, 64)   0           conv2_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_conv (Conv2D)    (None, 75, 75, 256)  16640       pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_conv (Conv2D)    (None, 75, 75, 256)  16640       conv2_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_bn (BatchNormali (None, 75, 75, 256)  1024        conv2_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_bn (BatchNormali (None, 75, 75, 256)  1024        conv2_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_add (Add)          (None, 75, 75, 256)  0           conv2_block1_0_bn[0][0]          \n",
      "                                                                 conv2_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_out (Activation)   (None, 75, 75, 256)  0           conv2_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_conv (Conv2D)    (None, 75, 75, 64)   16448       conv2_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_bn (BatchNormali (None, 75, 75, 64)   256         conv2_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_relu (Activation (None, 75, 75, 64)   0           conv2_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_conv (Conv2D)    (None, 75, 75, 64)   36928       conv2_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_bn (BatchNormali (None, 75, 75, 64)   256         conv2_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_relu (Activation (None, 75, 75, 64)   0           conv2_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_3_conv (Conv2D)    (None, 75, 75, 256)  16640       conv2_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_3_bn (BatchNormali (None, 75, 75, 256)  1024        conv2_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_add (Add)          (None, 75, 75, 256)  0           conv2_block1_out[0][0]           \n",
      "                                                                 conv2_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_out (Activation)   (None, 75, 75, 256)  0           conv2_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_conv (Conv2D)    (None, 75, 75, 64)   16448       conv2_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_bn (BatchNormali (None, 75, 75, 64)   256         conv2_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_relu (Activation (None, 75, 75, 64)   0           conv2_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_conv (Conv2D)    (None, 75, 75, 64)   36928       conv2_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_bn (BatchNormali (None, 75, 75, 64)   256         conv2_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_relu (Activation (None, 75, 75, 64)   0           conv2_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_3_conv (Conv2D)    (None, 75, 75, 256)  16640       conv2_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_3_bn (BatchNormali (None, 75, 75, 256)  1024        conv2_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_add (Add)          (None, 75, 75, 256)  0           conv2_block2_out[0][0]           \n",
      "                                                                 conv2_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_out (Activation)   (None, 75, 75, 256)  0           conv2_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_conv (Conv2D)    (None, 38, 38, 128)  32896       conv2_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_bn (BatchNormali (None, 38, 38, 128)  512         conv3_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_relu (Activation (None, 38, 38, 128)  0           conv3_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_conv (Conv2D)    (None, 38, 38, 128)  147584      conv3_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_bn (BatchNormali (None, 38, 38, 128)  512         conv3_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_relu (Activation (None, 38, 38, 128)  0           conv3_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_0_conv (Conv2D)    (None, 38, 38, 512)  131584      conv2_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_3_conv (Conv2D)    (None, 38, 38, 512)  66048       conv3_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_0_bn (BatchNormali (None, 38, 38, 512)  2048        conv3_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_3_bn (BatchNormali (None, 38, 38, 512)  2048        conv3_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_add (Add)          (None, 38, 38, 512)  0           conv3_block1_0_bn[0][0]          \n",
      "                                                                 conv3_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_out (Activation)   (None, 38, 38, 512)  0           conv3_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_conv (Conv2D)    (None, 38, 38, 128)  65664       conv3_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_bn (BatchNormali (None, 38, 38, 128)  512         conv3_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_relu (Activation (None, 38, 38, 128)  0           conv3_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_conv (Conv2D)    (None, 38, 38, 128)  147584      conv3_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_bn (BatchNormali (None, 38, 38, 128)  512         conv3_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_relu (Activation (None, 38, 38, 128)  0           conv3_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_3_conv (Conv2D)    (None, 38, 38, 512)  66048       conv3_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_3_bn (BatchNormali (None, 38, 38, 512)  2048        conv3_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_add (Add)          (None, 38, 38, 512)  0           conv3_block1_out[0][0]           \n",
      "                                                                 conv3_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_out (Activation)   (None, 38, 38, 512)  0           conv3_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_conv (Conv2D)    (None, 38, 38, 128)  65664       conv3_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_bn (BatchNormali (None, 38, 38, 128)  512         conv3_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_relu (Activation (None, 38, 38, 128)  0           conv3_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_conv (Conv2D)    (None, 38, 38, 128)  147584      conv3_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_bn (BatchNormali (None, 38, 38, 128)  512         conv3_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_relu (Activation (None, 38, 38, 128)  0           conv3_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_3_conv (Conv2D)    (None, 38, 38, 512)  66048       conv3_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_3_bn (BatchNormali (None, 38, 38, 512)  2048        conv3_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_add (Add)          (None, 38, 38, 512)  0           conv3_block2_out[0][0]           \n",
      "                                                                 conv3_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_out (Activation)   (None, 38, 38, 512)  0           conv3_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_conv (Conv2D)    (None, 38, 38, 128)  65664       conv3_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_bn (BatchNormali (None, 38, 38, 128)  512         conv3_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_relu (Activation (None, 38, 38, 128)  0           conv3_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_conv (Conv2D)    (None, 38, 38, 128)  147584      conv3_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_bn (BatchNormali (None, 38, 38, 128)  512         conv3_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_relu (Activation (None, 38, 38, 128)  0           conv3_block4_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_3_conv (Conv2D)    (None, 38, 38, 512)  66048       conv3_block4_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_3_bn (BatchNormali (None, 38, 38, 512)  2048        conv3_block4_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_add (Add)          (None, 38, 38, 512)  0           conv3_block3_out[0][0]           \n",
      "                                                                 conv3_block4_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_out (Activation)   (None, 38, 38, 512)  0           conv3_block4_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_conv (Conv2D)    (None, 19, 19, 256)  131328      conv3_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_bn (BatchNormali (None, 19, 19, 256)  1024        conv4_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_relu (Activation (None, 19, 19, 256)  0           conv4_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_conv (Conv2D)    (None, 19, 19, 256)  590080      conv4_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_bn (BatchNormali (None, 19, 19, 256)  1024        conv4_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_relu (Activation (None, 19, 19, 256)  0           conv4_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_0_conv (Conv2D)    (None, 19, 19, 1024) 525312      conv3_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_3_conv (Conv2D)    (None, 19, 19, 1024) 263168      conv4_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_0_bn (BatchNormali (None, 19, 19, 1024) 4096        conv4_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_3_bn (BatchNormali (None, 19, 19, 1024) 4096        conv4_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_add (Add)          (None, 19, 19, 1024) 0           conv4_block1_0_bn[0][0]          \n",
      "                                                                 conv4_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_out (Activation)   (None, 19, 19, 1024) 0           conv4_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_conv (Conv2D)    (None, 19, 19, 256)  262400      conv4_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_bn (BatchNormali (None, 19, 19, 256)  1024        conv4_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_relu (Activation (None, 19, 19, 256)  0           conv4_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_conv (Conv2D)    (None, 19, 19, 256)  590080      conv4_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_bn (BatchNormali (None, 19, 19, 256)  1024        conv4_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_relu (Activation (None, 19, 19, 256)  0           conv4_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_3_conv (Conv2D)    (None, 19, 19, 1024) 263168      conv4_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_3_bn (BatchNormali (None, 19, 19, 1024) 4096        conv4_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_add (Add)          (None, 19, 19, 1024) 0           conv4_block1_out[0][0]           \n",
      "                                                                 conv4_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_out (Activation)   (None, 19, 19, 1024) 0           conv4_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_conv (Conv2D)    (None, 19, 19, 256)  262400      conv4_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_bn (BatchNormali (None, 19, 19, 256)  1024        conv4_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_relu (Activation (None, 19, 19, 256)  0           conv4_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_conv (Conv2D)    (None, 19, 19, 256)  590080      conv4_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_bn (BatchNormali (None, 19, 19, 256)  1024        conv4_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_relu (Activation (None, 19, 19, 256)  0           conv4_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_3_conv (Conv2D)    (None, 19, 19, 1024) 263168      conv4_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_3_bn (BatchNormali (None, 19, 19, 1024) 4096        conv4_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_add (Add)          (None, 19, 19, 1024) 0           conv4_block2_out[0][0]           \n",
      "                                                                 conv4_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_out (Activation)   (None, 19, 19, 1024) 0           conv4_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_conv (Conv2D)    (None, 19, 19, 256)  262400      conv4_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_bn (BatchNormali (None, 19, 19, 256)  1024        conv4_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_relu (Activation (None, 19, 19, 256)  0           conv4_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_conv (Conv2D)    (None, 19, 19, 256)  590080      conv4_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_bn (BatchNormali (None, 19, 19, 256)  1024        conv4_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_relu (Activation (None, 19, 19, 256)  0           conv4_block4_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_3_conv (Conv2D)    (None, 19, 19, 1024) 263168      conv4_block4_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_3_bn (BatchNormali (None, 19, 19, 1024) 4096        conv4_block4_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_add (Add)          (None, 19, 19, 1024) 0           conv4_block3_out[0][0]           \n",
      "                                                                 conv4_block4_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_out (Activation)   (None, 19, 19, 1024) 0           conv4_block4_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_conv (Conv2D)    (None, 19, 19, 256)  262400      conv4_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_bn (BatchNormali (None, 19, 19, 256)  1024        conv4_block5_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_relu (Activation (None, 19, 19, 256)  0           conv4_block5_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_conv (Conv2D)    (None, 19, 19, 256)  590080      conv4_block5_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_bn (BatchNormali (None, 19, 19, 256)  1024        conv4_block5_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_relu (Activation (None, 19, 19, 256)  0           conv4_block5_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_3_conv (Conv2D)    (None, 19, 19, 1024) 263168      conv4_block5_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_3_bn (BatchNormali (None, 19, 19, 1024) 4096        conv4_block5_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_add (Add)          (None, 19, 19, 1024) 0           conv4_block4_out[0][0]           \n",
      "                                                                 conv4_block5_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_out (Activation)   (None, 19, 19, 1024) 0           conv4_block5_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_conv (Conv2D)    (None, 19, 19, 256)  262400      conv4_block5_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_bn (BatchNormali (None, 19, 19, 256)  1024        conv4_block6_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_relu (Activation (None, 19, 19, 256)  0           conv4_block6_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_conv (Conv2D)    (None, 19, 19, 256)  590080      conv4_block6_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_bn (BatchNormali (None, 19, 19, 256)  1024        conv4_block6_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_relu (Activation (None, 19, 19, 256)  0           conv4_block6_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_3_conv (Conv2D)    (None, 19, 19, 1024) 263168      conv4_block6_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_3_bn (BatchNormali (None, 19, 19, 1024) 4096        conv4_block6_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_add (Add)          (None, 19, 19, 1024) 0           conv4_block5_out[0][0]           \n",
      "                                                                 conv4_block6_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_out (Activation)   (None, 19, 19, 1024) 0           conv4_block6_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block7_1_conv (Conv2D)    (None, 19, 19, 256)  262400      conv4_block6_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block7_1_bn (BatchNormali (None, 19, 19, 256)  1024        conv4_block7_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block7_1_relu (Activation (None, 19, 19, 256)  0           conv4_block7_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block7_2_conv (Conv2D)    (None, 19, 19, 256)  590080      conv4_block7_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block7_2_bn (BatchNormali (None, 19, 19, 256)  1024        conv4_block7_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block7_2_relu (Activation (None, 19, 19, 256)  0           conv4_block7_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block7_3_conv (Conv2D)    (None, 19, 19, 1024) 263168      conv4_block7_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block7_3_bn (BatchNormali (None, 19, 19, 1024) 4096        conv4_block7_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block7_add (Add)          (None, 19, 19, 1024) 0           conv4_block6_out[0][0]           \n",
      "                                                                 conv4_block7_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block7_out (Activation)   (None, 19, 19, 1024) 0           conv4_block7_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block8_1_conv (Conv2D)    (None, 19, 19, 256)  262400      conv4_block7_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block8_1_bn (BatchNormali (None, 19, 19, 256)  1024        conv4_block8_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block8_1_relu (Activation (None, 19, 19, 256)  0           conv4_block8_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block8_2_conv (Conv2D)    (None, 19, 19, 256)  590080      conv4_block8_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block8_2_bn (BatchNormali (None, 19, 19, 256)  1024        conv4_block8_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block8_2_relu (Activation (None, 19, 19, 256)  0           conv4_block8_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block8_3_conv (Conv2D)    (None, 19, 19, 1024) 263168      conv4_block8_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block8_3_bn (BatchNormali (None, 19, 19, 1024) 4096        conv4_block8_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block8_add (Add)          (None, 19, 19, 1024) 0           conv4_block7_out[0][0]           \n",
      "                                                                 conv4_block8_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block8_out (Activation)   (None, 19, 19, 1024) 0           conv4_block8_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block9_1_conv (Conv2D)    (None, 19, 19, 256)  262400      conv4_block8_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block9_1_bn (BatchNormali (None, 19, 19, 256)  1024        conv4_block9_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block9_1_relu (Activation (None, 19, 19, 256)  0           conv4_block9_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block9_2_conv (Conv2D)    (None, 19, 19, 256)  590080      conv4_block9_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block9_2_bn (BatchNormali (None, 19, 19, 256)  1024        conv4_block9_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block9_2_relu (Activation (None, 19, 19, 256)  0           conv4_block9_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block9_3_conv (Conv2D)    (None, 19, 19, 1024) 263168      conv4_block9_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block9_3_bn (BatchNormali (None, 19, 19, 1024) 4096        conv4_block9_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block9_add (Add)          (None, 19, 19, 1024) 0           conv4_block8_out[0][0]           \n",
      "                                                                 conv4_block9_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block9_out (Activation)   (None, 19, 19, 1024) 0           conv4_block9_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block10_1_conv (Conv2D)   (None, 19, 19, 256)  262400      conv4_block9_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block10_1_bn (BatchNormal (None, 19, 19, 256)  1024        conv4_block10_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block10_1_relu (Activatio (None, 19, 19, 256)  0           conv4_block10_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block10_2_conv (Conv2D)   (None, 19, 19, 256)  590080      conv4_block10_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block10_2_bn (BatchNormal (None, 19, 19, 256)  1024        conv4_block10_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block10_2_relu (Activatio (None, 19, 19, 256)  0           conv4_block10_2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block10_3_conv (Conv2D)   (None, 19, 19, 1024) 263168      conv4_block10_2_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block10_3_bn (BatchNormal (None, 19, 19, 1024) 4096        conv4_block10_3_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block10_add (Add)         (None, 19, 19, 1024) 0           conv4_block9_out[0][0]           \n",
      "                                                                 conv4_block10_3_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block10_out (Activation)  (None, 19, 19, 1024) 0           conv4_block10_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block11_1_conv (Conv2D)   (None, 19, 19, 256)  262400      conv4_block10_out[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block11_1_bn (BatchNormal (None, 19, 19, 256)  1024        conv4_block11_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block11_1_relu (Activatio (None, 19, 19, 256)  0           conv4_block11_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block11_2_conv (Conv2D)   (None, 19, 19, 256)  590080      conv4_block11_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block11_2_bn (BatchNormal (None, 19, 19, 256)  1024        conv4_block11_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block11_2_relu (Activatio (None, 19, 19, 256)  0           conv4_block11_2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block11_3_conv (Conv2D)   (None, 19, 19, 1024) 263168      conv4_block11_2_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block11_3_bn (BatchNormal (None, 19, 19, 1024) 4096        conv4_block11_3_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block11_add (Add)         (None, 19, 19, 1024) 0           conv4_block10_out[0][0]          \n",
      "                                                                 conv4_block11_3_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block11_out (Activation)  (None, 19, 19, 1024) 0           conv4_block11_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block12_1_conv (Conv2D)   (None, 19, 19, 256)  262400      conv4_block11_out[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block12_1_bn (BatchNormal (None, 19, 19, 256)  1024        conv4_block12_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block12_1_relu (Activatio (None, 19, 19, 256)  0           conv4_block12_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block12_2_conv (Conv2D)   (None, 19, 19, 256)  590080      conv4_block12_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block12_2_bn (BatchNormal (None, 19, 19, 256)  1024        conv4_block12_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block12_2_relu (Activatio (None, 19, 19, 256)  0           conv4_block12_2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block12_3_conv (Conv2D)   (None, 19, 19, 1024) 263168      conv4_block12_2_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block12_3_bn (BatchNormal (None, 19, 19, 1024) 4096        conv4_block12_3_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block12_add (Add)         (None, 19, 19, 1024) 0           conv4_block11_out[0][0]          \n",
      "                                                                 conv4_block12_3_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block12_out (Activation)  (None, 19, 19, 1024) 0           conv4_block12_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block13_1_conv (Conv2D)   (None, 19, 19, 256)  262400      conv4_block12_out[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block13_1_bn (BatchNormal (None, 19, 19, 256)  1024        conv4_block13_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block13_1_relu (Activatio (None, 19, 19, 256)  0           conv4_block13_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block13_2_conv (Conv2D)   (None, 19, 19, 256)  590080      conv4_block13_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block13_2_bn (BatchNormal (None, 19, 19, 256)  1024        conv4_block13_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block13_2_relu (Activatio (None, 19, 19, 256)  0           conv4_block13_2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block13_3_conv (Conv2D)   (None, 19, 19, 1024) 263168      conv4_block13_2_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block13_3_bn (BatchNormal (None, 19, 19, 1024) 4096        conv4_block13_3_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block13_add (Add)         (None, 19, 19, 1024) 0           conv4_block12_out[0][0]          \n",
      "                                                                 conv4_block13_3_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block13_out (Activation)  (None, 19, 19, 1024) 0           conv4_block13_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block14_1_conv (Conv2D)   (None, 19, 19, 256)  262400      conv4_block13_out[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block14_1_bn (BatchNormal (None, 19, 19, 256)  1024        conv4_block14_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block14_1_relu (Activatio (None, 19, 19, 256)  0           conv4_block14_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block14_2_conv (Conv2D)   (None, 19, 19, 256)  590080      conv4_block14_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block14_2_bn (BatchNormal (None, 19, 19, 256)  1024        conv4_block14_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block14_2_relu (Activatio (None, 19, 19, 256)  0           conv4_block14_2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block14_3_conv (Conv2D)   (None, 19, 19, 1024) 263168      conv4_block14_2_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block14_3_bn (BatchNormal (None, 19, 19, 1024) 4096        conv4_block14_3_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block14_add (Add)         (None, 19, 19, 1024) 0           conv4_block13_out[0][0]          \n",
      "                                                                 conv4_block14_3_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block14_out (Activation)  (None, 19, 19, 1024) 0           conv4_block14_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block15_1_conv (Conv2D)   (None, 19, 19, 256)  262400      conv4_block14_out[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block15_1_bn (BatchNormal (None, 19, 19, 256)  1024        conv4_block15_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block15_1_relu (Activatio (None, 19, 19, 256)  0           conv4_block15_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block15_2_conv (Conv2D)   (None, 19, 19, 256)  590080      conv4_block15_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block15_2_bn (BatchNormal (None, 19, 19, 256)  1024        conv4_block15_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block15_2_relu (Activatio (None, 19, 19, 256)  0           conv4_block15_2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block15_3_conv (Conv2D)   (None, 19, 19, 1024) 263168      conv4_block15_2_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block15_3_bn (BatchNormal (None, 19, 19, 1024) 4096        conv4_block15_3_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block15_add (Add)         (None, 19, 19, 1024) 0           conv4_block14_out[0][0]          \n",
      "                                                                 conv4_block15_3_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block15_out (Activation)  (None, 19, 19, 1024) 0           conv4_block15_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block16_1_conv (Conv2D)   (None, 19, 19, 256)  262400      conv4_block15_out[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block16_1_bn (BatchNormal (None, 19, 19, 256)  1024        conv4_block16_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block16_1_relu (Activatio (None, 19, 19, 256)  0           conv4_block16_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block16_2_conv (Conv2D)   (None, 19, 19, 256)  590080      conv4_block16_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block16_2_bn (BatchNormal (None, 19, 19, 256)  1024        conv4_block16_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block16_2_relu (Activatio (None, 19, 19, 256)  0           conv4_block16_2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block16_3_conv (Conv2D)   (None, 19, 19, 1024) 263168      conv4_block16_2_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block16_3_bn (BatchNormal (None, 19, 19, 1024) 4096        conv4_block16_3_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block16_add (Add)         (None, 19, 19, 1024) 0           conv4_block15_out[0][0]          \n",
      "                                                                 conv4_block16_3_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block16_out (Activation)  (None, 19, 19, 1024) 0           conv4_block16_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block17_1_conv (Conv2D)   (None, 19, 19, 256)  262400      conv4_block16_out[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block17_1_bn (BatchNormal (None, 19, 19, 256)  1024        conv4_block17_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block17_1_relu (Activatio (None, 19, 19, 256)  0           conv4_block17_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block17_2_conv (Conv2D)   (None, 19, 19, 256)  590080      conv4_block17_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block17_2_bn (BatchNormal (None, 19, 19, 256)  1024        conv4_block17_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block17_2_relu (Activatio (None, 19, 19, 256)  0           conv4_block17_2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block17_3_conv (Conv2D)   (None, 19, 19, 1024) 263168      conv4_block17_2_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block17_3_bn (BatchNormal (None, 19, 19, 1024) 4096        conv4_block17_3_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block17_add (Add)         (None, 19, 19, 1024) 0           conv4_block16_out[0][0]          \n",
      "                                                                 conv4_block17_3_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block17_out (Activation)  (None, 19, 19, 1024) 0           conv4_block17_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block18_1_conv (Conv2D)   (None, 19, 19, 256)  262400      conv4_block17_out[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block18_1_bn (BatchNormal (None, 19, 19, 256)  1024        conv4_block18_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block18_1_relu (Activatio (None, 19, 19, 256)  0           conv4_block18_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block18_2_conv (Conv2D)   (None, 19, 19, 256)  590080      conv4_block18_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block18_2_bn (BatchNormal (None, 19, 19, 256)  1024        conv4_block18_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block18_2_relu (Activatio (None, 19, 19, 256)  0           conv4_block18_2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block18_3_conv (Conv2D)   (None, 19, 19, 1024) 263168      conv4_block18_2_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block18_3_bn (BatchNormal (None, 19, 19, 1024) 4096        conv4_block18_3_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block18_add (Add)         (None, 19, 19, 1024) 0           conv4_block17_out[0][0]          \n",
      "                                                                 conv4_block18_3_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block18_out (Activation)  (None, 19, 19, 1024) 0           conv4_block18_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block19_1_conv (Conv2D)   (None, 19, 19, 256)  262400      conv4_block18_out[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block19_1_bn (BatchNormal (None, 19, 19, 256)  1024        conv4_block19_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block19_1_relu (Activatio (None, 19, 19, 256)  0           conv4_block19_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block19_2_conv (Conv2D)   (None, 19, 19, 256)  590080      conv4_block19_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block19_2_bn (BatchNormal (None, 19, 19, 256)  1024        conv4_block19_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block19_2_relu (Activatio (None, 19, 19, 256)  0           conv4_block19_2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block19_3_conv (Conv2D)   (None, 19, 19, 1024) 263168      conv4_block19_2_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block19_3_bn (BatchNormal (None, 19, 19, 1024) 4096        conv4_block19_3_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block19_add (Add)         (None, 19, 19, 1024) 0           conv4_block18_out[0][0]          \n",
      "                                                                 conv4_block19_3_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block19_out (Activation)  (None, 19, 19, 1024) 0           conv4_block19_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block20_1_conv (Conv2D)   (None, 19, 19, 256)  262400      conv4_block19_out[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block20_1_bn (BatchNormal (None, 19, 19, 256)  1024        conv4_block20_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block20_1_relu (Activatio (None, 19, 19, 256)  0           conv4_block20_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block20_2_conv (Conv2D)   (None, 19, 19, 256)  590080      conv4_block20_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block20_2_bn (BatchNormal (None, 19, 19, 256)  1024        conv4_block20_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block20_2_relu (Activatio (None, 19, 19, 256)  0           conv4_block20_2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block20_3_conv (Conv2D)   (None, 19, 19, 1024) 263168      conv4_block20_2_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block20_3_bn (BatchNormal (None, 19, 19, 1024) 4096        conv4_block20_3_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block20_add (Add)         (None, 19, 19, 1024) 0           conv4_block19_out[0][0]          \n",
      "                                                                 conv4_block20_3_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block20_out (Activation)  (None, 19, 19, 1024) 0           conv4_block20_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block21_1_conv (Conv2D)   (None, 19, 19, 256)  262400      conv4_block20_out[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block21_1_bn (BatchNormal (None, 19, 19, 256)  1024        conv4_block21_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block21_1_relu (Activatio (None, 19, 19, 256)  0           conv4_block21_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block21_2_conv (Conv2D)   (None, 19, 19, 256)  590080      conv4_block21_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block21_2_bn (BatchNormal (None, 19, 19, 256)  1024        conv4_block21_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block21_2_relu (Activatio (None, 19, 19, 256)  0           conv4_block21_2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block21_3_conv (Conv2D)   (None, 19, 19, 1024) 263168      conv4_block21_2_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block21_3_bn (BatchNormal (None, 19, 19, 1024) 4096        conv4_block21_3_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block21_add (Add)         (None, 19, 19, 1024) 0           conv4_block20_out[0][0]          \n",
      "                                                                 conv4_block21_3_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block21_out (Activation)  (None, 19, 19, 1024) 0           conv4_block21_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block22_1_conv (Conv2D)   (None, 19, 19, 256)  262400      conv4_block21_out[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block22_1_bn (BatchNormal (None, 19, 19, 256)  1024        conv4_block22_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block22_1_relu (Activatio (None, 19, 19, 256)  0           conv4_block22_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block22_2_conv (Conv2D)   (None, 19, 19, 256)  590080      conv4_block22_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block22_2_bn (BatchNormal (None, 19, 19, 256)  1024        conv4_block22_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block22_2_relu (Activatio (None, 19, 19, 256)  0           conv4_block22_2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block22_3_conv (Conv2D)   (None, 19, 19, 1024) 263168      conv4_block22_2_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block22_3_bn (BatchNormal (None, 19, 19, 1024) 4096        conv4_block22_3_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block22_add (Add)         (None, 19, 19, 1024) 0           conv4_block21_out[0][0]          \n",
      "                                                                 conv4_block22_3_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block22_out (Activation)  (None, 19, 19, 1024) 0           conv4_block22_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block23_1_conv (Conv2D)   (None, 19, 19, 256)  262400      conv4_block22_out[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block23_1_bn (BatchNormal (None, 19, 19, 256)  1024        conv4_block23_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block23_1_relu (Activatio (None, 19, 19, 256)  0           conv4_block23_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block23_2_conv (Conv2D)   (None, 19, 19, 256)  590080      conv4_block23_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block23_2_bn (BatchNormal (None, 19, 19, 256)  1024        conv4_block23_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block23_2_relu (Activatio (None, 19, 19, 256)  0           conv4_block23_2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block23_3_conv (Conv2D)   (None, 19, 19, 1024) 263168      conv4_block23_2_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block23_3_bn (BatchNormal (None, 19, 19, 1024) 4096        conv4_block23_3_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block23_add (Add)         (None, 19, 19, 1024) 0           conv4_block22_out[0][0]          \n",
      "                                                                 conv4_block23_3_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block23_out (Activation)  (None, 19, 19, 1024) 0           conv4_block23_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_conv (Conv2D)    (None, 10, 10, 512)  524800      conv4_block23_out[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_bn (BatchNormali (None, 10, 10, 512)  2048        conv5_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_relu (Activation (None, 10, 10, 512)  0           conv5_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_conv (Conv2D)    (None, 10, 10, 512)  2359808     conv5_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_bn (BatchNormali (None, 10, 10, 512)  2048        conv5_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_relu (Activation (None, 10, 10, 512)  0           conv5_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_0_conv (Conv2D)    (None, 10, 10, 2048) 2099200     conv4_block23_out[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_3_conv (Conv2D)    (None, 10, 10, 2048) 1050624     conv5_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_0_bn (BatchNormali (None, 10, 10, 2048) 8192        conv5_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_3_bn (BatchNormali (None, 10, 10, 2048) 8192        conv5_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_add (Add)          (None, 10, 10, 2048) 0           conv5_block1_0_bn[0][0]          \n",
      "                                                                 conv5_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_out (Activation)   (None, 10, 10, 2048) 0           conv5_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_conv (Conv2D)    (None, 10, 10, 512)  1049088     conv5_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_bn (BatchNormali (None, 10, 10, 512)  2048        conv5_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_relu (Activation (None, 10, 10, 512)  0           conv5_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_conv (Conv2D)    (None, 10, 10, 512)  2359808     conv5_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_bn (BatchNormali (None, 10, 10, 512)  2048        conv5_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_relu (Activation (None, 10, 10, 512)  0           conv5_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_3_conv (Conv2D)    (None, 10, 10, 2048) 1050624     conv5_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_3_bn (BatchNormali (None, 10, 10, 2048) 8192        conv5_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_add (Add)          (None, 10, 10, 2048) 0           conv5_block1_out[0][0]           \n",
      "                                                                 conv5_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_out (Activation)   (None, 10, 10, 2048) 0           conv5_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_conv (Conv2D)    (None, 10, 10, 512)  1049088     conv5_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_bn (BatchNormali (None, 10, 10, 512)  2048        conv5_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_relu (Activation (None, 10, 10, 512)  0           conv5_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_conv (Conv2D)    (None, 10, 10, 512)  2359808     conv5_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_bn (BatchNormali (None, 10, 10, 512)  2048        conv5_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_relu (Activation (None, 10, 10, 512)  0           conv5_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_3_conv (Conv2D)    (None, 10, 10, 2048) 1050624     conv5_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_3_bn (BatchNormali (None, 10, 10, 2048) 8192        conv5_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_add (Add)          (None, 10, 10, 2048) 0           conv5_block2_out[0][0]           \n",
      "                                                                 conv5_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_out (Activation)   (None, 10, 10, 2048) 0           conv5_block3_add[0][0]           \n",
      "==================================================================================================\n",
      "Total params: 42,673,856\n",
      "Trainable params: 42,568,512\n",
      "Non-trainable params: 105,344\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Xception model\n",
    "model_name = \"ResNet101\"\n",
    "base_model = applications.ResNet101(include_top=False, weights=None,\n",
    "                                input_shape=(max_x, max_y, number_image_channels))\n",
    "base_model.trainable = True\n",
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-09 17:53:49.712847: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-09 17:53:49.718095: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-09 17:53:49.718537: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-09 17:53:49.719328: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-09 17:53:49.719864: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-09 17:53:49.720280: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-09 17:53:49.720690: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-09 17:53:50.115675: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-09 17:53:50.116144: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-09 17:53:50.116537: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-09 17:53:50.116928: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 10094 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3080 Ti, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         [(None, 299, 299, 3)]     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 299, 299, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 299, 299, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 149, 149, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 149, 149, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 149, 149, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 74, 74, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 74, 74, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 74, 74, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 74, 74, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 37, 37, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 37, 37, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 37, 37, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 37, 37, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 18, 18, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 18, 18, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 18, 18, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 18, 18, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 9, 9, 512)         0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 0\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# log-vgg pretrained - pu-setting\n",
    "model_name = \"log_vgg16\"\n",
    "fp_penalty_coef, fn_penalty_coef = 1, 1\n",
    "model_path = \"/home/shahrokh/projects/MLSpectrumAllocation/ML/data/pictures_299_299_transfer/log/noisy_std_1/\" + \\\n",
    "             \"pu_circle_su_circle_60/raw_power_min_max_norm/color/log_pu5_su6/20pus_5sus_8channels/models/vgg16/\" + \\\n",
    "             \"700000/best_model_lambda_0.1.h5\"\n",
    "model_path = \"ML/data/pictures_299_299_transfer/log/noisy_std_1/pu_circle_su_circle_60/\" + \\\n",
    "             \"raw_power_min_max_norm/color/log_5/pus_1_sus_3_channels_700k/models/700000/\" + \\\n",
    "             \"best_model_lambda_0_fit.h5\"\n",
    "base_model = models.load_model(model_path, \n",
    "                              custom_objects={ 'loss': custom_loss(fp_penalty_coef, fn_penalty_coef), \n",
    "                                               'fp_mae': fp_mae,\n",
    "                                               'mae':'mae', 'mse':'mse'})\n",
    "base_model.trainable = False\n",
    "base_model = base_model.layers[1]\n",
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-05 11:19:50.879198: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-05 11:19:50.882966: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-05 11:19:50.883259: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-05 11:19:50.883769: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-05 11:19:50.884674: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-05 11:19:50.885003: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-05 11:19:50.885279: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-05 11:19:51.183469: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-05 11:19:51.183786: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-05 11:19:51.184047: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-05 11:19:51.184295: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 10094 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3080 Ti, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"resnet152v2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 300, 300, 8) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1_pad (ZeroPadding2D)       (None, 306, 306, 8)  0           input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1_conv (Conv2D)             (None, 150, 150, 64) 25152       conv1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pad (ZeroPadding2D)       (None, 152, 152, 64) 0           conv1_conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pool (MaxPooling2D)       (None, 75, 75, 64)   0           pool1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_preact_bn (BatchNo (None, 75, 75, 64)   256         pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_preact_relu (Activ (None, 75, 75, 64)   0           conv2_block1_preact_bn[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_conv (Conv2D)    (None, 75, 75, 64)   4096        conv2_block1_preact_relu[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_bn (BatchNormali (None, 75, 75, 64)   256         conv2_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_relu (Activation (None, 75, 75, 64)   0           conv2_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_pad (ZeroPadding (None, 77, 77, 64)   0           conv2_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_conv (Conv2D)    (None, 75, 75, 64)   36864       conv2_block1_2_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_bn (BatchNormali (None, 75, 75, 64)   256         conv2_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_relu (Activation (None, 75, 75, 64)   0           conv2_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_conv (Conv2D)    (None, 75, 75, 256)  16640       conv2_block1_preact_relu[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_conv (Conv2D)    (None, 75, 75, 256)  16640       conv2_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_out (Add)          (None, 75, 75, 256)  0           conv2_block1_0_conv[0][0]        \n",
      "                                                                 conv2_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_preact_bn (BatchNo (None, 75, 75, 256)  1024        conv2_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_preact_relu (Activ (None, 75, 75, 256)  0           conv2_block2_preact_bn[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_conv (Conv2D)    (None, 75, 75, 64)   16384       conv2_block2_preact_relu[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_bn (BatchNormali (None, 75, 75, 64)   256         conv2_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_relu (Activation (None, 75, 75, 64)   0           conv2_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_pad (ZeroPadding (None, 77, 77, 64)   0           conv2_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_conv (Conv2D)    (None, 75, 75, 64)   36864       conv2_block2_2_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_bn (BatchNormali (None, 75, 75, 64)   256         conv2_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_relu (Activation (None, 75, 75, 64)   0           conv2_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_3_conv (Conv2D)    (None, 75, 75, 256)  16640       conv2_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_out (Add)          (None, 75, 75, 256)  0           conv2_block1_out[0][0]           \n",
      "                                                                 conv2_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_preact_bn (BatchNo (None, 75, 75, 256)  1024        conv2_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_preact_relu (Activ (None, 75, 75, 256)  0           conv2_block3_preact_bn[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_conv (Conv2D)    (None, 75, 75, 64)   16384       conv2_block3_preact_relu[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_bn (BatchNormali (None, 75, 75, 64)   256         conv2_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_relu (Activation (None, 75, 75, 64)   0           conv2_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_pad (ZeroPadding (None, 77, 77, 64)   0           conv2_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_conv (Conv2D)    (None, 38, 38, 64)   36864       conv2_block3_2_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_bn (BatchNormali (None, 38, 38, 64)   256         conv2_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_relu (Activation (None, 38, 38, 64)   0           conv2_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 38, 38, 256)  0           conv2_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_3_conv (Conv2D)    (None, 38, 38, 256)  16640       conv2_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_out (Add)          (None, 38, 38, 256)  0           max_pooling2d_3[0][0]            \n",
      "                                                                 conv2_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_preact_bn (BatchNo (None, 38, 38, 256)  1024        conv2_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_preact_relu (Activ (None, 38, 38, 256)  0           conv3_block1_preact_bn[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_conv (Conv2D)    (None, 38, 38, 128)  32768       conv3_block1_preact_relu[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_bn (BatchNormali (None, 38, 38, 128)  512         conv3_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_relu (Activation (None, 38, 38, 128)  0           conv3_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_pad (ZeroPadding (None, 40, 40, 128)  0           conv3_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_conv (Conv2D)    (None, 38, 38, 128)  147456      conv3_block1_2_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_bn (BatchNormali (None, 38, 38, 128)  512         conv3_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_relu (Activation (None, 38, 38, 128)  0           conv3_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_0_conv (Conv2D)    (None, 38, 38, 512)  131584      conv3_block1_preact_relu[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_3_conv (Conv2D)    (None, 38, 38, 512)  66048       conv3_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_out (Add)          (None, 38, 38, 512)  0           conv3_block1_0_conv[0][0]        \n",
      "                                                                 conv3_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_preact_bn (BatchNo (None, 38, 38, 512)  2048        conv3_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_preact_relu (Activ (None, 38, 38, 512)  0           conv3_block2_preact_bn[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_conv (Conv2D)    (None, 38, 38, 128)  65536       conv3_block2_preact_relu[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_bn (BatchNormali (None, 38, 38, 128)  512         conv3_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_relu (Activation (None, 38, 38, 128)  0           conv3_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_pad (ZeroPadding (None, 40, 40, 128)  0           conv3_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_conv (Conv2D)    (None, 38, 38, 128)  147456      conv3_block2_2_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_bn (BatchNormali (None, 38, 38, 128)  512         conv3_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_relu (Activation (None, 38, 38, 128)  0           conv3_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_3_conv (Conv2D)    (None, 38, 38, 512)  66048       conv3_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_out (Add)          (None, 38, 38, 512)  0           conv3_block1_out[0][0]           \n",
      "                                                                 conv3_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_preact_bn (BatchNo (None, 38, 38, 512)  2048        conv3_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_preact_relu (Activ (None, 38, 38, 512)  0           conv3_block3_preact_bn[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_conv (Conv2D)    (None, 38, 38, 128)  65536       conv3_block3_preact_relu[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_bn (BatchNormali (None, 38, 38, 128)  512         conv3_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_relu (Activation (None, 38, 38, 128)  0           conv3_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_pad (ZeroPadding (None, 40, 40, 128)  0           conv3_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_conv (Conv2D)    (None, 38, 38, 128)  147456      conv3_block3_2_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_bn (BatchNormali (None, 38, 38, 128)  512         conv3_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_relu (Activation (None, 38, 38, 128)  0           conv3_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_3_conv (Conv2D)    (None, 38, 38, 512)  66048       conv3_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_out (Add)          (None, 38, 38, 512)  0           conv3_block2_out[0][0]           \n",
      "                                                                 conv3_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_preact_bn (BatchNo (None, 38, 38, 512)  2048        conv3_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_preact_relu (Activ (None, 38, 38, 512)  0           conv3_block4_preact_bn[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_conv (Conv2D)    (None, 38, 38, 128)  65536       conv3_block4_preact_relu[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_bn (BatchNormali (None, 38, 38, 128)  512         conv3_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_relu (Activation (None, 38, 38, 128)  0           conv3_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_pad (ZeroPadding (None, 40, 40, 128)  0           conv3_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_conv (Conv2D)    (None, 38, 38, 128)  147456      conv3_block4_2_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_bn (BatchNormali (None, 38, 38, 128)  512         conv3_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_relu (Activation (None, 38, 38, 128)  0           conv3_block4_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_3_conv (Conv2D)    (None, 38, 38, 512)  66048       conv3_block4_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_out (Add)          (None, 38, 38, 512)  0           conv3_block3_out[0][0]           \n",
      "                                                                 conv3_block4_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block5_preact_bn (BatchNo (None, 38, 38, 512)  2048        conv3_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block5_preact_relu (Activ (None, 38, 38, 512)  0           conv3_block5_preact_bn[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block5_1_conv (Conv2D)    (None, 38, 38, 128)  65536       conv3_block5_preact_relu[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block5_1_bn (BatchNormali (None, 38, 38, 128)  512         conv3_block5_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block5_1_relu (Activation (None, 38, 38, 128)  0           conv3_block5_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block5_2_pad (ZeroPadding (None, 40, 40, 128)  0           conv3_block5_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block5_2_conv (Conv2D)    (None, 38, 38, 128)  147456      conv3_block5_2_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block5_2_bn (BatchNormali (None, 38, 38, 128)  512         conv3_block5_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block5_2_relu (Activation (None, 38, 38, 128)  0           conv3_block5_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block5_3_conv (Conv2D)    (None, 38, 38, 512)  66048       conv3_block5_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block5_out (Add)          (None, 38, 38, 512)  0           conv3_block4_out[0][0]           \n",
      "                                                                 conv3_block5_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block6_preact_bn (BatchNo (None, 38, 38, 512)  2048        conv3_block5_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block6_preact_relu (Activ (None, 38, 38, 512)  0           conv3_block6_preact_bn[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block6_1_conv (Conv2D)    (None, 38, 38, 128)  65536       conv3_block6_preact_relu[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block6_1_bn (BatchNormali (None, 38, 38, 128)  512         conv3_block6_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block6_1_relu (Activation (None, 38, 38, 128)  0           conv3_block6_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block6_2_pad (ZeroPadding (None, 40, 40, 128)  0           conv3_block6_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block6_2_conv (Conv2D)    (None, 38, 38, 128)  147456      conv3_block6_2_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block6_2_bn (BatchNormali (None, 38, 38, 128)  512         conv3_block6_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block6_2_relu (Activation (None, 38, 38, 128)  0           conv3_block6_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block6_3_conv (Conv2D)    (None, 38, 38, 512)  66048       conv3_block6_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block6_out (Add)          (None, 38, 38, 512)  0           conv3_block5_out[0][0]           \n",
      "                                                                 conv3_block6_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block7_preact_bn (BatchNo (None, 38, 38, 512)  2048        conv3_block6_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block7_preact_relu (Activ (None, 38, 38, 512)  0           conv3_block7_preact_bn[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block7_1_conv (Conv2D)    (None, 38, 38, 128)  65536       conv3_block7_preact_relu[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block7_1_bn (BatchNormali (None, 38, 38, 128)  512         conv3_block7_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block7_1_relu (Activation (None, 38, 38, 128)  0           conv3_block7_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block7_2_pad (ZeroPadding (None, 40, 40, 128)  0           conv3_block7_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block7_2_conv (Conv2D)    (None, 38, 38, 128)  147456      conv3_block7_2_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block7_2_bn (BatchNormali (None, 38, 38, 128)  512         conv3_block7_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block7_2_relu (Activation (None, 38, 38, 128)  0           conv3_block7_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block7_3_conv (Conv2D)    (None, 38, 38, 512)  66048       conv3_block7_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block7_out (Add)          (None, 38, 38, 512)  0           conv3_block6_out[0][0]           \n",
      "                                                                 conv3_block7_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block8_preact_bn (BatchNo (None, 38, 38, 512)  2048        conv3_block7_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block8_preact_relu (Activ (None, 38, 38, 512)  0           conv3_block8_preact_bn[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block8_1_conv (Conv2D)    (None, 38, 38, 128)  65536       conv3_block8_preact_relu[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block8_1_bn (BatchNormali (None, 38, 38, 128)  512         conv3_block8_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block8_1_relu (Activation (None, 38, 38, 128)  0           conv3_block8_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block8_2_pad (ZeroPadding (None, 40, 40, 128)  0           conv3_block8_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block8_2_conv (Conv2D)    (None, 19, 19, 128)  147456      conv3_block8_2_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block8_2_bn (BatchNormali (None, 19, 19, 128)  512         conv3_block8_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block8_2_relu (Activation (None, 19, 19, 128)  0           conv3_block8_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2D)  (None, 19, 19, 512)  0           conv3_block7_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block8_3_conv (Conv2D)    (None, 19, 19, 512)  66048       conv3_block8_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block8_out (Add)          (None, 19, 19, 512)  0           max_pooling2d_4[0][0]            \n",
      "                                                                 conv3_block8_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_preact_bn (BatchNo (None, 19, 19, 512)  2048        conv3_block8_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_preact_relu (Activ (None, 19, 19, 512)  0           conv4_block1_preact_bn[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_conv (Conv2D)    (None, 19, 19, 256)  131072      conv4_block1_preact_relu[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_bn (BatchNormali (None, 19, 19, 256)  1024        conv4_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_relu (Activation (None, 19, 19, 256)  0           conv4_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_pad (ZeroPadding (None, 21, 21, 256)  0           conv4_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_conv (Conv2D)    (None, 19, 19, 256)  589824      conv4_block1_2_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_bn (BatchNormali (None, 19, 19, 256)  1024        conv4_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_relu (Activation (None, 19, 19, 256)  0           conv4_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_0_conv (Conv2D)    (None, 19, 19, 1024) 525312      conv4_block1_preact_relu[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_3_conv (Conv2D)    (None, 19, 19, 1024) 263168      conv4_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_out (Add)          (None, 19, 19, 1024) 0           conv4_block1_0_conv[0][0]        \n",
      "                                                                 conv4_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_preact_bn (BatchNo (None, 19, 19, 1024) 4096        conv4_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_preact_relu (Activ (None, 19, 19, 1024) 0           conv4_block2_preact_bn[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_conv (Conv2D)    (None, 19, 19, 256)  262144      conv4_block2_preact_relu[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_bn (BatchNormali (None, 19, 19, 256)  1024        conv4_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_relu (Activation (None, 19, 19, 256)  0           conv4_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_pad (ZeroPadding (None, 21, 21, 256)  0           conv4_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_conv (Conv2D)    (None, 19, 19, 256)  589824      conv4_block2_2_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_bn (BatchNormali (None, 19, 19, 256)  1024        conv4_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_relu (Activation (None, 19, 19, 256)  0           conv4_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_3_conv (Conv2D)    (None, 19, 19, 1024) 263168      conv4_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_out (Add)          (None, 19, 19, 1024) 0           conv4_block1_out[0][0]           \n",
      "                                                                 conv4_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_preact_bn (BatchNo (None, 19, 19, 1024) 4096        conv4_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_preact_relu (Activ (None, 19, 19, 1024) 0           conv4_block3_preact_bn[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_conv (Conv2D)    (None, 19, 19, 256)  262144      conv4_block3_preact_relu[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_bn (BatchNormali (None, 19, 19, 256)  1024        conv4_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_relu (Activation (None, 19, 19, 256)  0           conv4_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_pad (ZeroPadding (None, 21, 21, 256)  0           conv4_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_conv (Conv2D)    (None, 19, 19, 256)  589824      conv4_block3_2_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_bn (BatchNormali (None, 19, 19, 256)  1024        conv4_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_relu (Activation (None, 19, 19, 256)  0           conv4_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_3_conv (Conv2D)    (None, 19, 19, 1024) 263168      conv4_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_out (Add)          (None, 19, 19, 1024) 0           conv4_block2_out[0][0]           \n",
      "                                                                 conv4_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_preact_bn (BatchNo (None, 19, 19, 1024) 4096        conv4_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_preact_relu (Activ (None, 19, 19, 1024) 0           conv4_block4_preact_bn[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_conv (Conv2D)    (None, 19, 19, 256)  262144      conv4_block4_preact_relu[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_bn (BatchNormali (None, 19, 19, 256)  1024        conv4_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_relu (Activation (None, 19, 19, 256)  0           conv4_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_pad (ZeroPadding (None, 21, 21, 256)  0           conv4_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_conv (Conv2D)    (None, 19, 19, 256)  589824      conv4_block4_2_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_bn (BatchNormali (None, 19, 19, 256)  1024        conv4_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_relu (Activation (None, 19, 19, 256)  0           conv4_block4_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_3_conv (Conv2D)    (None, 19, 19, 1024) 263168      conv4_block4_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_out (Add)          (None, 19, 19, 1024) 0           conv4_block3_out[0][0]           \n",
      "                                                                 conv4_block4_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_preact_bn (BatchNo (None, 19, 19, 1024) 4096        conv4_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_preact_relu (Activ (None, 19, 19, 1024) 0           conv4_block5_preact_bn[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_conv (Conv2D)    (None, 19, 19, 256)  262144      conv4_block5_preact_relu[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_bn (BatchNormali (None, 19, 19, 256)  1024        conv4_block5_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_relu (Activation (None, 19, 19, 256)  0           conv4_block5_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_pad (ZeroPadding (None, 21, 21, 256)  0           conv4_block5_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_conv (Conv2D)    (None, 19, 19, 256)  589824      conv4_block5_2_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_bn (BatchNormali (None, 19, 19, 256)  1024        conv4_block5_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_relu (Activation (None, 19, 19, 256)  0           conv4_block5_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_3_conv (Conv2D)    (None, 19, 19, 1024) 263168      conv4_block5_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_out (Add)          (None, 19, 19, 1024) 0           conv4_block4_out[0][0]           \n",
      "                                                                 conv4_block5_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_preact_bn (BatchNo (None, 19, 19, 1024) 4096        conv4_block5_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_preact_relu (Activ (None, 19, 19, 1024) 0           conv4_block6_preact_bn[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_conv (Conv2D)    (None, 19, 19, 256)  262144      conv4_block6_preact_relu[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_bn (BatchNormali (None, 19, 19, 256)  1024        conv4_block6_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_relu (Activation (None, 19, 19, 256)  0           conv4_block6_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_pad (ZeroPadding (None, 21, 21, 256)  0           conv4_block6_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_conv (Conv2D)    (None, 19, 19, 256)  589824      conv4_block6_2_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_bn (BatchNormali (None, 19, 19, 256)  1024        conv4_block6_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_relu (Activation (None, 19, 19, 256)  0           conv4_block6_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_3_conv (Conv2D)    (None, 19, 19, 1024) 263168      conv4_block6_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_out (Add)          (None, 19, 19, 1024) 0           conv4_block5_out[0][0]           \n",
      "                                                                 conv4_block6_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block7_preact_bn (BatchNo (None, 19, 19, 1024) 4096        conv4_block6_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block7_preact_relu (Activ (None, 19, 19, 1024) 0           conv4_block7_preact_bn[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block7_1_conv (Conv2D)    (None, 19, 19, 256)  262144      conv4_block7_preact_relu[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block7_1_bn (BatchNormali (None, 19, 19, 256)  1024        conv4_block7_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block7_1_relu (Activation (None, 19, 19, 256)  0           conv4_block7_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block7_2_pad (ZeroPadding (None, 21, 21, 256)  0           conv4_block7_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block7_2_conv (Conv2D)    (None, 19, 19, 256)  589824      conv4_block7_2_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block7_2_bn (BatchNormali (None, 19, 19, 256)  1024        conv4_block7_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block7_2_relu (Activation (None, 19, 19, 256)  0           conv4_block7_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block7_3_conv (Conv2D)    (None, 19, 19, 1024) 263168      conv4_block7_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block7_out (Add)          (None, 19, 19, 1024) 0           conv4_block6_out[0][0]           \n",
      "                                                                 conv4_block7_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block8_preact_bn (BatchNo (None, 19, 19, 1024) 4096        conv4_block7_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block8_preact_relu (Activ (None, 19, 19, 1024) 0           conv4_block8_preact_bn[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block8_1_conv (Conv2D)    (None, 19, 19, 256)  262144      conv4_block8_preact_relu[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block8_1_bn (BatchNormali (None, 19, 19, 256)  1024        conv4_block8_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block8_1_relu (Activation (None, 19, 19, 256)  0           conv4_block8_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block8_2_pad (ZeroPadding (None, 21, 21, 256)  0           conv4_block8_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block8_2_conv (Conv2D)    (None, 19, 19, 256)  589824      conv4_block8_2_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block8_2_bn (BatchNormali (None, 19, 19, 256)  1024        conv4_block8_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block8_2_relu (Activation (None, 19, 19, 256)  0           conv4_block8_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block8_3_conv (Conv2D)    (None, 19, 19, 1024) 263168      conv4_block8_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block8_out (Add)          (None, 19, 19, 1024) 0           conv4_block7_out[0][0]           \n",
      "                                                                 conv4_block8_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block9_preact_bn (BatchNo (None, 19, 19, 1024) 4096        conv4_block8_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block9_preact_relu (Activ (None, 19, 19, 1024) 0           conv4_block9_preact_bn[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block9_1_conv (Conv2D)    (None, 19, 19, 256)  262144      conv4_block9_preact_relu[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block9_1_bn (BatchNormali (None, 19, 19, 256)  1024        conv4_block9_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block9_1_relu (Activation (None, 19, 19, 256)  0           conv4_block9_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block9_2_pad (ZeroPadding (None, 21, 21, 256)  0           conv4_block9_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block9_2_conv (Conv2D)    (None, 19, 19, 256)  589824      conv4_block9_2_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block9_2_bn (BatchNormali (None, 19, 19, 256)  1024        conv4_block9_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block9_2_relu (Activation (None, 19, 19, 256)  0           conv4_block9_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block9_3_conv (Conv2D)    (None, 19, 19, 1024) 263168      conv4_block9_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block9_out (Add)          (None, 19, 19, 1024) 0           conv4_block8_out[0][0]           \n",
      "                                                                 conv4_block9_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block10_preact_bn (BatchN (None, 19, 19, 1024) 4096        conv4_block9_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block10_preact_relu (Acti (None, 19, 19, 1024) 0           conv4_block10_preact_bn[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block10_1_conv (Conv2D)   (None, 19, 19, 256)  262144      conv4_block10_preact_relu[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block10_1_bn (BatchNormal (None, 19, 19, 256)  1024        conv4_block10_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block10_1_relu (Activatio (None, 19, 19, 256)  0           conv4_block10_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block10_2_pad (ZeroPaddin (None, 21, 21, 256)  0           conv4_block10_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block10_2_conv (Conv2D)   (None, 19, 19, 256)  589824      conv4_block10_2_pad[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block10_2_bn (BatchNormal (None, 19, 19, 256)  1024        conv4_block10_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block10_2_relu (Activatio (None, 19, 19, 256)  0           conv4_block10_2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block10_3_conv (Conv2D)   (None, 19, 19, 1024) 263168      conv4_block10_2_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block10_out (Add)         (None, 19, 19, 1024) 0           conv4_block9_out[0][0]           \n",
      "                                                                 conv4_block10_3_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block11_preact_bn (BatchN (None, 19, 19, 1024) 4096        conv4_block10_out[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block11_preact_relu (Acti (None, 19, 19, 1024) 0           conv4_block11_preact_bn[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block11_1_conv (Conv2D)   (None, 19, 19, 256)  262144      conv4_block11_preact_relu[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block11_1_bn (BatchNormal (None, 19, 19, 256)  1024        conv4_block11_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block11_1_relu (Activatio (None, 19, 19, 256)  0           conv4_block11_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block11_2_pad (ZeroPaddin (None, 21, 21, 256)  0           conv4_block11_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block11_2_conv (Conv2D)   (None, 19, 19, 256)  589824      conv4_block11_2_pad[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block11_2_bn (BatchNormal (None, 19, 19, 256)  1024        conv4_block11_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block11_2_relu (Activatio (None, 19, 19, 256)  0           conv4_block11_2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block11_3_conv (Conv2D)   (None, 19, 19, 1024) 263168      conv4_block11_2_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block11_out (Add)         (None, 19, 19, 1024) 0           conv4_block10_out[0][0]          \n",
      "                                                                 conv4_block11_3_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block12_preact_bn (BatchN (None, 19, 19, 1024) 4096        conv4_block11_out[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block12_preact_relu (Acti (None, 19, 19, 1024) 0           conv4_block12_preact_bn[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block12_1_conv (Conv2D)   (None, 19, 19, 256)  262144      conv4_block12_preact_relu[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block12_1_bn (BatchNormal (None, 19, 19, 256)  1024        conv4_block12_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block12_1_relu (Activatio (None, 19, 19, 256)  0           conv4_block12_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block12_2_pad (ZeroPaddin (None, 21, 21, 256)  0           conv4_block12_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block12_2_conv (Conv2D)   (None, 19, 19, 256)  589824      conv4_block12_2_pad[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block12_2_bn (BatchNormal (None, 19, 19, 256)  1024        conv4_block12_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block12_2_relu (Activatio (None, 19, 19, 256)  0           conv4_block12_2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block12_3_conv (Conv2D)   (None, 19, 19, 1024) 263168      conv4_block12_2_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block12_out (Add)         (None, 19, 19, 1024) 0           conv4_block11_out[0][0]          \n",
      "                                                                 conv4_block12_3_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block13_preact_bn (BatchN (None, 19, 19, 1024) 4096        conv4_block12_out[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block13_preact_relu (Acti (None, 19, 19, 1024) 0           conv4_block13_preact_bn[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block13_1_conv (Conv2D)   (None, 19, 19, 256)  262144      conv4_block13_preact_relu[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block13_1_bn (BatchNormal (None, 19, 19, 256)  1024        conv4_block13_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block13_1_relu (Activatio (None, 19, 19, 256)  0           conv4_block13_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block13_2_pad (ZeroPaddin (None, 21, 21, 256)  0           conv4_block13_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block13_2_conv (Conv2D)   (None, 19, 19, 256)  589824      conv4_block13_2_pad[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block13_2_bn (BatchNormal (None, 19, 19, 256)  1024        conv4_block13_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block13_2_relu (Activatio (None, 19, 19, 256)  0           conv4_block13_2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block13_3_conv (Conv2D)   (None, 19, 19, 1024) 263168      conv4_block13_2_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block13_out (Add)         (None, 19, 19, 1024) 0           conv4_block12_out[0][0]          \n",
      "                                                                 conv4_block13_3_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block14_preact_bn (BatchN (None, 19, 19, 1024) 4096        conv4_block13_out[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block14_preact_relu (Acti (None, 19, 19, 1024) 0           conv4_block14_preact_bn[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block14_1_conv (Conv2D)   (None, 19, 19, 256)  262144      conv4_block14_preact_relu[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block14_1_bn (BatchNormal (None, 19, 19, 256)  1024        conv4_block14_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block14_1_relu (Activatio (None, 19, 19, 256)  0           conv4_block14_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block14_2_pad (ZeroPaddin (None, 21, 21, 256)  0           conv4_block14_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block14_2_conv (Conv2D)   (None, 19, 19, 256)  589824      conv4_block14_2_pad[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block14_2_bn (BatchNormal (None, 19, 19, 256)  1024        conv4_block14_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block14_2_relu (Activatio (None, 19, 19, 256)  0           conv4_block14_2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block14_3_conv (Conv2D)   (None, 19, 19, 1024) 263168      conv4_block14_2_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block14_out (Add)         (None, 19, 19, 1024) 0           conv4_block13_out[0][0]          \n",
      "                                                                 conv4_block14_3_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block15_preact_bn (BatchN (None, 19, 19, 1024) 4096        conv4_block14_out[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block15_preact_relu (Acti (None, 19, 19, 1024) 0           conv4_block15_preact_bn[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block15_1_conv (Conv2D)   (None, 19, 19, 256)  262144      conv4_block15_preact_relu[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block15_1_bn (BatchNormal (None, 19, 19, 256)  1024        conv4_block15_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block15_1_relu (Activatio (None, 19, 19, 256)  0           conv4_block15_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block15_2_pad (ZeroPaddin (None, 21, 21, 256)  0           conv4_block15_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block15_2_conv (Conv2D)   (None, 19, 19, 256)  589824      conv4_block15_2_pad[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block15_2_bn (BatchNormal (None, 19, 19, 256)  1024        conv4_block15_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block15_2_relu (Activatio (None, 19, 19, 256)  0           conv4_block15_2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block15_3_conv (Conv2D)   (None, 19, 19, 1024) 263168      conv4_block15_2_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block15_out (Add)         (None, 19, 19, 1024) 0           conv4_block14_out[0][0]          \n",
      "                                                                 conv4_block15_3_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block16_preact_bn (BatchN (None, 19, 19, 1024) 4096        conv4_block15_out[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block16_preact_relu (Acti (None, 19, 19, 1024) 0           conv4_block16_preact_bn[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block16_1_conv (Conv2D)   (None, 19, 19, 256)  262144      conv4_block16_preact_relu[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block16_1_bn (BatchNormal (None, 19, 19, 256)  1024        conv4_block16_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block16_1_relu (Activatio (None, 19, 19, 256)  0           conv4_block16_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block16_2_pad (ZeroPaddin (None, 21, 21, 256)  0           conv4_block16_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block16_2_conv (Conv2D)   (None, 19, 19, 256)  589824      conv4_block16_2_pad[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block16_2_bn (BatchNormal (None, 19, 19, 256)  1024        conv4_block16_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block16_2_relu (Activatio (None, 19, 19, 256)  0           conv4_block16_2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block16_3_conv (Conv2D)   (None, 19, 19, 1024) 263168      conv4_block16_2_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block16_out (Add)         (None, 19, 19, 1024) 0           conv4_block15_out[0][0]          \n",
      "                                                                 conv4_block16_3_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block17_preact_bn (BatchN (None, 19, 19, 1024) 4096        conv4_block16_out[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block17_preact_relu (Acti (None, 19, 19, 1024) 0           conv4_block17_preact_bn[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block17_1_conv (Conv2D)   (None, 19, 19, 256)  262144      conv4_block17_preact_relu[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block17_1_bn (BatchNormal (None, 19, 19, 256)  1024        conv4_block17_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block17_1_relu (Activatio (None, 19, 19, 256)  0           conv4_block17_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block17_2_pad (ZeroPaddin (None, 21, 21, 256)  0           conv4_block17_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block17_2_conv (Conv2D)   (None, 19, 19, 256)  589824      conv4_block17_2_pad[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block17_2_bn (BatchNormal (None, 19, 19, 256)  1024        conv4_block17_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block17_2_relu (Activatio (None, 19, 19, 256)  0           conv4_block17_2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block17_3_conv (Conv2D)   (None, 19, 19, 1024) 263168      conv4_block17_2_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block17_out (Add)         (None, 19, 19, 1024) 0           conv4_block16_out[0][0]          \n",
      "                                                                 conv4_block17_3_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block18_preact_bn (BatchN (None, 19, 19, 1024) 4096        conv4_block17_out[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block18_preact_relu (Acti (None, 19, 19, 1024) 0           conv4_block18_preact_bn[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block18_1_conv (Conv2D)   (None, 19, 19, 256)  262144      conv4_block18_preact_relu[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block18_1_bn (BatchNormal (None, 19, 19, 256)  1024        conv4_block18_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block18_1_relu (Activatio (None, 19, 19, 256)  0           conv4_block18_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block18_2_pad (ZeroPaddin (None, 21, 21, 256)  0           conv4_block18_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block18_2_conv (Conv2D)   (None, 19, 19, 256)  589824      conv4_block18_2_pad[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block18_2_bn (BatchNormal (None, 19, 19, 256)  1024        conv4_block18_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block18_2_relu (Activatio (None, 19, 19, 256)  0           conv4_block18_2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block18_3_conv (Conv2D)   (None, 19, 19, 1024) 263168      conv4_block18_2_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block18_out (Add)         (None, 19, 19, 1024) 0           conv4_block17_out[0][0]          \n",
      "                                                                 conv4_block18_3_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block19_preact_bn (BatchN (None, 19, 19, 1024) 4096        conv4_block18_out[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block19_preact_relu (Acti (None, 19, 19, 1024) 0           conv4_block19_preact_bn[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block19_1_conv (Conv2D)   (None, 19, 19, 256)  262144      conv4_block19_preact_relu[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block19_1_bn (BatchNormal (None, 19, 19, 256)  1024        conv4_block19_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block19_1_relu (Activatio (None, 19, 19, 256)  0           conv4_block19_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block19_2_pad (ZeroPaddin (None, 21, 21, 256)  0           conv4_block19_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block19_2_conv (Conv2D)   (None, 19, 19, 256)  589824      conv4_block19_2_pad[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block19_2_bn (BatchNormal (None, 19, 19, 256)  1024        conv4_block19_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block19_2_relu (Activatio (None, 19, 19, 256)  0           conv4_block19_2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block19_3_conv (Conv2D)   (None, 19, 19, 1024) 263168      conv4_block19_2_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block19_out (Add)         (None, 19, 19, 1024) 0           conv4_block18_out[0][0]          \n",
      "                                                                 conv4_block19_3_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block20_preact_bn (BatchN (None, 19, 19, 1024) 4096        conv4_block19_out[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block20_preact_relu (Acti (None, 19, 19, 1024) 0           conv4_block20_preact_bn[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block20_1_conv (Conv2D)   (None, 19, 19, 256)  262144      conv4_block20_preact_relu[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block20_1_bn (BatchNormal (None, 19, 19, 256)  1024        conv4_block20_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block20_1_relu (Activatio (None, 19, 19, 256)  0           conv4_block20_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block20_2_pad (ZeroPaddin (None, 21, 21, 256)  0           conv4_block20_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block20_2_conv (Conv2D)   (None, 19, 19, 256)  589824      conv4_block20_2_pad[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block20_2_bn (BatchNormal (None, 19, 19, 256)  1024        conv4_block20_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block20_2_relu (Activatio (None, 19, 19, 256)  0           conv4_block20_2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block20_3_conv (Conv2D)   (None, 19, 19, 1024) 263168      conv4_block20_2_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block20_out (Add)         (None, 19, 19, 1024) 0           conv4_block19_out[0][0]          \n",
      "                                                                 conv4_block20_3_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block21_preact_bn (BatchN (None, 19, 19, 1024) 4096        conv4_block20_out[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block21_preact_relu (Acti (None, 19, 19, 1024) 0           conv4_block21_preact_bn[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block21_1_conv (Conv2D)   (None, 19, 19, 256)  262144      conv4_block21_preact_relu[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block21_1_bn (BatchNormal (None, 19, 19, 256)  1024        conv4_block21_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block21_1_relu (Activatio (None, 19, 19, 256)  0           conv4_block21_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block21_2_pad (ZeroPaddin (None, 21, 21, 256)  0           conv4_block21_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block21_2_conv (Conv2D)   (None, 19, 19, 256)  589824      conv4_block21_2_pad[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block21_2_bn (BatchNormal (None, 19, 19, 256)  1024        conv4_block21_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block21_2_relu (Activatio (None, 19, 19, 256)  0           conv4_block21_2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block21_3_conv (Conv2D)   (None, 19, 19, 1024) 263168      conv4_block21_2_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block21_out (Add)         (None, 19, 19, 1024) 0           conv4_block20_out[0][0]          \n",
      "                                                                 conv4_block21_3_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block22_preact_bn (BatchN (None, 19, 19, 1024) 4096        conv4_block21_out[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block22_preact_relu (Acti (None, 19, 19, 1024) 0           conv4_block22_preact_bn[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block22_1_conv (Conv2D)   (None, 19, 19, 256)  262144      conv4_block22_preact_relu[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block22_1_bn (BatchNormal (None, 19, 19, 256)  1024        conv4_block22_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block22_1_relu (Activatio (None, 19, 19, 256)  0           conv4_block22_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block22_2_pad (ZeroPaddin (None, 21, 21, 256)  0           conv4_block22_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block22_2_conv (Conv2D)   (None, 19, 19, 256)  589824      conv4_block22_2_pad[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block22_2_bn (BatchNormal (None, 19, 19, 256)  1024        conv4_block22_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block22_2_relu (Activatio (None, 19, 19, 256)  0           conv4_block22_2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block22_3_conv (Conv2D)   (None, 19, 19, 1024) 263168      conv4_block22_2_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block22_out (Add)         (None, 19, 19, 1024) 0           conv4_block21_out[0][0]          \n",
      "                                                                 conv4_block22_3_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block23_preact_bn (BatchN (None, 19, 19, 1024) 4096        conv4_block22_out[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block23_preact_relu (Acti (None, 19, 19, 1024) 0           conv4_block23_preact_bn[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block23_1_conv (Conv2D)   (None, 19, 19, 256)  262144      conv4_block23_preact_relu[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block23_1_bn (BatchNormal (None, 19, 19, 256)  1024        conv4_block23_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block23_1_relu (Activatio (None, 19, 19, 256)  0           conv4_block23_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block23_2_pad (ZeroPaddin (None, 21, 21, 256)  0           conv4_block23_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block23_2_conv (Conv2D)   (None, 19, 19, 256)  589824      conv4_block23_2_pad[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block23_2_bn (BatchNormal (None, 19, 19, 256)  1024        conv4_block23_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block23_2_relu (Activatio (None, 19, 19, 256)  0           conv4_block23_2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block23_3_conv (Conv2D)   (None, 19, 19, 1024) 263168      conv4_block23_2_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block23_out (Add)         (None, 19, 19, 1024) 0           conv4_block22_out[0][0]          \n",
      "                                                                 conv4_block23_3_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block24_preact_bn (BatchN (None, 19, 19, 1024) 4096        conv4_block23_out[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block24_preact_relu (Acti (None, 19, 19, 1024) 0           conv4_block24_preact_bn[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block24_1_conv (Conv2D)   (None, 19, 19, 256)  262144      conv4_block24_preact_relu[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block24_1_bn (BatchNormal (None, 19, 19, 256)  1024        conv4_block24_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block24_1_relu (Activatio (None, 19, 19, 256)  0           conv4_block24_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block24_2_pad (ZeroPaddin (None, 21, 21, 256)  0           conv4_block24_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block24_2_conv (Conv2D)   (None, 19, 19, 256)  589824      conv4_block24_2_pad[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block24_2_bn (BatchNormal (None, 19, 19, 256)  1024        conv4_block24_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block24_2_relu (Activatio (None, 19, 19, 256)  0           conv4_block24_2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block24_3_conv (Conv2D)   (None, 19, 19, 1024) 263168      conv4_block24_2_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block24_out (Add)         (None, 19, 19, 1024) 0           conv4_block23_out[0][0]          \n",
      "                                                                 conv4_block24_3_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block25_preact_bn (BatchN (None, 19, 19, 1024) 4096        conv4_block24_out[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block25_preact_relu (Acti (None, 19, 19, 1024) 0           conv4_block25_preact_bn[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block25_1_conv (Conv2D)   (None, 19, 19, 256)  262144      conv4_block25_preact_relu[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block25_1_bn (BatchNormal (None, 19, 19, 256)  1024        conv4_block25_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block25_1_relu (Activatio (None, 19, 19, 256)  0           conv4_block25_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block25_2_pad (ZeroPaddin (None, 21, 21, 256)  0           conv4_block25_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block25_2_conv (Conv2D)   (None, 19, 19, 256)  589824      conv4_block25_2_pad[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block25_2_bn (BatchNormal (None, 19, 19, 256)  1024        conv4_block25_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block25_2_relu (Activatio (None, 19, 19, 256)  0           conv4_block25_2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block25_3_conv (Conv2D)   (None, 19, 19, 1024) 263168      conv4_block25_2_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block25_out (Add)         (None, 19, 19, 1024) 0           conv4_block24_out[0][0]          \n",
      "                                                                 conv4_block25_3_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block26_preact_bn (BatchN (None, 19, 19, 1024) 4096        conv4_block25_out[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block26_preact_relu (Acti (None, 19, 19, 1024) 0           conv4_block26_preact_bn[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block26_1_conv (Conv2D)   (None, 19, 19, 256)  262144      conv4_block26_preact_relu[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block26_1_bn (BatchNormal (None, 19, 19, 256)  1024        conv4_block26_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block26_1_relu (Activatio (None, 19, 19, 256)  0           conv4_block26_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block26_2_pad (ZeroPaddin (None, 21, 21, 256)  0           conv4_block26_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block26_2_conv (Conv2D)   (None, 19, 19, 256)  589824      conv4_block26_2_pad[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block26_2_bn (BatchNormal (None, 19, 19, 256)  1024        conv4_block26_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block26_2_relu (Activatio (None, 19, 19, 256)  0           conv4_block26_2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block26_3_conv (Conv2D)   (None, 19, 19, 1024) 263168      conv4_block26_2_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block26_out (Add)         (None, 19, 19, 1024) 0           conv4_block25_out[0][0]          \n",
      "                                                                 conv4_block26_3_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block27_preact_bn (BatchN (None, 19, 19, 1024) 4096        conv4_block26_out[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block27_preact_relu (Acti (None, 19, 19, 1024) 0           conv4_block27_preact_bn[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block27_1_conv (Conv2D)   (None, 19, 19, 256)  262144      conv4_block27_preact_relu[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block27_1_bn (BatchNormal (None, 19, 19, 256)  1024        conv4_block27_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block27_1_relu (Activatio (None, 19, 19, 256)  0           conv4_block27_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block27_2_pad (ZeroPaddin (None, 21, 21, 256)  0           conv4_block27_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block27_2_conv (Conv2D)   (None, 19, 19, 256)  589824      conv4_block27_2_pad[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block27_2_bn (BatchNormal (None, 19, 19, 256)  1024        conv4_block27_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block27_2_relu (Activatio (None, 19, 19, 256)  0           conv4_block27_2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block27_3_conv (Conv2D)   (None, 19, 19, 1024) 263168      conv4_block27_2_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block27_out (Add)         (None, 19, 19, 1024) 0           conv4_block26_out[0][0]          \n",
      "                                                                 conv4_block27_3_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block28_preact_bn (BatchN (None, 19, 19, 1024) 4096        conv4_block27_out[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block28_preact_relu (Acti (None, 19, 19, 1024) 0           conv4_block28_preact_bn[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block28_1_conv (Conv2D)   (None, 19, 19, 256)  262144      conv4_block28_preact_relu[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block28_1_bn (BatchNormal (None, 19, 19, 256)  1024        conv4_block28_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block28_1_relu (Activatio (None, 19, 19, 256)  0           conv4_block28_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block28_2_pad (ZeroPaddin (None, 21, 21, 256)  0           conv4_block28_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block28_2_conv (Conv2D)   (None, 19, 19, 256)  589824      conv4_block28_2_pad[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block28_2_bn (BatchNormal (None, 19, 19, 256)  1024        conv4_block28_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block28_2_relu (Activatio (None, 19, 19, 256)  0           conv4_block28_2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block28_3_conv (Conv2D)   (None, 19, 19, 1024) 263168      conv4_block28_2_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block28_out (Add)         (None, 19, 19, 1024) 0           conv4_block27_out[0][0]          \n",
      "                                                                 conv4_block28_3_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block29_preact_bn (BatchN (None, 19, 19, 1024) 4096        conv4_block28_out[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block29_preact_relu (Acti (None, 19, 19, 1024) 0           conv4_block29_preact_bn[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block29_1_conv (Conv2D)   (None, 19, 19, 256)  262144      conv4_block29_preact_relu[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block29_1_bn (BatchNormal (None, 19, 19, 256)  1024        conv4_block29_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block29_1_relu (Activatio (None, 19, 19, 256)  0           conv4_block29_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block29_2_pad (ZeroPaddin (None, 21, 21, 256)  0           conv4_block29_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block29_2_conv (Conv2D)   (None, 19, 19, 256)  589824      conv4_block29_2_pad[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block29_2_bn (BatchNormal (None, 19, 19, 256)  1024        conv4_block29_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block29_2_relu (Activatio (None, 19, 19, 256)  0           conv4_block29_2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block29_3_conv (Conv2D)   (None, 19, 19, 1024) 263168      conv4_block29_2_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block29_out (Add)         (None, 19, 19, 1024) 0           conv4_block28_out[0][0]          \n",
      "                                                                 conv4_block29_3_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block30_preact_bn (BatchN (None, 19, 19, 1024) 4096        conv4_block29_out[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block30_preact_relu (Acti (None, 19, 19, 1024) 0           conv4_block30_preact_bn[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block30_1_conv (Conv2D)   (None, 19, 19, 256)  262144      conv4_block30_preact_relu[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block30_1_bn (BatchNormal (None, 19, 19, 256)  1024        conv4_block30_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block30_1_relu (Activatio (None, 19, 19, 256)  0           conv4_block30_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block30_2_pad (ZeroPaddin (None, 21, 21, 256)  0           conv4_block30_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block30_2_conv (Conv2D)   (None, 19, 19, 256)  589824      conv4_block30_2_pad[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block30_2_bn (BatchNormal (None, 19, 19, 256)  1024        conv4_block30_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block30_2_relu (Activatio (None, 19, 19, 256)  0           conv4_block30_2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block30_3_conv (Conv2D)   (None, 19, 19, 1024) 263168      conv4_block30_2_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block30_out (Add)         (None, 19, 19, 1024) 0           conv4_block29_out[0][0]          \n",
      "                                                                 conv4_block30_3_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block31_preact_bn (BatchN (None, 19, 19, 1024) 4096        conv4_block30_out[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block31_preact_relu (Acti (None, 19, 19, 1024) 0           conv4_block31_preact_bn[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block31_1_conv (Conv2D)   (None, 19, 19, 256)  262144      conv4_block31_preact_relu[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block31_1_bn (BatchNormal (None, 19, 19, 256)  1024        conv4_block31_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block31_1_relu (Activatio (None, 19, 19, 256)  0           conv4_block31_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block31_2_pad (ZeroPaddin (None, 21, 21, 256)  0           conv4_block31_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block31_2_conv (Conv2D)   (None, 19, 19, 256)  589824      conv4_block31_2_pad[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block31_2_bn (BatchNormal (None, 19, 19, 256)  1024        conv4_block31_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block31_2_relu (Activatio (None, 19, 19, 256)  0           conv4_block31_2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block31_3_conv (Conv2D)   (None, 19, 19, 1024) 263168      conv4_block31_2_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block31_out (Add)         (None, 19, 19, 1024) 0           conv4_block30_out[0][0]          \n",
      "                                                                 conv4_block31_3_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block32_preact_bn (BatchN (None, 19, 19, 1024) 4096        conv4_block31_out[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block32_preact_relu (Acti (None, 19, 19, 1024) 0           conv4_block32_preact_bn[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block32_1_conv (Conv2D)   (None, 19, 19, 256)  262144      conv4_block32_preact_relu[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block32_1_bn (BatchNormal (None, 19, 19, 256)  1024        conv4_block32_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block32_1_relu (Activatio (None, 19, 19, 256)  0           conv4_block32_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block32_2_pad (ZeroPaddin (None, 21, 21, 256)  0           conv4_block32_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block32_2_conv (Conv2D)   (None, 19, 19, 256)  589824      conv4_block32_2_pad[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block32_2_bn (BatchNormal (None, 19, 19, 256)  1024        conv4_block32_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block32_2_relu (Activatio (None, 19, 19, 256)  0           conv4_block32_2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block32_3_conv (Conv2D)   (None, 19, 19, 1024) 263168      conv4_block32_2_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block32_out (Add)         (None, 19, 19, 1024) 0           conv4_block31_out[0][0]          \n",
      "                                                                 conv4_block32_3_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block33_preact_bn (BatchN (None, 19, 19, 1024) 4096        conv4_block32_out[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block33_preact_relu (Acti (None, 19, 19, 1024) 0           conv4_block33_preact_bn[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block33_1_conv (Conv2D)   (None, 19, 19, 256)  262144      conv4_block33_preact_relu[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block33_1_bn (BatchNormal (None, 19, 19, 256)  1024        conv4_block33_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block33_1_relu (Activatio (None, 19, 19, 256)  0           conv4_block33_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block33_2_pad (ZeroPaddin (None, 21, 21, 256)  0           conv4_block33_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block33_2_conv (Conv2D)   (None, 19, 19, 256)  589824      conv4_block33_2_pad[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block33_2_bn (BatchNormal (None, 19, 19, 256)  1024        conv4_block33_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block33_2_relu (Activatio (None, 19, 19, 256)  0           conv4_block33_2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block33_3_conv (Conv2D)   (None, 19, 19, 1024) 263168      conv4_block33_2_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block33_out (Add)         (None, 19, 19, 1024) 0           conv4_block32_out[0][0]          \n",
      "                                                                 conv4_block33_3_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block34_preact_bn (BatchN (None, 19, 19, 1024) 4096        conv4_block33_out[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block34_preact_relu (Acti (None, 19, 19, 1024) 0           conv4_block34_preact_bn[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block34_1_conv (Conv2D)   (None, 19, 19, 256)  262144      conv4_block34_preact_relu[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block34_1_bn (BatchNormal (None, 19, 19, 256)  1024        conv4_block34_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block34_1_relu (Activatio (None, 19, 19, 256)  0           conv4_block34_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block34_2_pad (ZeroPaddin (None, 21, 21, 256)  0           conv4_block34_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block34_2_conv (Conv2D)   (None, 19, 19, 256)  589824      conv4_block34_2_pad[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block34_2_bn (BatchNormal (None, 19, 19, 256)  1024        conv4_block34_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block34_2_relu (Activatio (None, 19, 19, 256)  0           conv4_block34_2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block34_3_conv (Conv2D)   (None, 19, 19, 1024) 263168      conv4_block34_2_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block34_out (Add)         (None, 19, 19, 1024) 0           conv4_block33_out[0][0]          \n",
      "                                                                 conv4_block34_3_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block35_preact_bn (BatchN (None, 19, 19, 1024) 4096        conv4_block34_out[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block35_preact_relu (Acti (None, 19, 19, 1024) 0           conv4_block35_preact_bn[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block35_1_conv (Conv2D)   (None, 19, 19, 256)  262144      conv4_block35_preact_relu[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block35_1_bn (BatchNormal (None, 19, 19, 256)  1024        conv4_block35_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block35_1_relu (Activatio (None, 19, 19, 256)  0           conv4_block35_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block35_2_pad (ZeroPaddin (None, 21, 21, 256)  0           conv4_block35_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block35_2_conv (Conv2D)   (None, 19, 19, 256)  589824      conv4_block35_2_pad[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block35_2_bn (BatchNormal (None, 19, 19, 256)  1024        conv4_block35_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block35_2_relu (Activatio (None, 19, 19, 256)  0           conv4_block35_2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block35_3_conv (Conv2D)   (None, 19, 19, 1024) 263168      conv4_block35_2_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block35_out (Add)         (None, 19, 19, 1024) 0           conv4_block34_out[0][0]          \n",
      "                                                                 conv4_block35_3_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block36_preact_bn (BatchN (None, 19, 19, 1024) 4096        conv4_block35_out[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block36_preact_relu (Acti (None, 19, 19, 1024) 0           conv4_block36_preact_bn[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block36_1_conv (Conv2D)   (None, 19, 19, 256)  262144      conv4_block36_preact_relu[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block36_1_bn (BatchNormal (None, 19, 19, 256)  1024        conv4_block36_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block36_1_relu (Activatio (None, 19, 19, 256)  0           conv4_block36_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block36_2_pad (ZeroPaddin (None, 21, 21, 256)  0           conv4_block36_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block36_2_conv (Conv2D)   (None, 10, 10, 256)  589824      conv4_block36_2_pad[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block36_2_bn (BatchNormal (None, 10, 10, 256)  1024        conv4_block36_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block36_2_relu (Activatio (None, 10, 10, 256)  0           conv4_block36_2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2D)  (None, 10, 10, 1024) 0           conv4_block35_out[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block36_3_conv (Conv2D)   (None, 10, 10, 1024) 263168      conv4_block36_2_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block36_out (Add)         (None, 10, 10, 1024) 0           max_pooling2d_5[0][0]            \n",
      "                                                                 conv4_block36_3_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_preact_bn (BatchNo (None, 10, 10, 1024) 4096        conv4_block36_out[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_preact_relu (Activ (None, 10, 10, 1024) 0           conv5_block1_preact_bn[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_conv (Conv2D)    (None, 10, 10, 512)  524288      conv5_block1_preact_relu[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_bn (BatchNormali (None, 10, 10, 512)  2048        conv5_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_relu (Activation (None, 10, 10, 512)  0           conv5_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_pad (ZeroPadding (None, 12, 12, 512)  0           conv5_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_conv (Conv2D)    (None, 10, 10, 512)  2359296     conv5_block1_2_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_bn (BatchNormali (None, 10, 10, 512)  2048        conv5_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_relu (Activation (None, 10, 10, 512)  0           conv5_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_0_conv (Conv2D)    (None, 10, 10, 2048) 2099200     conv5_block1_preact_relu[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_3_conv (Conv2D)    (None, 10, 10, 2048) 1050624     conv5_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_out (Add)          (None, 10, 10, 2048) 0           conv5_block1_0_conv[0][0]        \n",
      "                                                                 conv5_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_preact_bn (BatchNo (None, 10, 10, 2048) 8192        conv5_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_preact_relu (Activ (None, 10, 10, 2048) 0           conv5_block2_preact_bn[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_conv (Conv2D)    (None, 10, 10, 512)  1048576     conv5_block2_preact_relu[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_bn (BatchNormali (None, 10, 10, 512)  2048        conv5_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_relu (Activation (None, 10, 10, 512)  0           conv5_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_pad (ZeroPadding (None, 12, 12, 512)  0           conv5_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_conv (Conv2D)    (None, 10, 10, 512)  2359296     conv5_block2_2_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_bn (BatchNormali (None, 10, 10, 512)  2048        conv5_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_relu (Activation (None, 10, 10, 512)  0           conv5_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_3_conv (Conv2D)    (None, 10, 10, 2048) 1050624     conv5_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_out (Add)          (None, 10, 10, 2048) 0           conv5_block1_out[0][0]           \n",
      "                                                                 conv5_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_preact_bn (BatchNo (None, 10, 10, 2048) 8192        conv5_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_preact_relu (Activ (None, 10, 10, 2048) 0           conv5_block3_preact_bn[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_conv (Conv2D)    (None, 10, 10, 512)  1048576     conv5_block3_preact_relu[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_bn (BatchNormali (None, 10, 10, 512)  2048        conv5_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_relu (Activation (None, 10, 10, 512)  0           conv5_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_pad (ZeroPadding (None, 12, 12, 512)  0           conv5_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_conv (Conv2D)    (None, 10, 10, 512)  2359296     conv5_block3_2_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_bn (BatchNormali (None, 10, 10, 512)  2048        conv5_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_relu (Activation (None, 10, 10, 512)  0           conv5_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_3_conv (Conv2D)    (None, 10, 10, 2048) 1050624     conv5_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_out (Add)          (None, 10, 10, 2048) 0           conv5_block2_out[0][0]           \n",
      "                                                                 conv5_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "post_bn (BatchNormalization)    (None, 10, 10, 2048) 8192        conv5_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "post_relu (Activation)          (None, 10, 10, 2048) 0           post_bn[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 58,347,328\n",
      "Trainable params: 0\n",
      "Non-trainable params: 58,347,328\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# log-vgg pretrained - ss-setting\n",
    "model_name = \"ResNet152V2\"\n",
    "fp_penalty_coef, fn_penalty_coef = 1, 1\n",
    "base_model = models.load_model(\"/home/shahrokh/projects/MLSpectrumAllocation/ML/data/\" +\n",
    "                               \"pictures_299_299_transfer/log/noisy_std_1/pu_circle_su_circle_60/\" +\n",
    "                               \"raw_power_min_max_norm/color/log_pu5_su6/\" +\n",
    "                               \"variable_sensors_10_20_pus_5_sus_8_channels/models/ResNet152V2/\" + \n",
    "                               \"700000/best_model_lambda_0_1.h5\", \n",
    "                              custom_objects={ 'loss': custom_loss(fp_penalty_coef, fn_penalty_coef), \n",
    "                                               'fp_mae': fp_mae,\n",
    "                                               'mae':'mae', 'mse':'mse'})\n",
    "base_model = base_model.layers[1]\n",
    "base_model.trainable = False\n",
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vit_keras import vit, utils\n",
    "model_name = \"vit_l16\"\n",
    "def transformer_model():\n",
    "    model = vit.vit_l16(image_size=max_x, activation='linear',\n",
    "                        pretrained=True, include_top=True, pretrained_top=False, classes=1)\n",
    "    for l_index in range(len(model.layers) - 1):\n",
    "        model.layers[l_index].trainable = False\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = transformer_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn1.layers[1].trainable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn1 = cnn_model(0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 299, 299, 8)]     0         \n",
      "_________________________________________________________________\n",
      "vgg16 (Functional)           (None, 9, 9, 512)         14717568  \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d (Gl (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2048)              1050624   \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 2049      \n",
      "=================================================================\n",
      "Total params: 15,770,241\n",
      "Trainable params: 1,052,673\n",
      "Non-trainable params: 14,717,568\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "cnn1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tf.test.is_gpu_available()\n",
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_samples = [1024]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8192]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_samples "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_samples = [700000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number_samples: 1024 , New samples: 1024\n",
      "Validation size: 205 , starts: 1024 , ends: 1228\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-09 17:55:16.081126: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-09 17:55:17.513286: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8101\n",
      "2022-06-09 17:55:20.122008: I tensorflow/stream_executor/cuda/cuda_blas.cc:1760] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/64 - 8s - loss: 148.4075 - mse: 148.4075 - mae: 8.7763 - fp_mae: 4.3849 - val_loss: 61.2394 - val_mse: 61.2394 - val_mae: 4.3195 - val_fp_mae: 3.2842\n",
      "\n",
      "Epoch 00001: val_mae improved from inf to 4.31954, saving model to ML/data/pictures_299_299_transfer/splat/pu_circle_su_circle_60/raw_power_min_max_norm/color/log_pu5_su5/pus_1_sus_3_channels/models/log_vgg16/1024/best_model_lambda_0.h5\n",
      "Epoch 2/100\n",
      "64/64 - 3s - loss: 251.6637 - mse: 251.6637 - mae: 8.7278 - fp_mae: 4.4464 - val_loss: 36.6462 - val_mse: 36.6462 - val_mae: 4.5722 - val_fp_mae: 3.8844\n",
      "\n",
      "Epoch 00002: val_mae did not improve from 4.31954\n",
      "Epoch 3/100\n",
      "64/64 - 3s - loss: 272.3088 - mse: 272.3088 - mae: 7.6117 - fp_mae: 3.6448 - val_loss: 24.2086 - val_mse: 24.2086 - val_mae: 3.8205 - val_fp_mae: 1.5410\n",
      "\n",
      "Epoch 00003: val_mae improved from 4.31954 to 3.82047, saving model to ML/data/pictures_299_299_transfer/splat/pu_circle_su_circle_60/raw_power_min_max_norm/color/log_pu5_su5/pus_1_sus_3_channels/models/log_vgg16/1024/best_model_lambda_0.h5\n",
      "Epoch 4/100\n",
      "64/64 - 3s - loss: 209.8172 - mse: 209.8172 - mae: 7.1627 - fp_mae: 3.5537 - val_loss: 39.6484 - val_mse: 39.6484 - val_mae: 4.2845 - val_fp_mae: 2.8823\n",
      "\n",
      "Epoch 00004: val_mae did not improve from 3.82047\n",
      "Epoch 5/100\n",
      "64/64 - 3s - loss: 103.8562 - mse: 103.8562 - mae: 6.3541 - fp_mae: 3.1840 - val_loss: 28.3486 - val_mse: 28.3486 - val_mae: 4.3531 - val_fp_mae: 0.8947\n",
      "\n",
      "Epoch 00005: val_mae did not improve from 3.82047\n",
      "Epoch 6/100\n",
      "64/64 - 3s - loss: 73.7350 - mse: 73.7350 - mae: 5.4974 - fp_mae: 2.7522 - val_loss: 31.9481 - val_mse: 31.9481 - val_mae: 4.1739 - val_fp_mae: 3.2618\n",
      "\n",
      "Epoch 00006: val_mae did not improve from 3.82047\n",
      "Epoch 7/100\n",
      "64/64 - 3s - loss: 97.0760 - mse: 97.0760 - mae: 5.3580 - fp_mae: 2.6160 - val_loss: 26.0816 - val_mse: 26.0816 - val_mae: 3.7416 - val_fp_mae: 2.3661\n",
      "\n",
      "Epoch 00007: val_mae improved from 3.82047 to 3.74160, saving model to ML/data/pictures_299_299_transfer/splat/pu_circle_su_circle_60/raw_power_min_max_norm/color/log_pu5_su5/pus_1_sus_3_channels/models/log_vgg16/1024/best_model_lambda_0.h5\n",
      "Epoch 8/100\n",
      "64/64 - 3s - loss: 186.5648 - mse: 186.5648 - mae: 6.4061 - fp_mae: 3.2847 - val_loss: 46.3077 - val_mse: 46.3077 - val_mae: 4.1460 - val_fp_mae: 1.8155\n",
      "\n",
      "Epoch 00008: val_mae did not improve from 3.74160\n",
      "Epoch 9/100\n",
      "64/64 - 3s - loss: 150.6155 - mse: 150.6155 - mae: 6.1050 - fp_mae: 2.6557 - val_loss: 57.7256 - val_mse: 57.7256 - val_mae: 5.4902 - val_fp_mae: 1.9817\n",
      "\n",
      "Epoch 00009: val_mae did not improve from 3.74160\n",
      "Epoch 10/100\n",
      "64/64 - 3s - loss: 175.7496 - mse: 175.7496 - mae: 5.9328 - fp_mae: 3.1931 - val_loss: 24.5060 - val_mse: 24.5060 - val_mae: 3.8974 - val_fp_mae: 1.3572\n",
      "\n",
      "Epoch 00010: val_mae did not improve from 3.74160\n",
      "Epoch 11/100\n",
      "64/64 - 3s - loss: 242.6246 - mse: 242.6246 - mae: 5.8781 - fp_mae: 2.8344 - val_loss: 41.0642 - val_mse: 41.0642 - val_mae: 5.1130 - val_fp_mae: 1.0572\n",
      "\n",
      "Epoch 00011: val_mae did not improve from 3.74160\n",
      "Epoch 12/100\n",
      "64/64 - 3s - loss: 261.9811 - mse: 261.9811 - mae: 6.1848 - fp_mae: 2.9709 - val_loss: 77.8297 - val_mse: 77.8297 - val_mae: 7.4628 - val_fp_mae: 7.2993\n",
      "\n",
      "Epoch 00012: val_mae did not improve from 3.74160\n",
      "Epoch 13/100\n",
      "64/64 - 3s - loss: 125.8715 - mse: 125.8715 - mae: 5.8068 - fp_mae: 3.0280 - val_loss: 25.0920 - val_mse: 25.0920 - val_mae: 3.9866 - val_fp_mae: 1.1275\n",
      "\n",
      "Epoch 00013: val_mae did not improve from 3.74160\n",
      "Epoch 14/100\n",
      "64/64 - 3s - loss: 46.4367 - mse: 46.4367 - mae: 4.5831 - fp_mae: 2.1466 - val_loss: 22.5756 - val_mse: 22.5756 - val_mae: 3.6205 - val_fp_mae: 2.2608\n",
      "\n",
      "Epoch 00014: val_mae improved from 3.74160 to 3.62049, saving model to ML/data/pictures_299_299_transfer/splat/pu_circle_su_circle_60/raw_power_min_max_norm/color/log_pu5_su5/pus_1_sus_3_channels/models/log_vgg16/1024/best_model_lambda_0.h5\n",
      "Epoch 15/100\n",
      "64/64 - 3s - loss: 230.5652 - mse: 230.5652 - mae: 6.1369 - fp_mae: 3.0086 - val_loss: 33.1141 - val_mse: 33.1141 - val_mae: 3.9682 - val_fp_mae: 2.5110\n",
      "\n",
      "Epoch 00015: val_mae did not improve from 3.62049\n",
      "Epoch 16/100\n",
      "64/64 - 3s - loss: 173.6893 - mse: 173.6893 - mae: 5.1979 - fp_mae: 2.5746 - val_loss: 20.9619 - val_mse: 20.9619 - val_mae: 3.5377 - val_fp_mae: 1.9228\n",
      "\n",
      "Epoch 00016: val_mae improved from 3.62049 to 3.53774, saving model to ML/data/pictures_299_299_transfer/splat/pu_circle_su_circle_60/raw_power_min_max_norm/color/log_pu5_su5/pus_1_sus_3_channels/models/log_vgg16/1024/best_model_lambda_0.h5\n",
      "Epoch 17/100\n",
      "64/64 - 3s - loss: 46.4750 - mse: 46.4750 - mae: 4.2973 - fp_mae: 2.0940 - val_loss: 21.7041 - val_mse: 21.7041 - val_mae: 3.5272 - val_fp_mae: 2.1536\n",
      "\n",
      "Epoch 00017: val_mae improved from 3.53774 to 3.52718, saving model to ML/data/pictures_299_299_transfer/splat/pu_circle_su_circle_60/raw_power_min_max_norm/color/log_pu5_su5/pus_1_sus_3_channels/models/log_vgg16/1024/best_model_lambda_0.h5\n",
      "Epoch 18/100\n",
      "64/64 - 3s - loss: 87.6236 - mse: 87.6236 - mae: 4.7497 - fp_mae: 2.3506 - val_loss: 25.0567 - val_mse: 25.0567 - val_mae: 3.6953 - val_fp_mae: 1.7202\n",
      "\n",
      "Epoch 00018: val_mae did not improve from 3.52718\n",
      "Epoch 19/100\n",
      "64/64 - 3s - loss: 183.7152 - mse: 183.7152 - mae: 4.8969 - fp_mae: 2.5215 - val_loss: 23.2075 - val_mse: 23.2075 - val_mae: 3.6634 - val_fp_mae: 1.8137\n",
      "\n",
      "Epoch 00019: val_mae did not improve from 3.52718\n",
      "Epoch 20/100\n",
      "64/64 - 3s - loss: 71.8722 - mse: 71.8722 - mae: 4.4185 - fp_mae: 2.1881 - val_loss: 20.9003 - val_mse: 20.9003 - val_mae: 3.5485 - val_fp_mae: 1.7350\n",
      "\n",
      "Epoch 00020: val_mae did not improve from 3.52718\n",
      "Epoch 21/100\n",
      "64/64 - 3s - loss: 74.5637 - mse: 74.5637 - mae: 4.3037 - fp_mae: 2.1379 - val_loss: 24.0573 - val_mse: 24.0573 - val_mae: 3.6308 - val_fp_mae: 2.3781\n",
      "\n",
      "Epoch 00021: val_mae did not improve from 3.52718\n",
      "Epoch 22/100\n",
      "64/64 - 3s - loss: 238.3622 - mse: 238.3622 - mae: 5.1469 - fp_mae: 2.5207 - val_loss: 22.0522 - val_mse: 22.0522 - val_mae: 3.5579 - val_fp_mae: 1.6536\n",
      "\n",
      "Epoch 00022: val_mae did not improve from 3.52718\n",
      "Epoch 23/100\n",
      "64/64 - 3s - loss: 190.2594 - mse: 190.2594 - mae: 5.2573 - fp_mae: 2.4495 - val_loss: 24.6451 - val_mse: 24.6451 - val_mae: 3.7192 - val_fp_mae: 2.1877\n",
      "\n",
      "Epoch 00023: val_mae did not improve from 3.52718\n",
      "Epoch 24/100\n",
      "64/64 - 3s - loss: 481.5143 - mse: 481.5143 - mae: 6.2287 - fp_mae: 3.2728 - val_loss: 21.5382 - val_mse: 21.5382 - val_mae: 3.5859 - val_fp_mae: 1.8413\n",
      "\n",
      "Epoch 00024: val_mae did not improve from 3.52718\n",
      "Epoch 25/100\n",
      "64/64 - 3s - loss: 73.2671 - mse: 73.2671 - mae: 4.5975 - fp_mae: 2.2984 - val_loss: 23.4694 - val_mse: 23.4694 - val_mae: 3.6604 - val_fp_mae: 2.0225\n",
      "\n",
      "Epoch 00025: val_mae did not improve from 3.52718\n",
      "Epoch 26/100\n",
      "64/64 - 3s - loss: 178.9695 - mse: 178.9695 - mae: 5.2787 - fp_mae: 2.6069 - val_loss: 33.1471 - val_mse: 33.1471 - val_mae: 4.4661 - val_fp_mae: 1.1464\n",
      "\n",
      "Epoch 00026: val_mae did not improve from 3.52718\n",
      "Epoch 27/100\n",
      "64/64 - 3s - loss: 65.7544 - mse: 65.7544 - mae: 4.6148 - fp_mae: 2.2960 - val_loss: 21.4868 - val_mse: 21.4868 - val_mae: 3.5501 - val_fp_mae: 1.6121\n",
      "\n",
      "Epoch 00027: val_mae did not improve from 3.52718\n",
      "Epoch 28/100\n",
      "64/64 - 3s - loss: 74.1221 - mse: 74.1221 - mae: 4.4635 - fp_mae: 2.1906 - val_loss: 21.1072 - val_mse: 21.1072 - val_mae: 3.5288 - val_fp_mae: 1.7682\n",
      "\n",
      "Epoch 00028: val_mae did not improve from 3.52718\n",
      "Epoch 29/100\n",
      "64/64 - 3s - loss: 65.0328 - mse: 65.0328 - mae: 4.4109 - fp_mae: 2.1481 - val_loss: 25.7047 - val_mse: 25.7047 - val_mae: 4.0733 - val_fp_mae: 1.0219\n",
      "\n",
      "Epoch 00029: val_mae did not improve from 3.52718\n",
      "Epoch 30/100\n",
      "64/64 - 3s - loss: 169.3994 - mse: 169.3994 - mae: 4.7253 - fp_mae: 2.2006 - val_loss: 27.6576 - val_mse: 27.6576 - val_mae: 3.7110 - val_fp_mae: 2.6006\n",
      "\n",
      "Epoch 00030: val_mae did not improve from 3.52718\n",
      "Epoch 31/100\n",
      "64/64 - 3s - loss: 108.3677 - mse: 108.3677 - mae: 4.5857 - fp_mae: 2.4041 - val_loss: 22.8596 - val_mse: 22.8596 - val_mae: 3.6206 - val_fp_mae: 1.9304\n",
      "\n",
      "Epoch 00031: val_mae did not improve from 3.52718\n",
      "Epoch 32/100\n",
      "64/64 - 3s - loss: 65.1776 - mse: 65.1776 - mae: 4.3653 - fp_mae: 2.2520 - val_loss: 20.8954 - val_mse: 20.8954 - val_mae: 3.5506 - val_fp_mae: 1.6439\n",
      "\n",
      "Epoch 00032: val_mae did not improve from 3.52718\n",
      "Epoch 33/100\n",
      "64/64 - 3s - loss: 198.6288 - mse: 198.6288 - mae: 5.0338 - fp_mae: 2.3556 - val_loss: 25.0139 - val_mse: 25.0139 - val_mae: 3.6104 - val_fp_mae: 2.0892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00033: val_mae did not improve from 3.52718\n",
      "Epoch 34/100\n",
      "64/64 - 3s - loss: 74.4497 - mse: 74.4497 - mae: 4.4054 - fp_mae: 2.1672 - val_loss: 22.0496 - val_mse: 22.0496 - val_mae: 3.6604 - val_fp_mae: 1.5075\n",
      "\n",
      "Epoch 00034: val_mae did not improve from 3.52718\n",
      "Epoch 35/100\n",
      "64/64 - 3s - loss: 40.2764 - mse: 40.2764 - mae: 4.0487 - fp_mae: 2.0247 - val_loss: 21.2516 - val_mse: 21.2516 - val_mae: 3.6259 - val_fp_mae: 1.4485\n",
      "\n",
      "Epoch 00035: val_mae did not improve from 3.52718\n",
      "Epoch 36/100\n",
      "64/64 - 3s - loss: 123.5392 - mse: 123.5392 - mae: 4.6528 - fp_mae: 2.1547 - val_loss: 26.7002 - val_mse: 26.7002 - val_mae: 3.6880 - val_fp_mae: 2.4457\n",
      "\n",
      "Epoch 00036: val_mae did not improve from 3.52718\n",
      "Epoch 37/100\n",
      "64/64 - 3s - loss: 70.0416 - mse: 70.0416 - mae: 4.3516 - fp_mae: 2.3094 - val_loss: 23.4327 - val_mse: 23.4327 - val_mae: 3.8560 - val_fp_mae: 1.3017\n",
      "\n",
      "Epoch 00037: val_mae did not improve from 3.52718\n",
      "Epoch 38/100\n",
      "64/64 - 3s - loss: 46.0843 - mse: 46.0843 - mae: 4.1869 - fp_mae: 1.9942 - val_loss: 29.3085 - val_mse: 29.3085 - val_mae: 3.7011 - val_fp_mae: 1.9028\n",
      "\n",
      "Epoch 00038: val_mae did not improve from 3.52718\n",
      "Epoch 39/100\n",
      "64/64 - 3s - loss: 132.2702 - mse: 132.2702 - mae: 4.6494 - fp_mae: 2.4295 - val_loss: 26.0754 - val_mse: 26.0754 - val_mae: 3.9913 - val_fp_mae: 1.1727\n",
      "\n",
      "Epoch 00039: val_mae did not improve from 3.52718\n",
      "Epoch 40/100\n",
      "64/64 - 3s - loss: 160.7554 - mse: 160.7554 - mae: 4.5873 - fp_mae: 2.1059 - val_loss: 29.1472 - val_mse: 29.1472 - val_mae: 3.8079 - val_fp_mae: 1.9504\n",
      "\n",
      "Epoch 00040: val_mae did not improve from 3.52718\n",
      "Epoch 41/100\n",
      "64/64 - 3s - loss: 84.4725 - mse: 84.4725 - mae: 4.7171 - fp_mae: 2.2940 - val_loss: 31.1688 - val_mse: 31.1688 - val_mae: 3.9252 - val_fp_mae: 2.9796\n",
      "\n",
      "Epoch 00041: val_mae did not improve from 3.52718\n",
      "Epoch 42/100\n",
      "64/64 - 3s - loss: 153.3558 - mse: 153.3558 - mae: 4.4269 - fp_mae: 2.3616 - val_loss: 26.9701 - val_mse: 26.9701 - val_mae: 3.8583 - val_fp_mae: 1.1990\n",
      "\n",
      "Epoch 00042: val_mae did not improve from 3.52718\n",
      "Epoch 43/100\n",
      "64/64 - 3s - loss: 408.0302 - mse: 408.0302 - mae: 5.1927 - fp_mae: 2.4397 - val_loss: 22.5105 - val_mse: 22.5105 - val_mae: 3.6658 - val_fp_mae: 1.7951\n",
      "\n",
      "Epoch 00043: val_mae did not improve from 3.52718\n",
      "Epoch 44/100\n",
      "64/64 - 3s - loss: 55.2045 - mse: 55.2045 - mae: 4.4095 - fp_mae: 2.2375 - val_loss: 22.2075 - val_mse: 22.2075 - val_mae: 3.7383 - val_fp_mae: 1.2688\n",
      "\n",
      "Epoch 00044: val_mae did not improve from 3.52718\n",
      "Epoch 45/100\n",
      "64/64 - 3s - loss: 46.9791 - mse: 46.9791 - mae: 4.0479 - fp_mae: 2.0484 - val_loss: 20.7233 - val_mse: 20.7233 - val_mae: 3.5272 - val_fp_mae: 1.7111\n",
      "\n",
      "Epoch 00045: val_mae did not improve from 3.52718\n",
      "Epoch 46/100\n",
      "64/64 - 3s - loss: 59.1347 - mse: 59.1347 - mae: 4.1976 - fp_mae: 2.1578 - val_loss: 20.5838 - val_mse: 20.5838 - val_mae: 3.4808 - val_fp_mae: 1.9197\n",
      "\n",
      "Epoch 00046: val_mae improved from 3.52718 to 3.48078, saving model to ML/data/pictures_299_299_transfer/splat/pu_circle_su_circle_60/raw_power_min_max_norm/color/log_pu5_su5/pus_1_sus_3_channels/models/log_vgg16/1024/best_model_lambda_0.h5\n",
      "Epoch 47/100\n",
      "64/64 - 3s - loss: 78.1956 - mse: 78.1956 - mae: 4.2658 - fp_mae: 2.0034 - val_loss: 20.9008 - val_mse: 20.9008 - val_mae: 3.5470 - val_fp_mae: 1.6223\n",
      "\n",
      "Epoch 00047: val_mae did not improve from 3.48078\n",
      "Epoch 48/100\n",
      "64/64 - 3s - loss: 109.4239 - mse: 109.4239 - mae: 4.4424 - fp_mae: 2.3479 - val_loss: 23.1893 - val_mse: 23.1893 - val_mae: 3.8174 - val_fp_mae: 1.2875\n",
      "\n",
      "Epoch 00048: val_mae did not improve from 3.48078\n",
      "Epoch 49/100\n",
      "64/64 - 3s - loss: 309.8199 - mse: 309.8199 - mae: 4.9727 - fp_mae: 2.4716 - val_loss: 22.1781 - val_mse: 22.1781 - val_mae: 3.6225 - val_fp_mae: 1.4714\n",
      "\n",
      "Epoch 00049: val_mae did not improve from 3.48078\n",
      "Epoch 50/100\n",
      "64/64 - 3s - loss: 171.3981 - mse: 171.3981 - mae: 4.5430 - fp_mae: 2.2391 - val_loss: 22.5831 - val_mse: 22.5831 - val_mae: 3.5512 - val_fp_mae: 2.3873\n",
      "\n",
      "Epoch 00050: val_mae did not improve from 3.48078\n",
      "Epoch 51/100\n",
      "64/64 - 3s - loss: 62.5799 - mse: 62.5799 - mae: 4.1581 - fp_mae: 2.1348 - val_loss: 20.5722 - val_mse: 20.5722 - val_mae: 3.4920 - val_fp_mae: 1.6815\n",
      "\n",
      "Epoch 00051: val_mae did not improve from 3.48078\n",
      "Epoch 52/100\n",
      "64/64 - 3s - loss: 194.0686 - mse: 194.0686 - mae: 4.2872 - fp_mae: 2.1605 - val_loss: 26.8188 - val_mse: 26.8188 - val_mae: 3.7443 - val_fp_mae: 2.5233\n",
      "\n",
      "Epoch 00052: val_mae did not improve from 3.48078\n",
      "Epoch 53/100\n",
      "64/64 - 3s - loss: 92.5291 - mse: 92.5291 - mae: 4.3834 - fp_mae: 2.1369 - val_loss: 22.8228 - val_mse: 22.8228 - val_mae: 3.5836 - val_fp_mae: 2.3814\n",
      "\n",
      "Epoch 00053: val_mae did not improve from 3.48078\n",
      "Epoch 54/100\n",
      "64/64 - 3s - loss: 105.4135 - mse: 105.4135 - mae: 4.1239 - fp_mae: 2.1220 - val_loss: 22.2022 - val_mse: 22.2022 - val_mae: 3.6187 - val_fp_mae: 1.4321\n",
      "\n",
      "Epoch 00054: val_mae did not improve from 3.48078\n",
      "Epoch 55/100\n",
      "64/64 - 3s - loss: 61.3176 - mse: 61.3176 - mae: 3.9688 - fp_mae: 1.8606 - val_loss: 21.9263 - val_mse: 21.9263 - val_mae: 3.7219 - val_fp_mae: 1.3390\n",
      "\n",
      "Epoch 00055: val_mae did not improve from 3.48078\n",
      "Epoch 56/100\n",
      "64/64 - 3s - loss: 57.2402 - mse: 57.2402 - mae: 3.9884 - fp_mae: 2.0196 - val_loss: 22.8083 - val_mse: 22.8083 - val_mae: 3.6347 - val_fp_mae: 2.2594\n",
      "\n",
      "Epoch 00056: val_mae did not improve from 3.48078\n",
      "Epoch 57/100\n",
      "64/64 - 3s - loss: 62.9844 - mse: 62.9844 - mae: 3.9572 - fp_mae: 1.9862 - val_loss: 22.9838 - val_mse: 22.9838 - val_mae: 3.6395 - val_fp_mae: 2.2494\n",
      "\n",
      "Epoch 00057: val_mae did not improve from 3.48078\n",
      "Epoch 58/100\n",
      "64/64 - 3s - loss: 41.1941 - mse: 41.1941 - mae: 3.9478 - fp_mae: 1.9218 - val_loss: 21.3308 - val_mse: 21.3308 - val_mae: 3.5785 - val_fp_mae: 1.7404\n",
      "\n",
      "Epoch 00058: val_mae did not improve from 3.48078\n",
      "Epoch 59/100\n",
      "64/64 - 3s - loss: 26.5084 - mse: 26.5084 - mae: 3.6921 - fp_mae: 1.8148 - val_loss: 21.4044 - val_mse: 21.4044 - val_mae: 3.5606 - val_fp_mae: 1.7548\n",
      "\n",
      "Epoch 00059: val_mae did not improve from 3.48078\n",
      "Epoch 60/100\n",
      "64/64 - 3s - loss: 87.7343 - mse: 87.7343 - mae: 4.0548 - fp_mae: 2.0504 - val_loss: 22.9748 - val_mse: 22.9748 - val_mae: 3.5912 - val_fp_mae: 2.3527\n",
      "\n",
      "Epoch 00060: val_mae did not improve from 3.48078\n",
      "Epoch 61/100\n",
      "64/64 - 3s - loss: 86.3637 - mse: 86.3637 - mae: 4.1775 - fp_mae: 1.9643 - val_loss: 23.2326 - val_mse: 23.2326 - val_mae: 3.6281 - val_fp_mae: 2.5638\n",
      "\n",
      "Epoch 00061: val_mae did not improve from 3.48078\n",
      "Epoch 62/100\n",
      "64/64 - 3s - loss: 38.8586 - mse: 38.8586 - mae: 3.9502 - fp_mae: 2.1258 - val_loss: 22.9278 - val_mse: 22.9278 - val_mae: 3.6172 - val_fp_mae: 2.3965\n",
      "\n",
      "Epoch 00062: val_mae did not improve from 3.48078\n",
      "Epoch 63/100\n",
      "64/64 - 3s - loss: 38.5531 - mse: 38.5531 - mae: 3.8732 - fp_mae: 1.8675 - val_loss: 22.1114 - val_mse: 22.1114 - val_mae: 3.5956 - val_fp_mae: 2.0681\n",
      "\n",
      "Epoch 00063: val_mae did not improve from 3.48078\n",
      "Epoch 64/100\n",
      "64/64 - 3s - loss: 28.4591 - mse: 28.4591 - mae: 3.7485 - fp_mae: 1.8492 - val_loss: 20.9602 - val_mse: 20.9602 - val_mae: 3.5227 - val_fp_mae: 1.8120\n",
      "\n",
      "Epoch 00064: val_mae did not improve from 3.48078\n",
      "Epoch 65/100\n",
      "64/64 - 3s - loss: 34.1662 - mse: 34.1662 - mae: 3.8988 - fp_mae: 1.9645 - val_loss: 21.7140 - val_mse: 21.7140 - val_mae: 3.6427 - val_fp_mae: 1.4253\n",
      "\n",
      "Epoch 00065: val_mae did not improve from 3.48078\n",
      "Epoch 66/100\n",
      "64/64 - 3s - loss: 43.4286 - mse: 43.4286 - mae: 3.8834 - fp_mae: 1.9512 - val_loss: 20.9733 - val_mse: 20.9733 - val_mae: 3.5342 - val_fp_mae: 1.8244\n",
      "\n",
      "Epoch 00066: val_mae did not improve from 3.48078\n",
      "Epoch 67/100\n",
      "64/64 - 3s - loss: 107.5779 - mse: 107.5779 - mae: 4.1712 - fp_mae: 2.0378 - val_loss: 22.4386 - val_mse: 22.4386 - val_mae: 3.5556 - val_fp_mae: 2.3645\n",
      "\n",
      "Epoch 00067: val_mae did not improve from 3.48078\n",
      "Epoch 68/100\n",
      "64/64 - 3s - loss: 35.3270 - mse: 35.3270 - mae: 3.8285 - fp_mae: 1.9129 - val_loss: 20.9500 - val_mse: 20.9500 - val_mae: 3.5588 - val_fp_mae: 1.7360\n",
      "\n",
      "Epoch 00068: val_mae did not improve from 3.48078\n",
      "Epoch 69/100\n",
      "64/64 - 3s - loss: 43.8008 - mse: 43.8008 - mae: 3.7511 - fp_mae: 1.9001 - val_loss: 23.1633 - val_mse: 23.1633 - val_mae: 3.6124 - val_fp_mae: 2.3919\n",
      "\n",
      "Epoch 00069: val_mae did not improve from 3.48078\n",
      "Epoch 70/100\n",
      "64/64 - 3s - loss: 26.9257 - mse: 26.9257 - mae: 3.6631 - fp_mae: 1.8312 - val_loss: 21.4002 - val_mse: 21.4002 - val_mae: 3.5963 - val_fp_mae: 1.7159\n",
      "\n",
      "Epoch 00070: val_mae did not improve from 3.48078\n",
      "Epoch 71/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/64 - 3s - loss: 24.1507 - mse: 24.1507 - mae: 3.6577 - fp_mae: 1.7934 - val_loss: 20.9781 - val_mse: 20.9781 - val_mae: 3.5356 - val_fp_mae: 1.8112\n",
      "\n",
      "Epoch 00071: val_mae did not improve from 3.48078\n",
      "Epoch 72/100\n",
      "64/64 - 3s - loss: 37.1957 - mse: 37.1957 - mae: 3.8109 - fp_mae: 1.9069 - val_loss: 21.6597 - val_mse: 21.6597 - val_mae: 3.6245 - val_fp_mae: 1.7273\n",
      "\n",
      "Epoch 00072: val_mae did not improve from 3.48078\n",
      "Epoch 73/100\n",
      "64/64 - 3s - loss: 49.3647 - mse: 49.3647 - mae: 3.8090 - fp_mae: 1.8472 - val_loss: 21.4344 - val_mse: 21.4344 - val_mae: 3.5646 - val_fp_mae: 1.9976\n",
      "\n",
      "Epoch 00073: val_mae did not improve from 3.48078\n",
      "Epoch 74/100\n",
      "64/64 - 3s - loss: 37.1245 - mse: 37.1245 - mae: 3.8458 - fp_mae: 1.9449 - val_loss: 21.8435 - val_mse: 21.8435 - val_mae: 3.6437 - val_fp_mae: 1.5804\n",
      "\n",
      "Epoch 00074: val_mae did not improve from 3.48078\n",
      "Epoch 75/100\n",
      "64/64 - 3s - loss: 29.1168 - mse: 29.1168 - mae: 3.7509 - fp_mae: 1.8467 - val_loss: 22.1173 - val_mse: 22.1173 - val_mae: 3.6674 - val_fp_mae: 1.6878\n",
      "\n",
      "Epoch 00075: val_mae did not improve from 3.48078\n",
      "Epoch 76/100\n",
      "64/64 - 3s - loss: 25.4137 - mse: 25.4137 - mae: 3.6465 - fp_mae: 1.8076 - val_loss: 21.8673 - val_mse: 21.8673 - val_mae: 3.6236 - val_fp_mae: 1.7094\n",
      "\n",
      "Epoch 00076: val_mae did not improve from 3.48078\n",
      "Epoch 77/100\n",
      "64/64 - 3s - loss: 26.8630 - mse: 26.8630 - mae: 3.7556 - fp_mae: 1.8141 - val_loss: 21.5554 - val_mse: 21.5554 - val_mae: 3.6202 - val_fp_mae: 1.7857\n",
      "\n",
      "Epoch 00077: val_mae did not improve from 3.48078\n",
      "Epoch 78/100\n",
      "64/64 - 3s - loss: 32.4319 - mse: 32.4319 - mae: 3.7674 - fp_mae: 1.8521 - val_loss: 26.5497 - val_mse: 26.5497 - val_mae: 3.7851 - val_fp_mae: 2.9401\n",
      "\n",
      "Epoch 00078: val_mae did not improve from 3.48078\n",
      "Epoch 79/100\n",
      "64/64 - 3s - loss: 35.3646 - mse: 35.3646 - mae: 3.7323 - fp_mae: 1.8138 - val_loss: 22.3063 - val_mse: 22.3063 - val_mae: 3.5951 - val_fp_mae: 2.2916\n",
      "\n",
      "Epoch 00079: val_mae did not improve from 3.48078\n",
      "Epoch 80/100\n",
      "64/64 - 3s - loss: 37.4232 - mse: 37.4232 - mae: 3.8329 - fp_mae: 1.9298 - val_loss: 22.5827 - val_mse: 22.5827 - val_mae: 3.6432 - val_fp_mae: 2.1086\n",
      "\n",
      "Epoch 00080: val_mae did not improve from 3.48078\n",
      "Epoch 81/100\n",
      "64/64 - 3s - loss: 35.3958 - mse: 35.3958 - mae: 3.8294 - fp_mae: 1.8582 - val_loss: 20.8818 - val_mse: 20.8818 - val_mae: 3.5216 - val_fp_mae: 1.8365\n",
      "\n",
      "Epoch 00081: val_mae did not improve from 3.48078\n",
      "Epoch 82/100\n",
      "64/64 - 3s - loss: 56.6091 - mse: 56.6091 - mae: 3.9298 - fp_mae: 2.0118 - val_loss: 23.3083 - val_mse: 23.3083 - val_mae: 3.7006 - val_fp_mae: 2.0129\n",
      "\n",
      "Epoch 00082: val_mae did not improve from 3.48078\n",
      "Epoch 83/100\n",
      "64/64 - 3s - loss: 33.8793 - mse: 33.8793 - mae: 3.8111 - fp_mae: 1.8520 - val_loss: 21.9041 - val_mse: 21.9041 - val_mae: 3.5550 - val_fp_mae: 2.1930\n",
      "\n",
      "Epoch 00083: val_mae did not improve from 3.48078\n",
      "Epoch 84/100\n",
      "64/64 - 3s - loss: 32.9685 - mse: 32.9685 - mae: 3.7493 - fp_mae: 1.9076 - val_loss: 22.4878 - val_mse: 22.4878 - val_mae: 3.6169 - val_fp_mae: 2.1529\n",
      "\n",
      "Epoch 00084: val_mae did not improve from 3.48078\n",
      "Epoch 85/100\n",
      "64/64 - 3s - loss: 31.4961 - mse: 31.4961 - mae: 3.7540 - fp_mae: 1.8377 - val_loss: 21.5492 - val_mse: 21.5492 - val_mae: 3.6279 - val_fp_mae: 1.6030\n",
      "\n",
      "Epoch 00085: val_mae did not improve from 3.48078\n",
      "Epoch 86/100\n",
      "64/64 - 3s - loss: 32.3429 - mse: 32.3429 - mae: 3.6643 - fp_mae: 1.8402 - val_loss: 22.5102 - val_mse: 22.5102 - val_mae: 3.6456 - val_fp_mae: 1.9475\n",
      "\n",
      "Epoch 00086: val_mae did not improve from 3.48078\n",
      "Epoch 87/100\n",
      "64/64 - 3s - loss: 32.6077 - mse: 32.6077 - mae: 3.7177 - fp_mae: 1.7350 - val_loss: 23.0748 - val_mse: 23.0748 - val_mae: 3.6057 - val_fp_mae: 2.3793\n",
      "\n",
      "Epoch 00087: val_mae did not improve from 3.48078\n",
      "Epoch 88/100\n",
      "64/64 - 3s - loss: 32.0453 - mse: 32.0453 - mae: 3.6437 - fp_mae: 1.8672 - val_loss: 22.1488 - val_mse: 22.1488 - val_mae: 3.6955 - val_fp_mae: 1.4553\n",
      "\n",
      "Epoch 00088: val_mae did not improve from 3.48078\n",
      "Epoch 89/100\n",
      "64/64 - 3s - loss: 25.3457 - mse: 25.3457 - mae: 3.6609 - fp_mae: 1.8529 - val_loss: 22.0828 - val_mse: 22.0828 - val_mae: 3.6419 - val_fp_mae: 1.8730\n",
      "\n",
      "Epoch 00089: val_mae did not improve from 3.48078\n",
      "Epoch 90/100\n",
      "64/64 - 3s - loss: 28.2092 - mse: 28.2092 - mae: 3.6383 - fp_mae: 1.7588 - val_loss: 21.5749 - val_mse: 21.5749 - val_mae: 3.6439 - val_fp_mae: 1.6307\n",
      "\n",
      "Epoch 00090: val_mae did not improve from 3.48078\n",
      "Epoch 91/100\n",
      "64/64 - 3s - loss: 37.2476 - mse: 37.2476 - mae: 3.7038 - fp_mae: 1.8755 - val_loss: 21.6183 - val_mse: 21.6183 - val_mae: 3.5742 - val_fp_mae: 1.8370\n",
      "\n",
      "Epoch 00091: val_mae did not improve from 3.48078\n",
      "Epoch 92/100\n",
      "64/64 - 3s - loss: 47.2342 - mse: 47.2342 - mae: 3.7386 - fp_mae: 1.8060 - val_loss: 21.5481 - val_mse: 21.5481 - val_mae: 3.5979 - val_fp_mae: 1.8648\n",
      "\n",
      "Epoch 00092: val_mae did not improve from 3.48078\n",
      "Epoch 93/100\n",
      "64/64 - 3s - loss: 21.8273 - mse: 21.8273 - mae: 3.5562 - fp_mae: 1.7714 - val_loss: 22.2705 - val_mse: 22.2705 - val_mae: 3.6108 - val_fp_mae: 2.0802\n",
      "\n",
      "Epoch 00093: val_mae did not improve from 3.48078\n",
      "Epoch 94/100\n",
      "64/64 - 3s - loss: 26.9705 - mse: 26.9705 - mae: 3.6089 - fp_mae: 1.7738 - val_loss: 21.7498 - val_mse: 21.7498 - val_mae: 3.6077 - val_fp_mae: 1.9693\n",
      "\n",
      "Epoch 00094: val_mae did not improve from 3.48078\n",
      "Epoch 95/100\n",
      "64/64 - 3s - loss: 32.7288 - mse: 32.7288 - mae: 3.6160 - fp_mae: 1.7730 - val_loss: 22.4948 - val_mse: 22.4948 - val_mae: 3.6398 - val_fp_mae: 2.0282\n",
      "\n",
      "Epoch 00095: val_mae did not improve from 3.48078\n",
      "Epoch 96/100\n",
      "64/64 - 3s - loss: 26.7797 - mse: 26.7797 - mae: 3.6185 - fp_mae: 1.7652 - val_loss: 22.0957 - val_mse: 22.0957 - val_mae: 3.6107 - val_fp_mae: 2.0927\n",
      "\n",
      "Epoch 00096: val_mae did not improve from 3.48078\n",
      "Epoch 97/100\n",
      "64/64 - 3s - loss: 22.8161 - mse: 22.8161 - mae: 3.5248 - fp_mae: 1.7572 - val_loss: 21.5870 - val_mse: 21.5870 - val_mae: 3.5726 - val_fp_mae: 2.0054\n",
      "\n",
      "Epoch 00097: val_mae did not improve from 3.48078\n",
      "Epoch 98/100\n",
      "64/64 - 3s - loss: 21.7737 - mse: 21.7737 - mae: 3.5397 - fp_mae: 1.8152 - val_loss: 21.3423 - val_mse: 21.3423 - val_mae: 3.6008 - val_fp_mae: 1.6971\n",
      "\n",
      "Epoch 00098: val_mae did not improve from 3.48078\n",
      "Epoch 99/100\n",
      "64/64 - 3s - loss: 24.8915 - mse: 24.8915 - mae: 3.6151 - fp_mae: 1.7614 - val_loss: 21.3606 - val_mse: 21.3606 - val_mae: 3.5933 - val_fp_mae: 1.7478\n",
      "\n",
      "Epoch 00099: val_mae did not improve from 3.48078\n",
      "Epoch 100/100\n",
      "64/64 - 3s - loss: 28.1344 - mse: 28.1344 - mae: 3.6445 - fp_mae: 1.8347 - val_loss: 22.2793 - val_mse: 22.2793 - val_mae: 3.6607 - val_fp_mae: 1.7899\n",
      "\n",
      "Epoch 00100: val_mae did not improve from 3.48078\n",
      "\n",
      "Lambda: 0 , Time: 0:04:55\n",
      "Train Error(all epochs): 3.524770975112915 \n",
      " [8.776, 8.728, 7.612, 7.163, 6.354, 5.497, 5.358, 6.406, 6.105, 5.933, 5.878, 6.185, 5.807, 4.583, 6.137, 5.198, 4.297, 4.75, 4.897, 4.419, 4.304, 5.147, 5.257, 6.229, 4.597, 5.279, 4.615, 4.464, 4.411, 4.725, 4.586, 4.365, 5.034, 4.405, 4.049, 4.653, 4.352, 4.187, 4.649, 4.587, 4.717, 4.427, 5.193, 4.41, 4.048, 4.198, 4.266, 4.442, 4.973, 4.543, 4.158, 4.287, 4.383, 4.124, 3.969, 3.988, 3.957, 3.948, 3.692, 4.055, 4.178, 3.95, 3.873, 3.748, 3.899, 3.883, 4.171, 3.829, 3.751, 3.663, 3.658, 3.811, 3.809, 3.846, 3.751, 3.647, 3.756, 3.767, 3.732, 3.833, 3.829, 3.93, 3.811, 3.749, 3.754, 3.664, 3.718, 3.644, 3.661, 3.638, 3.704, 3.739, 3.556, 3.609, 3.616, 3.619, 3.525, 3.54, 3.615, 3.645]\n",
      "Train FP Error(all epochs): 1.7350200414657593 \n",
      " [4.385, 4.446, 3.645, 3.554, 3.184, 2.752, 2.616, 3.285, 2.656, 3.193, 2.834, 2.971, 3.028, 2.147, 3.009, 2.575, 2.094, 2.351, 2.522, 2.188, 2.138, 2.521, 2.449, 3.273, 2.298, 2.607, 2.296, 2.191, 2.148, 2.201, 2.404, 2.252, 2.356, 2.167, 2.025, 2.155, 2.309, 1.994, 2.43, 2.106, 2.294, 2.362, 2.44, 2.238, 2.048, 2.158, 2.003, 2.348, 2.472, 2.239, 2.135, 2.16, 2.137, 2.122, 1.861, 2.02, 1.986, 1.922, 1.815, 2.05, 1.964, 2.126, 1.867, 1.849, 1.965, 1.951, 2.038, 1.913, 1.9, 1.831, 1.793, 1.907, 1.847, 1.945, 1.847, 1.808, 1.814, 1.852, 1.814, 1.93, 1.858, 2.012, 1.852, 1.908, 1.838, 1.84, 1.735, 1.867, 1.853, 1.759, 1.875, 1.806, 1.771, 1.774, 1.773, 1.765, 1.757, 1.815, 1.761, 1.835]\n",
      "Val Error(all epochs): 3.4807839393615723 \n",
      " [4.32, 4.572, 3.82, 4.284, 4.353, 4.174, 3.742, 4.146, 5.49, 3.897, 5.113, 7.463, 3.987, 3.62, 3.968, 3.538, 3.527, 3.695, 3.663, 3.548, 3.631, 3.558, 3.719, 3.586, 3.66, 4.466, 3.55, 3.529, 4.073, 3.711, 3.621, 3.551, 3.61, 3.66, 3.626, 3.688, 3.856, 3.701, 3.991, 3.808, 3.925, 3.858, 3.666, 3.738, 3.527, 3.481, 3.547, 3.817, 3.623, 3.551, 3.492, 3.744, 3.584, 3.619, 3.722, 3.635, 3.64, 3.578, 3.561, 3.591, 3.628, 3.617, 3.596, 3.523, 3.643, 3.534, 3.556, 3.559, 3.612, 3.596, 3.536, 3.624, 3.565, 3.644, 3.667, 3.624, 3.62, 3.785, 3.595, 3.643, 3.522, 3.701, 3.555, 3.617, 3.628, 3.646, 3.606, 3.696, 3.642, 3.644, 3.574, 3.598, 3.611, 3.608, 3.64, 3.611, 3.573, 3.601, 3.593, 3.661]\n",
      "Val FP Error(all epochs): 0.8946508169174194 \n",
      " [3.284, 3.884, 1.541, 2.882, 0.895, 3.262, 2.366, 1.816, 1.982, 1.357, 1.057, 7.299, 1.128, 2.261, 2.511, 1.923, 2.154, 1.72, 1.814, 1.735, 2.378, 1.654, 2.188, 1.841, 2.023, 1.146, 1.612, 1.768, 1.022, 2.601, 1.93, 1.644, 2.089, 1.508, 1.449, 2.446, 1.302, 1.903, 1.173, 1.95, 2.98, 1.199, 1.795, 1.269, 1.711, 1.92, 1.622, 1.288, 1.471, 2.387, 1.681, 2.523, 2.381, 1.432, 1.339, 2.259, 2.249, 1.74, 1.755, 2.353, 2.564, 2.397, 2.068, 1.812, 1.425, 1.824, 2.364, 1.736, 2.392, 1.716, 1.811, 1.727, 1.998, 1.58, 1.688, 1.709, 1.786, 2.94, 2.292, 2.109, 1.837, 2.013, 2.193, 2.153, 1.603, 1.948, 2.379, 1.455, 1.873, 1.631, 1.837, 1.865, 2.08, 1.969, 2.028, 2.093, 2.005, 1.697, 1.748, 1.79]\n",
      "******FINE TUNNING ******\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "64/64 - 8s - loss: 27.7493 - mse: 27.7493 - mae: 3.8555 - fp_mae: 1.8859 - val_loss: 17.6468 - val_mse: 17.6468 - val_mae: 3.2654 - val_fp_mae: 1.2483\n",
      "\n",
      "Epoch 00001: val_mae improved from 3.48078 to 3.26540, saving model to ML/data/pictures_299_299_transfer/splat/pu_circle_su_circle_60/raw_power_min_max_norm/color/log_pu5_su5/pus_1_sus_3_channels/models/log_vgg16/1024/best_model_lambda_0.h5\n",
      "Epoch 2/50\n",
      "64/64 - 6s - loss: 16.2788 - mse: 16.2788 - mae: 3.0514 - fp_mae: 1.5505 - val_loss: 16.0061 - val_mse: 16.0061 - val_mae: 3.0883 - val_fp_mae: 1.2213\n",
      "\n",
      "Epoch 00002: val_mae improved from 3.26540 to 3.08825, saving model to ML/data/pictures_299_299_transfer/splat/pu_circle_su_circle_60/raw_power_min_max_norm/color/log_pu5_su5/pus_1_sus_3_channels/models/log_vgg16/1024/best_model_lambda_0.h5\n",
      "Epoch 3/50\n",
      "64/64 - 6s - loss: 12.6221 - mse: 12.6221 - mae: 2.6461 - fp_mae: 1.3142 - val_loss: 16.0382 - val_mse: 16.0382 - val_mae: 2.9453 - val_fp_mae: 1.5181\n",
      "\n",
      "Epoch 00003: val_mae improved from 3.08825 to 2.94526, saving model to ML/data/pictures_299_299_transfer/splat/pu_circle_su_circle_60/raw_power_min_max_norm/color/log_pu5_su5/pus_1_sus_3_channels/models/log_vgg16/1024/best_model_lambda_0.h5\n",
      "Epoch 4/50\n",
      "64/64 - 6s - loss: 9.6668 - mse: 9.6668 - mae: 2.3085 - fp_mae: 1.1326 - val_loss: 16.7365 - val_mse: 16.7365 - val_mae: 3.0799 - val_fp_mae: 1.7639\n",
      "\n",
      "Epoch 00004: val_mae did not improve from 2.94526\n",
      "Epoch 5/50\n",
      "64/64 - 7s - loss: 7.9647 - mse: 7.9647 - mae: 2.0897 - fp_mae: 1.0567 - val_loss: 17.0370 - val_mse: 17.0370 - val_mae: 3.0320 - val_fp_mae: 1.5671\n",
      "\n",
      "Epoch 00005: val_mae did not improve from 2.94526\n",
      "Epoch 6/50\n",
      "64/64 - 7s - loss: 7.1683 - mse: 7.1683 - mae: 1.9803 - fp_mae: 1.0081 - val_loss: 17.8641 - val_mse: 17.8641 - val_mae: 3.3639 - val_fp_mae: 1.0548\n",
      "\n",
      "Epoch 00006: val_mae did not improve from 2.94526\n",
      "Epoch 7/50\n",
      "64/64 - 7s - loss: 6.4863 - mse: 6.4863 - mae: 1.9251 - fp_mae: 0.9346 - val_loss: 18.6487 - val_mse: 18.6487 - val_mae: 3.1371 - val_fp_mae: 2.0277\n",
      "\n",
      "Epoch 00007: val_mae did not improve from 2.94526\n",
      "Epoch 8/50\n",
      "64/64 - 7s - loss: 5.2786 - mse: 5.2786 - mae: 1.7125 - fp_mae: 0.8297 - val_loss: 18.7741 - val_mse: 18.7741 - val_mae: 3.1705 - val_fp_mae: 1.7297\n",
      "\n",
      "Epoch 00008: val_mae did not improve from 2.94526\n",
      "Epoch 9/50\n",
      "64/64 - 6s - loss: 4.4893 - mse: 4.4893 - mae: 1.5879 - fp_mae: 0.7941 - val_loss: 17.5409 - val_mse: 17.5409 - val_mae: 3.1189 - val_fp_mae: 1.5153\n",
      "\n",
      "Epoch 00009: val_mae did not improve from 2.94526\n",
      "Epoch 10/50\n",
      "64/64 - 6s - loss: 3.7418 - mse: 3.7418 - mae: 1.4552 - fp_mae: 0.7145 - val_loss: 16.9446 - val_mse: 16.9446 - val_mae: 3.0201 - val_fp_mae: 1.7877\n",
      "\n",
      "Epoch 00010: val_mae did not improve from 2.94526\n",
      "Epoch 11/50\n",
      "64/64 - 6s - loss: 3.2186 - mse: 3.2186 - mae: 1.3737 - fp_mae: 0.6819 - val_loss: 16.7514 - val_mse: 16.7514 - val_mae: 3.0573 - val_fp_mae: 1.2824\n",
      "\n",
      "Epoch 00011: val_mae did not improve from 2.94526\n",
      "Epoch 12/50\n",
      "64/64 - 6s - loss: 3.3269 - mse: 3.3269 - mae: 1.3683 - fp_mae: 0.6373 - val_loss: 15.8250 - val_mse: 15.8250 - val_mae: 2.9130 - val_fp_mae: 1.5057\n",
      "\n",
      "Epoch 00012: val_mae improved from 2.94526 to 2.91300, saving model to ML/data/pictures_299_299_transfer/splat/pu_circle_su_circle_60/raw_power_min_max_norm/color/log_pu5_su5/pus_1_sus_3_channels/models/log_vgg16/1024/best_model_lambda_0.h5\n",
      "Epoch 13/50\n",
      "64/64 - 6s - loss: 3.1523 - mse: 3.1523 - mae: 1.3279 - fp_mae: 0.6749 - val_loss: 16.7722 - val_mse: 16.7722 - val_mae: 3.0292 - val_fp_mae: 1.4204\n",
      "\n",
      "Epoch 00013: val_mae did not improve from 2.91300\n",
      "Epoch 14/50\n",
      "64/64 - 6s - loss: 3.2110 - mse: 3.2110 - mae: 1.3209 - fp_mae: 0.6254 - val_loss: 15.7550 - val_mse: 15.7550 - val_mae: 2.9514 - val_fp_mae: 1.3524\n",
      "\n",
      "Epoch 00014: val_mae did not improve from 2.91300\n",
      "Epoch 15/50\n",
      "64/64 - 6s - loss: 3.3076 - mse: 3.3076 - mae: 1.3745 - fp_mae: 0.6613 - val_loss: 15.7035 - val_mse: 15.7035 - val_mae: 2.8764 - val_fp_mae: 1.4386\n",
      "\n",
      "Epoch 00015: val_mae improved from 2.91300 to 2.87639, saving model to ML/data/pictures_299_299_transfer/splat/pu_circle_su_circle_60/raw_power_min_max_norm/color/log_pu5_su5/pus_1_sus_3_channels/models/log_vgg16/1024/best_model_lambda_0.h5\n",
      "Epoch 16/50\n",
      "64/64 - 6s - loss: 3.1137 - mse: 3.1137 - mae: 1.3034 - fp_mae: 0.6557 - val_loss: 18.3249 - val_mse: 18.3249 - val_mae: 3.2876 - val_fp_mae: 1.5514\n",
      "\n",
      "Epoch 00016: val_mae did not improve from 2.87639\n",
      "Epoch 17/50\n",
      "64/64 - 6s - loss: 2.8859 - mse: 2.8859 - mae: 1.2788 - fp_mae: 0.6085 - val_loss: 16.1169 - val_mse: 16.1169 - val_mae: 2.8998 - val_fp_mae: 1.5030\n",
      "\n",
      "Epoch 00017: val_mae did not improve from 2.87639\n",
      "Epoch 18/50\n",
      "64/64 - 6s - loss: 2.7200 - mse: 2.7200 - mae: 1.2311 - fp_mae: 0.6001 - val_loss: 16.3108 - val_mse: 16.3108 - val_mae: 2.9981 - val_fp_mae: 1.5106\n",
      "\n",
      "Epoch 00018: val_mae did not improve from 2.87639\n",
      "Epoch 19/50\n",
      "64/64 - 6s - loss: 2.6847 - mse: 2.6847 - mae: 1.2273 - fp_mae: 0.6018 - val_loss: 15.6082 - val_mse: 15.6082 - val_mae: 2.8509 - val_fp_mae: 1.6389\n",
      "\n",
      "Epoch 00019: val_mae improved from 2.87639 to 2.85089, saving model to ML/data/pictures_299_299_transfer/splat/pu_circle_su_circle_60/raw_power_min_max_norm/color/log_pu5_su5/pus_1_sus_3_channels/models/log_vgg16/1024/best_model_lambda_0.h5\n",
      "Epoch 20/50\n",
      "64/64 - 6s - loss: 2.4161 - mse: 2.4161 - mae: 1.1346 - fp_mae: 0.5408 - val_loss: 16.3540 - val_mse: 16.3540 - val_mae: 2.9347 - val_fp_mae: 1.6352\n",
      "\n",
      "Epoch 00020: val_mae did not improve from 2.85089\n",
      "Epoch 21/50\n",
      "64/64 - 6s - loss: 2.2797 - mse: 2.2797 - mae: 1.1067 - fp_mae: 0.5205 - val_loss: 17.6828 - val_mse: 17.6828 - val_mae: 3.0593 - val_fp_mae: 2.0532\n",
      "\n",
      "Epoch 00021: val_mae did not improve from 2.85089\n",
      "Epoch 22/50\n",
      "64/64 - 7s - loss: 2.3287 - mse: 2.3287 - mae: 1.1346 - fp_mae: 0.5497 - val_loss: 17.7847 - val_mse: 17.7847 - val_mae: 3.0172 - val_fp_mae: 2.2041\n",
      "\n",
      "Epoch 00022: val_mae did not improve from 2.85089\n",
      "Epoch 23/50\n",
      "64/64 - 6s - loss: 2.3777 - mse: 2.3777 - mae: 1.1314 - fp_mae: 0.5631 - val_loss: 16.2355 - val_mse: 16.2355 - val_mae: 2.9880 - val_fp_mae: 1.5663\n",
      "\n",
      "Epoch 00023: val_mae did not improve from 2.85089\n",
      "Epoch 24/50\n",
      "64/64 - 6s - loss: 2.7345 - mse: 2.7345 - mae: 1.1766 - fp_mae: 0.5612 - val_loss: 17.4020 - val_mse: 17.4020 - val_mae: 3.2073 - val_fp_mae: 1.3970\n",
      "\n",
      "Epoch 00024: val_mae did not improve from 2.85089\n",
      "Epoch 25/50\n",
      "64/64 - 6s - loss: 2.3852 - mse: 2.3852 - mae: 1.1625 - fp_mae: 0.5620 - val_loss: 15.9281 - val_mse: 15.9281 - val_mae: 2.9336 - val_fp_mae: 1.5863\n",
      "\n",
      "Epoch 00025: val_mae did not improve from 2.85089\n",
      "Epoch 26/50\n",
      "64/64 - 7s - loss: 1.8282 - mse: 1.8282 - mae: 0.9874 - fp_mae: 0.4709 - val_loss: 16.0516 - val_mse: 16.0516 - val_mae: 2.9580 - val_fp_mae: 1.5649\n",
      "\n",
      "Epoch 00026: val_mae did not improve from 2.85089\n",
      "Epoch 27/50\n",
      "64/64 - 6s - loss: 2.0584 - mse: 2.0584 - mae: 1.0830 - fp_mae: 0.5232 - val_loss: 16.0381 - val_mse: 16.0381 - val_mae: 3.0053 - val_fp_mae: 1.4600\n",
      "\n",
      "Epoch 00027: val_mae did not improve from 2.85089\n",
      "Epoch 28/50\n",
      "64/64 - 6s - loss: 1.8720 - mse: 1.8720 - mae: 1.0079 - fp_mae: 0.5080 - val_loss: 15.6330 - val_mse: 15.6330 - val_mae: 2.9444 - val_fp_mae: 1.3728\n",
      "\n",
      "Epoch 00028: val_mae did not improve from 2.85089\n",
      "Epoch 29/50\n",
      "64/64 - 6s - loss: 2.0212 - mse: 2.0212 - mae: 1.0500 - fp_mae: 0.4869 - val_loss: 16.2138 - val_mse: 16.2138 - val_mae: 2.9728 - val_fp_mae: 1.6312\n",
      "\n",
      "Epoch 00029: val_mae did not improve from 2.85089\n",
      "Epoch 30/50\n",
      "64/64 - 6s - loss: 1.7928 - mse: 1.7928 - mae: 1.0222 - fp_mae: 0.5056 - val_loss: 14.6070 - val_mse: 14.6070 - val_mae: 2.8149 - val_fp_mae: 1.4346\n",
      "\n",
      "Epoch 00030: val_mae improved from 2.85089 to 2.81486, saving model to ML/data/pictures_299_299_transfer/splat/pu_circle_su_circle_60/raw_power_min_max_norm/color/log_pu5_su5/pus_1_sus_3_channels/models/log_vgg16/1024/best_model_lambda_0.h5\n",
      "Epoch 31/50\n",
      "64/64 - 6s - loss: 1.7359 - mse: 1.7359 - mae: 0.9943 - fp_mae: 0.4769 - val_loss: 15.8024 - val_mse: 15.8024 - val_mae: 2.8499 - val_fp_mae: 1.7667\n",
      "\n",
      "Epoch 00031: val_mae did not improve from 2.81486\n",
      "Epoch 32/50\n",
      "64/64 - 6s - loss: 1.9454 - mse: 1.9454 - mae: 1.0420 - fp_mae: 0.5109 - val_loss: 15.4758 - val_mse: 15.4758 - val_mae: 2.8633 - val_fp_mae: 1.5491\n",
      "\n",
      "Epoch 00032: val_mae did not improve from 2.81486\n",
      "Epoch 33/50\n",
      "64/64 - 7s - loss: 2.1875 - mse: 2.1875 - mae: 1.0363 - fp_mae: 0.5152 - val_loss: 15.1490 - val_mse: 15.1490 - val_mae: 2.8734 - val_fp_mae: 1.4425\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00033: val_mae did not improve from 2.81486\n",
      "Epoch 34/50\n",
      "64/64 - 7s - loss: 1.8568 - mse: 1.8568 - mae: 0.9836 - fp_mae: 0.4638 - val_loss: 15.2295 - val_mse: 15.2295 - val_mae: 2.8969 - val_fp_mae: 1.3076\n",
      "\n",
      "Epoch 00034: val_mae did not improve from 2.81486\n",
      "Epoch 35/50\n",
      "64/64 - 6s - loss: 1.5155 - mse: 1.5155 - mae: 0.9004 - fp_mae: 0.4459 - val_loss: 15.4290 - val_mse: 15.4290 - val_mae: 2.8964 - val_fp_mae: 1.5734\n",
      "\n",
      "Epoch 00035: val_mae did not improve from 2.81486\n",
      "Epoch 36/50\n",
      "64/64 - 6s - loss: 1.3710 - mse: 1.3710 - mae: 0.8813 - fp_mae: 0.4162 - val_loss: 15.5610 - val_mse: 15.5610 - val_mae: 2.9002 - val_fp_mae: 1.5400\n",
      "\n",
      "Epoch 00036: val_mae did not improve from 2.81486\n",
      "Epoch 37/50\n",
      "64/64 - 6s - loss: 1.9056 - mse: 1.9056 - mae: 1.0373 - fp_mae: 0.5111 - val_loss: 15.4958 - val_mse: 15.4958 - val_mae: 2.8944 - val_fp_mae: 1.4872\n",
      "\n",
      "Epoch 00037: val_mae did not improve from 2.81486\n",
      "Epoch 38/50\n",
      "64/64 - 6s - loss: 2.0828 - mse: 2.0828 - mae: 1.0858 - fp_mae: 0.5092 - val_loss: 14.7869 - val_mse: 14.7869 - val_mae: 2.7834 - val_fp_mae: 1.5590\n",
      "\n",
      "Epoch 00038: val_mae improved from 2.81486 to 2.78342, saving model to ML/data/pictures_299_299_transfer/splat/pu_circle_su_circle_60/raw_power_min_max_norm/color/log_pu5_su5/pus_1_sus_3_channels/models/log_vgg16/1024/best_model_lambda_0.h5\n",
      "Epoch 39/50\n",
      "64/64 - 6s - loss: 1.8614 - mse: 1.8614 - mae: 0.9917 - fp_mae: 0.4847 - val_loss: 15.6408 - val_mse: 15.6408 - val_mae: 2.8313 - val_fp_mae: 1.7440\n",
      "\n",
      "Epoch 00039: val_mae did not improve from 2.78342\n",
      "Epoch 40/50\n",
      "64/64 - 6s - loss: 1.5144 - mse: 1.5144 - mae: 0.9293 - fp_mae: 0.4669 - val_loss: 15.7643 - val_mse: 15.7643 - val_mae: 2.8303 - val_fp_mae: 1.6920\n",
      "\n",
      "Epoch 00040: val_mae did not improve from 2.78342\n",
      "Epoch 41/50\n",
      "64/64 - 6s - loss: 1.4107 - mse: 1.4107 - mae: 0.8893 - fp_mae: 0.4155 - val_loss: 15.8909 - val_mse: 15.8909 - val_mae: 2.8296 - val_fp_mae: 1.7899\n",
      "\n",
      "Epoch 00041: val_mae did not improve from 2.78342\n",
      "Epoch 42/50\n",
      "64/64 - 6s - loss: 1.6169 - mse: 1.6169 - mae: 0.9680 - fp_mae: 0.4762 - val_loss: 15.2744 - val_mse: 15.2744 - val_mae: 2.8229 - val_fp_mae: 1.5449\n",
      "\n",
      "Epoch 00042: val_mae did not improve from 2.78342\n",
      "Epoch 43/50\n",
      "64/64 - 6s - loss: 1.4573 - mse: 1.4573 - mae: 0.8937 - fp_mae: 0.4277 - val_loss: 14.7819 - val_mse: 14.7819 - val_mae: 2.7626 - val_fp_mae: 1.5577\n",
      "\n",
      "Epoch 00043: val_mae improved from 2.78342 to 2.76262, saving model to ML/data/pictures_299_299_transfer/splat/pu_circle_su_circle_60/raw_power_min_max_norm/color/log_pu5_su5/pus_1_sus_3_channels/models/log_vgg16/1024/best_model_lambda_0.h5\n",
      "Epoch 44/50\n",
      "64/64 - 6s - loss: 1.5998 - mse: 1.5998 - mae: 0.9051 - fp_mae: 0.4405 - val_loss: 15.3698 - val_mse: 15.3698 - val_mae: 2.8689 - val_fp_mae: 1.3025\n",
      "\n",
      "Epoch 00044: val_mae did not improve from 2.76262\n",
      "Epoch 45/50\n",
      "64/64 - 6s - loss: 1.9239 - mse: 1.9239 - mae: 1.0084 - fp_mae: 0.4832 - val_loss: 14.9826 - val_mse: 14.9826 - val_mae: 2.8194 - val_fp_mae: 1.5470\n",
      "\n",
      "Epoch 00045: val_mae did not improve from 2.76262\n",
      "Epoch 46/50\n",
      "64/64 - 6s - loss: 1.6211 - mse: 1.6211 - mae: 0.9695 - fp_mae: 0.4713 - val_loss: 15.5530 - val_mse: 15.5530 - val_mae: 2.8888 - val_fp_mae: 1.5275\n",
      "\n",
      "Epoch 00046: val_mae did not improve from 2.76262\n",
      "Epoch 47/50\n",
      "64/64 - 7s - loss: 1.5158 - mse: 1.5158 - mae: 0.9173 - fp_mae: 0.4520 - val_loss: 15.4739 - val_mse: 15.4739 - val_mae: 2.9073 - val_fp_mae: 1.5552\n",
      "\n",
      "Epoch 00047: val_mae did not improve from 2.76262\n",
      "Epoch 48/50\n",
      "64/64 - 6s - loss: 1.6016 - mse: 1.6016 - mae: 0.9488 - fp_mae: 0.4398 - val_loss: 15.5317 - val_mse: 15.5317 - val_mae: 2.8261 - val_fp_mae: 1.7531\n",
      "\n",
      "Epoch 00048: val_mae did not improve from 2.76262\n",
      "Epoch 49/50\n",
      "64/64 - 6s - loss: 1.6057 - mse: 1.6057 - mae: 0.9620 - fp_mae: 0.4738 - val_loss: 16.1100 - val_mse: 16.1100 - val_mae: 2.8855 - val_fp_mae: 1.8952\n",
      "\n",
      "Epoch 00049: val_mae did not improve from 2.76262\n",
      "Epoch 50/50\n",
      "64/64 - 6s - loss: 1.8368 - mse: 1.8368 - mae: 1.0217 - fp_mae: 0.4953 - val_loss: 15.3980 - val_mse: 15.3980 - val_mae: 2.8403 - val_fp_mae: 1.6697\n",
      "\n",
      "Epoch 00050: val_mae did not improve from 2.76262\n",
      "\n",
      "Lambda: 0 , Time: 0:05:46\n",
      "Train Error(all epochs): 0.8812554478645325 \n",
      " [3.855, 3.051, 2.646, 2.309, 2.09, 1.98, 1.925, 1.712, 1.588, 1.455, 1.374, 1.368, 1.328, 1.321, 1.374, 1.303, 1.279, 1.231, 1.227, 1.135, 1.107, 1.135, 1.131, 1.177, 1.163, 0.987, 1.083, 1.008, 1.05, 1.022, 0.994, 1.042, 1.036, 0.984, 0.9, 0.881, 1.037, 1.086, 0.992, 0.929, 0.889, 0.968, 0.894, 0.905, 1.008, 0.97, 0.917, 0.949, 0.962, 1.022]\n",
      "Train FP Error(all epochs): 0.41552433371543884 \n",
      " [1.886, 1.55, 1.314, 1.133, 1.057, 1.008, 0.935, 0.83, 0.794, 0.715, 0.682, 0.637, 0.675, 0.625, 0.661, 0.656, 0.609, 0.6, 0.602, 0.541, 0.521, 0.55, 0.563, 0.561, 0.562, 0.471, 0.523, 0.508, 0.487, 0.506, 0.477, 0.511, 0.515, 0.464, 0.446, 0.416, 0.511, 0.509, 0.485, 0.467, 0.416, 0.476, 0.428, 0.441, 0.483, 0.471, 0.452, 0.44, 0.474, 0.495]\n",
      "Val Error(all epochs): 2.7626237869262695 \n",
      " [3.265, 3.088, 2.945, 3.08, 3.032, 3.364, 3.137, 3.17, 3.119, 3.02, 3.057, 2.913, 3.029, 2.951, 2.876, 3.288, 2.9, 2.998, 2.851, 2.935, 3.059, 3.017, 2.988, 3.207, 2.934, 2.958, 3.005, 2.944, 2.973, 2.815, 2.85, 2.863, 2.873, 2.897, 2.896, 2.9, 2.894, 2.783, 2.831, 2.83, 2.83, 2.823, 2.763, 2.869, 2.819, 2.889, 2.907, 2.826, 2.885, 2.84]\n",
      "Val FP Error(all epochs): 1.0548286437988281 \n",
      " [1.248, 1.221, 1.518, 1.764, 1.567, 1.055, 2.028, 1.73, 1.515, 1.788, 1.282, 1.506, 1.42, 1.352, 1.439, 1.551, 1.503, 1.511, 1.639, 1.635, 2.053, 2.204, 1.566, 1.397, 1.586, 1.565, 1.46, 1.373, 1.631, 1.435, 1.767, 1.549, 1.443, 1.308, 1.573, 1.54, 1.487, 1.559, 1.744, 1.692, 1.79, 1.545, 1.558, 1.303, 1.547, 1.528, 1.555, 1.753, 1.895, 1.67]\n",
      "\n",
      "Trainig set size: 1024 , Time: 0:10:41 , best_lambda: 0 , min_  error: 2.763\n",
      "Test starts:  1229 , ends:  39999\n",
      "2424/2424 [==============================] - 77s 32ms/step - loss: 16.6624 - mse: 16.6624 - mae: 3.0690 - fp_mae: 1.7103\n",
      "average_error:  3.069 , fp_average_error:  1.71\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# CNN: support batching\n",
    "AUGMENTED = False\n",
    "TEST, CONSERVE = True, False\n",
    "mini_batch = 64 if max(max_x, max_y) == 1000 else 16\n",
    "epochs = 35 if max(max_x, max_y) == 1000 else 100\n",
    "MAX_QUEUE_SIZE, WORKERS = 28, 4\n",
    "fp_penalty_coef, fn_penalty_coef = 1, 1\n",
    "hyper_metric, mode = \"val_mae\", 'min'  # the metric that hyper parameters are tuned with\n",
    "prev_sample = 0\n",
    "lambda_vec = [0] \n",
    "average_diff_power, fp_mean_power = [],[] #[7.177, 8.088, 8.183], [3.438, 3.506, 2.662]\n",
    "best_lambda = []\n",
    "average_diff_power_conserve, fp_mean_power_conserve = [], []\n",
    "all_cnns = []\n",
    "if CONSERVE: # for conservative\n",
    "    prev_number_samples = [0] + number_samples[:-1]\n",
    "\n",
    "for num_sample_idx, number_sample in enumerate(number_samples):\n",
    "    if CONSERVE:\n",
    "        data_reg[prev_number_samples[num_sample_idx]:number_sample, -1] = data_reg[\n",
    "            prev_number_samples[num_sample_idx]:number_sample, -1] - 1.5 # conserv value\n",
    "    MODEL_PATH = '/'.join(image_dir.split('/')[:-1]) + '/models/' + (\"aug/\" if AUGMENTED else \"\") \\\n",
    "    + model_name + \"/\" + (\"conservative/\" if CONSERVE else \"\") + str(number_sample)\n",
    "    if not os.path.exists(MODEL_PATH):\n",
    "        os.makedirs(MODEL_PATH)\n",
    "    MODEL_PATH += \"/best_model_lambda_\"\n",
    "    if True:\n",
    "        cnns = [cnn_model(lamb, 0) for lamb in lambda_vec]\n",
    "        for cnn in cnns:\n",
    "#             cnn.compile(loss='mean_squared_error', optimizer='adam', metrics=['mse','mae', fp_mean])\n",
    "            # =optimizers.SGD(lr=0.1, momentum=0.9, decay=0.1/epochs, nesterov=False)\n",
    "            cnn.compile(loss=custom_loss(fp_penalty_coef, fn_penalty_coef), \n",
    "                        optimizer=optimizers.Adam(), \n",
    "                        metrics=['mse', 'mae', fp_mae])\n",
    "        checkpointers = [ModelCheckpoint(filepath=MODEL_PATH + str(lamb)+ '.h5',\n",
    "                                         verbose=1, save_best_only=True, \n",
    "                                         monitor=hyper_metric,\n",
    "                                         mode=mode)\n",
    "                         for lamb in lambda_vec]\n",
    "    else:\n",
    "        cnns = []\n",
    "        cnns = [models.load_model(MODEL_PATH + str(lamb) + '.h5', \n",
    "                                  custom_objects={ 'loss': custom_loss(fp_penalty_coef, fn_penalty_coef), \n",
    "                                                  'fp_mae': fp_mae }) \n",
    "                for lamb in lambda_vec]\n",
    "    number_start = time.time()\n",
    "    train_generator = DataBatchGenerator(\n",
    "        dataset=data_reg_train[:number_sample * 4] if AUGMENTED else data_reg[:number_sample],\n",
    "        batch_size=mini_batch,\n",
    "        start_idx=prev_sample,\n",
    "        number_image_channels=number_image_channels,\n",
    "        max_x=max_x, max_y=max_y, float_memory_used=float_memory_used,\n",
    "        image_dir=image_dir + (\"/aug\" if AUGMENTED else \"\"))\n",
    "    \n",
    "\n",
    "    val_size = math.ceil(number_sample * validation_size)\n",
    "#     val_size = data_reg.shape[0] - number_sample\n",
    "    val_generator = DataBatchGenerator(dataset=data_reg[number_sample:number_sample+val_size], \n",
    "                                       batch_size=mini_batch,\n",
    "                                       start_idx=number_sample,\n",
    "                                       number_image_channels=number_image_channels,\n",
    "                                       max_x=max_x, max_y=max_y, \n",
    "                                       float_memory_used=float_memory_used,\n",
    "                                      image_dir=\"/\".join(image_dir.split(\"/\")[:-1]) + \"/images\")\n",
    "  \n",
    "    print('number_samples:', number_sample, \", New samples:\", number_sample - prev_sample)\n",
    "    print(\"Validation size:\", val_size, \", starts:\", number_sample, \", ends:\", number_sample + val_size - 1)\n",
    "    \n",
    "    for lamb_idx, lamb in enumerate(lambda_vec):\n",
    "#     for lamb_idx, lamb in enumerate(lambda_vec[:len(lambda_vec) - num_sample_idx//2]):\n",
    "#         if num_sample_idx == 3 and lamb_idx < 4:\n",
    "#             continue\n",
    "        lambda_start = time.time()\n",
    "        cnns[lamb_idx].fit(train_generator, epochs=epochs, verbose=2,\n",
    "                           validation_data=val_generator, \n",
    "                           shuffle=True, callbacks=[checkpointers[lamb_idx]], \n",
    "                           workers=WORKERS, max_queue_size=MAX_QUEUE_SIZE, \n",
    "                           use_multiprocessing=False)\n",
    "        \n",
    "        print(\"\\nLambda:\", lamb, \", Time:\", str(datetime.timedelta(seconds=int(time.time() - lambda_start))))\n",
    "        print(\"Train Error(all epochs):\", min(cnns[lamb_idx].history.history['mae']), '\\n', \n",
    "              [round(val, 3) for val in cnns[lamb_idx].history.history['mae']])\n",
    "        print(\"Train FP Error(all epochs):\", min(cnns[lamb_idx].history.history['fp_mae']), '\\n',\n",
    "              [round(val,3) for val in cnns[lamb_idx].history.history['fp_mae']])\n",
    "        print(\"Val Error(all epochs):\", min(cnns[lamb_idx].history.history['val_mae']), '\\n', \n",
    "              [round(val,3) for val in cnns[lamb_idx].history.history['val_mae']])\n",
    "        print(\"Val FP Error(all epochs):\", min(cnns[lamb_idx].history.history['val_fp_mae']), '\\n',\n",
    "              [round(val,3) for val in cnns[lamb_idx].history.history['val_fp_mae']])\n",
    "    # ******************** fine-tunning *******\n",
    "    print(\"******FINE TUNNING ******\")\n",
    "    # reloading the best\n",
    "    cnns = [models.load_model(MODEL_PATH + str(lamb) + '.h5', \n",
    "                              custom_objects={ 'loss': custom_loss(fp_penalty_coef, fn_penalty_coef), \n",
    "                                               'fp_mae': fp_mae,\n",
    "                                               'mae':'mae', 'mse':'mse'}) for lamb in lambda_vec]\n",
    "    for cnn in cnns:\n",
    "        cnn.trainable = True\n",
    "        cnn.compile(loss=custom_loss(fp_penalty_coef, fn_penalty_coef), \n",
    "                    optimizer=optimizers.Adam(1e-4), \n",
    "                    metrics=['mse', 'mae', fp_mae])\n",
    "    train_generator = DataBatchGenerator(\n",
    "        dataset=data_reg_train[:number_sample * 4] if AUGMENTED else data_reg[:number_sample],\n",
    "        batch_size=mini_batch,\n",
    "        start_idx=prev_sample,\n",
    "        number_image_channels=number_image_channels,\n",
    "        max_x=max_x, max_y=max_y, float_memory_used=float_memory_used,\n",
    "        image_dir=image_dir + (\"/aug\" if AUGMENTED else \"\"))\n",
    "    val_generator = DataBatchGenerator(dataset=data_reg[number_sample:number_sample+val_size], \n",
    "                                       batch_size=mini_batch,\n",
    "                                       start_idx=number_sample,\n",
    "                                       number_image_channels=number_image_channels,\n",
    "                                       max_x=max_x, max_y=max_y, \n",
    "                                       float_memory_used=float_memory_used,\n",
    "                                      image_dir=\"/\".join(image_dir.split(\"/\")[:-1]) + \"/images\")\n",
    "    for lamb_idx, lamb in enumerate(lambda_vec):\n",
    "#     for lamb_idx, lamb in enumerate(lambda_vec[:len(lambda_vec) - num_sample_idx//2]):\n",
    "#         if num_sample_idx == 3 and lamb_idx < 4:\n",
    "#             continue\n",
    "        lambda_start = time.time()\n",
    "        cnns[lamb_idx].fit(train_generator, epochs=int(epochs//2), verbose=2,\n",
    "                           validation_data=val_generator, \n",
    "                           shuffle=True, callbacks=[checkpointers[lamb_idx]], \n",
    "                           workers=WORKERS, max_queue_size=MAX_QUEUE_SIZE, \n",
    "                           use_multiprocessing=False)\n",
    "        \n",
    "        print(\"\\nLambda:\", lamb, \", Time:\", str(datetime.timedelta(seconds=int(time.time() - lambda_start))))\n",
    "        print(\"Train Error(all epochs):\", min(cnns[lamb_idx].history.history['mae']), '\\n', \n",
    "              [round(val, 3) for val in cnns[lamb_idx].history.history['mae']])\n",
    "        print(\"Train FP Error(all epochs):\", min(cnns[lamb_idx].history.history['fp_mae']), '\\n',\n",
    "              [round(val,3) for val in cnns[lamb_idx].history.history['fp_mae']])\n",
    "        print(\"Val Error(all epochs):\", min(cnns[lamb_idx].history.history['val_mae']), '\\n', \n",
    "              [round(val,3) for val in cnns[lamb_idx].history.history['val_mae']])\n",
    "        print(\"Val FP Error(all epochs):\", min(cnns[lamb_idx].history.history['val_fp_mae']), '\\n',\n",
    "              [round(val,3) for val in cnns[lamb_idx].history.history['val_fp_mae']])\n",
    "    \n",
    "    models_min_mae = [min(cnns[lam_idx].history.history[hyper_metric]) for\n",
    "                      lam_idx,_ in enumerate(lambda_vec)]\n",
    "    best_lamb_idx = models_min_mae.index(min(models_min_mae))\n",
    "    best_lambda.append(lambda_vec[best_lamb_idx])\n",
    "    print(\"\\nTrainig set size:\", number_sample, \", Time:\", str(datetime.timedelta(seconds=int(time.time() - \n",
    "                                                                                              number_start))),\n",
    "          \", best_lambda:\", lambda_vec[best_lamb_idx], \", min_\" , (\"fp_\" if hyper_metric == \"val_fp_mae\" else \"\"),\n",
    "          \"error:\", round(min(models_min_mae), 3))\n",
    "    all_cnns.append(cnns)\n",
    "    del cnns, train_generator, val_generator\n",
    "    \n",
    "    if TEST:\n",
    "        # evaluating test images\n",
    "        best_model = None\n",
    "        best_model = models.load_model(MODEL_PATH + str(lambda_vec[best_lamb_idx]) + '.h5', \n",
    "                                       custom_objects={ 'loss': custom_loss(fp_penalty_coef, fn_penalty_coef), \n",
    "                                                       'fp_mae': fp_mae,\n",
    "                                                      'mae':'mae', 'mse':'mse'})\n",
    "        test_generator = DataBatchGenerator(dataset=data_reg[number_sample + val_size:], \n",
    "                                            batch_size=mini_batch,\n",
    "                                            start_idx=number_sample + val_size, \n",
    "                                            number_image_channels=number_image_channels,\n",
    "                                            max_x=max_x, max_y=max_y, float_memory_used=float_memory_used)\n",
    "\n",
    "        print(\"Test starts: \", number_sample + val_size, \", ends: \", data_reg.shape[0] - 1)\n",
    "        time.sleep(1)\n",
    "        test_res = best_model.evaluate(test_generator, verbose=1, \n",
    "                                       workers=WORKERS, max_queue_size=MAX_QUEUE_SIZE, use_multiprocessing=False)\n",
    "        \n",
    "        test_mae_idx, test_fp_mae_idx = [best_model.metrics_names.index(mtrc) \n",
    "                                         for mtrc in ['mae','fp_mae']]\n",
    "        test_mae, test_fp_mae = test_res[test_mae_idx], test_res[test_fp_mae_idx]\n",
    "        average_diff_power.append(round(test_mae, 3))\n",
    "        fp_mean_power.append(round(test_fp_mae, 3))\n",
    "        print('average_error: ', average_diff_power[-1], ', fp_average_error: ', \n",
    "              fp_mean_power[-1])\n",
    "        \n",
    "        if False:\n",
    "            test_generator_conserve = DataBatchGenerator(dataset=data_reg[number_sample + val_size:], \n",
    "                                                         batch_size=mini_batch,\n",
    "                                                         start_idx=number_sample + val_size, \n",
    "                                                         number_image_channels=number_image_channels,\n",
    "                                                         max_x=max_x, max_y=max_y, \n",
    "                                                         float_memory_used=float_memory_used, \n",
    "                                                         conserve=1)\n",
    "            test_res_conserve = best_model.evaluate(test_generator_conserve, verbose=1, \n",
    "                                                    workers=WORKERS, max_queue_size=MAX_QUEUE_SIZE, \n",
    "                                                    use_multiprocessing=False)\n",
    "            test_mae_cons, test_fp_mae_cons = test_res_conserve[test_mae_idx], test_res_conserve[test_fp_mae_idx]\n",
    "            average_diff_power_conserve.append(round(test_mae_cons, 3))\n",
    "            fp_mean_power_conserve.append(round(test_fp_mae_cons, 3))\n",
    "            print('Conserve, average_error: ', average_diff_power_conserve[-1], ', fp_average_error: ',\n",
    "                 fp_mean_power_conserve[-1])\n",
    "        print(\"\\n\\n\")\n",
    "\n",
    "        \n",
    "#         var_f = open('/'.join(image_dir.split('/')[:-1]) +  '/models/' + model_name + \"/\"\n",
    "#                      + intensity_degradation + '_' + str(slope) + '_' + \n",
    "#                      dtime + \".dat\", \"wb\") # file for saving results\n",
    "#         pickle.dump([average_diff_power, fp_mean_power, number_samples, best_lambda, \n",
    "#                      dataset_name, max_dataset_name, average_diff_power_conserve, fp_mean_power_conserve,\n",
    "#                      checkpointers],\n",
    "#                     file=var_f)\n",
    "#         var_f.close()\n",
    "        del best_model, test_generator\n",
    "#     prev_sample = number_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving images once to save time\n",
    "# run this cell just for creating images\n",
    "def creating_image_max_su_tot(start, end):\n",
    "    # for image_num in range(115, data_reg.shape[0]):\n",
    "    # for image_num in range(1625, 5000):\n",
    "    for row_idx in tqdm.tqdm(range(start, end+1)):  #4463, data_reg.shape[0]\n",
    "        row_sample = data_reg[row_idx]\n",
    "        row_sample[int(row_sample[0]) * 3 + 1] = 1\n",
    "        for su_idx in range(int(data_max_su_tot[row_idx][0])):\n",
    "            row_sample[int(row_sample[0]) * 3 + 2 : \n",
    "                       int(row_sample[0]) * 3 + 4] = data_max_su_tot[row_idx][1 + su_idx * 3: \n",
    "                                                                              1 + su_idx * 3 + 2]\n",
    "            image = create_image(data=row_sample, \n",
    "                                 slope=slope, style=style, noise_floor=noise_floor,\n",
    "                                 pu_shape=pu_shape, su_shape=su_shape, su_param=su_param, \n",
    "                                 sensors_num=(sensors_num if sensors else 0), \n",
    "                                 intensity_degradation=intensity_degradation, \n",
    "                                 max_pu_power=0.0 if not sensors else -60,\n",
    "                                 max_su_power=40.0)\n",
    "            if False and style == \"image_intensity\":\n",
    "                if number_image_channels != 3:\n",
    "                    image = np.append(np.array(image[0]), np.zeros((3-number_image_channels,max_x, max_y), \n",
    "                                                                   dtype=float_memory_used), axis=0)\n",
    "                image_save = np.swapaxes(image, 0, 2)\n",
    "                plt.imsave(max_su_image_dir + '/image' + str(image_num)+'.png', image_save)\n",
    "            elif style == \"raw_power_min_max_norm\" or style == \"raw_power_zscore_norm\" or style == \"image_intensity\":\n",
    "        #         np.save(max_su_image_dir + '/image' + str(image_num), image)\n",
    "    #             np.savez_compressed(f\"{image_dir}{(600000 + image_num)//100000}/image{600000 + image_num}\",\n",
    "    #                                 a=np.expand_dims(image,0))\n",
    "                np.savez_compressed(f\"{max_su_image_dir}/image{row_idx * 5 + su_idx}\",\n",
    "                                    a=np.expand_dims(image,0))\n",
    "            del image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|█▉                                    | 138/2680 [03:39<1:10:00,  1.65s/it]"
     ]
    }
   ],
   "source": [
    "model_name = \"log_vgg16_max_su_total\"\n",
    "max_su_image_dir = '/'.join(image_dir.split('/')[:-1]) + \"/\" + model_name + \"/images\"\n",
    "if not os.path.exists(max_su_image_dir):\n",
    "        os.makedirs(max_su_image_dir)\n",
    "jobs = []\n",
    "proc_sizes = [data_reg.shape[0]//number_of_proccessors] * (number_of_proccessors)\n",
    "proc_sizes[-1] += data_reg.shape[0]%number_of_proccessors\n",
    "proc_idx = [(sum(proc_sizes[:i]), sum(proc_sizes[:i+1])-1) for i in range(number_of_proccessors)]\n",
    "\n",
    "for i in range(number_of_proccessors):\n",
    "    p = Process(target=creating_image_max_su_tot, args=(proc_idx[i][0], proc_idx[i][1]))\n",
    "    jobs.append(p)\n",
    "    p.start()\n",
    "for i in range(number_of_proccessors):\n",
    "    jobs[i].join()\n",
    "\n",
    "for i in range(number_of_proccessors):\n",
    "    jobs[i].terminate()\n",
    "    jobs[i].close()\n",
    "del jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128 finished.\n"
     ]
    }
   ],
   "source": [
    "# Multi-SU using single deep-alloc and another NN\n",
    "# Create dataset\n",
    "\n",
    "number_samples = [128, 256, 512, 1024, 2048, 4096]\n",
    "fp_penalty_coef, fn_penalty_coef = 1, 1\n",
    "dataset_path = '/'.join(image_dir.split('/')[:-1]) + \"/\" + model_name\n",
    "\n",
    "model_path = \"/home/shahrokh/projects/MLSpectrumAllocation/ML/data/pictures_299_299_transfer/\" + \\\n",
    "             \"splat/pu_circle_su_circle_60/raw_power_min_max_norm/color/log_pu5_su5/\" + \\\n",
    "             \"pus_1_sus_3_channels/models/log_vgg16/\"\n",
    "\n",
    "for number_sample in number_samples:\n",
    "    model = models.load_model(f\"{model_path}/{number_sample}/best_model_lambda_0.h5\",\n",
    "                              custom_objects={ 'loss': custom_loss(fp_penalty_coef, fn_penalty_coef),\n",
    "                                              'fp_mae': fp_mae,\n",
    "                                              'mae':'mae', 'mse':'mse'})\n",
    "    model.trainable = False\n",
    "    number_dataset_path = f\"{dataset_path}/{number_sample}\"\n",
    "    if not os.path.exists(number_dataset_path):\n",
    "        os.makedirs(number_dataset_path)\n",
    "    X = np.zeros((data_reg.shape[0], max_sus_num * 3), dtype=float)\n",
    "    y = np.zeros((data_reg.shape[0], max_sus_num), dtype = float)\n",
    "    for row_idx in range(data_reg.shape[0]):\n",
    "        row_sample = data_reg[row_idx]\n",
    "        row_sample[int(row_sample[0]) * 3 + 1] = 1\n",
    "        for su_idx in range(int(data_max_su_tot[row_idx][0])):\n",
    "            X[row_idx][su_idx * 3: su_idx * 3 + 2] = data_max_su_tot[row_idx][1 + su_idx * 3: 1 + su_idx * 3 + 2]\n",
    "            y[row_idx][su_idx] = data_max_su_tot[row_idx][1 + su_idx * 3 + 2]\n",
    "            row_sample[int(row_sample[0]) * 3 + 2 : int(row_sample[0]) * 3 + 4] = X[row_idx][su_idx * 3: \n",
    "                                                                                             su_idx * 3 + 2]\n",
    "            image = create_image(data=row_sample, \n",
    "                                 slope=slope, style=style, noise_floor=noise_floor,\n",
    "                                 pu_shape=pu_shape, su_shape=su_shape, su_param=su_param, \n",
    "                                 sensors_num=(sensors_num if sensors else 0), \n",
    "                                 intensity_degradation=intensity_degradation, \n",
    "                                 max_pu_power=0.0 if not sensors else -60,\n",
    "                                 max_su_power=40.0)\n",
    "            image = np.expand_dims(image,0)\n",
    "            X[row_idx][su_idx * 3 + 2] = model.predict(image)[0][0]\n",
    "    np.savetxt(f\"{number_dataset_path}/X.txt\", X, delimiter=\",\")\n",
    "    np.savetxt(f\"{number_dataset_path}/y.txt\", y, delimiter=\",\")\n",
    "    print(f\"{number_sample} finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ML/data/pictures_299_299_transfer/splat/pu_circle_su_circle_60/raw_power_min_max_norm/color/log_pu5_su5/20pus_5sus_3channels/images'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "imm = read_image(3, image_dir=max_su_image_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x1080 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "fig, ax = plt.subplots(figsize=(15,15))         # Sample figsize in inches\n",
    "sns.heatmap(imm[0,0,:,:,2], ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5.   , 80.   , 56.   ,  0.313, 80.   , 80.   , -3.125, 53.   ,\n",
       "       38.   ,  5.898, 99.   , 95.   , 17.5  , 66.   , 52.   , -9.893])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_max_su_tot[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 10.   ,  72.   ,  73.   , -21.032,  91.   ,  30.   , -25.484,\n",
       "        25.   ,  57.   ,  -1.238,  62.   ,  72.   ,  -1.277,  14.   ,\n",
       "        50.   , -25.147,  51.   ,  75.   , -11.996,  67.   ,  17.   ,\n",
       "        -0.44 ,  36.   ,  58.   , -22.813,  23.   ,  83.   , -12.931,\n",
       "        12.   ,  85.   , -27.937,   1.   ,  80.   ,  56.   ,   8.803,\n",
       "        80.   ,  80.   , -49.85 ,  53.   ,  38.   , -37.92 ,  99.   ,\n",
       "        95.   , -37.8  ,  66.   ,  52.   ,  38.317,   0.   ,     nan,\n",
       "           nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
       "           nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
       "           nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
       "           nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
       "           nan, -51.79 ])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_reg[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_sample[int(row_sample[0])*3 + 1] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 10.   ,  72.   ,  73.   , -21.032,  91.   ,  30.   , -25.484,\n",
       "        25.   ,  57.   ,  -1.238,  62.   ,  72.   ,  -1.277,  14.   ,\n",
       "        50.   , -25.147,  51.   ,  75.   , -11.996,  67.   ,  17.   ,\n",
       "        -0.44 ,  36.   ,  58.   , -22.813,  23.   ,  83.   , -12.931,\n",
       "        12.   ,  85.   , -27.937,   1.   ,  80.   ,  56.   ,   8.803,\n",
       "        80.   ,  80.   , -49.85 ,  53.   ,  38.   , -37.92 ,  99.   ,\n",
       "        95.   , -37.8  ,  66.   ,  52.   ,  38.317,   0.   ,     nan,\n",
       "           nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
       "           nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
       "           nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
       "           nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
       "           nan, -51.79 ])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ML/data/pictures_299_299_transfer/splat/pu_circle_su_circle_60/raw_power_min_max_norm/color/log_pu5_su5/20pus_5sus_3channelslog_vgg16_max_su_total'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = create_image(data=data_reg[image_num], slope=slope, style=style, \n",
    "                             noise_floor=noise_floor,\n",
    "                             pu_shape=pu_shape, su_shape=su_shape, su_param=su_param, \n",
    "                             sensors_num=(sensors_num if sensors else 0), \n",
    "                             intensity_degradation=intensity_degradation, \n",
    "                             max_pu_power=0.0 if not sensors else -60,\n",
    "                             max_su_power=40.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7.018]\n",
      "[3.323]\n",
      "[0]\n"
     ]
    }
   ],
   "source": [
    "print(average_diff_power)\n",
    "print(fp_mean_power)\n",
    "print(best_lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1935/1935 [==============================] - 166s 86ms/step - loss: 76.8849 - mse: 75.6603 - mae: 6.9288 - fp_mae: 3.3564\n"
     ]
    }
   ],
   "source": [
    "test_res = best_model.evaluate(test_generator, verbose=1, \n",
    "                               workers=WORKERS, max_queue_size=MAX_QUEUE_SIZE, use_multiprocessing=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aug, 400 sensors\n",
    "[6.973, 6.9, 6.78, 6.583, 6.51, 6.395]\n",
    "[3.423, 3.234, 3.611, 3.509, 3.391, 3.234]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 400 sensors\n",
    "[7.018, 6.98, 6.88, 6.755, 6.621, 6.497]\n",
    "[3.323, 3.179, 3.93, 3.422, 3.459, 3.519]\n",
    "[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6.847, 6.856]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[6.847, 6.856 (6.608), 6.827 (6.782),7.038 (6.678, 6.621), 6.32 (6.20)]\n",
    "[3.473, 3.691, 3.297, 4.085 (3.8, 3.459), 3.34 (3.01)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aug\n",
    "[6.83, 6.812 , 6.72, 6.51, 6.252]\n",
    "[3.37, 3.583 , 3.14, 3.39, 3.503]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnns[0].save(MODEL_PATH + \"0_fit.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\n",
    "ii, jj = 2, 2\n",
    "b = [a[i][j] for i in range(max(0, ii - 1), min(2, ii+1) + 1) for j in range(max(0, jj-1), min(2, jj+1)+1)\n",
    "     if not (i ==ii and j==jj)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = {1:2, 3:4}\n",
    "[c[i] for i in c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### MULTIPLE - SUS\n",
    "def db_to_dec(db: float):\n",
    "    return 10 ** (db/10)\n",
    "def dec_to_db(dec: float):\n",
    "    return -float('inf') if dec <= 0 else 10 * math.log10(dec)\n",
    "\n",
    "def greedy_sus(pus, requesting_sus, model, pl_info, number_channel = 1):\n",
    "    # pus: (x, y, p), requesting_sus: (x, y)\n",
    "    def best_su_candidate(pus, active_sus, requesting_sus):\n",
    "        if len(requesting_sus) == 1:\n",
    "            return requesting_sus[0][0]\n",
    "        # active_sus: (x, y, allocated_power), requesting_sus: (req_id, x, y)\n",
    "        power_map_cell_size, area_size = 50, 1000\n",
    "        cell_weight, neighbour_weight = 0.5, 0.5\n",
    "        power_map = [[0] * int(area_size // power_map_cell_size) for _ in range(int(area_size // power_map_cell_size))]\n",
    "        \n",
    "        for pu_num in range(int(len(pus)//3)):\n",
    "            x, y, dec_p = pus[pu_num*3], pus[pu_num * 3 + 1], db_to_dec(pus[pu_num*3 + 2])\n",
    "            power_map[int(x * cell_size // power_map_cell_size)][int(y * cell_size // power_map_cell_size)] += dec_p\n",
    "        for su in active_sus:\n",
    "            x, y, dec_p = su[0], su[1], db_to_dec(su[2])\n",
    "            power_map[int(x * cell_size // power_map_cell_size)][int(y * cell_size // power_map_cell_size)] += dec_p\n",
    "        \n",
    "        power_score = [[0] * len(power_map[0]) for _ in range(len(power_map))]\n",
    "        # updating power score\n",
    "        for i in range(len(power_map)):\n",
    "            for j in range(len(power_map[0])):\n",
    "                power_score[i][j] = cell_weight * power_map[i][j] + neighbour_weight * sum(\n",
    "                [power_map[ii][jj] for ii in range(max(0, i - 1), min(i + 1, len(power_map))) \n",
    "                 for jj in range(max(0, j - 1), min(j + 1, len(power_map[0]))) if not (i == ii and j == jj)])\n",
    "        # find su with lowest weight\n",
    "        best_su_req, min_score = -1, float('inf')\n",
    "        for req_id, x, y in requesting_sus:\n",
    "            if power_score[int(x * cell_size // power_map_cell_size)][int(x * cell_size // power_map_cell_size)] < min_score:\n",
    "                min_score = power_score[int(x * cell_size // power_map_cell_size)][int(x * cell_size // power_map_cell_size)]\n",
    "                best_su_req = req_id\n",
    "        return best_su_req         \n",
    "    active_sus = [{} for _ in range(number_channel)] # (request_id: power_allocated)\n",
    "    assigned_req = set()\n",
    "    for _ in range(len(requesting_sus)):\n",
    "        max_allocated_power, best_channel, best_req_id = -float('inf'), -1, -1\n",
    "        for ch in range(number_channel):\n",
    "            nex_req_id = best_su_candidate(pus, [(*requesting_sus[i], active_sus[ch][i]) for i in active_sus[ch]],\n",
    "                                          [(idd, *requesting_sus[idd]) for idd in range(len(requesting_sus))\n",
    "                                           if idd not in assigned_req])\n",
    "            su_lst = []\n",
    "            for i in active_sus[ch]:\n",
    "                su_lst += [*requesting_sus[i]]\n",
    "                su_lst.append(active_sus[ch][i])\n",
    "            requesting_data = [len(pus)//3] + pus + [len(active_sus[ch]) + 1] + su_lst + requesting_sus[nex_req_id]\n",
    "            requesting_image = np.expand_dims(create_image(requesting_data, slope=slope, style=style, \n",
    "                                                           noise_floor=noise_floor, pu_shape=pu_shape, su_shape=su_shape,\n",
    "                                                           su_param=su_param, sensors_num=(sensors_num if sensors else 0), \n",
    "                                                           intensity_degradation=intensity_degradation, \n",
    "                                                           max_pu_power=0.0 if not sensors else -30,\n",
    "                                                           max_su_power=0.0), 0)\n",
    "            predicted_power = model.predict(requesting_image)\n",
    "            if predicted_power[0][0] > max_allocated_power:\n",
    "                max_allocated_power = predicted_power[0][0]\n",
    "                best_channel, best_req_id = ch, nex_req_id\n",
    "        active_sus[best_channel][best_req_id] = max_allocated_power\n",
    "        assigned_req.add(best_req_id)\n",
    "    res = []\n",
    "    for ch in range(number_channel):\n",
    "        for req_id in active_sus[ch]:\n",
    "            res.append((req_id, active_sus[ch][req_id], ch))\n",
    "    return res\n",
    "\n",
    "def fair_sus(pus, requesting_sus, model, pl_info, number_channel = 1):\n",
    "    # pus: (x, y, p), requesting_sus: (x, y)\n",
    "    def best_su_candidate(pus, active_sus, requesting_sus):\n",
    "        if len(requesting_sus) == 1:\n",
    "            return requesting_sus[0][0]\n",
    "        # active_sus: (x, y, allocated_power), requesting_sus: (req_id, x, y)\n",
    "        power_map_cell_size, area_size = 50, 1000\n",
    "        cell_weight, neighbour_weight = 0.5, 0.5\n",
    "        power_map = [[0] * int(area_size // power_map_cell_size) for _ in range(int(area_size // power_map_cell_size))]\n",
    "        \n",
    "        for pu_num in range(int(len(pus)//3)):\n",
    "            x, y, dec_p = pus[pu_num*3], pus[pu_num * 3 + 1], db_to_dec(pus[pu_num*3 + 2])\n",
    "            power_map[int(x * cell_size // power_map_cell_size)][int(y * cell_size // power_map_cell_size)] += dec_p\n",
    "        for su in active_sus:\n",
    "            x, y, dec_p = su[0], su[1], db_to_dec(su[2])\n",
    "            power_map[int(x * cell_size // power_map_cell_size)][int(y * cell_size // power_map_cell_size)] += dec_p\n",
    "        \n",
    "        power_score = [[0] * len(power_map[0]) for _ in range(len(power_map))]\n",
    "        # updating power score\n",
    "        for i in range(len(power_map)):\n",
    "            for j in range(len(power_map[0])):\n",
    "                power_score[i][j] = cell_weight * power_map[i][j] + neighbour_weight * sum(\n",
    "                [power_map[ii][jj] for ii in range(max(0, i - 1), min(i + 1, len(power_map))) \n",
    "                 for jj in range(max(0, j - 1), min(j + 1, len(power_map[0]))) if not (i == ii and j == jj)])\n",
    "        # find su with lowest weight\n",
    "        best_su_req, min_score = -1, float('inf')\n",
    "        for req_id, x, y in requesting_sus:\n",
    "            if power_score[int(x * cell_size // power_map_cell_size)][int(x * cell_size // power_map_cell_size)] < min_score:\n",
    "                min_score = power_score[int(x * cell_size // power_map_cell_size)][int(x * cell_size // power_map_cell_size)]\n",
    "                best_su_req = req_id\n",
    "        return best_su_req         \n",
    "    active_sus = [{} for _ in range(number_channel)] # (request_id: power_allocated)\n",
    "    assigned_req = set()\n",
    "    for _ in range(len(requesting_sus)):\n",
    "        max_allocated_power, best_channel, best_req_id = -float('inf'), -1, -1\n",
    "        for ch in range(number_channel):\n",
    "            nex_req_id = best_su_candidate(pus, [(*requesting_sus[i], active_sus[ch][i]) for i in active_sus[ch]],\n",
    "                                          [(idd, *requesting_sus[idd]) for idd in range(len(requesting_sus))\n",
    "                                           if idd not in assigned_req])\n",
    "            su_lst = []\n",
    "            for i in active_sus[ch]:\n",
    "                su_lst += [*requesting_sus[i]]\n",
    "                su_lst.append(active_sus[ch][i])\n",
    "            requesting_data = [len(pus)//3] + pus + [len(active_sus[ch]) + 1] + su_lst + requesting_sus[nex_req_id]\n",
    "            requesting_image = np.expand_dims(create_image(requesting_data, slope=slope, style=style, \n",
    "                                                           noise_floor=noise_floor, pu_shape=pu_shape, su_shape=su_shape,\n",
    "                                                           su_param=su_param, sensors_num=(sensors_num if sensors else 0), \n",
    "                                                           intensity_degradation=intensity_degradation, \n",
    "                                                           max_pu_power=0.0 if not sensors else -30,\n",
    "                                                           max_su_power=0.0), 0)\n",
    "            predicted_power = model.predict(requesting_image)\n",
    "            if predicted_power[0][0] > max_allocated_power:\n",
    "                max_allocated_power = predicted_power[0][0]\n",
    "                best_channel, best_req_id = ch, nex_req_id\n",
    "        if max_allocated_power < -20:\n",
    "            # it's less than threshold. sort active su w.r.t to best_req_id and try to decrease their power\n",
    "            def dist(p1, p2):\n",
    "                return ((p1[0] - p2[0]) ** 2 + (p1[1] - p2[1]) ** 2) ** 0.5\n",
    "            dist_info = []\n",
    "            for ch in range(number_channel):\n",
    "                for i in active_sus[ch]:\n",
    "                    dist_info.append((ch, i, dist(requesting_sus[i], requesting_sus[best_req_id])))\n",
    "            dist_info.sort(key=lambda x:x[2])\n",
    "            SATISFIED = False\n",
    "            for i in range(len(dist_info)):\n",
    "                candid_ch, candid_su_id  = dist_info[i][0], dist_info[i][1]\n",
    "                candid_old_pow = active_sus[candid_ch][candid_su_id]\n",
    "                if  candid_old_pow > -10:\n",
    "                    candid_new_pow = candid_old_pow\n",
    "                    while candid_new_pow - 5 > -20.0:\n",
    "                        candid_new_pow -= 5\n",
    "                        # try this new power\n",
    "                        su_lst = []\n",
    "                        for ii in active_sus[candid_ch]:\n",
    "                            su_lst += [*requesting_sus[ii]]\n",
    "                            if ii == candid_su_id:\n",
    "                                su_lst.append(candid_new_pow)\n",
    "                            else:\n",
    "                                su_lst.append(active_sus[candid_ch][ii])\n",
    "                        requesting_data = [len(pus)//3] + pus + [len(active_sus[candid_ch]) + 1] + su_lst + requesting_sus[best_req_id]\n",
    "                        requesting_image = np.expand_dims(create_image(requesting_data, slope=slope, style=style, \n",
    "                                                                       noise_floor=noise_floor, pu_shape=pu_shape, su_shape=su_shape,\n",
    "                                                                       su_param=su_param, sensors_num=(sensors_num if sensors else 0), \n",
    "                                                                       intensity_degradation=intensity_degradation, \n",
    "                                                                       max_pu_power=0.0 if not sensors else -30,\n",
    "                                                                       max_su_power=0.0), 0)\n",
    "                        predicted_power = model.predict(requesting_image)[0][0]\n",
    "                        if predicted_power > -20.0:\n",
    "                            SATISFIED = True\n",
    "                            break\n",
    "                    if SATISFIED:\n",
    "                        break\n",
    "            if SATISFIED:\n",
    "                best_channel, max_allocated_power = candid_ch, predicted_power\n",
    "                active_sus[candid_ch][candid_su_id] = candid_new_pow  #update the candid su\n",
    "                    \n",
    "        active_sus[best_channel][best_req_id] = max_allocated_power\n",
    "        assigned_req.add(best_req_id)\n",
    "    res = []\n",
    "    for ch in range(number_channel):\n",
    "        for req_id in active_sus[ch]:\n",
    "            res.append((req_id, active_sus[ch][req_id], ch))\n",
    "    return res\n",
    "\n",
    "#     return [(i, active_sus[ch][i][0][0]) for i in active_sus[ch] for ch in range(number_channel)]\n",
    "#     req_dec_power = [db_to_dec(active_sus[ch][i][0][0]) for i in active_sus[ch] for ch in range(number_channel)]\n",
    "#     return sum(req_dec_power), max(req_dec_power) / min(req_dec_power)\n",
    "\n",
    "def random_sus(pus, requesting_sus, model, pl_info, number_channel = 1):\n",
    "    active_sus = [{}  for _ in range(number_channel)] # (request_id: power_allocated)\n",
    "    assigned_req = set()\n",
    "    for nex_req_id in range(len(requesting_sus)):\n",
    "        max_allocated_power, best_channel, best_req_id = -float('inf'), -1, -1\n",
    "        for ch in range(number_channel):\n",
    "            su_lst = []\n",
    "            for i in active_sus[ch]:\n",
    "                su_lst += [*requesting_sus[i]]\n",
    "                su_lst.append(active_sus[ch][i])\n",
    "            requesting_data = [len(pus)//3] + pus + [len(active_sus[ch]) + 1] + su_lst + requesting_sus[nex_req_id]\n",
    "            requesting_image = np.expand_dims(create_image(requesting_data, slope=slope, style=style, \n",
    "                                                           noise_floor=noise_floor, pu_shape=pu_shape, su_shape=su_shape,\n",
    "                                                           su_param=su_param, sensors_num=(sensors_num if sensors else 0), \n",
    "                                                           intensity_degradation=intensity_degradation, \n",
    "                                                           max_pu_power=0.0 if not sensors else -30,\n",
    "                                                           max_su_power=0.0), 0)\n",
    "            predicted_power = model.predict(requesting_image)\n",
    "            if predicted_power[0][0] > max_allocated_power:\n",
    "                max_allocated_power = predicted_power[0][0]\n",
    "                best_channel, best_req_id = ch, nex_req_id\n",
    "        active_sus[best_channel][best_req_id] = max_allocated_power\n",
    "        assigned_req.add(best_req_id)\n",
    "    res = []\n",
    "    for ch in range(number_channel):\n",
    "        for req_id in active_sus[ch]:\n",
    "            res.append((req_id, active_sus[ch][req_id], ch))\n",
    "    return res\n",
    "#     return [(req_id, active_sus[ch][req_id], ch) for req_id in active_sus[ch] for ch in range(number_channel)]\n",
    "#     req_dec_power = [db_to_dec(active_sus[ch][i][0][0]) for i in active_sus[ch] for ch in range(number_channel)]\n",
    "#     return sum(req_dec_power), max(req_dec_power) / min(req_dec_power)\n",
    "    \n",
    "def multiple_sus(data_reg, train_set_size, pl_info, model_path, number_channel):\n",
    "#     random_sum_power, random_max_min_ratio = [[] for _ in range(len(train_set_size))], [[] for _ in range(len(train_set_size))]\n",
    "#     greedy_sum_power, greedy_max_min_ratio = [[] for _ in range(len(train_set_size))], [[] for _ in range(len(train_set_size))]\n",
    "    random_res, greedy_res = [[] for _ in range(len(train_set_size))], [[] for _ in range(len(train_set_size))]\n",
    "    fair_res = [[] for _ in range(len(train_set_size))]\n",
    "    for ind, train_size in enumerate(train_set_size):\n",
    "        print(f\"Train size: {train_size}\")\n",
    "        model = models.load_model(f\"{model_path}/{train_size}/best_model_lambda_0.1.h5\",\n",
    "                                 custom_objects={ 'loss': custom_loss(1, 1), \n",
    "                                               'fp_mae': fp_mae,\n",
    "                                               'mae':'mae', 'mse':'mse'})\n",
    "        for i in tqdm.tqdm(range(len(data_reg))):\n",
    "            pu_num = int(data_reg[i][0])\n",
    "            pus = np.ndarray.tolist(data_reg[i][1:1 + pu_num * 3])\n",
    "            su_num = int(data_reg[i][1 + pu_num * 3])\n",
    "            if su_num < 4:\n",
    "                continue\n",
    "            sus = []\n",
    "            for su_ind in range(su_num):\n",
    "                sus.append(np.ndarray.tolist(data_reg[i][2 + pu_num * 3 + su_ind * 3:4 + pu_num * 3 + su_ind * 3]))\n",
    "#             random_tot_power, random_ratio = random_sus(pus, sus, model, pl_info, 1)\n",
    "#             print(random_tot_power)\n",
    "#             print(random_ratio)\n",
    "#             greedy_tot_power, greedy_ratio = greedy_sus(pus, sus, model, pl_info, 1)\n",
    "#             print(greedy_tot_power)\n",
    "#             print(greedy_ratio)\n",
    "#             random_sum_power[ind].append(random_tot_power)\n",
    "#             random_max_min_ratio[ind].append(random_ratio)\n",
    "#             greedy_sum_power[ind].append(greedy_tot_power)\n",
    "#             greedy_max_min_ratio[ind].append(greedy_ratio)\n",
    "            fair_res[ind].append(fair_sus(pus, sus, model, pl_info, number_channel))\n",
    "            random_res[ind].append(random_sus(pus, sus, model, pl_info, number_channel))\n",
    "            greedy_res[ind].append(greedy_sus(pus, sus, model, pl_info, number_channel))\n",
    "            var_f = open('/'.join(image_dir.split('/')[:-1]) + \"/multi_res_num_channel\"\n",
    "                     + str(number_channel) + \"_\" + intensity_degradation + '_' + str(slope) + '_' + \n",
    "                     dtime + \".dat\", \"wb\") # file for saving results\n",
    "            pickle.dump([random_res, greedy_res, fair_res],  file=var_f)\n",
    "            var_f.close()\n",
    "            \n",
    "#     return random_sum_power, random_max_min_ratio, greedy_sum_power, greedy_max_min_ratio\n",
    "    return random_res, greedy_res, fair_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rand_res, greedy_res, fair_res = multiple_sus(data_reg[:600], number_samples, None,\n",
    "                                              \"ML/data/pictures_299_299_transfer/splat/pu_circle_su_circle_60/raw_power_min_max_norm/color/log_5/pus_5_sus_3_channels/models/log_vgg16\",\n",
    "                                              4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_res, greedy_res, fair_res = pickle.load( open( \"ML/data/pictures_299_299_transfer/splat/pu_circle_su_circle_60/raw_power_min_max_norm/color/log_pu5_su5/pus_20_sus_3_channels/multi_res_num_channel1_log_5__202110_2115_18.dat\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_calc(data_arr):\n",
    "    min_data, tot_power = [], []\n",
    "    for data in data_arr:\n",
    "#         min_arr = [min(val, key=lambda x:x[1])[1] for val in data]\n",
    "        min_arr = [(sorted(val, key=lambda x:x[1])[-1][1] - sorted(val, key=lambda x:x[1])[0][1]) for val in data]\n",
    "#         print(min_arr)\n",
    "        min_data.append(sum(min_arr)/len(min_arr))\n",
    "        sum_arr = [dec_to_db(sum([db_to_dec(val[1]) for val in fair_res_sng]))for fair_res_sng in data]\n",
    "        tot_power.append(sum(sum_arr)/len(sum_arr))\n",
    "    return [round(min_, 2) for min_ in min_data], [round(tot,2) for tot in tot_power]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fair_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minn, tot = multi_calc(fair_res)\n",
    "print(minn)\n",
    "print(tot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minn, tot = multi_calc(greedy_res)\n",
    "print(minn)\n",
    "print(tot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minn, tot = multi_calc(rand_res)\n",
    "print(minn)\n",
    "print(tot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[dec_to_db(sum([db_to_dec(x) for x in [xx] * 4])) for xx in [6.29, 7.17, 6.3, 7.8, 7.95, 8.07]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "greedy_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[[10609.339866896524],\n",
    " [27745.525760029166],\n",
    " [33981.2176874612],\n",
    " [83093.78437330532],\n",
    " [230199.57169850808],\n",
    " [303634.6974403381]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(number_samples)\n",
    "print(average_diff_power)\n",
    "print(fp_mean_power)\n",
    "print(best_lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[128, 256, 512, 1024, 2048, 4096]\n",
    "[8.678, 7.595, 7.392, 6.968, 6.701, 6.521]\n",
    "[5.619, 4.375, 3.38,  4.032, 3.282, 3.4]\n",
    "[0.1, 0, 1, 10, 0.1 0.1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnns[0].summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnns[0].summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = DataBatchGenerator(dataset=data_reg[0:number_sample], batch_size=16,\n",
    "                                         start_idx=0, number_image_channels=number_image_channels,\n",
    "                                         max_x=max_x, max_y=max_y, float_memory_used=float_memory_used)\n",
    "    \n",
    "\n",
    "# val_size = math.ceil(number_sample * validation_size)\n",
    "val_generator = DataBatchGenerator(dataset=data_reg[number_sample:number_sample+val_size], \n",
    "                                   batch_size=32,\n",
    "                                   start_idx=number_sample,\n",
    "                                   number_image_channels=number_image_channels,\n",
    "                                   max_x=max_x, max_y=max_y, \n",
    "                                   float_memory_used=float_memory_used,\n",
    "                                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = DataBatchGenerator(dataset=data_reg_train[prev_sample:number_sample], batch_size=16,\n",
    "                                         start_idx=0, number_image_channels=number_image_channels,\n",
    "                                         max_x=max_x, max_y=max_y, float_memory_used=float_memory_used)\n",
    "    \n",
    "\n",
    "val_size = math.ceil(number_sample/4 * validation_size)\n",
    "val_generator = DataBatchGenerator(dataset=data_reg[number_sample//4:number_sample//4+val_size], \n",
    "                                       batch_size=mini_batch,\n",
    "                                       start_idx=number_sample//4,\n",
    "                                       number_image_channels=number_image_channels,\n",
    "                                       max_x=max_x, max_y=max_y, \n",
    "                                       float_memory_used=float_memory_used,\n",
    "                                      image_dir=\"/\".join(image_dir.split(\"/\")[:-1]) + \"/images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = models.load_model(MODEL_PATH + str(0) + '.h5', \n",
    "                              custom_objects={ 'loss': custom_loss(fp_penalty_coef, fn_penalty_coef), \n",
    "                                               'fp_mae': fp_mae,\n",
    "                                               'mae':'mae', 'mse':'mse'})\n",
    "best_model.layers[1].trainable = True\n",
    "best_model.compile(loss=custom_loss(fp_penalty_coef, fn_penalty_coef), \n",
    "                        optimizer=optimizers.Adam(1e-5), \n",
    "                        metrics=['mse', 'mae', fp_mae])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.fit(train_generator, epochs=40, verbose=2,\n",
    "                   validation_data=val_generator, \n",
    "                   shuffle=True, callbacks=[checkpointers[0]], \n",
    "                   workers=WORKERS, max_queue_size=MAX_QUEUE_SIZE, \n",
    "                   use_multiprocessing=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.save(MODEL_PATH + \"0.1_fit2.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.save(\"ML/data/pictures_299_299_transfer/log/noisy_std_1/pu_circle_su_circle_60/raw_power_min_max_norm/color/log_5/pus_1_sus_3_channels/models/700000/best_model_lambda_0_fit.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_sample + val_size+320*3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model1 = models.load_model(MODEL_PATH + str(0.1) + '.h5', \n",
    "                                       custom_objects={ 'loss': custom_loss(fp_penalty_coef, fn_penalty_coef), \n",
    "                                                       'fp_mae': fp_mae,\n",
    "                                                      'mae':'mae', 'mse':'mse'})\n",
    "test_generator = DataBatchGenerator(dataset=data_reg[number_sample + val_size:], \n",
    "                                    batch_size=32,\n",
    "                                    start_idx=number_sample + val_size, \n",
    "                                    number_image_channels=number_image_channels,\n",
    "                                    max_x=max_x, max_y=max_y, float_memory_used=float_memory_used,\n",
    "                                   image_dir=\"/\".join(image_dir.split(\"/\")[:-1]) + \"/images\")\n",
    "\n",
    "print(\"Test starts: \", number_sample + val_size, \", ends: \", data_reg.shape[0] - 1)\n",
    "time.sleep(1)\n",
    "test_res = best_model1.evaluate(test_generator, verbose=1, \n",
    "                                workers=WORKERS, max_queue_size=MAX_QUEUE_SIZE, use_multiprocessing=False)\n",
    "        \n",
    "test_mae_idx, test_fp_mae_idx = [best_model1.metrics_names.index(mtrc) \n",
    "                                for mtrc in ['mae','fp_mae']]\n",
    "test_mae, test_fp_mae = test_res[test_mae_idx], test_res[test_fp_mae_idx]\n",
    "average_diff_power.append(round(test_mae, 3))\n",
    "fp_mean_power.append(round(test_fp_mae, 3))\n",
    "print('average_error: ', average_diff_power[-1], ', fp_average_error: ', \n",
    "        fp_mean_power[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:research] *",
   "language": "python",
   "name": "conda-env-research-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
