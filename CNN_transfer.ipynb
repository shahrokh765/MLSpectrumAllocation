{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, Input,optimizers, Sequential\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import regularizers, Model\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras import applications\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from collections import namedtuple\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import datetime, time\n",
    "import os, sys\n",
    "import tqdm\n",
    "import gc\n",
    "from multiprocessing import Process\n",
    "Point = namedtuple('Point', ('x', 'y'))\n",
    "Circle = namedtuple('Circle', ('r'))\n",
    "Square = namedtuple('Square', ('side'))\n",
    "Rectangle = namedtuple('Rectangle', ('length', 'width'))\n",
    "PointWithDistance = namedtuple('PointWithDistance', ('p', 'dist'))\n",
    "float_memory_used = 'float32'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INIT\n",
    "# PART 1\n",
    "number_samples = [128, 256, 512, 1024, 4096] \n",
    "validation_size, noise_floor = 0.2, -110.0\n",
    "su_power = 0 # this is not actually su power just a number to show there is an SU in its image\n",
    "max_x, max_y, number_image_channels, su_szie = 299, 299, 3, 60  # su_size:30 for 1000, 10 for 100\n",
    "cell_size, pixel_expansion = 1000 / max_x, max_x / 100\n",
    "pu_shape, su_shape = 'circle', 'circle' # shape = {'circle', 'square', 'point'}\n",
    "style = \"raw_power_min_max_norm\"  # {\"raw_power_zscore_norm\", \"image_intensity\", \"raw_power_min_max_norm\"}\n",
    "intensity_degradation, slope, su_slope = 'log', 5, 5  # 'log', 'linear', slope 3 for 1000, 5 for 100\n",
    "max_pus_num, max_sus_num = 20, 5\n",
    "propagation_model = 'splat' # 'splat', 'log', 'testbed'\n",
    "noise, std = False, 1 # False for splat\n",
    "if su_shape == 'circle':\n",
    "    su_param = Circle(su_szie)\n",
    "elif su_shape == 'square':\n",
    "    su_param = Square(su_szie)\n",
    "else:\n",
    "    su_param = None\n",
    "    \n",
    "sensors = False\n",
    "if sensors:\n",
    "    sensors_num = 225\n",
    "    sensors_file_path = f\"data/sensors/square{100}/{sensors_num}/sensors.txt\"\n",
    "# num_pus = (data_reg.shape[1] - 3)//3\n",
    "\n",
    "# PART 2\n",
    "number_of_proccessors = 12\n",
    "memory_size_allowed = 4 # in Gigabyte\n",
    "float_size = 0\n",
    "if float_memory_used == \"float16\":\n",
    "    float_size = 16\n",
    "elif float_memory_used == \"float\" or \"float32\":\n",
    "    float_size = 32\n",
    "elif float_memory_used == \"float8\":\n",
    "    float_size = 8\n",
    "\n",
    "\n",
    "batch_size = int(memory_size_allowed / (max_x * max_y * number_image_channels * float_size/(8 * 1024 ** 3)))\n",
    "\n",
    "\n",
    "dtime = datetime.datetime.now().strftime('_%Y%m_%d%H_%M')\n",
    "color = \"color\" if number_image_channels > 1 else \"gray\"\n",
    "image_dir = 'ML/data/pictures_' + str(max_x) + '_' + str(max_y) + '_transfer/' + propagation_model + (\n",
    "    \"/noisy_std_\" + str(std) if noise else \"\") + '/pu_' + pu_shape + '_su_' + su_shape + '_' + (\n",
    "    \"\" if su_shape == 'point' else str(su_szie)) + \"/\" + style + \"/\" + color +'/' + (\n",
    "    \"\" if pu_shape == 'point' and su_shape == 'point' else (f\"{intensity_degradation}_pu{slope}_su{su_slope}\")) + (\n",
    "    \"/\" + str(sensors_num) + \"sensors\" if sensors else f\"/{max_pus_num}pus\") + \\\n",
    "        f\"_{max_sus_num}sus_{number_image_channels}channels\" + \"/images\"\n",
    "\n",
    "if not os.path.exists(image_dir):\n",
    "        os.makedirs(image_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dir "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_dir = \"/home/shahrokh/projects/MLSpectrumAllocation/ML/data/pictures_299_299_transfer/log/\" +\\\n",
    "#             \"noisy_std_1/pu_circle_su_circle_60/raw_power_min_max_norm/color/log_pu5_su6/\" +\\\n",
    "#             \"variable_sensors_10_20_pus_5_sus_8_channels/images\"\n",
    "sensors_location = {}\n",
    "for sensor_num in [49, 100, 225, 400, 625]:\n",
    "    sensors_location[sensor_num] = []\n",
    "    with open(f\"data/sensors/square{100}/{sensor_num}/sensors.txt\", 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            line = line.split(',')\n",
    "            sensors_location[sensor_num].append(Point(int(float(line[0])), int(float(line[1]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.hist(data_reg[0:100:1, :100], bins='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD DATA\n",
    "MAX_SU_TOTAL = True\n",
    "num_columns = (sensors_num + 1 if sensors else max_pus_num * 3 + 1) + max_sus_num * 3 + 2\n",
    "# num_columns = max_pus_num * 3 + 1 + max_sus_num * 3 + 2\n",
    "cols = [i for i in range(num_columns)]\n",
    "dataset_name = \"dynamic_pus_using_pus_50000_min10_max20PUs_5SUs_square100grid_splat_2022_06_09_13_24.txt\"\n",
    "max_dataset_name = \"dynamic_pus_max_power_50000_min10_max20PUs_5SUs_square100grid_splat_2022_06_09_13_24.txt\"\n",
    "if MAX_SU_TOTAL:\n",
    "    max_su_total_dataset_name = \"dynamic_pus_maximum_total_sus50000_min10_max20PUs_5SUs_square100grid_splat_2022_06_09_13_24.txt\"\n",
    "with open('/'.join(image_dir.split('/')[:-1]) + '/datasets' + dtime + '.txt', 'w') as set_file:\n",
    "    set_file.write(dataset_name + \"\\n\")\n",
    "    set_file.write(max_dataset_name)\n",
    "    if MAX_SU_TOTAL:\n",
    "        set_file.write(max_su_total_dataset_name)\n",
    "\n",
    "dataframe = pd.read_csv('data/' \n",
    "                        + dataset_name, delimiter=',', header=None, names=cols)\n",
    "dataframe_max = pd.read_csv('data/' \n",
    "                            + max_dataset_name, delimiter=',', header=None)\n",
    "if MAX_SU_TOTAL:\n",
    "    dataframe_max_su_total = pd.read_csv('data/' + max_su_total_dataset_name, delimiter=\",\", header=None,\n",
    "                                        names=[i for i in range(max_sus_num * 3 + 1)])\n",
    "\n",
    "dataframe.reset_index(drop=True, inplace=True)\n",
    "dataframe_max.reset_index(drop=True, inplace=True)\n",
    "if MAX_SU_TOTAL:\n",
    "    dataframe_max_su_total.reset_index(drop=True, inplace=True)\n",
    "dataframe_max[dataframe_max.shape[1] - 1] = dataframe_max[dataframe_max.shape[1] - 1].astype(float)\n",
    "\n",
    "dataframe_tot = pd.concat([dataframe, dataframe_max.iloc[:, dataframe_max.columns.values[-1:]]], axis=1,\n",
    "                        ignore_index=True)\n",
    "\n",
    "idx = dataframe_tot[dataframe_tot[dataframe_tot.columns[-1]] == -float('inf')].index\n",
    "dataframe_tot.drop(idx, inplace=True)\n",
    "if MAX_SU_TOTAL:\n",
    "    dataframe_max_su_total.drop(idx, inplace=True)\n",
    "\n",
    "data_reg = dataframe_tot.values\n",
    "data_reg[data_reg < noise_floor] = noise_floor\n",
    "if MAX_SU_TOTAL:\n",
    "    data_max_su_tot = dataframe_max_su_total.values\n",
    "\n",
    "if False and sensors:\n",
    "    sensors_location = []\n",
    "    with open(sensors_file_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            line = line.split(',')\n",
    "            sensors_location.append(Point(int(float(line[0])), int(float(line[1]))))\n",
    "del dataframe, dataframe_tot, dataframe_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([225.   , -87.793, -84.376, -83.528, -81.822, -75.905, -66.582,\n",
       "       -74.179, -73.021, -69.737, -57.872, -67.844, -70.076, -70.457,\n",
       "       -73.44 , -77.393, -85.975, -82.953, -74.184, -79.187, -77.676,\n",
       "       -76.026, -73.896, -73.696, -70.641, -65.995, -68.487, -69.938,\n",
       "       -69.732, -69.752, -72.175, -86.43 , -83.924, -78.311, -77.882,\n",
       "       -75.753, -72.736, -69.31 , -72.285, -71.614, -70.488, -65.709,\n",
       "       -64.755, -64.035, -65.505, -70.898, -87.179, -85.966, -80.402,\n",
       "       -80.979, -77.596, -72.971, -68.651, -72.127, -71.146, -68.837,\n",
       "       -67.846, -64.873, -45.862, -66.383, -69.093, -87.078, -85.946,\n",
       "       -84.992, -80.244, -79.394, -76.972, -76.694, -77.052, -70.44 ,\n",
       "       -71.027, -68.209, -63.084, -62.659, -67.373, -67.48 , -87.243,\n",
       "       -87.241, -86.551, -80.209, -78.736, -77.566, -74.604, -74.698,\n",
       "       -71.771, -70.846, -67.554, -60.771, -60.797, -67.122, -67.29 ,\n",
       "       -88.056, -84.096, -86.428, -85.156, -83.467, -81.174, -80.467,\n",
       "       -78.297, -71.936, -68.293, -70.53 , -66.053, -65.932, -68.541,\n",
       "       -70.025, -84.677, -78.195, -81.945, -82.068, -82.39 , -85.014,\n",
       "       -81.664, -78.841, -77.831, -67.678, -70.942, -69.837, -68.264,\n",
       "       -69.62 , -68.98 , -78.345, -78.29 , -77.721, -78.132, -83.258,\n",
       "       -82.132, -81.343, -82.106, -81.984, -75.755, -75.314, -76.444,\n",
       "       -76.062, -79.373, -79.554, -71.326, -71.278, -71.376, -72.143,\n",
       "       -79.739, -79.912, -81.152, -81.207, -79.252, -79.425, -79.278,\n",
       "       -80.805, -80.783, -83.097, -83.002, -68.73 , -67.88 , -71.206,\n",
       "       -73.005, -73.188, -79.479, -81.783, -79.901, -79.246, -81.386,\n",
       "       -82.424, -82.067, -83.273, -83.307, -83.621, -62.642, -59.683,\n",
       "       -68.902, -73.457, -73.56 , -79.847, -78.544, -75.762, -72.591,\n",
       "       -79.643, -82.243, -82.596, -83.624, -83.468, -86.624, -65.589,\n",
       "       -64.938, -69.609, -73.977, -73.435, -79.186, -77.75 , -77.441,\n",
       "       -75.175, -80.391, -82.766, -84.377, -85.999, -86.419, -89.037,\n",
       "       -71.871, -71.128, -68.329, -68.942, -79.485, -78.764, -72.52 ,\n",
       "       -80.078, -82.047, -82.88 , -83.454, -84.624, -87.134, -89.314,\n",
       "       -90.114, -71.249, -71.182, -71.696, -78.546, -78.982, -83.174,\n",
       "       -81.237, -81.857, -82.319, -82.554, -83.109, -87.486, -87.946,\n",
       "       -90.222, -90.552,   1.   ,  97.   ,  82.   , -33.251,   1.   ,\n",
       "        19.261])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_reg[56,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_reg = data_reg[:30000][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_reg = data_reg[:40000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_reg_train = data_reg[12000][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32162, 79)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_reg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5.   , 30.   , 80.   , 12.773, 68.   , 91.   , 18.252, 66.   ,\n",
       "       56.   ,  9.604, 30.   , 82.   , 10.786, 28.   ,  8.   , 11.592])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_max_su_tot[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 19.   ,  38.   ,  65.   , -19.658,  69.   ,  49.   ,  -7.351,\n",
       "        88.   ,  42.   ,  -6.322,  28.   ,  17.   ,  -5.694,  61.   ,\n",
       "        45.   , -17.43 ,   2.   ,  55.   , -24.392,  57.   ,  95.   ,\n",
       "       -20.778,  52.   ,  79.   , -21.744,  52.   ,   5.   , -13.93 ,\n",
       "         6.   ,  39.   ,  -6.269,  87.   ,   8.   , -21.177,  26.   ,\n",
       "        80.   , -20.406,  19.   ,  63.   , -15.858,  30.   ,  19.   ,\n",
       "       -13.793,  27.   ,  50.   , -17.062,  95.   ,  95.   , -29.368,\n",
       "        39.   ,  71.   ,  -3.309,  52.   ,  59.   , -11.98 ,  38.   ,\n",
       "        16.   , -22.553,   1.   ,  95.   ,  30.   ,  -7.839,   1.   ,\n",
       "           nan,     nan,     nan,  11.077])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_reg[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(data_reg[:, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidian_distance(p1: Point, p2: Point):\n",
    "    return ((p1.x - p2.x) ** 2 + (p1.y - p2.y) ** 2) ** 0.5 * cell_size\n",
    "\n",
    "def calculate_mu_sigma(data, num_pus):\n",
    "    sum_non_noise = 0\n",
    "    for pu_n in range(num_pus): # calculate mu\n",
    "        sum_non_noise += data[pu_n*3+2]\n",
    "    mu = ((max_x * max_y - num_pus) * noise_floor + sum_non_noise)/(max_x * max_y)\n",
    "    sum_square = 0\n",
    "    for pu_n in range(num_pus): # calculate sigma\n",
    "        sum_square += (data[pu_n*3+2]-mu)**2\n",
    "    sum_square += (max_x * max_y - num_pus) * (noise_floor - mu)**2\n",
    "    sigma = math.sqrt(sum_square/(max_x * max_y))\n",
    "    return mu, sigma\n",
    "\n",
    "def get_pu_param(pu_shape: str, intensity_degradation: str, pu_p: float, noise_floor: float, slope: float):\n",
    "    pu_param = None\n",
    "    if pu_shape == 'circle':\n",
    "        if intensity_degradation == \"linear\":\n",
    "            pu_param = Circle(int((pu_p - noise_floor) / slope)) # linear\n",
    "        elif intensity_degradation == \"log\":\n",
    "            pu_param = Circle(int(10 ** ((pu_p - noise_floor) / (10 *slope)))) # log_based\n",
    "    elif pu_shape == 'square':\n",
    "        if intensity_degradation == \"linear\":\n",
    "            pu_param = Square(int(2 ** 0.5 * (pu_p - noise_floor) / slope)) # linear\n",
    "        elif intensity_degradation == \"log\":\n",
    "            pu_param = Square(int(2 ** 0.5 * 10 ** ((pu_p - noise_floor) / (10 *slope)))) # log_based\n",
    "    elif pu_shape == 'point':\n",
    "        pu_param = None\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported PU shape(create_image)! \", pu_shape)\n",
    "    return pu_param\n",
    "\n",
    "def create_image(data, slope, sensors_num, style=\"raw_power_z_score\", noise_floor=-90, pu_shape= 'circle',\n",
    "                 pu_param=None, su_shape='circle', su_param=None, intensity_degradation=\"log\", \n",
    "                 max_pu_power: float=0, max_su_power: float=0):  \n",
    "    # style = {\"raw_power_zscore_norm\", \"image_intensity\", \"raw_power_min_max_norm\"}\n",
    "    # intensity_degradation= {\"log\", \"linear\"}\n",
    "    # if param is None, it's automatically calculated. Highest brightness(or power value) (255 or 1.) would\n",
    "    # assigned to the center(PU location) and radius(side) would be calculated based on its power, slope, and noise floor.\n",
    "    # If it is given, intensity(power) of pixel beside center would be calculated in the same fashin with an exception that \n",
    "    # intensity below zero(noise_floor) would be replaced by zero(noise_floor)\n",
    "    if style == \"raw_power_min_max_norm\":\n",
    "        # In this way, PUs' location are replaced with their power(dBm) and the power would fade with \n",
    "        # slope till gets noise_floor(in circle shape)\n",
    "        \n",
    "        # creating pu matrix\n",
    "        image = np.zeros((max_x, max_y, number_image_channels), dtype=float_memory_used)\n",
    "        if not sensors:\n",
    "            pus_num = int(data[0])\n",
    "#             print(pus_num)\n",
    "            for pu_i in range(pus_num):\n",
    "                pu_x = max(0, min(max_x-1, int(data[pu_i * 3 + 1] * pixel_expansion))) \n",
    "                pu_y = max(0, min(max_x-1, int(data[pu_i * 3 + 2] * pixel_expansion)))\n",
    "                pu_p = data[pu_i * 3 + 3]\n",
    "                pu_channel = int(abs(pu_p)//5) if number_image_channels > 3 else 0\n",
    "#                 print(pu_x, pu_y, pu_p)\n",
    "                if pu_param is None:\n",
    "                    pu_param_p = get_pu_param(pu_shape, intensity_degradation, pu_p, noise_floor, slope)\n",
    "                else:\n",
    "                    pu_param_p = pu_param\n",
    "                points = points_inside_shape(center=Point(pu_x, pu_y),\n",
    "                                             shape=pu_shape, param=pu_param_p)\n",
    "                for point in points:\n",
    "                    if 0 <= point.p.x < max_x and 0 <= point.p.y < max_y: # TODO should pass image size\n",
    "                        if intensity_degradation == \"linear\":\n",
    "                            image[int(abs(pu_p))//10][point.p.x][point.p.y] += (pu_p - slope * point.dist - noise_floor)/(\n",
    "                                max_pu_power - noise_floor)\n",
    "                        elif intensity_degradation == \"log\":\n",
    "                            if point.dist < 1:\n",
    "                                image[point.p.x][point.p.y][pu_channel] += (pu_p - noise_floor) / (max_pu_power - noise_floor)\n",
    "                            else:\n",
    "                                image[point.p.x][point.p.y][pu_channel] += (pu_p - slope * 10*math.log10(point.dist) - noise_floor)/(\n",
    "                                    max_pu_power - noise_floor)\n",
    "        else:\n",
    "            ss_channels_num = number_image_channels - 2\n",
    "            ss_param, ss_shape = pu_param, pu_shape\n",
    "            sensors_num = int(data[0])\n",
    "            sensors_num_at_each_row = int(sensors_num ** 0.5)\n",
    "            for ss_i in range(sensors_num):\n",
    "                ss_x = max(0, min(max_x-1, int(sensors_location[sensors_num][ss_i].x * pixel_expansion)))\n",
    "                ss_y =  max(0, min(max_x-1, int(sensors_location[sensors_num][ss_i].y * pixel_expansion)))\n",
    "                ss_p = max(noise_floor, data[ss_i+1])\n",
    "#                 ss_x, ss_y, ss_p = max(0, min(max_x-1, int(sensors_location[ss_i].x * pixel_expansion))), max(\n",
    "#                     0, min(max_x-1, int(sensors_location[ss_i].y * pixel_expansion))), max(noise_floor, data[ss_i])\n",
    "#                 ss_channel = 0 \n",
    "# #                 if -62.5 <= ss_p < -50.0:\n",
    "# #                     ss_channel = 1\n",
    "# #                 elif -70.0 <= ss_p < -62.6:\n",
    "# #                     ss_channel = 2\n",
    "# #                 elif -77.5 <= ss_p < -70.0:\n",
    "# #                     ss_channel = 3\n",
    "# #                 elif -85.0 <= ss_p < -77.5:\n",
    "# #                     ss_channel = 4\n",
    "# # #                 elif -70.0 <= ss_p < -65.0:\n",
    "# # #                     ss_channel = 5\n",
    "# #                 elif ss_p < -85.0:\n",
    "# #                     ss_channel = 5\n",
    "#                 if -70 <= ss_p < -60.0:\n",
    "#                     ss_channel = 1\n",
    "#                 elif -80.0 <= ss_p < -70:\n",
    "#                     ss_channel = 2\n",
    "#                 elif -90.0 <= ss_p < -80.0:\n",
    "#                     ss_channel = 3\n",
    "#                 elif -100.0 <= ss_p < -90.0:\n",
    "#                     ss_channel = 4\n",
    "#                 elif ss_p < -100.0:\n",
    "#                     ss_channel = 5\n",
    "#                 ss_channel = 0\n",
    "                ss_row, ss_cols = ss_i // sensors_num_at_each_row, ss_i % sensors_num_at_each_row\n",
    "                ss_channel = ss_cols % ss_channels_num\n",
    "                if ss_row % 2:\n",
    "                    ss_channel = (ss_channel + ss_channels_num // 2) % ss_channels_num\n",
    "                if ss_param is None:\n",
    "                    ss_param_p = get_pu_param(ss_shape, intensity_degradation, ss_p, noise_floor, slope)\n",
    "                else:\n",
    "                    ss_param_p = ss_param\n",
    "                points = points_inside_shape(center=Point(ss_x, ss_y), shape=ss_shape, param=ss_param_p)\n",
    "                for point in points:\n",
    "                    if 0 <= point.p.x < max_x and 0 <= point.p.y < max_y: # TODO should pass image size\n",
    "                        if intensity_degradation == \"linear\":\n",
    "                            image[point.p.x][point.p.y][ss_channel] += (ss_p - slope * point.dist - noise_floor)/(\n",
    "                                max_pu_power - noise_floor)\n",
    "                        elif intensity_degradation == \"log\":\n",
    "                            if point.dist < 1:\n",
    "                                image[point.p.x][point.p.y][ss_channel] += (ss_p - noise_floor) / (max_pu_power - noise_floor)\n",
    "                            else:\n",
    "                                image[point.p.x][point.p.y][ss_channel] += (ss_p - slope * 10*math.log10(point.dist) - noise_floor)/(\n",
    "                                    max_pu_power - noise_floor)\n",
    "        del points\n",
    "        # creating su matrix\n",
    "        su_num_idx = sensors_num + 1 if sensors else (pus_num * 3 + 1)\n",
    "        su_num = int(data[su_num_idx])\n",
    "#         print(su_num)\n",
    "#         su_num = (len(data) - pus_num * (3 if not sensors else 1)) // 2\n",
    "#         if not (len(data) - pus_num * (3 if not sensors else 1)) % 2:\n",
    "#             raise ValueError(\"Data provided is not correct; can't get SUs' information(create_image)\")\n",
    "        if su_param is None:\n",
    "            # if su_param is unavailable, a circle(square) with radius(side) 1 is created\n",
    "            if su_shape == 'circle':\n",
    "                su_param = Circle(1)\n",
    "            elif su_shape == 'square':\n",
    "                su_param = Square(1)\n",
    "            elif su_shape == 'point':\n",
    "                su_param = None\n",
    "            else:\n",
    "                raise ValueError(\"Unsupported SU shape(create_image)! \", su_shape)\n",
    "        \n",
    "        for su_i in range(su_num - 1):\n",
    "            su_x = max(0, min(max_x-1, int(data[su_num_idx + su_i * 3 + 1] * pixel_expansion)))\n",
    "            su_y = max(0, min(max_x-1, int(data[su_num_idx + su_i * 3 + 2] * pixel_expansion)))\n",
    "            su_p = data[su_num_idx + su_i * 3 + 3]\n",
    "#             su_p = su_intensity\n",
    "            su_param_p = get_pu_param(su_shape, intensity_degradation, su_p, noise_floor, su_slope)\n",
    "            points = points_inside_shape(center=Point(su_x, su_y),\n",
    "                                         param=su_param_p, shape=su_shape)\n",
    "            su_channel = 0 if number_image_channels == 1 else -2\n",
    "            for point in points:\n",
    "                if 0 <= point.p.x < max_x and 0 <= point.p.y < max_y: # TODO should pass image size\n",
    "                    if intensity_degradation == \"linear\":\n",
    "                            su_val = (su_p - su_slope * point.dist - noise_floor)/(max_su_power - noise_floor)\n",
    "                    elif intensity_degradation == \"log\":\n",
    "                        if point.dist < 1:\n",
    "                            su_val = (su_p - noise_floor) / (max_su_power - noise_floor)\n",
    "                        else:\n",
    "                            su_val = (su_p - su_slope * 10*math.log10(point.dist) - noise_floor)/(\n",
    "                                max_su_power - noise_floor)\n",
    "                    image[point.p.x][point.p.y][su_channel] += su_val\n",
    "            del points\n",
    "        # the last and  target SU\n",
    "        su_intensity = 1.\n",
    "        su_x = max(0, min(max_x-1, int(data[su_num_idx + (su_num - 1) * 3 + 1] * pixel_expansion)))\n",
    "        su_y = max(0, min(max_x-1, int(data[su_num_idx + (su_num - 1) * 3 + 2] * pixel_expansion)))\n",
    "#         print(su_x, su_y)\n",
    "#         print(su_shape)\n",
    "#         print(su_param)\n",
    "        points = points_inside_shape(center=Point(su_x, su_y),\n",
    "                                     param=su_param, shape=su_shape)\n",
    "        su_channel = -1\n",
    "        for point in points:\n",
    "            if 0 <= point.p.x < max_x and 0 <= point.p.y < max_y: # TODO should pass image size\n",
    "                image[point.p.x][point.p.y][su_channel] += su_intensity\n",
    "        del points\n",
    "        return image\n",
    "        \n",
    "#         pu_image = [[(noise_floor - mu)/sigma] * max_y for _ in range(max_x)]\n",
    "    elif style == \"image_intensity\":\n",
    "        # creating PU image\n",
    "        image = np.zeros((1,number_image_channels,max_x, max_y), dtype=float_memory_used)\n",
    "        pus_num = int(data[0])\n",
    "        for pu_i in range(pus_num):\n",
    "            pu_x = max(0, min(max_x-1, int(data[pu_i * 3 + 1] * pixel_expansion))) \n",
    "            pu_y = max(0, min(max_x-1, int(data[pu_i * 3 + 2] * pixel_expansion)))\n",
    "            pu_p = data[pu_i * 3 + 3]\n",
    "            \n",
    "            if pu_param is None:\n",
    "                pu_param_p = get_pu_param(pu_shape, intensity_degradation, pu_p, noise_floor, slope)\n",
    "            else:\n",
    "                pu_param_p = pu_param\n",
    "            \n",
    "            points = points_inside_shape(center=Point(pu_x, pu_y), shape=pu_shape, param=pu_param_p)\n",
    "            for point in points:\n",
    "                if 0 <= point.p.x < max_x and 0 <= point.p.y < max_y: # TODO should pass image size\n",
    "                    if intensity_degradation == \"linear\":\n",
    "                        image[0][0][point.p.x][point.p.y] += (pu_p - slope * point.dist - noise_floor)/(\n",
    "                            max_pu_power - noise_floor)\n",
    "                    elif intensity_degradation == \"log\":\n",
    "                        if point.dist < 1:\n",
    "#                             image[0][0][point.p.x][point.p.y] += (pu_p - noise_floor) / (max_pu_power - noise_floor)\n",
    "                            image[0][0][point.p.x][point.p.y] += 0.1\n",
    "                        else:\n",
    "#                             image[0][0][point.p.x][point.p.y] += (pu_p - slope * 10*math.log10(point.dist) - noise_floor)/(\n",
    "#                                 max_pu_power - noise_floor)\n",
    "                            image[0][0][point.p.x][point.p.y] += 0.1\n",
    "                        \n",
    "        # creating SU image\n",
    "        del points\n",
    "        # creating su matrix\n",
    "        su_num_idx = sensors_num if sensors else (pus_num * 3 + 1)\n",
    "        su_num = int(data[su_num_idx])\n",
    "#         print(su_num)\n",
    "#         su_num = (len(data) - pus_num * (3 if not sensors else 1)) // 2\n",
    "#         if not (len(data) - pus_num * (3 if not sensors else 1)) % 2:\n",
    "#             raise ValueError(\"Data provided is not correct; can't get SUs' information(create_image)\")\n",
    "        if su_param is None:\n",
    "            # if su_param is unavailable, a circle(square) with radius(side) 1 is created\n",
    "            if su_shape == 'circle':\n",
    "                su_param = Circle(1)\n",
    "            elif su_shape == 'square':\n",
    "                su_param = Square(1)\n",
    "            elif su_shape == 'point':\n",
    "                su_param = None\n",
    "            else:\n",
    "                raise ValueError(\"Unsupported SU shape(create_image)! \", su_shape)\n",
    "        \n",
    "        for su_i in range(su_num - 1):\n",
    "            su_x = max(0, min(max_x-1, int(data[su_num_idx + su_i * 3 + 1])))\n",
    "            su_y = max(0, min(max_x-1, int(data[su_num_idx + su_i * 3 + 2])))\n",
    "            su_p = data[su_num_idx + su_i * 3 + 3]\n",
    "            \n",
    "#             su_p = su_intensity\n",
    "            points = points_inside_shape(center=Point(su_x, su_y), param=su_param, shape=su_shape)\n",
    "            su_channel = 0 if number_image_channels == 1 else -1\n",
    "            for point in points:\n",
    "                if 0 <= point.p.x < max_x and 0 <= point.p.y < max_y: # TODO should pass image size\n",
    "                    if intensity_degradation == \"linear\":\n",
    "                            su_val = (su_p - slope * point.dist - noise_floor)/(max_pu_power - noise_floor)\n",
    "                    elif intensity_degradation == \"log\":\n",
    "                        if point.dist < 1:\n",
    "                            su_val = (su_p - noise_floor) / (max_pu_power - noise_floor)\n",
    "                        else:\n",
    "                            su_val = (su_p - slope * 10*math.log10(point.dist) - noise_floor)/(\n",
    "                                max_pu_power - noise_floor)\n",
    "                    image[0][su_channel][point.p.x][point.p.y] += su_val\n",
    "            del points\n",
    "        # the last and  target SU\n",
    "        su_intensity = 1.\n",
    "        su_x = max(0, min(max_x-1, int(data[su_num_idx + (su_num - 1) * 3 + 1])))\n",
    "        su_y = max(0, min(max_x-1, int(data[su_num_idx + (su_num - 1) * 3 + 2])))\n",
    "#         print(su_x, su_y)\n",
    "        points = points_inside_shape(center=Point(su_x, su_y), param=su_param, shape=su_shape)\n",
    "        su_channel = 0 if number_image_channels == 1 else -1\n",
    "        for point in points:\n",
    "            if 0 <= point.p.x < max_x and 0 <= point.p.y < max_y: # TODO should pass image size\n",
    "                image[0][su_channel][point.p.x][point.p.y] += su_intensity\n",
    "        del points\n",
    "        return image       \n",
    "            \n",
    "    else:\n",
    "        raise ValueError(\"Unsupported style(create_image)! \", style)\n",
    "        \n",
    "def points_inside_shape(center: Point, shape: str, param)-> list:\n",
    "    # This function returns points+distance around center with defined shape\n",
    "    if shape == 'circle':\n",
    "        # First creates points inside a square(around orgigin) with 2*r side and then remove those with distance > r.\n",
    "        # Shift all remaining around center. O(4r^2)\n",
    "        r, origin = param.r, Point(0, 0)\n",
    "        square_points = set((Point(x, y) for x in range(max(-int(r/cell_size), -max_x), \n",
    "                             min(int(r/cell_size), max_x) + 1) \n",
    "                             for y in range(max(-int(r/cell_size), -max_y), min(int(r/cell_size), max_y) + 1)))\n",
    "        points = []\n",
    "        while square_points:\n",
    "            p = square_points.pop()\n",
    "            dist = euclidian_distance(p, origin)\n",
    "            if dist <= r:\n",
    "                points.append(PointWithDistance(Point(p.x + center.x, p.y + center.y), dist))\n",
    "                if p.x != 0:\n",
    "                    points.append(PointWithDistance(Point(-p.x + center.x, p.y + center.y), dist))\n",
    "                    square_points.remove(Point(-p.x, p.y))\n",
    "                if p.y != 0:\n",
    "                    points.append(PointWithDistance(Point(p.x + center.x, -p.y + center.y), dist))\n",
    "                    square_points.remove(Point(p.x, -p.y))\n",
    "                if p.x != 0 and p.y != 0:\n",
    "                    points.append(PointWithDistance(Point(-p.x + center.x, -p.y + center.y), dist))\n",
    "                    square_points.remove(Point(-p.x, -p.y))\n",
    "        del square_points\n",
    "        return points\n",
    "    elif shape == 'square':\n",
    "        half_side = param.side // 2\n",
    "        return [PointWithDistance(Point(x, y), euclidian_distance(Point(x, y), center)) for x in range(-half_side + center.x,\n",
    "                                                                                               half_side + center.x+1) \n",
    "                         for y in range(-half_side + center.y, half_side + center.y + 1)]\n",
    "    elif shape == 'point':\n",
    "        return [PointWithDistance(center, 0)]\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported shape(points_inside_shape)! \", shape)\n",
    "        \n",
    "def read_image(image_num, image_dir=image_dir):\n",
    "    if False and style == \"image_intensity\":\n",
    "        image = plt.imread(image_dir + '/image' + str(image_num)+'.png')\n",
    "        image = np.swapaxes(image, 0, 2)\n",
    "        image = np.array(image[:number_image_channels], dtype=float_memory_used).reshape(1, number_image_channels, max_x, max_y)\n",
    "    elif  style == \"raw_power_min_max_norm\" or style == \"raw_power_zscore_norm\" or style == \"image_intensity\":\n",
    "        suffix = 'npz'  # npy, npz\n",
    "#         image = np.load(f\"{image_dir}/images{image_num//100000}/image{image_num}.{suffix}\") \n",
    "        image = np.load(f\"{image_dir}/image{image_num}.{suffix}\") \n",
    "        if type(image) == np.lib.npyio.NpzFile:\n",
    "            image = image['a']\n",
    "    \n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 2048\n",
    "data_reg_train = np.repeat(data_reg[:train_size], 4, axis=0)\n",
    "image_state = [\"\", \"rot\", \"lr\", \"ud\"] * train_size\n",
    "p = np.random.permutation(train_size*4)\n",
    "data_reg_train = data_reg_train[p]\n",
    "new_image_state = []\n",
    "for idx in range(train_size*4):\n",
    "    new_image_state.append(image_state[p[idx]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def creating_image(start, end):\n",
    "    # for image_num in range(115, data_reg.shape[0]):\n",
    "    # for image_num in range(1625, 5000):\n",
    "    for image_num in tqdm.tqdm(range(start, end+1)):  #4463, data_reg.shape[0]\n",
    "        image = create_image(data=data_reg_train[image_num], slope=slope, style=style, \n",
    "                             noise_floor=noise_floor,\n",
    "                             pu_shape=pu_shape, su_shape=su_shape, su_param=su_param, \n",
    "                             sensors_num=(sensors_num if sensors else 0), \n",
    "                             intensity_degradation=intensity_degradation, \n",
    "                             max_pu_power=0.0 if not sensors else -60)\n",
    "        if new_image_state[image_num] == \"rot\":\n",
    "            image = np.rot90(image, 2)\n",
    "        elif new_image_state[image_num] == \"lr\":\n",
    "            image = np.fliplr(image)\n",
    "        elif new_image_state[image_num] == \"ud\":\n",
    "            image = np.flipud(image)\n",
    "        np.savez_compressed(image_dir + '/aug/image' + str(image_num), a=np.expand_dims(image,0 ))\n",
    "        \n",
    "        del image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_reg_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2260025/1061041538.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mjobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mproc_sizes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdata_reg_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mnumber_of_proccessors\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnumber_of_proccessors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mproc_sizes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mdata_reg_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0mnumber_of_proccessors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mproc_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproc_sizes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproc_sizes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumber_of_proccessors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data_reg_train' is not defined"
     ]
    }
   ],
   "source": [
    "jobs = []\n",
    "proc_sizes = [data_reg_train.shape[0]//number_of_proccessors] * (number_of_proccessors)\n",
    "proc_sizes[-1] += data_reg_train.shape[0]%number_of_proccessors\n",
    "proc_idx = [(sum(proc_sizes[:i]), sum(proc_sizes[:i+1])-1) for i in range(number_of_proccessors)]\n",
    "\n",
    "for i in range(number_of_proccessors):\n",
    "    p = Process(target=creating_image, args=(proc_idx[i][0], proc_idx[i][1]))\n",
    "    jobs.append(p)\n",
    "    p.start()\n",
    "for i in range(number_of_proccessors):\n",
    "    jobs[i].join()\n",
    "\n",
    "for i in range(number_of_proccessors):\n",
    "    jobs[i].terminate()\n",
    "    jobs[i].close()\n",
    "del jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving images once to save time\n",
    "# run this cell just for creating images\n",
    "def creating_image(start, end):\n",
    "    # for image_num in range(115, data_reg.shape[0]):\n",
    "    # for image_num in range(1625, 5000):\n",
    "    for image_num in tqdm.tqdm(range(start, end+1)):  #4463, data_reg.shape[0]\n",
    "        image = create_image(data=data_reg[image_num], slope=slope, style=style, \n",
    "                             noise_floor=noise_floor,\n",
    "                             pu_shape=pu_shape, su_shape=su_shape, su_param=su_param, \n",
    "                             sensors_num=(sensors_num if sensors else 0), \n",
    "                             intensity_degradation=intensity_degradation, \n",
    "                             max_pu_power=0.0 if not sensors else -60,\n",
    "                             max_su_power=40.0)\n",
    "        if False and style == \"image_intensity\":\n",
    "            if number_image_channels != 3:\n",
    "                image = np.append(np.array(image[0]), np.zeros((3-number_image_channels,max_x, max_y), \n",
    "                                                               dtype=float_memory_used), axis=0)\n",
    "            image_save = np.swapaxes(image, 0, 2)\n",
    "            plt.imsave(image_dir + '/image' + str(image_num)+'.png', image_save)\n",
    "        elif style == \"raw_power_min_max_norm\" or style == \"raw_power_zscore_norm\" or style == \"image_intensity\":\n",
    "    #         np.save(image_dir + '/image' + str(image_num), image)\n",
    "#             np.savez_compressed(f\"{image_dir}{(600000 + image_num)//100000}/image{600000 + image_num}\",\n",
    "#                                 a=np.expand_dims(image,0))\n",
    "            np.savez_compressed(f\"{image_dir}/image{image_num}\",\n",
    "                                a=np.expand_dims(image,0))\n",
    "        del image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|██████████████████████████████████████▎| 3711/3772 [29:41<00:27,  2.21it/s]\n",
      "100%|███████████████████████████████████████| 3774/3774 [29:45<00:00,  2.11it/s]\n",
      "100%|███████████████████████████████████████| 3772/3772 [29:46<00:00,  2.11it/s]\n",
      "100%|███████████████████████████████████████| 3772/3772 [29:53<00:00,  2.10it/s]\n",
      "100%|███████████████████████████████████████| 3772/3772 [29:55<00:00,  2.10it/s]\n",
      " 99%|██████████████████████████████████████▊| 3749/3772 [29:57<00:07,  3.10it/s]\n",
      "100%|███████████████████████████████████████| 3772/3772 [29:57<00:00,  2.10it/s]\n",
      "100%|███████████████████████████████████████| 3772/3772 [29:58<00:00,  2.10it/s]\n",
      "100%|███████████████████████████████████████| 3772/3772 [29:58<00:00,  2.10it/s]\n",
      "100%|███████████████████████████████████████| 3772/3772 [29:59<00:00,  2.10it/s]\n",
      "100%|███████████████████████████████████████| 3772/3772 [30:03<00:00,  2.09it/s]\n",
      "100%|███████████████████████████████████████| 3772/3772 [30:03<00:00,  2.09it/s]\n"
     ]
    }
   ],
   "source": [
    "jobs = []\n",
    "proc_sizes = [data_reg.shape[0]//number_of_proccessors] * (number_of_proccessors)\n",
    "proc_sizes[-1] += data_reg.shape[0]%number_of_proccessors\n",
    "proc_idx = [(sum(proc_sizes[:i]), sum(proc_sizes[:i+1])-1) for i in range(number_of_proccessors)]\n",
    "\n",
    "for i in range(number_of_proccessors):\n",
    "    p = Process(target=creating_image, args=(proc_idx[i][0], proc_idx[i][1]))\n",
    "    jobs.append(p)\n",
    "    p.start()\n",
    "for i in range(number_of_proccessors):\n",
    "    jobs[i].join()\n",
    "\n",
    "for i in range(number_of_proccessors):\n",
    "    jobs[i].terminate()\n",
    "    jobs[i].close()\n",
    "del jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sensors_location' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2290659/2692102101.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msensors_location\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m225\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m225\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'sensors_location' is not defined"
     ]
    }
   ],
   "source": [
    "sensors_location[225][225]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 49.   , -67.973, -50.789, -72.345, -72.867, -60.808, -68.141,\n",
       "       -73.054, -65.912, -66.931, -81.242, -78.197, -68.693, -68.276,\n",
       "       -75.171, -76.76 , -68.616, -79.3  , -76.408, -70.583, -66.942,\n",
       "       -68.405, -74.092, -75.976, -75.671, -76.973, -78.145, -68.874,\n",
       "       -67.967, -74.006, -71.048, -81.523, -77.934, -76.878, -76.477,\n",
       "       -78.388, -55.4  , -73.306, -76.155, -82.771, -78.345, -84.242,\n",
       "       -85.043, -68.135, -85.481, -85.117, -79.689, -85.389, -88.734,\n",
       "       -89.126,   1.   ,  27.   ,  52.   ,  35.852,   0.   ,  22.075])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_reg[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/shahrokh/projects/MLSpectrumAllocation/ML/data/pictures_299_299_transfer/log/noisy_std_1/pu_circle_su_circle_60/raw_power_min_max_norm/color/log_pu5_su6/variable_sensors_10_20_pus_5_sus_8_channels/images'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "imm = read_image(1)\n",
    "new_imm = np.zeros((max_x, max_y), dtype=float_memory_used)\n",
    "for x in range(max_x):\n",
    "    for y in range(max_y):\n",
    "        new_imm[x][y] = imm[0][x][y][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39.059"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(data_reg[:,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imm = read_image(0, image_dir=f\"{image_dir}\")#, image_dir=\"/home/shahrokh/projects/MLSpectrumAllocation/ML/data/pictures_299_299_transfer/splat/pu_circle_su_circle_60/raw_power_min_max_norm/color/log_5/pus_1_sus_3_channels/images\")\n",
    "immm = np.zeros((max_x, max_y, number_image_channels), dtype='float')\n",
    "for x in range(max_x):\n",
    "    for y in range(max_y):\n",
    "        for ch in range(number_image_channels):\n",
    "            immm[x][y][ch] = float(imm[0][x][y][ch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "immm[immm>1] = 1.0\n",
    "plt.imshow(immm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## immm[immm>1] = 1.0\n",
    "plt.imshow(immm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_reg[123853,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rotated_im = np.rot90(immm, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "img_rot = ndimage.rotate(immm, -90, reshape=False)\n",
    "img_rot = np.rot90(immm,2)\n",
    "plt.imshow(img_rot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 12.   ,  83.   ,  80.   ,  -6.513,  57.   ,  14.   , -16.233,\n",
       "        80.   ,   9.   ,  -4.876,  74.   ,  75.   , -26.081,  42.   ,\n",
       "        49.   ,  -4.702,  36.   ,   7.   , -20.297,  23.   ,  73.   ,\n",
       "        -5.415,  87.   ,  80.   ,  -8.679,   2.   ,  14.   ,  -1.331,\n",
       "        15.   ,  20.   , -16.955,  29.   ,  88.   ,  -2.918,  93.   ,\n",
       "        87.   , -12.061,   1.   ,  65.   ,  28.   , -31.732,   1.   ,\n",
       "           nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
       "           nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
       "           nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
       "           nan,     nan,     nan,  14.658])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_reg[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sensors_location[49]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x1080 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "fig, ax = plt.subplots(figsize=(15,15))         # Sample figsize in inches\n",
    "sns.heatmap(imm[0,:,:,2], ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_model(kernel_lam, bias_lam):\n",
    "    inputs = Input(shape=(max_x, max_y, number_image_channels))\n",
    "    convolution_filter, dense_filter = 'relu', 'linear'\n",
    "    convolution_init, dense_init = \"lecun_normal\", \"RandomNormal\"\n",
    "    # We make sure that the base_model is running in inference mode here,\n",
    "    # by passing `training=False`. This is important for fine-tuning, as you will\n",
    "    # learn in a few paragraphs.\n",
    "#     base_model.trainable = False\n",
    "    cnn = base_model(inputs, training=False)\n",
    "#     cnn = base_model(inputs)\n",
    "    \n",
    "#     cnn = layers.Dense(1024, activation=convolution_filter, kernel_regularizer=regularizers.l2(kernel_lam),\n",
    "#                              bias_regularizer=regularizers.l2(bias_lam), kernel_initializer=convolution_init)(cnn)\n",
    "#     cnn = layers.Dropout(0.5)(cnn)\n",
    "    if True:\n",
    "        # Convert features of shape `base_model.output_shape[1:]` to vectors\n",
    "        cnn = layers.GlobalAveragePooling2D()(cnn)\n",
    "        cnn = layers.Dense(2048, activation=convolution_filter, kernel_regularizer=regularizers.l2(kernel_lam),\n",
    "                             bias_regularizer=regularizers.l2(bias_lam), kernel_initializer=convolution_init)(cnn)\n",
    "        cnn = layers.Dropout(0.5)(cnn)\n",
    "        outputs = layers.Dense(1, activation=dense_filter, kernel_regularizer=regularizers.l2(kernel_lam),\n",
    "                             bias_regularizer=regularizers.l2(bias_lam), kernel_initializer=dense_init)(cnn)\n",
    "        return Model(inputs, outputs)\n",
    "    else:\n",
    "        data_format=\"channels_last\"\n",
    "        filter_shape, pool_size = (1, 1), (2,2)\n",
    "        cnn = layers.Conv2D(512, filter_shape, padding='same', \n",
    "                            activation=convolution_filter, data_format=data_format, \n",
    "                            kernel_regularizer=regularizers.l2(kernel_lam), \n",
    "                            bias_regularizer=regularizers.l2(bias_lam),\n",
    "                         kernel_initializer=convolution_init)(cnn)\n",
    "        cnn = layers.Conv2D(512, filter_shape,padding='same', activation=convolution_filter, data_format=data_format, \n",
    "                         kernel_regularizer=regularizers.l2(kernel_lam), bias_regularizer=regularizers.l2(bias_lam),\n",
    "                         kernel_initializer=convolution_init)(cnn)\n",
    "        cnn = layers.Conv2D(512, filter_shape,padding='same', activation=convolution_filter, data_format=data_format, \n",
    "                         kernel_regularizer=regularizers.l2(kernel_lam), bias_regularizer=regularizers.l2(bias_lam),\n",
    "                         kernel_initializer=convolution_init)(cnn)\n",
    "        cnn = layers.MaxPooling2D(pool_size=pool_size, data_format=data_format)(cnn)\n",
    "        \n",
    "        cnn = layers.Conv2D(512, filter_shape,padding='same', activation=convolution_filter, data_format=data_format, \n",
    "                         kernel_regularizer=regularizers.l2(kernel_lam), bias_regularizer=regularizers.l2(bias_lam),\n",
    "                         kernel_initializer=convolution_init)(cnn)\n",
    "        cnn = layers.Conv2D(512, filter_shape,padding='same', activation=convolution_filter,\n",
    "                            data_format=data_format, \n",
    "                         kernel_regularizer=regularizers.l2(kernel_lam), bias_regularizer=regularizers.l2(bias_lam),\n",
    "                         kernel_initializer=convolution_init)(cnn)\n",
    "        cnn = layers.Conv2D(512, filter_shape,padding='same', activation=convolution_filter, data_format=data_format, \n",
    "                         kernel_regularizer=regularizers.l2(kernel_lam), bias_regularizer=regularizers.l2(bias_lam),\n",
    "                         kernel_initializer=convolution_init)(cnn)\n",
    "        cnn = layers.MaxPooling2D(pool_size=pool_size, data_format=data_format)(cnn)\n",
    "        \n",
    "        cnn = layers.Conv2D(512, filter_shape,padding='same', activation=convolution_filter, data_format=data_format, \n",
    "                         kernel_regularizer=regularizers.l2(kernel_lam), bias_regularizer=regularizers.l2(bias_lam),\n",
    "                         kernel_initializer=convolution_init)(cnn)\n",
    "        cnn = layers.Conv2D(512, filter_shape,padding='same', activation=convolution_filter,\n",
    "                            data_format=data_format, \n",
    "                         kernel_regularizer=regularizers.l2(kernel_lam), bias_regularizer=regularizers.l2(bias_lam),\n",
    "                         kernel_initializer=convolution_init)(cnn)\n",
    "        cnn = layers.Conv2D(512, filter_shape,padding='same', activation=convolution_filter, data_format=data_format, \n",
    "                         kernel_regularizer=regularizers.l2(kernel_lam), bias_regularizer=regularizers.l2(bias_lam),\n",
    "                         kernel_initializer=convolution_init)(cnn)\n",
    "        cnn = layers.MaxPooling2D(pool_size=pool_size, data_format=data_format)(cnn)\n",
    "        cnn = layers.Conv2D(512, filter_shape,padding='same', activation=convolution_filter, data_format=data_format, \n",
    "                         kernel_regularizer=regularizers.l2(kernel_lam), bias_regularizer=regularizers.l2(bias_lam),\n",
    "                         kernel_initializer=convolution_init)(cnn)\n",
    "        cnn = layers.Conv2D(128, filter_shape,padding='same', activation=convolution_filter,\n",
    "                            data_format=data_format, \n",
    "                         kernel_regularizer=regularizers.l2(kernel_lam), bias_regularizer=regularizers.l2(bias_lam),\n",
    "                         kernel_initializer=convolution_init)(cnn)\n",
    "        cnn = layers.Conv2D(32, filter_shape,padding='same', activation=convolution_filter, data_format=data_format, \n",
    "                         kernel_regularizer=regularizers.l2(kernel_lam), bias_regularizer=regularizers.l2(bias_lam),\n",
    "                         kernel_initializer=convolution_init)(cnn)\n",
    "        cnn = layers.Conv2D(8, filter_shape,padding='same', activation=convolution_filter, data_format=data_format, \n",
    "                         kernel_regularizer=regularizers.l2(kernel_lam), bias_regularizer=regularizers.l2(bias_lam),\n",
    "                         kernel_initializer=convolution_init)(cnn)\n",
    "        cnn = layers.Conv2D(1, filter_shape,padding='same', activation=convolution_filter, data_format=data_format, \n",
    "                         kernel_regularizer=regularizers.l2(kernel_lam), bias_regularizer=regularizers.l2(bias_lam),\n",
    "                         kernel_initializer=convolution_init)(cnn)\n",
    "        cnn = layers.GlobalAveragePooling2D()(cnn)\n",
    "        return Model(inputs, cnn)\n",
    "    \n",
    "\n",
    "class DataBatchGenerator(Sequence):\n",
    "    def __init__(self, dataset:np.ndarray, batch_size:int, start_idx:int,\n",
    "                 number_image_channels:int,\n",
    "                 max_x, max_y, float_memory_used, image_dir = image_dir, conserve=0):\n",
    "#         print(dataset.shape[0])\n",
    "#         print(\"generator initiated\")\n",
    "        self.dataset, self.batch_size, self.start_idx = dataset, batch_size, start_idx\n",
    "        self.number_image_channels, self.max_x, self.max_y = number_image_channels, max_x, max_y\n",
    "        self.float_memory_used = float_memory_used\n",
    "        self.conserve = conserve\n",
    "        self._image_dir = image_dir\n",
    "    \n",
    "    def __len__(self):\n",
    "        return np.ceil(self.dataset.shape[0] / self.batch_size).astype(int)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "#         print('generator yielded a batch %d' % idx)\n",
    "        size = min(self.dataset.shape[0] - idx * self.batch_size, self.batch_size)\n",
    "        batch_x = np.empty((size, self.max_x, self.max_y, self.number_image_channels), \n",
    "                           dtype=self.float_memory_used)\n",
    "        batch_y = np.empty((size), dtype=self.float_memory_used)\n",
    "        for i in range(size):\n",
    "            batch_x[i] = read_image(self.start_idx + idx * self.batch_size + i, self._image_dir)\n",
    "            batch_y[i] = self.dataset[idx * self.batch_size + i][- 1 - self.conserve]\n",
    "        return batch_x, batch_y\n",
    "\n",
    "class PredictBatchGenerator(Sequence):\n",
    "    def __init__(self, dataset_size: int, batch_size: int, start_idx: int,\n",
    "                 number_image_channels: int, max_x: int,\n",
    "                 max_y: int, float_memory_used,\n",
    "                 image_dir: str):\n",
    "        self.dataset_size = dataset_size\n",
    "        self.batch_size, self.start_idx = batch_size, start_idx\n",
    "        self.number_image_channels, self.max_x, self.max_y = number_image_channels, max_x, max_y\n",
    "        self.float_memory_used = float_memory_used\n",
    "        self.image_dir = image_dir\n",
    "    \n",
    "    def __len__(self):\n",
    "        return np.ceil(self.dataset_size / self.batch_size).astype(int)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        size = min(self.dataset_size - idx * self.batch_size, self.batch_size)\n",
    "        batch_x = np.empty((size, self.max_x, self.max_y, self.number_image_channels),\n",
    "                           dtype=self.float_memory_used)\n",
    "        for i in range(size):\n",
    "            batch_x[i] = read_image(self.start_idx + idx * self.batch_size + i, self.image_dir)\n",
    "        return batch_x\n",
    "        \n",
    "    \n",
    "def custom_loss(fp_penalty_coef, fn_penalty_coef):\n",
    "    # custom loss function that penalize false positive and negative differently\n",
    "    def loss(y_true, y_pred):\n",
    "        res = y_pred - y_true\n",
    "        res = tf.where(res > 0, res * fp_penalty_coef, res * fn_penalty_coef)\n",
    "        return K.mean(K.square(res))\n",
    "    return loss\n",
    "\n",
    "def fp_mae(y_true, y_pred):\n",
    "    # custom metric that replace false negative with zero and return the mean of new vector\n",
    "    res = y_pred - y_true\n",
    "    res = tf.nn.relu(res)\n",
    "#     res = tf.where(res <= 0, 0, res)\n",
    "    return K.mean(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shahrokh/miniconda3/envs/research/lib/python3.9/site-packages/keras/applications/imagenet_utils.py:331: UserWarning: This model usually expects 1 or 3 input channels. However, it was passed an input_shape with 8 input channels.\n",
      "  warnings.warn('This model usually expects 1 or 3 input channels. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"resnet101\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_17 (InputLayer)           [(None, 300, 300, 8) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1_pad (ZeroPadding2D)       (None, 306, 306, 8)  0           input_17[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1_conv (Conv2D)             (None, 150, 150, 64) 25152       conv1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1_bn (BatchNormalization)   (None, 150, 150, 64) 256         conv1_conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1_relu (Activation)         (None, 150, 150, 64) 0           conv1_bn[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pad (ZeroPadding2D)       (None, 152, 152, 64) 0           conv1_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pool (MaxPooling2D)       (None, 75, 75, 64)   0           pool1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_conv (Conv2D)    (None, 75, 75, 64)   4160        pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_bn (BatchNormali (None, 75, 75, 64)   256         conv2_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_relu (Activation (None, 75, 75, 64)   0           conv2_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_conv (Conv2D)    (None, 75, 75, 64)   36928       conv2_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_bn (BatchNormali (None, 75, 75, 64)   256         conv2_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_relu (Activation (None, 75, 75, 64)   0           conv2_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_conv (Conv2D)    (None, 75, 75, 256)  16640       pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_conv (Conv2D)    (None, 75, 75, 256)  16640       conv2_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_bn (BatchNormali (None, 75, 75, 256)  1024        conv2_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_bn (BatchNormali (None, 75, 75, 256)  1024        conv2_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_add (Add)          (None, 75, 75, 256)  0           conv2_block1_0_bn[0][0]          \n",
      "                                                                 conv2_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_out (Activation)   (None, 75, 75, 256)  0           conv2_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_conv (Conv2D)    (None, 75, 75, 64)   16448       conv2_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_bn (BatchNormali (None, 75, 75, 64)   256         conv2_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_relu (Activation (None, 75, 75, 64)   0           conv2_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_conv (Conv2D)    (None, 75, 75, 64)   36928       conv2_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_bn (BatchNormali (None, 75, 75, 64)   256         conv2_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_relu (Activation (None, 75, 75, 64)   0           conv2_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_3_conv (Conv2D)    (None, 75, 75, 256)  16640       conv2_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_3_bn (BatchNormali (None, 75, 75, 256)  1024        conv2_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_add (Add)          (None, 75, 75, 256)  0           conv2_block1_out[0][0]           \n",
      "                                                                 conv2_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_out (Activation)   (None, 75, 75, 256)  0           conv2_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_conv (Conv2D)    (None, 75, 75, 64)   16448       conv2_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_bn (BatchNormali (None, 75, 75, 64)   256         conv2_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_relu (Activation (None, 75, 75, 64)   0           conv2_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_conv (Conv2D)    (None, 75, 75, 64)   36928       conv2_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_bn (BatchNormali (None, 75, 75, 64)   256         conv2_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_relu (Activation (None, 75, 75, 64)   0           conv2_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_3_conv (Conv2D)    (None, 75, 75, 256)  16640       conv2_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_3_bn (BatchNormali (None, 75, 75, 256)  1024        conv2_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_add (Add)          (None, 75, 75, 256)  0           conv2_block2_out[0][0]           \n",
      "                                                                 conv2_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_out (Activation)   (None, 75, 75, 256)  0           conv2_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_conv (Conv2D)    (None, 38, 38, 128)  32896       conv2_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_bn (BatchNormali (None, 38, 38, 128)  512         conv3_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_relu (Activation (None, 38, 38, 128)  0           conv3_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_conv (Conv2D)    (None, 38, 38, 128)  147584      conv3_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_bn (BatchNormali (None, 38, 38, 128)  512         conv3_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_relu (Activation (None, 38, 38, 128)  0           conv3_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_0_conv (Conv2D)    (None, 38, 38, 512)  131584      conv2_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_3_conv (Conv2D)    (None, 38, 38, 512)  66048       conv3_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_0_bn (BatchNormali (None, 38, 38, 512)  2048        conv3_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_3_bn (BatchNormali (None, 38, 38, 512)  2048        conv3_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_add (Add)          (None, 38, 38, 512)  0           conv3_block1_0_bn[0][0]          \n",
      "                                                                 conv3_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_out (Activation)   (None, 38, 38, 512)  0           conv3_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_conv (Conv2D)    (None, 38, 38, 128)  65664       conv3_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_bn (BatchNormali (None, 38, 38, 128)  512         conv3_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_relu (Activation (None, 38, 38, 128)  0           conv3_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_conv (Conv2D)    (None, 38, 38, 128)  147584      conv3_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_bn (BatchNormali (None, 38, 38, 128)  512         conv3_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_relu (Activation (None, 38, 38, 128)  0           conv3_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_3_conv (Conv2D)    (None, 38, 38, 512)  66048       conv3_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_3_bn (BatchNormali (None, 38, 38, 512)  2048        conv3_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_add (Add)          (None, 38, 38, 512)  0           conv3_block1_out[0][0]           \n",
      "                                                                 conv3_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_out (Activation)   (None, 38, 38, 512)  0           conv3_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_conv (Conv2D)    (None, 38, 38, 128)  65664       conv3_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_bn (BatchNormali (None, 38, 38, 128)  512         conv3_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_relu (Activation (None, 38, 38, 128)  0           conv3_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_conv (Conv2D)    (None, 38, 38, 128)  147584      conv3_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_bn (BatchNormali (None, 38, 38, 128)  512         conv3_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_relu (Activation (None, 38, 38, 128)  0           conv3_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_3_conv (Conv2D)    (None, 38, 38, 512)  66048       conv3_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_3_bn (BatchNormali (None, 38, 38, 512)  2048        conv3_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_add (Add)          (None, 38, 38, 512)  0           conv3_block2_out[0][0]           \n",
      "                                                                 conv3_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_out (Activation)   (None, 38, 38, 512)  0           conv3_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_conv (Conv2D)    (None, 38, 38, 128)  65664       conv3_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_bn (BatchNormali (None, 38, 38, 128)  512         conv3_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_relu (Activation (None, 38, 38, 128)  0           conv3_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_conv (Conv2D)    (None, 38, 38, 128)  147584      conv3_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_bn (BatchNormali (None, 38, 38, 128)  512         conv3_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_relu (Activation (None, 38, 38, 128)  0           conv3_block4_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_3_conv (Conv2D)    (None, 38, 38, 512)  66048       conv3_block4_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_3_bn (BatchNormali (None, 38, 38, 512)  2048        conv3_block4_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_add (Add)          (None, 38, 38, 512)  0           conv3_block3_out[0][0]           \n",
      "                                                                 conv3_block4_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_out (Activation)   (None, 38, 38, 512)  0           conv3_block4_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_conv (Conv2D)    (None, 19, 19, 256)  131328      conv3_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_bn (BatchNormali (None, 19, 19, 256)  1024        conv4_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_relu (Activation (None, 19, 19, 256)  0           conv4_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_conv (Conv2D)    (None, 19, 19, 256)  590080      conv4_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_bn (BatchNormali (None, 19, 19, 256)  1024        conv4_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_relu (Activation (None, 19, 19, 256)  0           conv4_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_0_conv (Conv2D)    (None, 19, 19, 1024) 525312      conv3_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_3_conv (Conv2D)    (None, 19, 19, 1024) 263168      conv4_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_0_bn (BatchNormali (None, 19, 19, 1024) 4096        conv4_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_3_bn (BatchNormali (None, 19, 19, 1024) 4096        conv4_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_add (Add)          (None, 19, 19, 1024) 0           conv4_block1_0_bn[0][0]          \n",
      "                                                                 conv4_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_out (Activation)   (None, 19, 19, 1024) 0           conv4_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_conv (Conv2D)    (None, 19, 19, 256)  262400      conv4_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_bn (BatchNormali (None, 19, 19, 256)  1024        conv4_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_relu (Activation (None, 19, 19, 256)  0           conv4_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_conv (Conv2D)    (None, 19, 19, 256)  590080      conv4_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_bn (BatchNormali (None, 19, 19, 256)  1024        conv4_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_relu (Activation (None, 19, 19, 256)  0           conv4_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_3_conv (Conv2D)    (None, 19, 19, 1024) 263168      conv4_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_3_bn (BatchNormali (None, 19, 19, 1024) 4096        conv4_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_add (Add)          (None, 19, 19, 1024) 0           conv4_block1_out[0][0]           \n",
      "                                                                 conv4_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_out (Activation)   (None, 19, 19, 1024) 0           conv4_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_conv (Conv2D)    (None, 19, 19, 256)  262400      conv4_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_bn (BatchNormali (None, 19, 19, 256)  1024        conv4_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_relu (Activation (None, 19, 19, 256)  0           conv4_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_conv (Conv2D)    (None, 19, 19, 256)  590080      conv4_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_bn (BatchNormali (None, 19, 19, 256)  1024        conv4_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_relu (Activation (None, 19, 19, 256)  0           conv4_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_3_conv (Conv2D)    (None, 19, 19, 1024) 263168      conv4_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_3_bn (BatchNormali (None, 19, 19, 1024) 4096        conv4_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_add (Add)          (None, 19, 19, 1024) 0           conv4_block2_out[0][0]           \n",
      "                                                                 conv4_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_out (Activation)   (None, 19, 19, 1024) 0           conv4_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_conv (Conv2D)    (None, 19, 19, 256)  262400      conv4_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_bn (BatchNormali (None, 19, 19, 256)  1024        conv4_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_relu (Activation (None, 19, 19, 256)  0           conv4_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_conv (Conv2D)    (None, 19, 19, 256)  590080      conv4_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_bn (BatchNormali (None, 19, 19, 256)  1024        conv4_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_relu (Activation (None, 19, 19, 256)  0           conv4_block4_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_3_conv (Conv2D)    (None, 19, 19, 1024) 263168      conv4_block4_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_3_bn (BatchNormali (None, 19, 19, 1024) 4096        conv4_block4_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_add (Add)          (None, 19, 19, 1024) 0           conv4_block3_out[0][0]           \n",
      "                                                                 conv4_block4_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_out (Activation)   (None, 19, 19, 1024) 0           conv4_block4_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_conv (Conv2D)    (None, 19, 19, 256)  262400      conv4_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_bn (BatchNormali (None, 19, 19, 256)  1024        conv4_block5_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_relu (Activation (None, 19, 19, 256)  0           conv4_block5_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_conv (Conv2D)    (None, 19, 19, 256)  590080      conv4_block5_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_bn (BatchNormali (None, 19, 19, 256)  1024        conv4_block5_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_relu (Activation (None, 19, 19, 256)  0           conv4_block5_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_3_conv (Conv2D)    (None, 19, 19, 1024) 263168      conv4_block5_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_3_bn (BatchNormali (None, 19, 19, 1024) 4096        conv4_block5_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_add (Add)          (None, 19, 19, 1024) 0           conv4_block4_out[0][0]           \n",
      "                                                                 conv4_block5_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_out (Activation)   (None, 19, 19, 1024) 0           conv4_block5_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_conv (Conv2D)    (None, 19, 19, 256)  262400      conv4_block5_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_bn (BatchNormali (None, 19, 19, 256)  1024        conv4_block6_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_relu (Activation (None, 19, 19, 256)  0           conv4_block6_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_conv (Conv2D)    (None, 19, 19, 256)  590080      conv4_block6_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_bn (BatchNormali (None, 19, 19, 256)  1024        conv4_block6_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_relu (Activation (None, 19, 19, 256)  0           conv4_block6_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_3_conv (Conv2D)    (None, 19, 19, 1024) 263168      conv4_block6_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_3_bn (BatchNormali (None, 19, 19, 1024) 4096        conv4_block6_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_add (Add)          (None, 19, 19, 1024) 0           conv4_block5_out[0][0]           \n",
      "                                                                 conv4_block6_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_out (Activation)   (None, 19, 19, 1024) 0           conv4_block6_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block7_1_conv (Conv2D)    (None, 19, 19, 256)  262400      conv4_block6_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block7_1_bn (BatchNormali (None, 19, 19, 256)  1024        conv4_block7_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block7_1_relu (Activation (None, 19, 19, 256)  0           conv4_block7_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block7_2_conv (Conv2D)    (None, 19, 19, 256)  590080      conv4_block7_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block7_2_bn (BatchNormali (None, 19, 19, 256)  1024        conv4_block7_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block7_2_relu (Activation (None, 19, 19, 256)  0           conv4_block7_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block7_3_conv (Conv2D)    (None, 19, 19, 1024) 263168      conv4_block7_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block7_3_bn (BatchNormali (None, 19, 19, 1024) 4096        conv4_block7_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block7_add (Add)          (None, 19, 19, 1024) 0           conv4_block6_out[0][0]           \n",
      "                                                                 conv4_block7_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block7_out (Activation)   (None, 19, 19, 1024) 0           conv4_block7_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block8_1_conv (Conv2D)    (None, 19, 19, 256)  262400      conv4_block7_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block8_1_bn (BatchNormali (None, 19, 19, 256)  1024        conv4_block8_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block8_1_relu (Activation (None, 19, 19, 256)  0           conv4_block8_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block8_2_conv (Conv2D)    (None, 19, 19, 256)  590080      conv4_block8_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block8_2_bn (BatchNormali (None, 19, 19, 256)  1024        conv4_block8_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block8_2_relu (Activation (None, 19, 19, 256)  0           conv4_block8_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block8_3_conv (Conv2D)    (None, 19, 19, 1024) 263168      conv4_block8_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block8_3_bn (BatchNormali (None, 19, 19, 1024) 4096        conv4_block8_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block8_add (Add)          (None, 19, 19, 1024) 0           conv4_block7_out[0][0]           \n",
      "                                                                 conv4_block8_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block8_out (Activation)   (None, 19, 19, 1024) 0           conv4_block8_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block9_1_conv (Conv2D)    (None, 19, 19, 256)  262400      conv4_block8_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block9_1_bn (BatchNormali (None, 19, 19, 256)  1024        conv4_block9_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block9_1_relu (Activation (None, 19, 19, 256)  0           conv4_block9_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block9_2_conv (Conv2D)    (None, 19, 19, 256)  590080      conv4_block9_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block9_2_bn (BatchNormali (None, 19, 19, 256)  1024        conv4_block9_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block9_2_relu (Activation (None, 19, 19, 256)  0           conv4_block9_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block9_3_conv (Conv2D)    (None, 19, 19, 1024) 263168      conv4_block9_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block9_3_bn (BatchNormali (None, 19, 19, 1024) 4096        conv4_block9_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block9_add (Add)          (None, 19, 19, 1024) 0           conv4_block8_out[0][0]           \n",
      "                                                                 conv4_block9_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block9_out (Activation)   (None, 19, 19, 1024) 0           conv4_block9_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block10_1_conv (Conv2D)   (None, 19, 19, 256)  262400      conv4_block9_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block10_1_bn (BatchNormal (None, 19, 19, 256)  1024        conv4_block10_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block10_1_relu (Activatio (None, 19, 19, 256)  0           conv4_block10_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block10_2_conv (Conv2D)   (None, 19, 19, 256)  590080      conv4_block10_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block10_2_bn (BatchNormal (None, 19, 19, 256)  1024        conv4_block10_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block10_2_relu (Activatio (None, 19, 19, 256)  0           conv4_block10_2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block10_3_conv (Conv2D)   (None, 19, 19, 1024) 263168      conv4_block10_2_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block10_3_bn (BatchNormal (None, 19, 19, 1024) 4096        conv4_block10_3_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block10_add (Add)         (None, 19, 19, 1024) 0           conv4_block9_out[0][0]           \n",
      "                                                                 conv4_block10_3_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block10_out (Activation)  (None, 19, 19, 1024) 0           conv4_block10_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block11_1_conv (Conv2D)   (None, 19, 19, 256)  262400      conv4_block10_out[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block11_1_bn (BatchNormal (None, 19, 19, 256)  1024        conv4_block11_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block11_1_relu (Activatio (None, 19, 19, 256)  0           conv4_block11_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block11_2_conv (Conv2D)   (None, 19, 19, 256)  590080      conv4_block11_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block11_2_bn (BatchNormal (None, 19, 19, 256)  1024        conv4_block11_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block11_2_relu (Activatio (None, 19, 19, 256)  0           conv4_block11_2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block11_3_conv (Conv2D)   (None, 19, 19, 1024) 263168      conv4_block11_2_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block11_3_bn (BatchNormal (None, 19, 19, 1024) 4096        conv4_block11_3_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block11_add (Add)         (None, 19, 19, 1024) 0           conv4_block10_out[0][0]          \n",
      "                                                                 conv4_block11_3_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block11_out (Activation)  (None, 19, 19, 1024) 0           conv4_block11_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block12_1_conv (Conv2D)   (None, 19, 19, 256)  262400      conv4_block11_out[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block12_1_bn (BatchNormal (None, 19, 19, 256)  1024        conv4_block12_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block12_1_relu (Activatio (None, 19, 19, 256)  0           conv4_block12_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block12_2_conv (Conv2D)   (None, 19, 19, 256)  590080      conv4_block12_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block12_2_bn (BatchNormal (None, 19, 19, 256)  1024        conv4_block12_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block12_2_relu (Activatio (None, 19, 19, 256)  0           conv4_block12_2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block12_3_conv (Conv2D)   (None, 19, 19, 1024) 263168      conv4_block12_2_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block12_3_bn (BatchNormal (None, 19, 19, 1024) 4096        conv4_block12_3_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block12_add (Add)         (None, 19, 19, 1024) 0           conv4_block11_out[0][0]          \n",
      "                                                                 conv4_block12_3_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block12_out (Activation)  (None, 19, 19, 1024) 0           conv4_block12_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block13_1_conv (Conv2D)   (None, 19, 19, 256)  262400      conv4_block12_out[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block13_1_bn (BatchNormal (None, 19, 19, 256)  1024        conv4_block13_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block13_1_relu (Activatio (None, 19, 19, 256)  0           conv4_block13_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block13_2_conv (Conv2D)   (None, 19, 19, 256)  590080      conv4_block13_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block13_2_bn (BatchNormal (None, 19, 19, 256)  1024        conv4_block13_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block13_2_relu (Activatio (None, 19, 19, 256)  0           conv4_block13_2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block13_3_conv (Conv2D)   (None, 19, 19, 1024) 263168      conv4_block13_2_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block13_3_bn (BatchNormal (None, 19, 19, 1024) 4096        conv4_block13_3_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block13_add (Add)         (None, 19, 19, 1024) 0           conv4_block12_out[0][0]          \n",
      "                                                                 conv4_block13_3_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block13_out (Activation)  (None, 19, 19, 1024) 0           conv4_block13_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block14_1_conv (Conv2D)   (None, 19, 19, 256)  262400      conv4_block13_out[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block14_1_bn (BatchNormal (None, 19, 19, 256)  1024        conv4_block14_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block14_1_relu (Activatio (None, 19, 19, 256)  0           conv4_block14_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block14_2_conv (Conv2D)   (None, 19, 19, 256)  590080      conv4_block14_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block14_2_bn (BatchNormal (None, 19, 19, 256)  1024        conv4_block14_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block14_2_relu (Activatio (None, 19, 19, 256)  0           conv4_block14_2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block14_3_conv (Conv2D)   (None, 19, 19, 1024) 263168      conv4_block14_2_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block14_3_bn (BatchNormal (None, 19, 19, 1024) 4096        conv4_block14_3_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block14_add (Add)         (None, 19, 19, 1024) 0           conv4_block13_out[0][0]          \n",
      "                                                                 conv4_block14_3_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block14_out (Activation)  (None, 19, 19, 1024) 0           conv4_block14_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block15_1_conv (Conv2D)   (None, 19, 19, 256)  262400      conv4_block14_out[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block15_1_bn (BatchNormal (None, 19, 19, 256)  1024        conv4_block15_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block15_1_relu (Activatio (None, 19, 19, 256)  0           conv4_block15_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block15_2_conv (Conv2D)   (None, 19, 19, 256)  590080      conv4_block15_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block15_2_bn (BatchNormal (None, 19, 19, 256)  1024        conv4_block15_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block15_2_relu (Activatio (None, 19, 19, 256)  0           conv4_block15_2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block15_3_conv (Conv2D)   (None, 19, 19, 1024) 263168      conv4_block15_2_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block15_3_bn (BatchNormal (None, 19, 19, 1024) 4096        conv4_block15_3_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block15_add (Add)         (None, 19, 19, 1024) 0           conv4_block14_out[0][0]          \n",
      "                                                                 conv4_block15_3_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block15_out (Activation)  (None, 19, 19, 1024) 0           conv4_block15_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block16_1_conv (Conv2D)   (None, 19, 19, 256)  262400      conv4_block15_out[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block16_1_bn (BatchNormal (None, 19, 19, 256)  1024        conv4_block16_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block16_1_relu (Activatio (None, 19, 19, 256)  0           conv4_block16_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block16_2_conv (Conv2D)   (None, 19, 19, 256)  590080      conv4_block16_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block16_2_bn (BatchNormal (None, 19, 19, 256)  1024        conv4_block16_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block16_2_relu (Activatio (None, 19, 19, 256)  0           conv4_block16_2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block16_3_conv (Conv2D)   (None, 19, 19, 1024) 263168      conv4_block16_2_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block16_3_bn (BatchNormal (None, 19, 19, 1024) 4096        conv4_block16_3_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block16_add (Add)         (None, 19, 19, 1024) 0           conv4_block15_out[0][0]          \n",
      "                                                                 conv4_block16_3_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block16_out (Activation)  (None, 19, 19, 1024) 0           conv4_block16_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block17_1_conv (Conv2D)   (None, 19, 19, 256)  262400      conv4_block16_out[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block17_1_bn (BatchNormal (None, 19, 19, 256)  1024        conv4_block17_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block17_1_relu (Activatio (None, 19, 19, 256)  0           conv4_block17_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block17_2_conv (Conv2D)   (None, 19, 19, 256)  590080      conv4_block17_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block17_2_bn (BatchNormal (None, 19, 19, 256)  1024        conv4_block17_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block17_2_relu (Activatio (None, 19, 19, 256)  0           conv4_block17_2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block17_3_conv (Conv2D)   (None, 19, 19, 1024) 263168      conv4_block17_2_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block17_3_bn (BatchNormal (None, 19, 19, 1024) 4096        conv4_block17_3_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block17_add (Add)         (None, 19, 19, 1024) 0           conv4_block16_out[0][0]          \n",
      "                                                                 conv4_block17_3_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block17_out (Activation)  (None, 19, 19, 1024) 0           conv4_block17_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block18_1_conv (Conv2D)   (None, 19, 19, 256)  262400      conv4_block17_out[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block18_1_bn (BatchNormal (None, 19, 19, 256)  1024        conv4_block18_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block18_1_relu (Activatio (None, 19, 19, 256)  0           conv4_block18_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block18_2_conv (Conv2D)   (None, 19, 19, 256)  590080      conv4_block18_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block18_2_bn (BatchNormal (None, 19, 19, 256)  1024        conv4_block18_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block18_2_relu (Activatio (None, 19, 19, 256)  0           conv4_block18_2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block18_3_conv (Conv2D)   (None, 19, 19, 1024) 263168      conv4_block18_2_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block18_3_bn (BatchNormal (None, 19, 19, 1024) 4096        conv4_block18_3_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block18_add (Add)         (None, 19, 19, 1024) 0           conv4_block17_out[0][0]          \n",
      "                                                                 conv4_block18_3_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block18_out (Activation)  (None, 19, 19, 1024) 0           conv4_block18_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block19_1_conv (Conv2D)   (None, 19, 19, 256)  262400      conv4_block18_out[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block19_1_bn (BatchNormal (None, 19, 19, 256)  1024        conv4_block19_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block19_1_relu (Activatio (None, 19, 19, 256)  0           conv4_block19_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block19_2_conv (Conv2D)   (None, 19, 19, 256)  590080      conv4_block19_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block19_2_bn (BatchNormal (None, 19, 19, 256)  1024        conv4_block19_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block19_2_relu (Activatio (None, 19, 19, 256)  0           conv4_block19_2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block19_3_conv (Conv2D)   (None, 19, 19, 1024) 263168      conv4_block19_2_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block19_3_bn (BatchNormal (None, 19, 19, 1024) 4096        conv4_block19_3_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block19_add (Add)         (None, 19, 19, 1024) 0           conv4_block18_out[0][0]          \n",
      "                                                                 conv4_block19_3_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block19_out (Activation)  (None, 19, 19, 1024) 0           conv4_block19_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block20_1_conv (Conv2D)   (None, 19, 19, 256)  262400      conv4_block19_out[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block20_1_bn (BatchNormal (None, 19, 19, 256)  1024        conv4_block20_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block20_1_relu (Activatio (None, 19, 19, 256)  0           conv4_block20_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block20_2_conv (Conv2D)   (None, 19, 19, 256)  590080      conv4_block20_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block20_2_bn (BatchNormal (None, 19, 19, 256)  1024        conv4_block20_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block20_2_relu (Activatio (None, 19, 19, 256)  0           conv4_block20_2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block20_3_conv (Conv2D)   (None, 19, 19, 1024) 263168      conv4_block20_2_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block20_3_bn (BatchNormal (None, 19, 19, 1024) 4096        conv4_block20_3_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block20_add (Add)         (None, 19, 19, 1024) 0           conv4_block19_out[0][0]          \n",
      "                                                                 conv4_block20_3_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block20_out (Activation)  (None, 19, 19, 1024) 0           conv4_block20_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block21_1_conv (Conv2D)   (None, 19, 19, 256)  262400      conv4_block20_out[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block21_1_bn (BatchNormal (None, 19, 19, 256)  1024        conv4_block21_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block21_1_relu (Activatio (None, 19, 19, 256)  0           conv4_block21_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block21_2_conv (Conv2D)   (None, 19, 19, 256)  590080      conv4_block21_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block21_2_bn (BatchNormal (None, 19, 19, 256)  1024        conv4_block21_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block21_2_relu (Activatio (None, 19, 19, 256)  0           conv4_block21_2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block21_3_conv (Conv2D)   (None, 19, 19, 1024) 263168      conv4_block21_2_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block21_3_bn (BatchNormal (None, 19, 19, 1024) 4096        conv4_block21_3_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block21_add (Add)         (None, 19, 19, 1024) 0           conv4_block20_out[0][0]          \n",
      "                                                                 conv4_block21_3_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block21_out (Activation)  (None, 19, 19, 1024) 0           conv4_block21_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block22_1_conv (Conv2D)   (None, 19, 19, 256)  262400      conv4_block21_out[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block22_1_bn (BatchNormal (None, 19, 19, 256)  1024        conv4_block22_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block22_1_relu (Activatio (None, 19, 19, 256)  0           conv4_block22_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block22_2_conv (Conv2D)   (None, 19, 19, 256)  590080      conv4_block22_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block22_2_bn (BatchNormal (None, 19, 19, 256)  1024        conv4_block22_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block22_2_relu (Activatio (None, 19, 19, 256)  0           conv4_block22_2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block22_3_conv (Conv2D)   (None, 19, 19, 1024) 263168      conv4_block22_2_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block22_3_bn (BatchNormal (None, 19, 19, 1024) 4096        conv4_block22_3_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block22_add (Add)         (None, 19, 19, 1024) 0           conv4_block21_out[0][0]          \n",
      "                                                                 conv4_block22_3_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block22_out (Activation)  (None, 19, 19, 1024) 0           conv4_block22_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block23_1_conv (Conv2D)   (None, 19, 19, 256)  262400      conv4_block22_out[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block23_1_bn (BatchNormal (None, 19, 19, 256)  1024        conv4_block23_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block23_1_relu (Activatio (None, 19, 19, 256)  0           conv4_block23_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block23_2_conv (Conv2D)   (None, 19, 19, 256)  590080      conv4_block23_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block23_2_bn (BatchNormal (None, 19, 19, 256)  1024        conv4_block23_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block23_2_relu (Activatio (None, 19, 19, 256)  0           conv4_block23_2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block23_3_conv (Conv2D)   (None, 19, 19, 1024) 263168      conv4_block23_2_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block23_3_bn (BatchNormal (None, 19, 19, 1024) 4096        conv4_block23_3_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block23_add (Add)         (None, 19, 19, 1024) 0           conv4_block22_out[0][0]          \n",
      "                                                                 conv4_block23_3_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block23_out (Activation)  (None, 19, 19, 1024) 0           conv4_block23_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_conv (Conv2D)    (None, 10, 10, 512)  524800      conv4_block23_out[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_bn (BatchNormali (None, 10, 10, 512)  2048        conv5_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_relu (Activation (None, 10, 10, 512)  0           conv5_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_conv (Conv2D)    (None, 10, 10, 512)  2359808     conv5_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_bn (BatchNormali (None, 10, 10, 512)  2048        conv5_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_relu (Activation (None, 10, 10, 512)  0           conv5_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_0_conv (Conv2D)    (None, 10, 10, 2048) 2099200     conv4_block23_out[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_3_conv (Conv2D)    (None, 10, 10, 2048) 1050624     conv5_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_0_bn (BatchNormali (None, 10, 10, 2048) 8192        conv5_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_3_bn (BatchNormali (None, 10, 10, 2048) 8192        conv5_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_add (Add)          (None, 10, 10, 2048) 0           conv5_block1_0_bn[0][0]          \n",
      "                                                                 conv5_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_out (Activation)   (None, 10, 10, 2048) 0           conv5_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_conv (Conv2D)    (None, 10, 10, 512)  1049088     conv5_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_bn (BatchNormali (None, 10, 10, 512)  2048        conv5_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_relu (Activation (None, 10, 10, 512)  0           conv5_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_conv (Conv2D)    (None, 10, 10, 512)  2359808     conv5_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_bn (BatchNormali (None, 10, 10, 512)  2048        conv5_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_relu (Activation (None, 10, 10, 512)  0           conv5_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_3_conv (Conv2D)    (None, 10, 10, 2048) 1050624     conv5_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_3_bn (BatchNormali (None, 10, 10, 2048) 8192        conv5_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_add (Add)          (None, 10, 10, 2048) 0           conv5_block1_out[0][0]           \n",
      "                                                                 conv5_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_out (Activation)   (None, 10, 10, 2048) 0           conv5_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_conv (Conv2D)    (None, 10, 10, 512)  1049088     conv5_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_bn (BatchNormali (None, 10, 10, 512)  2048        conv5_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_relu (Activation (None, 10, 10, 512)  0           conv5_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_conv (Conv2D)    (None, 10, 10, 512)  2359808     conv5_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_bn (BatchNormali (None, 10, 10, 512)  2048        conv5_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_relu (Activation (None, 10, 10, 512)  0           conv5_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_3_conv (Conv2D)    (None, 10, 10, 2048) 1050624     conv5_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_3_bn (BatchNormali (None, 10, 10, 2048) 8192        conv5_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_add (Add)          (None, 10, 10, 2048) 0           conv5_block2_out[0][0]           \n",
      "                                                                 conv5_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_out (Activation)   (None, 10, 10, 2048) 0           conv5_block3_add[0][0]           \n",
      "==================================================================================================\n",
      "Total params: 42,673,856\n",
      "Trainable params: 42,568,512\n",
      "Non-trainable params: 105,344\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Xception model\n",
    "model_name = \"ResNet101\"\n",
    "base_model = applications.ResNet101(include_top=False, weights=None,\n",
    "                                input_shape=(max_x, max_y, number_image_channels))\n",
    "base_model.trainable = True\n",
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-09 17:53:49.712847: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-09 17:53:49.718095: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-09 17:53:49.718537: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-09 17:53:49.719328: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-09 17:53:49.719864: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-09 17:53:49.720280: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-09 17:53:49.720690: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-09 17:53:50.115675: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-09 17:53:50.116144: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-09 17:53:50.116537: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-09 17:53:50.116928: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 10094 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3080 Ti, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         [(None, 299, 299, 3)]     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 299, 299, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 299, 299, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 149, 149, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 149, 149, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 149, 149, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 74, 74, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 74, 74, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 74, 74, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 74, 74, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 37, 37, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 37, 37, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 37, 37, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 37, 37, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 18, 18, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 18, 18, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 18, 18, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 18, 18, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 9, 9, 512)         0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 0\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# log-vgg pretrained - pu-setting\n",
    "model_name = \"log_vgg16\"\n",
    "fp_penalty_coef, fn_penalty_coef = 1, 1\n",
    "model_path = \"/home/shahrokh/projects/MLSpectrumAllocation/ML/data/pictures_299_299_transfer/log/noisy_std_1/\" + \\\n",
    "             \"pu_circle_su_circle_60/raw_power_min_max_norm/color/log_pu5_su6/20pus_5sus_8channels/models/vgg16/\" + \\\n",
    "             \"700000/best_model_lambda_0.1.h5\"\n",
    "model_path = \"ML/data/pictures_299_299_transfer/log/noisy_std_1/pu_circle_su_circle_60/\" + \\\n",
    "             \"raw_power_min_max_norm/color/log_5/pus_1_sus_3_channels_700k/models/700000/\" + \\\n",
    "             \"best_model_lambda_0_fit.h5\"\n",
    "base_model = models.load_model(model_path, \n",
    "                              custom_objects={ 'loss': custom_loss(fp_penalty_coef, fn_penalty_coef), \n",
    "                                               'fp_mae': fp_mae,\n",
    "                                               'mae':'mae', 'mse':'mse'})\n",
    "base_model.trainable = False\n",
    "base_model = base_model.layers[1]\n",
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-05 11:19:50.879198: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-05 11:19:50.882966: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-05 11:19:50.883259: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-05 11:19:50.883769: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-05 11:19:50.884674: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-05 11:19:50.885003: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-05 11:19:50.885279: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-05 11:19:51.183469: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-05 11:19:51.183786: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-05 11:19:51.184047: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-05 11:19:51.184295: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 10094 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3080 Ti, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"resnet152v2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 300, 300, 8) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1_pad (ZeroPadding2D)       (None, 306, 306, 8)  0           input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1_conv (Conv2D)             (None, 150, 150, 64) 25152       conv1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pad (ZeroPadding2D)       (None, 152, 152, 64) 0           conv1_conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pool (MaxPooling2D)       (None, 75, 75, 64)   0           pool1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_preact_bn (BatchNo (None, 75, 75, 64)   256         pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_preact_relu (Activ (None, 75, 75, 64)   0           conv2_block1_preact_bn[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_conv (Conv2D)    (None, 75, 75, 64)   4096        conv2_block1_preact_relu[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_bn (BatchNormali (None, 75, 75, 64)   256         conv2_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_relu (Activation (None, 75, 75, 64)   0           conv2_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_pad (ZeroPadding (None, 77, 77, 64)   0           conv2_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_conv (Conv2D)    (None, 75, 75, 64)   36864       conv2_block1_2_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_bn (BatchNormali (None, 75, 75, 64)   256         conv2_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_relu (Activation (None, 75, 75, 64)   0           conv2_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_conv (Conv2D)    (None, 75, 75, 256)  16640       conv2_block1_preact_relu[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_conv (Conv2D)    (None, 75, 75, 256)  16640       conv2_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_out (Add)          (None, 75, 75, 256)  0           conv2_block1_0_conv[0][0]        \n",
      "                                                                 conv2_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_preact_bn (BatchNo (None, 75, 75, 256)  1024        conv2_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_preact_relu (Activ (None, 75, 75, 256)  0           conv2_block2_preact_bn[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_conv (Conv2D)    (None, 75, 75, 64)   16384       conv2_block2_preact_relu[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_bn (BatchNormali (None, 75, 75, 64)   256         conv2_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_relu (Activation (None, 75, 75, 64)   0           conv2_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_pad (ZeroPadding (None, 77, 77, 64)   0           conv2_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_conv (Conv2D)    (None, 75, 75, 64)   36864       conv2_block2_2_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_bn (BatchNormali (None, 75, 75, 64)   256         conv2_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_relu (Activation (None, 75, 75, 64)   0           conv2_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_3_conv (Conv2D)    (None, 75, 75, 256)  16640       conv2_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_out (Add)          (None, 75, 75, 256)  0           conv2_block1_out[0][0]           \n",
      "                                                                 conv2_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_preact_bn (BatchNo (None, 75, 75, 256)  1024        conv2_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_preact_relu (Activ (None, 75, 75, 256)  0           conv2_block3_preact_bn[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_conv (Conv2D)    (None, 75, 75, 64)   16384       conv2_block3_preact_relu[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_bn (BatchNormali (None, 75, 75, 64)   256         conv2_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_relu (Activation (None, 75, 75, 64)   0           conv2_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_pad (ZeroPadding (None, 77, 77, 64)   0           conv2_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_conv (Conv2D)    (None, 38, 38, 64)   36864       conv2_block3_2_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_bn (BatchNormali (None, 38, 38, 64)   256         conv2_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_relu (Activation (None, 38, 38, 64)   0           conv2_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 38, 38, 256)  0           conv2_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_3_conv (Conv2D)    (None, 38, 38, 256)  16640       conv2_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_out (Add)          (None, 38, 38, 256)  0           max_pooling2d_3[0][0]            \n",
      "                                                                 conv2_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_preact_bn (BatchNo (None, 38, 38, 256)  1024        conv2_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_preact_relu (Activ (None, 38, 38, 256)  0           conv3_block1_preact_bn[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_conv (Conv2D)    (None, 38, 38, 128)  32768       conv3_block1_preact_relu[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_bn (BatchNormali (None, 38, 38, 128)  512         conv3_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_relu (Activation (None, 38, 38, 128)  0           conv3_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_pad (ZeroPadding (None, 40, 40, 128)  0           conv3_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_conv (Conv2D)    (None, 38, 38, 128)  147456      conv3_block1_2_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_bn (BatchNormali (None, 38, 38, 128)  512         conv3_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_relu (Activation (None, 38, 38, 128)  0           conv3_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_0_conv (Conv2D)    (None, 38, 38, 512)  131584      conv3_block1_preact_relu[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_3_conv (Conv2D)    (None, 38, 38, 512)  66048       conv3_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_out (Add)          (None, 38, 38, 512)  0           conv3_block1_0_conv[0][0]        \n",
      "                                                                 conv3_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_preact_bn (BatchNo (None, 38, 38, 512)  2048        conv3_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_preact_relu (Activ (None, 38, 38, 512)  0           conv3_block2_preact_bn[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_conv (Conv2D)    (None, 38, 38, 128)  65536       conv3_block2_preact_relu[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_bn (BatchNormali (None, 38, 38, 128)  512         conv3_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_relu (Activation (None, 38, 38, 128)  0           conv3_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_pad (ZeroPadding (None, 40, 40, 128)  0           conv3_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_conv (Conv2D)    (None, 38, 38, 128)  147456      conv3_block2_2_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_bn (BatchNormali (None, 38, 38, 128)  512         conv3_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_relu (Activation (None, 38, 38, 128)  0           conv3_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_3_conv (Conv2D)    (None, 38, 38, 512)  66048       conv3_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_out (Add)          (None, 38, 38, 512)  0           conv3_block1_out[0][0]           \n",
      "                                                                 conv3_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_preact_bn (BatchNo (None, 38, 38, 512)  2048        conv3_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_preact_relu (Activ (None, 38, 38, 512)  0           conv3_block3_preact_bn[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_conv (Conv2D)    (None, 38, 38, 128)  65536       conv3_block3_preact_relu[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_bn (BatchNormali (None, 38, 38, 128)  512         conv3_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_relu (Activation (None, 38, 38, 128)  0           conv3_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_pad (ZeroPadding (None, 40, 40, 128)  0           conv3_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_conv (Conv2D)    (None, 38, 38, 128)  147456      conv3_block3_2_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_bn (BatchNormali (None, 38, 38, 128)  512         conv3_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_relu (Activation (None, 38, 38, 128)  0           conv3_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_3_conv (Conv2D)    (None, 38, 38, 512)  66048       conv3_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_out (Add)          (None, 38, 38, 512)  0           conv3_block2_out[0][0]           \n",
      "                                                                 conv3_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_preact_bn (BatchNo (None, 38, 38, 512)  2048        conv3_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_preact_relu (Activ (None, 38, 38, 512)  0           conv3_block4_preact_bn[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_conv (Conv2D)    (None, 38, 38, 128)  65536       conv3_block4_preact_relu[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_bn (BatchNormali (None, 38, 38, 128)  512         conv3_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_relu (Activation (None, 38, 38, 128)  0           conv3_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_pad (ZeroPadding (None, 40, 40, 128)  0           conv3_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_conv (Conv2D)    (None, 38, 38, 128)  147456      conv3_block4_2_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_bn (BatchNormali (None, 38, 38, 128)  512         conv3_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_relu (Activation (None, 38, 38, 128)  0           conv3_block4_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_3_conv (Conv2D)    (None, 38, 38, 512)  66048       conv3_block4_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_out (Add)          (None, 38, 38, 512)  0           conv3_block3_out[0][0]           \n",
      "                                                                 conv3_block4_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block5_preact_bn (BatchNo (None, 38, 38, 512)  2048        conv3_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block5_preact_relu (Activ (None, 38, 38, 512)  0           conv3_block5_preact_bn[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block5_1_conv (Conv2D)    (None, 38, 38, 128)  65536       conv3_block5_preact_relu[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block5_1_bn (BatchNormali (None, 38, 38, 128)  512         conv3_block5_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block5_1_relu (Activation (None, 38, 38, 128)  0           conv3_block5_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block5_2_pad (ZeroPadding (None, 40, 40, 128)  0           conv3_block5_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block5_2_conv (Conv2D)    (None, 38, 38, 128)  147456      conv3_block5_2_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block5_2_bn (BatchNormali (None, 38, 38, 128)  512         conv3_block5_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block5_2_relu (Activation (None, 38, 38, 128)  0           conv3_block5_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block5_3_conv (Conv2D)    (None, 38, 38, 512)  66048       conv3_block5_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block5_out (Add)          (None, 38, 38, 512)  0           conv3_block4_out[0][0]           \n",
      "                                                                 conv3_block5_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block6_preact_bn (BatchNo (None, 38, 38, 512)  2048        conv3_block5_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block6_preact_relu (Activ (None, 38, 38, 512)  0           conv3_block6_preact_bn[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block6_1_conv (Conv2D)    (None, 38, 38, 128)  65536       conv3_block6_preact_relu[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block6_1_bn (BatchNormali (None, 38, 38, 128)  512         conv3_block6_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block6_1_relu (Activation (None, 38, 38, 128)  0           conv3_block6_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block6_2_pad (ZeroPadding (None, 40, 40, 128)  0           conv3_block6_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block6_2_conv (Conv2D)    (None, 38, 38, 128)  147456      conv3_block6_2_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block6_2_bn (BatchNormali (None, 38, 38, 128)  512         conv3_block6_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block6_2_relu (Activation (None, 38, 38, 128)  0           conv3_block6_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block6_3_conv (Conv2D)    (None, 38, 38, 512)  66048       conv3_block6_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block6_out (Add)          (None, 38, 38, 512)  0           conv3_block5_out[0][0]           \n",
      "                                                                 conv3_block6_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block7_preact_bn (BatchNo (None, 38, 38, 512)  2048        conv3_block6_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block7_preact_relu (Activ (None, 38, 38, 512)  0           conv3_block7_preact_bn[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block7_1_conv (Conv2D)    (None, 38, 38, 128)  65536       conv3_block7_preact_relu[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block7_1_bn (BatchNormali (None, 38, 38, 128)  512         conv3_block7_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block7_1_relu (Activation (None, 38, 38, 128)  0           conv3_block7_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block7_2_pad (ZeroPadding (None, 40, 40, 128)  0           conv3_block7_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block7_2_conv (Conv2D)    (None, 38, 38, 128)  147456      conv3_block7_2_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block7_2_bn (BatchNormali (None, 38, 38, 128)  512         conv3_block7_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block7_2_relu (Activation (None, 38, 38, 128)  0           conv3_block7_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block7_3_conv (Conv2D)    (None, 38, 38, 512)  66048       conv3_block7_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block7_out (Add)          (None, 38, 38, 512)  0           conv3_block6_out[0][0]           \n",
      "                                                                 conv3_block7_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block8_preact_bn (BatchNo (None, 38, 38, 512)  2048        conv3_block7_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block8_preact_relu (Activ (None, 38, 38, 512)  0           conv3_block8_preact_bn[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block8_1_conv (Conv2D)    (None, 38, 38, 128)  65536       conv3_block8_preact_relu[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block8_1_bn (BatchNormali (None, 38, 38, 128)  512         conv3_block8_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block8_1_relu (Activation (None, 38, 38, 128)  0           conv3_block8_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block8_2_pad (ZeroPadding (None, 40, 40, 128)  0           conv3_block8_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block8_2_conv (Conv2D)    (None, 19, 19, 128)  147456      conv3_block8_2_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block8_2_bn (BatchNormali (None, 19, 19, 128)  512         conv3_block8_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block8_2_relu (Activation (None, 19, 19, 128)  0           conv3_block8_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2D)  (None, 19, 19, 512)  0           conv3_block7_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block8_3_conv (Conv2D)    (None, 19, 19, 512)  66048       conv3_block8_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block8_out (Add)          (None, 19, 19, 512)  0           max_pooling2d_4[0][0]            \n",
      "                                                                 conv3_block8_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_preact_bn (BatchNo (None, 19, 19, 512)  2048        conv3_block8_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_preact_relu (Activ (None, 19, 19, 512)  0           conv4_block1_preact_bn[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_conv (Conv2D)    (None, 19, 19, 256)  131072      conv4_block1_preact_relu[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_bn (BatchNormali (None, 19, 19, 256)  1024        conv4_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_relu (Activation (None, 19, 19, 256)  0           conv4_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_pad (ZeroPadding (None, 21, 21, 256)  0           conv4_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_conv (Conv2D)    (None, 19, 19, 256)  589824      conv4_block1_2_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_bn (BatchNormali (None, 19, 19, 256)  1024        conv4_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_relu (Activation (None, 19, 19, 256)  0           conv4_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_0_conv (Conv2D)    (None, 19, 19, 1024) 525312      conv4_block1_preact_relu[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_3_conv (Conv2D)    (None, 19, 19, 1024) 263168      conv4_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_out (Add)          (None, 19, 19, 1024) 0           conv4_block1_0_conv[0][0]        \n",
      "                                                                 conv4_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_preact_bn (BatchNo (None, 19, 19, 1024) 4096        conv4_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_preact_relu (Activ (None, 19, 19, 1024) 0           conv4_block2_preact_bn[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_conv (Conv2D)    (None, 19, 19, 256)  262144      conv4_block2_preact_relu[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_bn (BatchNormali (None, 19, 19, 256)  1024        conv4_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_relu (Activation (None, 19, 19, 256)  0           conv4_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_pad (ZeroPadding (None, 21, 21, 256)  0           conv4_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_conv (Conv2D)    (None, 19, 19, 256)  589824      conv4_block2_2_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_bn (BatchNormali (None, 19, 19, 256)  1024        conv4_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_relu (Activation (None, 19, 19, 256)  0           conv4_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_3_conv (Conv2D)    (None, 19, 19, 1024) 263168      conv4_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_out (Add)          (None, 19, 19, 1024) 0           conv4_block1_out[0][0]           \n",
      "                                                                 conv4_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_preact_bn (BatchNo (None, 19, 19, 1024) 4096        conv4_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_preact_relu (Activ (None, 19, 19, 1024) 0           conv4_block3_preact_bn[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_conv (Conv2D)    (None, 19, 19, 256)  262144      conv4_block3_preact_relu[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_bn (BatchNormali (None, 19, 19, 256)  1024        conv4_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_relu (Activation (None, 19, 19, 256)  0           conv4_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_pad (ZeroPadding (None, 21, 21, 256)  0           conv4_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_conv (Conv2D)    (None, 19, 19, 256)  589824      conv4_block3_2_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_bn (BatchNormali (None, 19, 19, 256)  1024        conv4_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_relu (Activation (None, 19, 19, 256)  0           conv4_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_3_conv (Conv2D)    (None, 19, 19, 1024) 263168      conv4_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_out (Add)          (None, 19, 19, 1024) 0           conv4_block2_out[0][0]           \n",
      "                                                                 conv4_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_preact_bn (BatchNo (None, 19, 19, 1024) 4096        conv4_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_preact_relu (Activ (None, 19, 19, 1024) 0           conv4_block4_preact_bn[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_conv (Conv2D)    (None, 19, 19, 256)  262144      conv4_block4_preact_relu[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_bn (BatchNormali (None, 19, 19, 256)  1024        conv4_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_relu (Activation (None, 19, 19, 256)  0           conv4_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_pad (ZeroPadding (None, 21, 21, 256)  0           conv4_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_conv (Conv2D)    (None, 19, 19, 256)  589824      conv4_block4_2_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_bn (BatchNormali (None, 19, 19, 256)  1024        conv4_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_relu (Activation (None, 19, 19, 256)  0           conv4_block4_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_3_conv (Conv2D)    (None, 19, 19, 1024) 263168      conv4_block4_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_out (Add)          (None, 19, 19, 1024) 0           conv4_block3_out[0][0]           \n",
      "                                                                 conv4_block4_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_preact_bn (BatchNo (None, 19, 19, 1024) 4096        conv4_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_preact_relu (Activ (None, 19, 19, 1024) 0           conv4_block5_preact_bn[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_conv (Conv2D)    (None, 19, 19, 256)  262144      conv4_block5_preact_relu[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_bn (BatchNormali (None, 19, 19, 256)  1024        conv4_block5_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_relu (Activation (None, 19, 19, 256)  0           conv4_block5_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_pad (ZeroPadding (None, 21, 21, 256)  0           conv4_block5_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_conv (Conv2D)    (None, 19, 19, 256)  589824      conv4_block5_2_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_bn (BatchNormali (None, 19, 19, 256)  1024        conv4_block5_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_relu (Activation (None, 19, 19, 256)  0           conv4_block5_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_3_conv (Conv2D)    (None, 19, 19, 1024) 263168      conv4_block5_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_out (Add)          (None, 19, 19, 1024) 0           conv4_block4_out[0][0]           \n",
      "                                                                 conv4_block5_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_preact_bn (BatchNo (None, 19, 19, 1024) 4096        conv4_block5_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_preact_relu (Activ (None, 19, 19, 1024) 0           conv4_block6_preact_bn[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_conv (Conv2D)    (None, 19, 19, 256)  262144      conv4_block6_preact_relu[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_bn (BatchNormali (None, 19, 19, 256)  1024        conv4_block6_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_relu (Activation (None, 19, 19, 256)  0           conv4_block6_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_pad (ZeroPadding (None, 21, 21, 256)  0           conv4_block6_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_conv (Conv2D)    (None, 19, 19, 256)  589824      conv4_block6_2_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_bn (BatchNormali (None, 19, 19, 256)  1024        conv4_block6_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_relu (Activation (None, 19, 19, 256)  0           conv4_block6_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_3_conv (Conv2D)    (None, 19, 19, 1024) 263168      conv4_block6_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_out (Add)          (None, 19, 19, 1024) 0           conv4_block5_out[0][0]           \n",
      "                                                                 conv4_block6_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block7_preact_bn (BatchNo (None, 19, 19, 1024) 4096        conv4_block6_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block7_preact_relu (Activ (None, 19, 19, 1024) 0           conv4_block7_preact_bn[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block7_1_conv (Conv2D)    (None, 19, 19, 256)  262144      conv4_block7_preact_relu[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block7_1_bn (BatchNormali (None, 19, 19, 256)  1024        conv4_block7_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block7_1_relu (Activation (None, 19, 19, 256)  0           conv4_block7_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block7_2_pad (ZeroPadding (None, 21, 21, 256)  0           conv4_block7_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block7_2_conv (Conv2D)    (None, 19, 19, 256)  589824      conv4_block7_2_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block7_2_bn (BatchNormali (None, 19, 19, 256)  1024        conv4_block7_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block7_2_relu (Activation (None, 19, 19, 256)  0           conv4_block7_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block7_3_conv (Conv2D)    (None, 19, 19, 1024) 263168      conv4_block7_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block7_out (Add)          (None, 19, 19, 1024) 0           conv4_block6_out[0][0]           \n",
      "                                                                 conv4_block7_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block8_preact_bn (BatchNo (None, 19, 19, 1024) 4096        conv4_block7_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block8_preact_relu (Activ (None, 19, 19, 1024) 0           conv4_block8_preact_bn[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block8_1_conv (Conv2D)    (None, 19, 19, 256)  262144      conv4_block8_preact_relu[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block8_1_bn (BatchNormali (None, 19, 19, 256)  1024        conv4_block8_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block8_1_relu (Activation (None, 19, 19, 256)  0           conv4_block8_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block8_2_pad (ZeroPadding (None, 21, 21, 256)  0           conv4_block8_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block8_2_conv (Conv2D)    (None, 19, 19, 256)  589824      conv4_block8_2_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block8_2_bn (BatchNormali (None, 19, 19, 256)  1024        conv4_block8_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block8_2_relu (Activation (None, 19, 19, 256)  0           conv4_block8_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block8_3_conv (Conv2D)    (None, 19, 19, 1024) 263168      conv4_block8_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block8_out (Add)          (None, 19, 19, 1024) 0           conv4_block7_out[0][0]           \n",
      "                                                                 conv4_block8_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block9_preact_bn (BatchNo (None, 19, 19, 1024) 4096        conv4_block8_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block9_preact_relu (Activ (None, 19, 19, 1024) 0           conv4_block9_preact_bn[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block9_1_conv (Conv2D)    (None, 19, 19, 256)  262144      conv4_block9_preact_relu[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block9_1_bn (BatchNormali (None, 19, 19, 256)  1024        conv4_block9_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block9_1_relu (Activation (None, 19, 19, 256)  0           conv4_block9_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block9_2_pad (ZeroPadding (None, 21, 21, 256)  0           conv4_block9_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block9_2_conv (Conv2D)    (None, 19, 19, 256)  589824      conv4_block9_2_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block9_2_bn (BatchNormali (None, 19, 19, 256)  1024        conv4_block9_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block9_2_relu (Activation (None, 19, 19, 256)  0           conv4_block9_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block9_3_conv (Conv2D)    (None, 19, 19, 1024) 263168      conv4_block9_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block9_out (Add)          (None, 19, 19, 1024) 0           conv4_block8_out[0][0]           \n",
      "                                                                 conv4_block9_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block10_preact_bn (BatchN (None, 19, 19, 1024) 4096        conv4_block9_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block10_preact_relu (Acti (None, 19, 19, 1024) 0           conv4_block10_preact_bn[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block10_1_conv (Conv2D)   (None, 19, 19, 256)  262144      conv4_block10_preact_relu[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block10_1_bn (BatchNormal (None, 19, 19, 256)  1024        conv4_block10_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block10_1_relu (Activatio (None, 19, 19, 256)  0           conv4_block10_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block10_2_pad (ZeroPaddin (None, 21, 21, 256)  0           conv4_block10_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block10_2_conv (Conv2D)   (None, 19, 19, 256)  589824      conv4_block10_2_pad[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block10_2_bn (BatchNormal (None, 19, 19, 256)  1024        conv4_block10_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block10_2_relu (Activatio (None, 19, 19, 256)  0           conv4_block10_2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block10_3_conv (Conv2D)   (None, 19, 19, 1024) 263168      conv4_block10_2_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block10_out (Add)         (None, 19, 19, 1024) 0           conv4_block9_out[0][0]           \n",
      "                                                                 conv4_block10_3_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block11_preact_bn (BatchN (None, 19, 19, 1024) 4096        conv4_block10_out[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block11_preact_relu (Acti (None, 19, 19, 1024) 0           conv4_block11_preact_bn[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block11_1_conv (Conv2D)   (None, 19, 19, 256)  262144      conv4_block11_preact_relu[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block11_1_bn (BatchNormal (None, 19, 19, 256)  1024        conv4_block11_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block11_1_relu (Activatio (None, 19, 19, 256)  0           conv4_block11_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block11_2_pad (ZeroPaddin (None, 21, 21, 256)  0           conv4_block11_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block11_2_conv (Conv2D)   (None, 19, 19, 256)  589824      conv4_block11_2_pad[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block11_2_bn (BatchNormal (None, 19, 19, 256)  1024        conv4_block11_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block11_2_relu (Activatio (None, 19, 19, 256)  0           conv4_block11_2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block11_3_conv (Conv2D)   (None, 19, 19, 1024) 263168      conv4_block11_2_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block11_out (Add)         (None, 19, 19, 1024) 0           conv4_block10_out[0][0]          \n",
      "                                                                 conv4_block11_3_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block12_preact_bn (BatchN (None, 19, 19, 1024) 4096        conv4_block11_out[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block12_preact_relu (Acti (None, 19, 19, 1024) 0           conv4_block12_preact_bn[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block12_1_conv (Conv2D)   (None, 19, 19, 256)  262144      conv4_block12_preact_relu[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block12_1_bn (BatchNormal (None, 19, 19, 256)  1024        conv4_block12_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block12_1_relu (Activatio (None, 19, 19, 256)  0           conv4_block12_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block12_2_pad (ZeroPaddin (None, 21, 21, 256)  0           conv4_block12_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block12_2_conv (Conv2D)   (None, 19, 19, 256)  589824      conv4_block12_2_pad[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block12_2_bn (BatchNormal (None, 19, 19, 256)  1024        conv4_block12_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block12_2_relu (Activatio (None, 19, 19, 256)  0           conv4_block12_2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block12_3_conv (Conv2D)   (None, 19, 19, 1024) 263168      conv4_block12_2_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block12_out (Add)         (None, 19, 19, 1024) 0           conv4_block11_out[0][0]          \n",
      "                                                                 conv4_block12_3_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block13_preact_bn (BatchN (None, 19, 19, 1024) 4096        conv4_block12_out[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block13_preact_relu (Acti (None, 19, 19, 1024) 0           conv4_block13_preact_bn[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block13_1_conv (Conv2D)   (None, 19, 19, 256)  262144      conv4_block13_preact_relu[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block13_1_bn (BatchNormal (None, 19, 19, 256)  1024        conv4_block13_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block13_1_relu (Activatio (None, 19, 19, 256)  0           conv4_block13_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block13_2_pad (ZeroPaddin (None, 21, 21, 256)  0           conv4_block13_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block13_2_conv (Conv2D)   (None, 19, 19, 256)  589824      conv4_block13_2_pad[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block13_2_bn (BatchNormal (None, 19, 19, 256)  1024        conv4_block13_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block13_2_relu (Activatio (None, 19, 19, 256)  0           conv4_block13_2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block13_3_conv (Conv2D)   (None, 19, 19, 1024) 263168      conv4_block13_2_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block13_out (Add)         (None, 19, 19, 1024) 0           conv4_block12_out[0][0]          \n",
      "                                                                 conv4_block13_3_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block14_preact_bn (BatchN (None, 19, 19, 1024) 4096        conv4_block13_out[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block14_preact_relu (Acti (None, 19, 19, 1024) 0           conv4_block14_preact_bn[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block14_1_conv (Conv2D)   (None, 19, 19, 256)  262144      conv4_block14_preact_relu[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block14_1_bn (BatchNormal (None, 19, 19, 256)  1024        conv4_block14_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block14_1_relu (Activatio (None, 19, 19, 256)  0           conv4_block14_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block14_2_pad (ZeroPaddin (None, 21, 21, 256)  0           conv4_block14_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block14_2_conv (Conv2D)   (None, 19, 19, 256)  589824      conv4_block14_2_pad[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block14_2_bn (BatchNormal (None, 19, 19, 256)  1024        conv4_block14_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block14_2_relu (Activatio (None, 19, 19, 256)  0           conv4_block14_2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block14_3_conv (Conv2D)   (None, 19, 19, 1024) 263168      conv4_block14_2_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block14_out (Add)         (None, 19, 19, 1024) 0           conv4_block13_out[0][0]          \n",
      "                                                                 conv4_block14_3_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block15_preact_bn (BatchN (None, 19, 19, 1024) 4096        conv4_block14_out[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block15_preact_relu (Acti (None, 19, 19, 1024) 0           conv4_block15_preact_bn[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block15_1_conv (Conv2D)   (None, 19, 19, 256)  262144      conv4_block15_preact_relu[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block15_1_bn (BatchNormal (None, 19, 19, 256)  1024        conv4_block15_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block15_1_relu (Activatio (None, 19, 19, 256)  0           conv4_block15_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block15_2_pad (ZeroPaddin (None, 21, 21, 256)  0           conv4_block15_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block15_2_conv (Conv2D)   (None, 19, 19, 256)  589824      conv4_block15_2_pad[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block15_2_bn (BatchNormal (None, 19, 19, 256)  1024        conv4_block15_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block15_2_relu (Activatio (None, 19, 19, 256)  0           conv4_block15_2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block15_3_conv (Conv2D)   (None, 19, 19, 1024) 263168      conv4_block15_2_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block15_out (Add)         (None, 19, 19, 1024) 0           conv4_block14_out[0][0]          \n",
      "                                                                 conv4_block15_3_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block16_preact_bn (BatchN (None, 19, 19, 1024) 4096        conv4_block15_out[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block16_preact_relu (Acti (None, 19, 19, 1024) 0           conv4_block16_preact_bn[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block16_1_conv (Conv2D)   (None, 19, 19, 256)  262144      conv4_block16_preact_relu[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block16_1_bn (BatchNormal (None, 19, 19, 256)  1024        conv4_block16_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block16_1_relu (Activatio (None, 19, 19, 256)  0           conv4_block16_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block16_2_pad (ZeroPaddin (None, 21, 21, 256)  0           conv4_block16_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block16_2_conv (Conv2D)   (None, 19, 19, 256)  589824      conv4_block16_2_pad[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block16_2_bn (BatchNormal (None, 19, 19, 256)  1024        conv4_block16_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block16_2_relu (Activatio (None, 19, 19, 256)  0           conv4_block16_2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block16_3_conv (Conv2D)   (None, 19, 19, 1024) 263168      conv4_block16_2_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block16_out (Add)         (None, 19, 19, 1024) 0           conv4_block15_out[0][0]          \n",
      "                                                                 conv4_block16_3_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block17_preact_bn (BatchN (None, 19, 19, 1024) 4096        conv4_block16_out[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block17_preact_relu (Acti (None, 19, 19, 1024) 0           conv4_block17_preact_bn[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block17_1_conv (Conv2D)   (None, 19, 19, 256)  262144      conv4_block17_preact_relu[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block17_1_bn (BatchNormal (None, 19, 19, 256)  1024        conv4_block17_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block17_1_relu (Activatio (None, 19, 19, 256)  0           conv4_block17_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block17_2_pad (ZeroPaddin (None, 21, 21, 256)  0           conv4_block17_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block17_2_conv (Conv2D)   (None, 19, 19, 256)  589824      conv4_block17_2_pad[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block17_2_bn (BatchNormal (None, 19, 19, 256)  1024        conv4_block17_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block17_2_relu (Activatio (None, 19, 19, 256)  0           conv4_block17_2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block17_3_conv (Conv2D)   (None, 19, 19, 1024) 263168      conv4_block17_2_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block17_out (Add)         (None, 19, 19, 1024) 0           conv4_block16_out[0][0]          \n",
      "                                                                 conv4_block17_3_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block18_preact_bn (BatchN (None, 19, 19, 1024) 4096        conv4_block17_out[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block18_preact_relu (Acti (None, 19, 19, 1024) 0           conv4_block18_preact_bn[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block18_1_conv (Conv2D)   (None, 19, 19, 256)  262144      conv4_block18_preact_relu[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block18_1_bn (BatchNormal (None, 19, 19, 256)  1024        conv4_block18_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block18_1_relu (Activatio (None, 19, 19, 256)  0           conv4_block18_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block18_2_pad (ZeroPaddin (None, 21, 21, 256)  0           conv4_block18_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block18_2_conv (Conv2D)   (None, 19, 19, 256)  589824      conv4_block18_2_pad[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block18_2_bn (BatchNormal (None, 19, 19, 256)  1024        conv4_block18_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block18_2_relu (Activatio (None, 19, 19, 256)  0           conv4_block18_2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block18_3_conv (Conv2D)   (None, 19, 19, 1024) 263168      conv4_block18_2_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block18_out (Add)         (None, 19, 19, 1024) 0           conv4_block17_out[0][0]          \n",
      "                                                                 conv4_block18_3_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block19_preact_bn (BatchN (None, 19, 19, 1024) 4096        conv4_block18_out[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block19_preact_relu (Acti (None, 19, 19, 1024) 0           conv4_block19_preact_bn[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block19_1_conv (Conv2D)   (None, 19, 19, 256)  262144      conv4_block19_preact_relu[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block19_1_bn (BatchNormal (None, 19, 19, 256)  1024        conv4_block19_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block19_1_relu (Activatio (None, 19, 19, 256)  0           conv4_block19_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block19_2_pad (ZeroPaddin (None, 21, 21, 256)  0           conv4_block19_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block19_2_conv (Conv2D)   (None, 19, 19, 256)  589824      conv4_block19_2_pad[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block19_2_bn (BatchNormal (None, 19, 19, 256)  1024        conv4_block19_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block19_2_relu (Activatio (None, 19, 19, 256)  0           conv4_block19_2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block19_3_conv (Conv2D)   (None, 19, 19, 1024) 263168      conv4_block19_2_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block19_out (Add)         (None, 19, 19, 1024) 0           conv4_block18_out[0][0]          \n",
      "                                                                 conv4_block19_3_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block20_preact_bn (BatchN (None, 19, 19, 1024) 4096        conv4_block19_out[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block20_preact_relu (Acti (None, 19, 19, 1024) 0           conv4_block20_preact_bn[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block20_1_conv (Conv2D)   (None, 19, 19, 256)  262144      conv4_block20_preact_relu[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block20_1_bn (BatchNormal (None, 19, 19, 256)  1024        conv4_block20_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block20_1_relu (Activatio (None, 19, 19, 256)  0           conv4_block20_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block20_2_pad (ZeroPaddin (None, 21, 21, 256)  0           conv4_block20_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block20_2_conv (Conv2D)   (None, 19, 19, 256)  589824      conv4_block20_2_pad[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block20_2_bn (BatchNormal (None, 19, 19, 256)  1024        conv4_block20_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block20_2_relu (Activatio (None, 19, 19, 256)  0           conv4_block20_2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block20_3_conv (Conv2D)   (None, 19, 19, 1024) 263168      conv4_block20_2_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block20_out (Add)         (None, 19, 19, 1024) 0           conv4_block19_out[0][0]          \n",
      "                                                                 conv4_block20_3_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block21_preact_bn (BatchN (None, 19, 19, 1024) 4096        conv4_block20_out[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block21_preact_relu (Acti (None, 19, 19, 1024) 0           conv4_block21_preact_bn[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block21_1_conv (Conv2D)   (None, 19, 19, 256)  262144      conv4_block21_preact_relu[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block21_1_bn (BatchNormal (None, 19, 19, 256)  1024        conv4_block21_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block21_1_relu (Activatio (None, 19, 19, 256)  0           conv4_block21_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block21_2_pad (ZeroPaddin (None, 21, 21, 256)  0           conv4_block21_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block21_2_conv (Conv2D)   (None, 19, 19, 256)  589824      conv4_block21_2_pad[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block21_2_bn (BatchNormal (None, 19, 19, 256)  1024        conv4_block21_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block21_2_relu (Activatio (None, 19, 19, 256)  0           conv4_block21_2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block21_3_conv (Conv2D)   (None, 19, 19, 1024) 263168      conv4_block21_2_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block21_out (Add)         (None, 19, 19, 1024) 0           conv4_block20_out[0][0]          \n",
      "                                                                 conv4_block21_3_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block22_preact_bn (BatchN (None, 19, 19, 1024) 4096        conv4_block21_out[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block22_preact_relu (Acti (None, 19, 19, 1024) 0           conv4_block22_preact_bn[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block22_1_conv (Conv2D)   (None, 19, 19, 256)  262144      conv4_block22_preact_relu[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block22_1_bn (BatchNormal (None, 19, 19, 256)  1024        conv4_block22_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block22_1_relu (Activatio (None, 19, 19, 256)  0           conv4_block22_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block22_2_pad (ZeroPaddin (None, 21, 21, 256)  0           conv4_block22_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block22_2_conv (Conv2D)   (None, 19, 19, 256)  589824      conv4_block22_2_pad[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block22_2_bn (BatchNormal (None, 19, 19, 256)  1024        conv4_block22_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block22_2_relu (Activatio (None, 19, 19, 256)  0           conv4_block22_2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block22_3_conv (Conv2D)   (None, 19, 19, 1024) 263168      conv4_block22_2_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block22_out (Add)         (None, 19, 19, 1024) 0           conv4_block21_out[0][0]          \n",
      "                                                                 conv4_block22_3_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block23_preact_bn (BatchN (None, 19, 19, 1024) 4096        conv4_block22_out[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block23_preact_relu (Acti (None, 19, 19, 1024) 0           conv4_block23_preact_bn[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block23_1_conv (Conv2D)   (None, 19, 19, 256)  262144      conv4_block23_preact_relu[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block23_1_bn (BatchNormal (None, 19, 19, 256)  1024        conv4_block23_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block23_1_relu (Activatio (None, 19, 19, 256)  0           conv4_block23_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block23_2_pad (ZeroPaddin (None, 21, 21, 256)  0           conv4_block23_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block23_2_conv (Conv2D)   (None, 19, 19, 256)  589824      conv4_block23_2_pad[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block23_2_bn (BatchNormal (None, 19, 19, 256)  1024        conv4_block23_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block23_2_relu (Activatio (None, 19, 19, 256)  0           conv4_block23_2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block23_3_conv (Conv2D)   (None, 19, 19, 1024) 263168      conv4_block23_2_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block23_out (Add)         (None, 19, 19, 1024) 0           conv4_block22_out[0][0]          \n",
      "                                                                 conv4_block23_3_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block24_preact_bn (BatchN (None, 19, 19, 1024) 4096        conv4_block23_out[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block24_preact_relu (Acti (None, 19, 19, 1024) 0           conv4_block24_preact_bn[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block24_1_conv (Conv2D)   (None, 19, 19, 256)  262144      conv4_block24_preact_relu[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block24_1_bn (BatchNormal (None, 19, 19, 256)  1024        conv4_block24_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block24_1_relu (Activatio (None, 19, 19, 256)  0           conv4_block24_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block24_2_pad (ZeroPaddin (None, 21, 21, 256)  0           conv4_block24_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block24_2_conv (Conv2D)   (None, 19, 19, 256)  589824      conv4_block24_2_pad[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block24_2_bn (BatchNormal (None, 19, 19, 256)  1024        conv4_block24_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block24_2_relu (Activatio (None, 19, 19, 256)  0           conv4_block24_2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block24_3_conv (Conv2D)   (None, 19, 19, 1024) 263168      conv4_block24_2_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block24_out (Add)         (None, 19, 19, 1024) 0           conv4_block23_out[0][0]          \n",
      "                                                                 conv4_block24_3_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block25_preact_bn (BatchN (None, 19, 19, 1024) 4096        conv4_block24_out[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block25_preact_relu (Acti (None, 19, 19, 1024) 0           conv4_block25_preact_bn[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block25_1_conv (Conv2D)   (None, 19, 19, 256)  262144      conv4_block25_preact_relu[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block25_1_bn (BatchNormal (None, 19, 19, 256)  1024        conv4_block25_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block25_1_relu (Activatio (None, 19, 19, 256)  0           conv4_block25_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block25_2_pad (ZeroPaddin (None, 21, 21, 256)  0           conv4_block25_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block25_2_conv (Conv2D)   (None, 19, 19, 256)  589824      conv4_block25_2_pad[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block25_2_bn (BatchNormal (None, 19, 19, 256)  1024        conv4_block25_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block25_2_relu (Activatio (None, 19, 19, 256)  0           conv4_block25_2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block25_3_conv (Conv2D)   (None, 19, 19, 1024) 263168      conv4_block25_2_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block25_out (Add)         (None, 19, 19, 1024) 0           conv4_block24_out[0][0]          \n",
      "                                                                 conv4_block25_3_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block26_preact_bn (BatchN (None, 19, 19, 1024) 4096        conv4_block25_out[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block26_preact_relu (Acti (None, 19, 19, 1024) 0           conv4_block26_preact_bn[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block26_1_conv (Conv2D)   (None, 19, 19, 256)  262144      conv4_block26_preact_relu[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block26_1_bn (BatchNormal (None, 19, 19, 256)  1024        conv4_block26_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block26_1_relu (Activatio (None, 19, 19, 256)  0           conv4_block26_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block26_2_pad (ZeroPaddin (None, 21, 21, 256)  0           conv4_block26_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block26_2_conv (Conv2D)   (None, 19, 19, 256)  589824      conv4_block26_2_pad[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block26_2_bn (BatchNormal (None, 19, 19, 256)  1024        conv4_block26_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block26_2_relu (Activatio (None, 19, 19, 256)  0           conv4_block26_2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block26_3_conv (Conv2D)   (None, 19, 19, 1024) 263168      conv4_block26_2_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block26_out (Add)         (None, 19, 19, 1024) 0           conv4_block25_out[0][0]          \n",
      "                                                                 conv4_block26_3_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block27_preact_bn (BatchN (None, 19, 19, 1024) 4096        conv4_block26_out[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block27_preact_relu (Acti (None, 19, 19, 1024) 0           conv4_block27_preact_bn[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block27_1_conv (Conv2D)   (None, 19, 19, 256)  262144      conv4_block27_preact_relu[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block27_1_bn (BatchNormal (None, 19, 19, 256)  1024        conv4_block27_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block27_1_relu (Activatio (None, 19, 19, 256)  0           conv4_block27_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block27_2_pad (ZeroPaddin (None, 21, 21, 256)  0           conv4_block27_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block27_2_conv (Conv2D)   (None, 19, 19, 256)  589824      conv4_block27_2_pad[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block27_2_bn (BatchNormal (None, 19, 19, 256)  1024        conv4_block27_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block27_2_relu (Activatio (None, 19, 19, 256)  0           conv4_block27_2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block27_3_conv (Conv2D)   (None, 19, 19, 1024) 263168      conv4_block27_2_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block27_out (Add)         (None, 19, 19, 1024) 0           conv4_block26_out[0][0]          \n",
      "                                                                 conv4_block27_3_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block28_preact_bn (BatchN (None, 19, 19, 1024) 4096        conv4_block27_out[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block28_preact_relu (Acti (None, 19, 19, 1024) 0           conv4_block28_preact_bn[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block28_1_conv (Conv2D)   (None, 19, 19, 256)  262144      conv4_block28_preact_relu[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block28_1_bn (BatchNormal (None, 19, 19, 256)  1024        conv4_block28_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block28_1_relu (Activatio (None, 19, 19, 256)  0           conv4_block28_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block28_2_pad (ZeroPaddin (None, 21, 21, 256)  0           conv4_block28_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block28_2_conv (Conv2D)   (None, 19, 19, 256)  589824      conv4_block28_2_pad[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block28_2_bn (BatchNormal (None, 19, 19, 256)  1024        conv4_block28_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block28_2_relu (Activatio (None, 19, 19, 256)  0           conv4_block28_2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block28_3_conv (Conv2D)   (None, 19, 19, 1024) 263168      conv4_block28_2_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block28_out (Add)         (None, 19, 19, 1024) 0           conv4_block27_out[0][0]          \n",
      "                                                                 conv4_block28_3_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block29_preact_bn (BatchN (None, 19, 19, 1024) 4096        conv4_block28_out[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block29_preact_relu (Acti (None, 19, 19, 1024) 0           conv4_block29_preact_bn[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block29_1_conv (Conv2D)   (None, 19, 19, 256)  262144      conv4_block29_preact_relu[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block29_1_bn (BatchNormal (None, 19, 19, 256)  1024        conv4_block29_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block29_1_relu (Activatio (None, 19, 19, 256)  0           conv4_block29_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block29_2_pad (ZeroPaddin (None, 21, 21, 256)  0           conv4_block29_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block29_2_conv (Conv2D)   (None, 19, 19, 256)  589824      conv4_block29_2_pad[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block29_2_bn (BatchNormal (None, 19, 19, 256)  1024        conv4_block29_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block29_2_relu (Activatio (None, 19, 19, 256)  0           conv4_block29_2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block29_3_conv (Conv2D)   (None, 19, 19, 1024) 263168      conv4_block29_2_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block29_out (Add)         (None, 19, 19, 1024) 0           conv4_block28_out[0][0]          \n",
      "                                                                 conv4_block29_3_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block30_preact_bn (BatchN (None, 19, 19, 1024) 4096        conv4_block29_out[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block30_preact_relu (Acti (None, 19, 19, 1024) 0           conv4_block30_preact_bn[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block30_1_conv (Conv2D)   (None, 19, 19, 256)  262144      conv4_block30_preact_relu[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block30_1_bn (BatchNormal (None, 19, 19, 256)  1024        conv4_block30_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block30_1_relu (Activatio (None, 19, 19, 256)  0           conv4_block30_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block30_2_pad (ZeroPaddin (None, 21, 21, 256)  0           conv4_block30_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block30_2_conv (Conv2D)   (None, 19, 19, 256)  589824      conv4_block30_2_pad[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block30_2_bn (BatchNormal (None, 19, 19, 256)  1024        conv4_block30_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block30_2_relu (Activatio (None, 19, 19, 256)  0           conv4_block30_2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block30_3_conv (Conv2D)   (None, 19, 19, 1024) 263168      conv4_block30_2_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block30_out (Add)         (None, 19, 19, 1024) 0           conv4_block29_out[0][0]          \n",
      "                                                                 conv4_block30_3_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block31_preact_bn (BatchN (None, 19, 19, 1024) 4096        conv4_block30_out[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block31_preact_relu (Acti (None, 19, 19, 1024) 0           conv4_block31_preact_bn[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block31_1_conv (Conv2D)   (None, 19, 19, 256)  262144      conv4_block31_preact_relu[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block31_1_bn (BatchNormal (None, 19, 19, 256)  1024        conv4_block31_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block31_1_relu (Activatio (None, 19, 19, 256)  0           conv4_block31_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block31_2_pad (ZeroPaddin (None, 21, 21, 256)  0           conv4_block31_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block31_2_conv (Conv2D)   (None, 19, 19, 256)  589824      conv4_block31_2_pad[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block31_2_bn (BatchNormal (None, 19, 19, 256)  1024        conv4_block31_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block31_2_relu (Activatio (None, 19, 19, 256)  0           conv4_block31_2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block31_3_conv (Conv2D)   (None, 19, 19, 1024) 263168      conv4_block31_2_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block31_out (Add)         (None, 19, 19, 1024) 0           conv4_block30_out[0][0]          \n",
      "                                                                 conv4_block31_3_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block32_preact_bn (BatchN (None, 19, 19, 1024) 4096        conv4_block31_out[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block32_preact_relu (Acti (None, 19, 19, 1024) 0           conv4_block32_preact_bn[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block32_1_conv (Conv2D)   (None, 19, 19, 256)  262144      conv4_block32_preact_relu[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block32_1_bn (BatchNormal (None, 19, 19, 256)  1024        conv4_block32_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block32_1_relu (Activatio (None, 19, 19, 256)  0           conv4_block32_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block32_2_pad (ZeroPaddin (None, 21, 21, 256)  0           conv4_block32_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block32_2_conv (Conv2D)   (None, 19, 19, 256)  589824      conv4_block32_2_pad[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block32_2_bn (BatchNormal (None, 19, 19, 256)  1024        conv4_block32_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block32_2_relu (Activatio (None, 19, 19, 256)  0           conv4_block32_2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block32_3_conv (Conv2D)   (None, 19, 19, 1024) 263168      conv4_block32_2_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block32_out (Add)         (None, 19, 19, 1024) 0           conv4_block31_out[0][0]          \n",
      "                                                                 conv4_block32_3_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block33_preact_bn (BatchN (None, 19, 19, 1024) 4096        conv4_block32_out[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block33_preact_relu (Acti (None, 19, 19, 1024) 0           conv4_block33_preact_bn[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block33_1_conv (Conv2D)   (None, 19, 19, 256)  262144      conv4_block33_preact_relu[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block33_1_bn (BatchNormal (None, 19, 19, 256)  1024        conv4_block33_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block33_1_relu (Activatio (None, 19, 19, 256)  0           conv4_block33_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block33_2_pad (ZeroPaddin (None, 21, 21, 256)  0           conv4_block33_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block33_2_conv (Conv2D)   (None, 19, 19, 256)  589824      conv4_block33_2_pad[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block33_2_bn (BatchNormal (None, 19, 19, 256)  1024        conv4_block33_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block33_2_relu (Activatio (None, 19, 19, 256)  0           conv4_block33_2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block33_3_conv (Conv2D)   (None, 19, 19, 1024) 263168      conv4_block33_2_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block33_out (Add)         (None, 19, 19, 1024) 0           conv4_block32_out[0][0]          \n",
      "                                                                 conv4_block33_3_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block34_preact_bn (BatchN (None, 19, 19, 1024) 4096        conv4_block33_out[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block34_preact_relu (Acti (None, 19, 19, 1024) 0           conv4_block34_preact_bn[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block34_1_conv (Conv2D)   (None, 19, 19, 256)  262144      conv4_block34_preact_relu[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block34_1_bn (BatchNormal (None, 19, 19, 256)  1024        conv4_block34_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block34_1_relu (Activatio (None, 19, 19, 256)  0           conv4_block34_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block34_2_pad (ZeroPaddin (None, 21, 21, 256)  0           conv4_block34_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block34_2_conv (Conv2D)   (None, 19, 19, 256)  589824      conv4_block34_2_pad[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block34_2_bn (BatchNormal (None, 19, 19, 256)  1024        conv4_block34_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block34_2_relu (Activatio (None, 19, 19, 256)  0           conv4_block34_2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block34_3_conv (Conv2D)   (None, 19, 19, 1024) 263168      conv4_block34_2_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block34_out (Add)         (None, 19, 19, 1024) 0           conv4_block33_out[0][0]          \n",
      "                                                                 conv4_block34_3_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block35_preact_bn (BatchN (None, 19, 19, 1024) 4096        conv4_block34_out[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block35_preact_relu (Acti (None, 19, 19, 1024) 0           conv4_block35_preact_bn[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block35_1_conv (Conv2D)   (None, 19, 19, 256)  262144      conv4_block35_preact_relu[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block35_1_bn (BatchNormal (None, 19, 19, 256)  1024        conv4_block35_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block35_1_relu (Activatio (None, 19, 19, 256)  0           conv4_block35_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block35_2_pad (ZeroPaddin (None, 21, 21, 256)  0           conv4_block35_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block35_2_conv (Conv2D)   (None, 19, 19, 256)  589824      conv4_block35_2_pad[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block35_2_bn (BatchNormal (None, 19, 19, 256)  1024        conv4_block35_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block35_2_relu (Activatio (None, 19, 19, 256)  0           conv4_block35_2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block35_3_conv (Conv2D)   (None, 19, 19, 1024) 263168      conv4_block35_2_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block35_out (Add)         (None, 19, 19, 1024) 0           conv4_block34_out[0][0]          \n",
      "                                                                 conv4_block35_3_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block36_preact_bn (BatchN (None, 19, 19, 1024) 4096        conv4_block35_out[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block36_preact_relu (Acti (None, 19, 19, 1024) 0           conv4_block36_preact_bn[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block36_1_conv (Conv2D)   (None, 19, 19, 256)  262144      conv4_block36_preact_relu[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block36_1_bn (BatchNormal (None, 19, 19, 256)  1024        conv4_block36_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block36_1_relu (Activatio (None, 19, 19, 256)  0           conv4_block36_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block36_2_pad (ZeroPaddin (None, 21, 21, 256)  0           conv4_block36_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block36_2_conv (Conv2D)   (None, 10, 10, 256)  589824      conv4_block36_2_pad[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block36_2_bn (BatchNormal (None, 10, 10, 256)  1024        conv4_block36_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block36_2_relu (Activatio (None, 10, 10, 256)  0           conv4_block36_2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2D)  (None, 10, 10, 1024) 0           conv4_block35_out[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block36_3_conv (Conv2D)   (None, 10, 10, 1024) 263168      conv4_block36_2_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block36_out (Add)         (None, 10, 10, 1024) 0           max_pooling2d_5[0][0]            \n",
      "                                                                 conv4_block36_3_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_preact_bn (BatchNo (None, 10, 10, 1024) 4096        conv4_block36_out[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_preact_relu (Activ (None, 10, 10, 1024) 0           conv5_block1_preact_bn[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_conv (Conv2D)    (None, 10, 10, 512)  524288      conv5_block1_preact_relu[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_bn (BatchNormali (None, 10, 10, 512)  2048        conv5_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_relu (Activation (None, 10, 10, 512)  0           conv5_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_pad (ZeroPadding (None, 12, 12, 512)  0           conv5_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_conv (Conv2D)    (None, 10, 10, 512)  2359296     conv5_block1_2_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_bn (BatchNormali (None, 10, 10, 512)  2048        conv5_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_relu (Activation (None, 10, 10, 512)  0           conv5_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_0_conv (Conv2D)    (None, 10, 10, 2048) 2099200     conv5_block1_preact_relu[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_3_conv (Conv2D)    (None, 10, 10, 2048) 1050624     conv5_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_out (Add)          (None, 10, 10, 2048) 0           conv5_block1_0_conv[0][0]        \n",
      "                                                                 conv5_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_preact_bn (BatchNo (None, 10, 10, 2048) 8192        conv5_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_preact_relu (Activ (None, 10, 10, 2048) 0           conv5_block2_preact_bn[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_conv (Conv2D)    (None, 10, 10, 512)  1048576     conv5_block2_preact_relu[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_bn (BatchNormali (None, 10, 10, 512)  2048        conv5_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_relu (Activation (None, 10, 10, 512)  0           conv5_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_pad (ZeroPadding (None, 12, 12, 512)  0           conv5_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_conv (Conv2D)    (None, 10, 10, 512)  2359296     conv5_block2_2_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_bn (BatchNormali (None, 10, 10, 512)  2048        conv5_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_relu (Activation (None, 10, 10, 512)  0           conv5_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_3_conv (Conv2D)    (None, 10, 10, 2048) 1050624     conv5_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_out (Add)          (None, 10, 10, 2048) 0           conv5_block1_out[0][0]           \n",
      "                                                                 conv5_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_preact_bn (BatchNo (None, 10, 10, 2048) 8192        conv5_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_preact_relu (Activ (None, 10, 10, 2048) 0           conv5_block3_preact_bn[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_conv (Conv2D)    (None, 10, 10, 512)  1048576     conv5_block3_preact_relu[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_bn (BatchNormali (None, 10, 10, 512)  2048        conv5_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_relu (Activation (None, 10, 10, 512)  0           conv5_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_pad (ZeroPadding (None, 12, 12, 512)  0           conv5_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_conv (Conv2D)    (None, 10, 10, 512)  2359296     conv5_block3_2_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_bn (BatchNormali (None, 10, 10, 512)  2048        conv5_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_relu (Activation (None, 10, 10, 512)  0           conv5_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_3_conv (Conv2D)    (None, 10, 10, 2048) 1050624     conv5_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_out (Add)          (None, 10, 10, 2048) 0           conv5_block2_out[0][0]           \n",
      "                                                                 conv5_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "post_bn (BatchNormalization)    (None, 10, 10, 2048) 8192        conv5_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "post_relu (Activation)          (None, 10, 10, 2048) 0           post_bn[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 58,347,328\n",
      "Trainable params: 0\n",
      "Non-trainable params: 58,347,328\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# log-vgg pretrained - ss-setting\n",
    "model_name = \"ResNet152V2\"\n",
    "fp_penalty_coef, fn_penalty_coef = 1, 1\n",
    "base_model = models.load_model(\"/home/shahrokh/projects/MLSpectrumAllocation/ML/data/\" +\n",
    "                               \"pictures_299_299_transfer/log/noisy_std_1/pu_circle_su_circle_60/\" +\n",
    "                               \"raw_power_min_max_norm/color/log_pu5_su6/\" +\n",
    "                               \"variable_sensors_10_20_pus_5_sus_8_channels/models/ResNet152V2/\" + \n",
    "                               \"700000/best_model_lambda_0_1.h5\", \n",
    "                              custom_objects={ 'loss': custom_loss(fp_penalty_coef, fn_penalty_coef), \n",
    "                                               'fp_mae': fp_mae,\n",
    "                                               'mae':'mae', 'mse':'mse'})\n",
    "base_model = base_model.layers[1]\n",
    "base_model.trainable = False\n",
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vit_keras import vit, utils\n",
    "model_name = \"vit_l16\"\n",
    "def transformer_model():\n",
    "    model = vit.vit_l16(image_size=max_x, activation='linear',\n",
    "                        pretrained=True, include_top=True, pretrained_top=False, classes=1)\n",
    "    for l_index in range(len(model.layers) - 1):\n",
    "        model.layers[l_index].trainable = False\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = transformer_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn1.layers[1].trainable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn1 = cnn_model(0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 299, 299, 8)]     0         \n",
      "_________________________________________________________________\n",
      "vgg16 (Functional)           (None, 9, 9, 512)         14717568  \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d (Gl (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2048)              1050624   \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 2049      \n",
      "=================================================================\n",
      "Total params: 15,770,241\n",
      "Trainable params: 1,052,673\n",
      "Non-trainable params: 14,717,568\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "cnn1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tf.test.is_gpu_available()\n",
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_samples = [1024]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8192]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_samples "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_samples = [700000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number_samples: 1024 , New samples: 1024\n",
      "Validation size: 205 , starts: 1024 , ends: 1228\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-09 17:55:16.081126: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-09 17:55:17.513286: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8101\n",
      "2022-06-09 17:55:20.122008: I tensorflow/stream_executor/cuda/cuda_blas.cc:1760] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/64 - 8s - loss: 148.4075 - mse: 148.4075 - mae: 8.7763 - fp_mae: 4.3849 - val_loss: 61.2394 - val_mse: 61.2394 - val_mae: 4.3195 - val_fp_mae: 3.2842\n",
      "\n",
      "Epoch 00001: val_mae improved from inf to 4.31954, saving model to ML/data/pictures_299_299_transfer/splat/pu_circle_su_circle_60/raw_power_min_max_norm/color/log_pu5_su5/pus_1_sus_3_channels/models/log_vgg16/1024/best_model_lambda_0.h5\n",
      "Epoch 2/100\n",
      "64/64 - 3s - loss: 251.6637 - mse: 251.6637 - mae: 8.7278 - fp_mae: 4.4464 - val_loss: 36.6462 - val_mse: 36.6462 - val_mae: 4.5722 - val_fp_mae: 3.8844\n",
      "\n",
      "Epoch 00002: val_mae did not improve from 4.31954\n",
      "Epoch 3/100\n",
      "64/64 - 3s - loss: 272.3088 - mse: 272.3088 - mae: 7.6117 - fp_mae: 3.6448 - val_loss: 24.2086 - val_mse: 24.2086 - val_mae: 3.8205 - val_fp_mae: 1.5410\n",
      "\n",
      "Epoch 00003: val_mae improved from 4.31954 to 3.82047, saving model to ML/data/pictures_299_299_transfer/splat/pu_circle_su_circle_60/raw_power_min_max_norm/color/log_pu5_su5/pus_1_sus_3_channels/models/log_vgg16/1024/best_model_lambda_0.h5\n",
      "Epoch 4/100\n",
      "64/64 - 3s - loss: 209.8172 - mse: 209.8172 - mae: 7.1627 - fp_mae: 3.5537 - val_loss: 39.6484 - val_mse: 39.6484 - val_mae: 4.2845 - val_fp_mae: 2.8823\n",
      "\n",
      "Epoch 00004: val_mae did not improve from 3.82047\n",
      "Epoch 5/100\n",
      "64/64 - 3s - loss: 103.8562 - mse: 103.8562 - mae: 6.3541 - fp_mae: 3.1840 - val_loss: 28.3486 - val_mse: 28.3486 - val_mae: 4.3531 - val_fp_mae: 0.8947\n",
      "\n",
      "Epoch 00005: val_mae did not improve from 3.82047\n",
      "Epoch 6/100\n",
      "64/64 - 3s - loss: 73.7350 - mse: 73.7350 - mae: 5.4974 - fp_mae: 2.7522 - val_loss: 31.9481 - val_mse: 31.9481 - val_mae: 4.1739 - val_fp_mae: 3.2618\n",
      "\n",
      "Epoch 00006: val_mae did not improve from 3.82047\n",
      "Epoch 7/100\n",
      "64/64 - 3s - loss: 97.0760 - mse: 97.0760 - mae: 5.3580 - fp_mae: 2.6160 - val_loss: 26.0816 - val_mse: 26.0816 - val_mae: 3.7416 - val_fp_mae: 2.3661\n",
      "\n",
      "Epoch 00007: val_mae improved from 3.82047 to 3.74160, saving model to ML/data/pictures_299_299_transfer/splat/pu_circle_su_circle_60/raw_power_min_max_norm/color/log_pu5_su5/pus_1_sus_3_channels/models/log_vgg16/1024/best_model_lambda_0.h5\n",
      "Epoch 8/100\n",
      "64/64 - 3s - loss: 186.5648 - mse: 186.5648 - mae: 6.4061 - fp_mae: 3.2847 - val_loss: 46.3077 - val_mse: 46.3077 - val_mae: 4.1460 - val_fp_mae: 1.8155\n",
      "\n",
      "Epoch 00008: val_mae did not improve from 3.74160\n",
      "Epoch 9/100\n",
      "64/64 - 3s - loss: 150.6155 - mse: 150.6155 - mae: 6.1050 - fp_mae: 2.6557 - val_loss: 57.7256 - val_mse: 57.7256 - val_mae: 5.4902 - val_fp_mae: 1.9817\n",
      "\n",
      "Epoch 00009: val_mae did not improve from 3.74160\n",
      "Epoch 10/100\n",
      "64/64 - 3s - loss: 175.7496 - mse: 175.7496 - mae: 5.9328 - fp_mae: 3.1931 - val_loss: 24.5060 - val_mse: 24.5060 - val_mae: 3.8974 - val_fp_mae: 1.3572\n",
      "\n",
      "Epoch 00010: val_mae did not improve from 3.74160\n",
      "Epoch 11/100\n",
      "64/64 - 3s - loss: 242.6246 - mse: 242.6246 - mae: 5.8781 - fp_mae: 2.8344 - val_loss: 41.0642 - val_mse: 41.0642 - val_mae: 5.1130 - val_fp_mae: 1.0572\n",
      "\n",
      "Epoch 00011: val_mae did not improve from 3.74160\n",
      "Epoch 12/100\n",
      "64/64 - 3s - loss: 261.9811 - mse: 261.9811 - mae: 6.1848 - fp_mae: 2.9709 - val_loss: 77.8297 - val_mse: 77.8297 - val_mae: 7.4628 - val_fp_mae: 7.2993\n",
      "\n",
      "Epoch 00012: val_mae did not improve from 3.74160\n",
      "Epoch 13/100\n",
      "64/64 - 3s - loss: 125.8715 - mse: 125.8715 - mae: 5.8068 - fp_mae: 3.0280 - val_loss: 25.0920 - val_mse: 25.0920 - val_mae: 3.9866 - val_fp_mae: 1.1275\n",
      "\n",
      "Epoch 00013: val_mae did not improve from 3.74160\n",
      "Epoch 14/100\n",
      "64/64 - 3s - loss: 46.4367 - mse: 46.4367 - mae: 4.5831 - fp_mae: 2.1466 - val_loss: 22.5756 - val_mse: 22.5756 - val_mae: 3.6205 - val_fp_mae: 2.2608\n",
      "\n",
      "Epoch 00014: val_mae improved from 3.74160 to 3.62049, saving model to ML/data/pictures_299_299_transfer/splat/pu_circle_su_circle_60/raw_power_min_max_norm/color/log_pu5_su5/pus_1_sus_3_channels/models/log_vgg16/1024/best_model_lambda_0.h5\n",
      "Epoch 15/100\n",
      "64/64 - 3s - loss: 230.5652 - mse: 230.5652 - mae: 6.1369 - fp_mae: 3.0086 - val_loss: 33.1141 - val_mse: 33.1141 - val_mae: 3.9682 - val_fp_mae: 2.5110\n",
      "\n",
      "Epoch 00015: val_mae did not improve from 3.62049\n",
      "Epoch 16/100\n",
      "64/64 - 3s - loss: 173.6893 - mse: 173.6893 - mae: 5.1979 - fp_mae: 2.5746 - val_loss: 20.9619 - val_mse: 20.9619 - val_mae: 3.5377 - val_fp_mae: 1.9228\n",
      "\n",
      "Epoch 00016: val_mae improved from 3.62049 to 3.53774, saving model to ML/data/pictures_299_299_transfer/splat/pu_circle_su_circle_60/raw_power_min_max_norm/color/log_pu5_su5/pus_1_sus_3_channels/models/log_vgg16/1024/best_model_lambda_0.h5\n",
      "Epoch 17/100\n",
      "64/64 - 3s - loss: 46.4750 - mse: 46.4750 - mae: 4.2973 - fp_mae: 2.0940 - val_loss: 21.7041 - val_mse: 21.7041 - val_mae: 3.5272 - val_fp_mae: 2.1536\n",
      "\n",
      "Epoch 00017: val_mae improved from 3.53774 to 3.52718, saving model to ML/data/pictures_299_299_transfer/splat/pu_circle_su_circle_60/raw_power_min_max_norm/color/log_pu5_su5/pus_1_sus_3_channels/models/log_vgg16/1024/best_model_lambda_0.h5\n",
      "Epoch 18/100\n",
      "64/64 - 3s - loss: 87.6236 - mse: 87.6236 - mae: 4.7497 - fp_mae: 2.3506 - val_loss: 25.0567 - val_mse: 25.0567 - val_mae: 3.6953 - val_fp_mae: 1.7202\n",
      "\n",
      "Epoch 00018: val_mae did not improve from 3.52718\n",
      "Epoch 19/100\n",
      "64/64 - 3s - loss: 183.7152 - mse: 183.7152 - mae: 4.8969 - fp_mae: 2.5215 - val_loss: 23.2075 - val_mse: 23.2075 - val_mae: 3.6634 - val_fp_mae: 1.8137\n",
      "\n",
      "Epoch 00019: val_mae did not improve from 3.52718\n",
      "Epoch 20/100\n",
      "64/64 - 3s - loss: 71.8722 - mse: 71.8722 - mae: 4.4185 - fp_mae: 2.1881 - val_loss: 20.9003 - val_mse: 20.9003 - val_mae: 3.5485 - val_fp_mae: 1.7350\n",
      "\n",
      "Epoch 00020: val_mae did not improve from 3.52718\n",
      "Epoch 21/100\n",
      "64/64 - 3s - loss: 74.5637 - mse: 74.5637 - mae: 4.3037 - fp_mae: 2.1379 - val_loss: 24.0573 - val_mse: 24.0573 - val_mae: 3.6308 - val_fp_mae: 2.3781\n",
      "\n",
      "Epoch 00021: val_mae did not improve from 3.52718\n",
      "Epoch 22/100\n",
      "64/64 - 3s - loss: 238.3622 - mse: 238.3622 - mae: 5.1469 - fp_mae: 2.5207 - val_loss: 22.0522 - val_mse: 22.0522 - val_mae: 3.5579 - val_fp_mae: 1.6536\n",
      "\n",
      "Epoch 00022: val_mae did not improve from 3.52718\n",
      "Epoch 23/100\n",
      "64/64 - 3s - loss: 190.2594 - mse: 190.2594 - mae: 5.2573 - fp_mae: 2.4495 - val_loss: 24.6451 - val_mse: 24.6451 - val_mae: 3.7192 - val_fp_mae: 2.1877\n",
      "\n",
      "Epoch 00023: val_mae did not improve from 3.52718\n",
      "Epoch 24/100\n",
      "64/64 - 3s - loss: 481.5143 - mse: 481.5143 - mae: 6.2287 - fp_mae: 3.2728 - val_loss: 21.5382 - val_mse: 21.5382 - val_mae: 3.5859 - val_fp_mae: 1.8413\n",
      "\n",
      "Epoch 00024: val_mae did not improve from 3.52718\n",
      "Epoch 25/100\n",
      "64/64 - 3s - loss: 73.2671 - mse: 73.2671 - mae: 4.5975 - fp_mae: 2.2984 - val_loss: 23.4694 - val_mse: 23.4694 - val_mae: 3.6604 - val_fp_mae: 2.0225\n",
      "\n",
      "Epoch 00025: val_mae did not improve from 3.52718\n",
      "Epoch 26/100\n",
      "64/64 - 3s - loss: 178.9695 - mse: 178.9695 - mae: 5.2787 - fp_mae: 2.6069 - val_loss: 33.1471 - val_mse: 33.1471 - val_mae: 4.4661 - val_fp_mae: 1.1464\n",
      "\n",
      "Epoch 00026: val_mae did not improve from 3.52718\n",
      "Epoch 27/100\n",
      "64/64 - 3s - loss: 65.7544 - mse: 65.7544 - mae: 4.6148 - fp_mae: 2.2960 - val_loss: 21.4868 - val_mse: 21.4868 - val_mae: 3.5501 - val_fp_mae: 1.6121\n",
      "\n",
      "Epoch 00027: val_mae did not improve from 3.52718\n",
      "Epoch 28/100\n",
      "64/64 - 3s - loss: 74.1221 - mse: 74.1221 - mae: 4.4635 - fp_mae: 2.1906 - val_loss: 21.1072 - val_mse: 21.1072 - val_mae: 3.5288 - val_fp_mae: 1.7682\n",
      "\n",
      "Epoch 00028: val_mae did not improve from 3.52718\n",
      "Epoch 29/100\n",
      "64/64 - 3s - loss: 65.0328 - mse: 65.0328 - mae: 4.4109 - fp_mae: 2.1481 - val_loss: 25.7047 - val_mse: 25.7047 - val_mae: 4.0733 - val_fp_mae: 1.0219\n",
      "\n",
      "Epoch 00029: val_mae did not improve from 3.52718\n",
      "Epoch 30/100\n",
      "64/64 - 3s - loss: 169.3994 - mse: 169.3994 - mae: 4.7253 - fp_mae: 2.2006 - val_loss: 27.6576 - val_mse: 27.6576 - val_mae: 3.7110 - val_fp_mae: 2.6006\n",
      "\n",
      "Epoch 00030: val_mae did not improve from 3.52718\n",
      "Epoch 31/100\n",
      "64/64 - 3s - loss: 108.3677 - mse: 108.3677 - mae: 4.5857 - fp_mae: 2.4041 - val_loss: 22.8596 - val_mse: 22.8596 - val_mae: 3.6206 - val_fp_mae: 1.9304\n",
      "\n",
      "Epoch 00031: val_mae did not improve from 3.52718\n",
      "Epoch 32/100\n",
      "64/64 - 3s - loss: 65.1776 - mse: 65.1776 - mae: 4.3653 - fp_mae: 2.2520 - val_loss: 20.8954 - val_mse: 20.8954 - val_mae: 3.5506 - val_fp_mae: 1.6439\n",
      "\n",
      "Epoch 00032: val_mae did not improve from 3.52718\n",
      "Epoch 33/100\n",
      "64/64 - 3s - loss: 198.6288 - mse: 198.6288 - mae: 5.0338 - fp_mae: 2.3556 - val_loss: 25.0139 - val_mse: 25.0139 - val_mae: 3.6104 - val_fp_mae: 2.0892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00033: val_mae did not improve from 3.52718\n",
      "Epoch 34/100\n",
      "64/64 - 3s - loss: 74.4497 - mse: 74.4497 - mae: 4.4054 - fp_mae: 2.1672 - val_loss: 22.0496 - val_mse: 22.0496 - val_mae: 3.6604 - val_fp_mae: 1.5075\n",
      "\n",
      "Epoch 00034: val_mae did not improve from 3.52718\n",
      "Epoch 35/100\n",
      "64/64 - 3s - loss: 40.2764 - mse: 40.2764 - mae: 4.0487 - fp_mae: 2.0247 - val_loss: 21.2516 - val_mse: 21.2516 - val_mae: 3.6259 - val_fp_mae: 1.4485\n",
      "\n",
      "Epoch 00035: val_mae did not improve from 3.52718\n",
      "Epoch 36/100\n",
      "64/64 - 3s - loss: 123.5392 - mse: 123.5392 - mae: 4.6528 - fp_mae: 2.1547 - val_loss: 26.7002 - val_mse: 26.7002 - val_mae: 3.6880 - val_fp_mae: 2.4457\n",
      "\n",
      "Epoch 00036: val_mae did not improve from 3.52718\n",
      "Epoch 37/100\n",
      "64/64 - 3s - loss: 70.0416 - mse: 70.0416 - mae: 4.3516 - fp_mae: 2.3094 - val_loss: 23.4327 - val_mse: 23.4327 - val_mae: 3.8560 - val_fp_mae: 1.3017\n",
      "\n",
      "Epoch 00037: val_mae did not improve from 3.52718\n",
      "Epoch 38/100\n",
      "64/64 - 3s - loss: 46.0843 - mse: 46.0843 - mae: 4.1869 - fp_mae: 1.9942 - val_loss: 29.3085 - val_mse: 29.3085 - val_mae: 3.7011 - val_fp_mae: 1.9028\n",
      "\n",
      "Epoch 00038: val_mae did not improve from 3.52718\n",
      "Epoch 39/100\n",
      "64/64 - 3s - loss: 132.2702 - mse: 132.2702 - mae: 4.6494 - fp_mae: 2.4295 - val_loss: 26.0754 - val_mse: 26.0754 - val_mae: 3.9913 - val_fp_mae: 1.1727\n",
      "\n",
      "Epoch 00039: val_mae did not improve from 3.52718\n",
      "Epoch 40/100\n",
      "64/64 - 3s - loss: 160.7554 - mse: 160.7554 - mae: 4.5873 - fp_mae: 2.1059 - val_loss: 29.1472 - val_mse: 29.1472 - val_mae: 3.8079 - val_fp_mae: 1.9504\n",
      "\n",
      "Epoch 00040: val_mae did not improve from 3.52718\n",
      "Epoch 41/100\n",
      "64/64 - 3s - loss: 84.4725 - mse: 84.4725 - mae: 4.7171 - fp_mae: 2.2940 - val_loss: 31.1688 - val_mse: 31.1688 - val_mae: 3.9252 - val_fp_mae: 2.9796\n",
      "\n",
      "Epoch 00041: val_mae did not improve from 3.52718\n",
      "Epoch 42/100\n",
      "64/64 - 3s - loss: 153.3558 - mse: 153.3558 - mae: 4.4269 - fp_mae: 2.3616 - val_loss: 26.9701 - val_mse: 26.9701 - val_mae: 3.8583 - val_fp_mae: 1.1990\n",
      "\n",
      "Epoch 00042: val_mae did not improve from 3.52718\n",
      "Epoch 43/100\n",
      "64/64 - 3s - loss: 408.0302 - mse: 408.0302 - mae: 5.1927 - fp_mae: 2.4397 - val_loss: 22.5105 - val_mse: 22.5105 - val_mae: 3.6658 - val_fp_mae: 1.7951\n",
      "\n",
      "Epoch 00043: val_mae did not improve from 3.52718\n",
      "Epoch 44/100\n",
      "64/64 - 3s - loss: 55.2045 - mse: 55.2045 - mae: 4.4095 - fp_mae: 2.2375 - val_loss: 22.2075 - val_mse: 22.2075 - val_mae: 3.7383 - val_fp_mae: 1.2688\n",
      "\n",
      "Epoch 00044: val_mae did not improve from 3.52718\n",
      "Epoch 45/100\n",
      "64/64 - 3s - loss: 46.9791 - mse: 46.9791 - mae: 4.0479 - fp_mae: 2.0484 - val_loss: 20.7233 - val_mse: 20.7233 - val_mae: 3.5272 - val_fp_mae: 1.7111\n",
      "\n",
      "Epoch 00045: val_mae did not improve from 3.52718\n",
      "Epoch 46/100\n",
      "64/64 - 3s - loss: 59.1347 - mse: 59.1347 - mae: 4.1976 - fp_mae: 2.1578 - val_loss: 20.5838 - val_mse: 20.5838 - val_mae: 3.4808 - val_fp_mae: 1.9197\n",
      "\n",
      "Epoch 00046: val_mae improved from 3.52718 to 3.48078, saving model to ML/data/pictures_299_299_transfer/splat/pu_circle_su_circle_60/raw_power_min_max_norm/color/log_pu5_su5/pus_1_sus_3_channels/models/log_vgg16/1024/best_model_lambda_0.h5\n",
      "Epoch 47/100\n",
      "64/64 - 3s - loss: 78.1956 - mse: 78.1956 - mae: 4.2658 - fp_mae: 2.0034 - val_loss: 20.9008 - val_mse: 20.9008 - val_mae: 3.5470 - val_fp_mae: 1.6223\n",
      "\n",
      "Epoch 00047: val_mae did not improve from 3.48078\n",
      "Epoch 48/100\n",
      "64/64 - 3s - loss: 109.4239 - mse: 109.4239 - mae: 4.4424 - fp_mae: 2.3479 - val_loss: 23.1893 - val_mse: 23.1893 - val_mae: 3.8174 - val_fp_mae: 1.2875\n",
      "\n",
      "Epoch 00048: val_mae did not improve from 3.48078\n",
      "Epoch 49/100\n",
      "64/64 - 3s - loss: 309.8199 - mse: 309.8199 - mae: 4.9727 - fp_mae: 2.4716 - val_loss: 22.1781 - val_mse: 22.1781 - val_mae: 3.6225 - val_fp_mae: 1.4714\n",
      "\n",
      "Epoch 00049: val_mae did not improve from 3.48078\n",
      "Epoch 50/100\n",
      "64/64 - 3s - loss: 171.3981 - mse: 171.3981 - mae: 4.5430 - fp_mae: 2.2391 - val_loss: 22.5831 - val_mse: 22.5831 - val_mae: 3.5512 - val_fp_mae: 2.3873\n",
      "\n",
      "Epoch 00050: val_mae did not improve from 3.48078\n",
      "Epoch 51/100\n",
      "64/64 - 3s - loss: 62.5799 - mse: 62.5799 - mae: 4.1581 - fp_mae: 2.1348 - val_loss: 20.5722 - val_mse: 20.5722 - val_mae: 3.4920 - val_fp_mae: 1.6815\n",
      "\n",
      "Epoch 00051: val_mae did not improve from 3.48078\n",
      "Epoch 52/100\n",
      "64/64 - 3s - loss: 194.0686 - mse: 194.0686 - mae: 4.2872 - fp_mae: 2.1605 - val_loss: 26.8188 - val_mse: 26.8188 - val_mae: 3.7443 - val_fp_mae: 2.5233\n",
      "\n",
      "Epoch 00052: val_mae did not improve from 3.48078\n",
      "Epoch 53/100\n",
      "64/64 - 3s - loss: 92.5291 - mse: 92.5291 - mae: 4.3834 - fp_mae: 2.1369 - val_loss: 22.8228 - val_mse: 22.8228 - val_mae: 3.5836 - val_fp_mae: 2.3814\n",
      "\n",
      "Epoch 00053: val_mae did not improve from 3.48078\n",
      "Epoch 54/100\n",
      "64/64 - 3s - loss: 105.4135 - mse: 105.4135 - mae: 4.1239 - fp_mae: 2.1220 - val_loss: 22.2022 - val_mse: 22.2022 - val_mae: 3.6187 - val_fp_mae: 1.4321\n",
      "\n",
      "Epoch 00054: val_mae did not improve from 3.48078\n",
      "Epoch 55/100\n",
      "64/64 - 3s - loss: 61.3176 - mse: 61.3176 - mae: 3.9688 - fp_mae: 1.8606 - val_loss: 21.9263 - val_mse: 21.9263 - val_mae: 3.7219 - val_fp_mae: 1.3390\n",
      "\n",
      "Epoch 00055: val_mae did not improve from 3.48078\n",
      "Epoch 56/100\n",
      "64/64 - 3s - loss: 57.2402 - mse: 57.2402 - mae: 3.9884 - fp_mae: 2.0196 - val_loss: 22.8083 - val_mse: 22.8083 - val_mae: 3.6347 - val_fp_mae: 2.2594\n",
      "\n",
      "Epoch 00056: val_mae did not improve from 3.48078\n",
      "Epoch 57/100\n",
      "64/64 - 3s - loss: 62.9844 - mse: 62.9844 - mae: 3.9572 - fp_mae: 1.9862 - val_loss: 22.9838 - val_mse: 22.9838 - val_mae: 3.6395 - val_fp_mae: 2.2494\n",
      "\n",
      "Epoch 00057: val_mae did not improve from 3.48078\n",
      "Epoch 58/100\n",
      "64/64 - 3s - loss: 41.1941 - mse: 41.1941 - mae: 3.9478 - fp_mae: 1.9218 - val_loss: 21.3308 - val_mse: 21.3308 - val_mae: 3.5785 - val_fp_mae: 1.7404\n",
      "\n",
      "Epoch 00058: val_mae did not improve from 3.48078\n",
      "Epoch 59/100\n",
      "64/64 - 3s - loss: 26.5084 - mse: 26.5084 - mae: 3.6921 - fp_mae: 1.8148 - val_loss: 21.4044 - val_mse: 21.4044 - val_mae: 3.5606 - val_fp_mae: 1.7548\n",
      "\n",
      "Epoch 00059: val_mae did not improve from 3.48078\n",
      "Epoch 60/100\n",
      "64/64 - 3s - loss: 87.7343 - mse: 87.7343 - mae: 4.0548 - fp_mae: 2.0504 - val_loss: 22.9748 - val_mse: 22.9748 - val_mae: 3.5912 - val_fp_mae: 2.3527\n",
      "\n",
      "Epoch 00060: val_mae did not improve from 3.48078\n",
      "Epoch 61/100\n",
      "64/64 - 3s - loss: 86.3637 - mse: 86.3637 - mae: 4.1775 - fp_mae: 1.9643 - val_loss: 23.2326 - val_mse: 23.2326 - val_mae: 3.6281 - val_fp_mae: 2.5638\n",
      "\n",
      "Epoch 00061: val_mae did not improve from 3.48078\n",
      "Epoch 62/100\n",
      "64/64 - 3s - loss: 38.8586 - mse: 38.8586 - mae: 3.9502 - fp_mae: 2.1258 - val_loss: 22.9278 - val_mse: 22.9278 - val_mae: 3.6172 - val_fp_mae: 2.3965\n",
      "\n",
      "Epoch 00062: val_mae did not improve from 3.48078\n",
      "Epoch 63/100\n",
      "64/64 - 3s - loss: 38.5531 - mse: 38.5531 - mae: 3.8732 - fp_mae: 1.8675 - val_loss: 22.1114 - val_mse: 22.1114 - val_mae: 3.5956 - val_fp_mae: 2.0681\n",
      "\n",
      "Epoch 00063: val_mae did not improve from 3.48078\n",
      "Epoch 64/100\n",
      "64/64 - 3s - loss: 28.4591 - mse: 28.4591 - mae: 3.7485 - fp_mae: 1.8492 - val_loss: 20.9602 - val_mse: 20.9602 - val_mae: 3.5227 - val_fp_mae: 1.8120\n",
      "\n",
      "Epoch 00064: val_mae did not improve from 3.48078\n",
      "Epoch 65/100\n",
      "64/64 - 3s - loss: 34.1662 - mse: 34.1662 - mae: 3.8988 - fp_mae: 1.9645 - val_loss: 21.7140 - val_mse: 21.7140 - val_mae: 3.6427 - val_fp_mae: 1.4253\n",
      "\n",
      "Epoch 00065: val_mae did not improve from 3.48078\n",
      "Epoch 66/100\n",
      "64/64 - 3s - loss: 43.4286 - mse: 43.4286 - mae: 3.8834 - fp_mae: 1.9512 - val_loss: 20.9733 - val_mse: 20.9733 - val_mae: 3.5342 - val_fp_mae: 1.8244\n",
      "\n",
      "Epoch 00066: val_mae did not improve from 3.48078\n",
      "Epoch 67/100\n",
      "64/64 - 3s - loss: 107.5779 - mse: 107.5779 - mae: 4.1712 - fp_mae: 2.0378 - val_loss: 22.4386 - val_mse: 22.4386 - val_mae: 3.5556 - val_fp_mae: 2.3645\n",
      "\n",
      "Epoch 00067: val_mae did not improve from 3.48078\n",
      "Epoch 68/100\n",
      "64/64 - 3s - loss: 35.3270 - mse: 35.3270 - mae: 3.8285 - fp_mae: 1.9129 - val_loss: 20.9500 - val_mse: 20.9500 - val_mae: 3.5588 - val_fp_mae: 1.7360\n",
      "\n",
      "Epoch 00068: val_mae did not improve from 3.48078\n",
      "Epoch 69/100\n",
      "64/64 - 3s - loss: 43.8008 - mse: 43.8008 - mae: 3.7511 - fp_mae: 1.9001 - val_loss: 23.1633 - val_mse: 23.1633 - val_mae: 3.6124 - val_fp_mae: 2.3919\n",
      "\n",
      "Epoch 00069: val_mae did not improve from 3.48078\n",
      "Epoch 70/100\n",
      "64/64 - 3s - loss: 26.9257 - mse: 26.9257 - mae: 3.6631 - fp_mae: 1.8312 - val_loss: 21.4002 - val_mse: 21.4002 - val_mae: 3.5963 - val_fp_mae: 1.7159\n",
      "\n",
      "Epoch 00070: val_mae did not improve from 3.48078\n",
      "Epoch 71/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/64 - 3s - loss: 24.1507 - mse: 24.1507 - mae: 3.6577 - fp_mae: 1.7934 - val_loss: 20.9781 - val_mse: 20.9781 - val_mae: 3.5356 - val_fp_mae: 1.8112\n",
      "\n",
      "Epoch 00071: val_mae did not improve from 3.48078\n",
      "Epoch 72/100\n",
      "64/64 - 3s - loss: 37.1957 - mse: 37.1957 - mae: 3.8109 - fp_mae: 1.9069 - val_loss: 21.6597 - val_mse: 21.6597 - val_mae: 3.6245 - val_fp_mae: 1.7273\n",
      "\n",
      "Epoch 00072: val_mae did not improve from 3.48078\n",
      "Epoch 73/100\n",
      "64/64 - 3s - loss: 49.3647 - mse: 49.3647 - mae: 3.8090 - fp_mae: 1.8472 - val_loss: 21.4344 - val_mse: 21.4344 - val_mae: 3.5646 - val_fp_mae: 1.9976\n",
      "\n",
      "Epoch 00073: val_mae did not improve from 3.48078\n",
      "Epoch 74/100\n",
      "64/64 - 3s - loss: 37.1245 - mse: 37.1245 - mae: 3.8458 - fp_mae: 1.9449 - val_loss: 21.8435 - val_mse: 21.8435 - val_mae: 3.6437 - val_fp_mae: 1.5804\n",
      "\n",
      "Epoch 00074: val_mae did not improve from 3.48078\n",
      "Epoch 75/100\n",
      "64/64 - 3s - loss: 29.1168 - mse: 29.1168 - mae: 3.7509 - fp_mae: 1.8467 - val_loss: 22.1173 - val_mse: 22.1173 - val_mae: 3.6674 - val_fp_mae: 1.6878\n",
      "\n",
      "Epoch 00075: val_mae did not improve from 3.48078\n",
      "Epoch 76/100\n",
      "64/64 - 3s - loss: 25.4137 - mse: 25.4137 - mae: 3.6465 - fp_mae: 1.8076 - val_loss: 21.8673 - val_mse: 21.8673 - val_mae: 3.6236 - val_fp_mae: 1.7094\n",
      "\n",
      "Epoch 00076: val_mae did not improve from 3.48078\n",
      "Epoch 77/100\n",
      "64/64 - 3s - loss: 26.8630 - mse: 26.8630 - mae: 3.7556 - fp_mae: 1.8141 - val_loss: 21.5554 - val_mse: 21.5554 - val_mae: 3.6202 - val_fp_mae: 1.7857\n",
      "\n",
      "Epoch 00077: val_mae did not improve from 3.48078\n",
      "Epoch 78/100\n",
      "64/64 - 3s - loss: 32.4319 - mse: 32.4319 - mae: 3.7674 - fp_mae: 1.8521 - val_loss: 26.5497 - val_mse: 26.5497 - val_mae: 3.7851 - val_fp_mae: 2.9401\n",
      "\n",
      "Epoch 00078: val_mae did not improve from 3.48078\n",
      "Epoch 79/100\n",
      "64/64 - 3s - loss: 35.3646 - mse: 35.3646 - mae: 3.7323 - fp_mae: 1.8138 - val_loss: 22.3063 - val_mse: 22.3063 - val_mae: 3.5951 - val_fp_mae: 2.2916\n",
      "\n",
      "Epoch 00079: val_mae did not improve from 3.48078\n",
      "Epoch 80/100\n",
      "64/64 - 3s - loss: 37.4232 - mse: 37.4232 - mae: 3.8329 - fp_mae: 1.9298 - val_loss: 22.5827 - val_mse: 22.5827 - val_mae: 3.6432 - val_fp_mae: 2.1086\n",
      "\n",
      "Epoch 00080: val_mae did not improve from 3.48078\n",
      "Epoch 81/100\n",
      "64/64 - 3s - loss: 35.3958 - mse: 35.3958 - mae: 3.8294 - fp_mae: 1.8582 - val_loss: 20.8818 - val_mse: 20.8818 - val_mae: 3.5216 - val_fp_mae: 1.8365\n",
      "\n",
      "Epoch 00081: val_mae did not improve from 3.48078\n",
      "Epoch 82/100\n",
      "64/64 - 3s - loss: 56.6091 - mse: 56.6091 - mae: 3.9298 - fp_mae: 2.0118 - val_loss: 23.3083 - val_mse: 23.3083 - val_mae: 3.7006 - val_fp_mae: 2.0129\n",
      "\n",
      "Epoch 00082: val_mae did not improve from 3.48078\n",
      "Epoch 83/100\n",
      "64/64 - 3s - loss: 33.8793 - mse: 33.8793 - mae: 3.8111 - fp_mae: 1.8520 - val_loss: 21.9041 - val_mse: 21.9041 - val_mae: 3.5550 - val_fp_mae: 2.1930\n",
      "\n",
      "Epoch 00083: val_mae did not improve from 3.48078\n",
      "Epoch 84/100\n",
      "64/64 - 3s - loss: 32.9685 - mse: 32.9685 - mae: 3.7493 - fp_mae: 1.9076 - val_loss: 22.4878 - val_mse: 22.4878 - val_mae: 3.6169 - val_fp_mae: 2.1529\n",
      "\n",
      "Epoch 00084: val_mae did not improve from 3.48078\n",
      "Epoch 85/100\n",
      "64/64 - 3s - loss: 31.4961 - mse: 31.4961 - mae: 3.7540 - fp_mae: 1.8377 - val_loss: 21.5492 - val_mse: 21.5492 - val_mae: 3.6279 - val_fp_mae: 1.6030\n",
      "\n",
      "Epoch 00085: val_mae did not improve from 3.48078\n",
      "Epoch 86/100\n",
      "64/64 - 3s - loss: 32.3429 - mse: 32.3429 - mae: 3.6643 - fp_mae: 1.8402 - val_loss: 22.5102 - val_mse: 22.5102 - val_mae: 3.6456 - val_fp_mae: 1.9475\n",
      "\n",
      "Epoch 00086: val_mae did not improve from 3.48078\n",
      "Epoch 87/100\n",
      "64/64 - 3s - loss: 32.6077 - mse: 32.6077 - mae: 3.7177 - fp_mae: 1.7350 - val_loss: 23.0748 - val_mse: 23.0748 - val_mae: 3.6057 - val_fp_mae: 2.3793\n",
      "\n",
      "Epoch 00087: val_mae did not improve from 3.48078\n",
      "Epoch 88/100\n",
      "64/64 - 3s - loss: 32.0453 - mse: 32.0453 - mae: 3.6437 - fp_mae: 1.8672 - val_loss: 22.1488 - val_mse: 22.1488 - val_mae: 3.6955 - val_fp_mae: 1.4553\n",
      "\n",
      "Epoch 00088: val_mae did not improve from 3.48078\n",
      "Epoch 89/100\n",
      "64/64 - 3s - loss: 25.3457 - mse: 25.3457 - mae: 3.6609 - fp_mae: 1.8529 - val_loss: 22.0828 - val_mse: 22.0828 - val_mae: 3.6419 - val_fp_mae: 1.8730\n",
      "\n",
      "Epoch 00089: val_mae did not improve from 3.48078\n",
      "Epoch 90/100\n",
      "64/64 - 3s - loss: 28.2092 - mse: 28.2092 - mae: 3.6383 - fp_mae: 1.7588 - val_loss: 21.5749 - val_mse: 21.5749 - val_mae: 3.6439 - val_fp_mae: 1.6307\n",
      "\n",
      "Epoch 00090: val_mae did not improve from 3.48078\n",
      "Epoch 91/100\n",
      "64/64 - 3s - loss: 37.2476 - mse: 37.2476 - mae: 3.7038 - fp_mae: 1.8755 - val_loss: 21.6183 - val_mse: 21.6183 - val_mae: 3.5742 - val_fp_mae: 1.8370\n",
      "\n",
      "Epoch 00091: val_mae did not improve from 3.48078\n",
      "Epoch 92/100\n",
      "64/64 - 3s - loss: 47.2342 - mse: 47.2342 - mae: 3.7386 - fp_mae: 1.8060 - val_loss: 21.5481 - val_mse: 21.5481 - val_mae: 3.5979 - val_fp_mae: 1.8648\n",
      "\n",
      "Epoch 00092: val_mae did not improve from 3.48078\n",
      "Epoch 93/100\n",
      "64/64 - 3s - loss: 21.8273 - mse: 21.8273 - mae: 3.5562 - fp_mae: 1.7714 - val_loss: 22.2705 - val_mse: 22.2705 - val_mae: 3.6108 - val_fp_mae: 2.0802\n",
      "\n",
      "Epoch 00093: val_mae did not improve from 3.48078\n",
      "Epoch 94/100\n",
      "64/64 - 3s - loss: 26.9705 - mse: 26.9705 - mae: 3.6089 - fp_mae: 1.7738 - val_loss: 21.7498 - val_mse: 21.7498 - val_mae: 3.6077 - val_fp_mae: 1.9693\n",
      "\n",
      "Epoch 00094: val_mae did not improve from 3.48078\n",
      "Epoch 95/100\n",
      "64/64 - 3s - loss: 32.7288 - mse: 32.7288 - mae: 3.6160 - fp_mae: 1.7730 - val_loss: 22.4948 - val_mse: 22.4948 - val_mae: 3.6398 - val_fp_mae: 2.0282\n",
      "\n",
      "Epoch 00095: val_mae did not improve from 3.48078\n",
      "Epoch 96/100\n",
      "64/64 - 3s - loss: 26.7797 - mse: 26.7797 - mae: 3.6185 - fp_mae: 1.7652 - val_loss: 22.0957 - val_mse: 22.0957 - val_mae: 3.6107 - val_fp_mae: 2.0927\n",
      "\n",
      "Epoch 00096: val_mae did not improve from 3.48078\n",
      "Epoch 97/100\n",
      "64/64 - 3s - loss: 22.8161 - mse: 22.8161 - mae: 3.5248 - fp_mae: 1.7572 - val_loss: 21.5870 - val_mse: 21.5870 - val_mae: 3.5726 - val_fp_mae: 2.0054\n",
      "\n",
      "Epoch 00097: val_mae did not improve from 3.48078\n",
      "Epoch 98/100\n",
      "64/64 - 3s - loss: 21.7737 - mse: 21.7737 - mae: 3.5397 - fp_mae: 1.8152 - val_loss: 21.3423 - val_mse: 21.3423 - val_mae: 3.6008 - val_fp_mae: 1.6971\n",
      "\n",
      "Epoch 00098: val_mae did not improve from 3.48078\n",
      "Epoch 99/100\n",
      "64/64 - 3s - loss: 24.8915 - mse: 24.8915 - mae: 3.6151 - fp_mae: 1.7614 - val_loss: 21.3606 - val_mse: 21.3606 - val_mae: 3.5933 - val_fp_mae: 1.7478\n",
      "\n",
      "Epoch 00099: val_mae did not improve from 3.48078\n",
      "Epoch 100/100\n",
      "64/64 - 3s - loss: 28.1344 - mse: 28.1344 - mae: 3.6445 - fp_mae: 1.8347 - val_loss: 22.2793 - val_mse: 22.2793 - val_mae: 3.6607 - val_fp_mae: 1.7899\n",
      "\n",
      "Epoch 00100: val_mae did not improve from 3.48078\n",
      "\n",
      "Lambda: 0 , Time: 0:04:55\n",
      "Train Error(all epochs): 3.524770975112915 \n",
      " [8.776, 8.728, 7.612, 7.163, 6.354, 5.497, 5.358, 6.406, 6.105, 5.933, 5.878, 6.185, 5.807, 4.583, 6.137, 5.198, 4.297, 4.75, 4.897, 4.419, 4.304, 5.147, 5.257, 6.229, 4.597, 5.279, 4.615, 4.464, 4.411, 4.725, 4.586, 4.365, 5.034, 4.405, 4.049, 4.653, 4.352, 4.187, 4.649, 4.587, 4.717, 4.427, 5.193, 4.41, 4.048, 4.198, 4.266, 4.442, 4.973, 4.543, 4.158, 4.287, 4.383, 4.124, 3.969, 3.988, 3.957, 3.948, 3.692, 4.055, 4.178, 3.95, 3.873, 3.748, 3.899, 3.883, 4.171, 3.829, 3.751, 3.663, 3.658, 3.811, 3.809, 3.846, 3.751, 3.647, 3.756, 3.767, 3.732, 3.833, 3.829, 3.93, 3.811, 3.749, 3.754, 3.664, 3.718, 3.644, 3.661, 3.638, 3.704, 3.739, 3.556, 3.609, 3.616, 3.619, 3.525, 3.54, 3.615, 3.645]\n",
      "Train FP Error(all epochs): 1.7350200414657593 \n",
      " [4.385, 4.446, 3.645, 3.554, 3.184, 2.752, 2.616, 3.285, 2.656, 3.193, 2.834, 2.971, 3.028, 2.147, 3.009, 2.575, 2.094, 2.351, 2.522, 2.188, 2.138, 2.521, 2.449, 3.273, 2.298, 2.607, 2.296, 2.191, 2.148, 2.201, 2.404, 2.252, 2.356, 2.167, 2.025, 2.155, 2.309, 1.994, 2.43, 2.106, 2.294, 2.362, 2.44, 2.238, 2.048, 2.158, 2.003, 2.348, 2.472, 2.239, 2.135, 2.16, 2.137, 2.122, 1.861, 2.02, 1.986, 1.922, 1.815, 2.05, 1.964, 2.126, 1.867, 1.849, 1.965, 1.951, 2.038, 1.913, 1.9, 1.831, 1.793, 1.907, 1.847, 1.945, 1.847, 1.808, 1.814, 1.852, 1.814, 1.93, 1.858, 2.012, 1.852, 1.908, 1.838, 1.84, 1.735, 1.867, 1.853, 1.759, 1.875, 1.806, 1.771, 1.774, 1.773, 1.765, 1.757, 1.815, 1.761, 1.835]\n",
      "Val Error(all epochs): 3.4807839393615723 \n",
      " [4.32, 4.572, 3.82, 4.284, 4.353, 4.174, 3.742, 4.146, 5.49, 3.897, 5.113, 7.463, 3.987, 3.62, 3.968, 3.538, 3.527, 3.695, 3.663, 3.548, 3.631, 3.558, 3.719, 3.586, 3.66, 4.466, 3.55, 3.529, 4.073, 3.711, 3.621, 3.551, 3.61, 3.66, 3.626, 3.688, 3.856, 3.701, 3.991, 3.808, 3.925, 3.858, 3.666, 3.738, 3.527, 3.481, 3.547, 3.817, 3.623, 3.551, 3.492, 3.744, 3.584, 3.619, 3.722, 3.635, 3.64, 3.578, 3.561, 3.591, 3.628, 3.617, 3.596, 3.523, 3.643, 3.534, 3.556, 3.559, 3.612, 3.596, 3.536, 3.624, 3.565, 3.644, 3.667, 3.624, 3.62, 3.785, 3.595, 3.643, 3.522, 3.701, 3.555, 3.617, 3.628, 3.646, 3.606, 3.696, 3.642, 3.644, 3.574, 3.598, 3.611, 3.608, 3.64, 3.611, 3.573, 3.601, 3.593, 3.661]\n",
      "Val FP Error(all epochs): 0.8946508169174194 \n",
      " [3.284, 3.884, 1.541, 2.882, 0.895, 3.262, 2.366, 1.816, 1.982, 1.357, 1.057, 7.299, 1.128, 2.261, 2.511, 1.923, 2.154, 1.72, 1.814, 1.735, 2.378, 1.654, 2.188, 1.841, 2.023, 1.146, 1.612, 1.768, 1.022, 2.601, 1.93, 1.644, 2.089, 1.508, 1.449, 2.446, 1.302, 1.903, 1.173, 1.95, 2.98, 1.199, 1.795, 1.269, 1.711, 1.92, 1.622, 1.288, 1.471, 2.387, 1.681, 2.523, 2.381, 1.432, 1.339, 2.259, 2.249, 1.74, 1.755, 2.353, 2.564, 2.397, 2.068, 1.812, 1.425, 1.824, 2.364, 1.736, 2.392, 1.716, 1.811, 1.727, 1.998, 1.58, 1.688, 1.709, 1.786, 2.94, 2.292, 2.109, 1.837, 2.013, 2.193, 2.153, 1.603, 1.948, 2.379, 1.455, 1.873, 1.631, 1.837, 1.865, 2.08, 1.969, 2.028, 2.093, 2.005, 1.697, 1.748, 1.79]\n",
      "******FINE TUNNING ******\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "64/64 - 8s - loss: 27.7493 - mse: 27.7493 - mae: 3.8555 - fp_mae: 1.8859 - val_loss: 17.6468 - val_mse: 17.6468 - val_mae: 3.2654 - val_fp_mae: 1.2483\n",
      "\n",
      "Epoch 00001: val_mae improved from 3.48078 to 3.26540, saving model to ML/data/pictures_299_299_transfer/splat/pu_circle_su_circle_60/raw_power_min_max_norm/color/log_pu5_su5/pus_1_sus_3_channels/models/log_vgg16/1024/best_model_lambda_0.h5\n",
      "Epoch 2/50\n",
      "64/64 - 6s - loss: 16.2788 - mse: 16.2788 - mae: 3.0514 - fp_mae: 1.5505 - val_loss: 16.0061 - val_mse: 16.0061 - val_mae: 3.0883 - val_fp_mae: 1.2213\n",
      "\n",
      "Epoch 00002: val_mae improved from 3.26540 to 3.08825, saving model to ML/data/pictures_299_299_transfer/splat/pu_circle_su_circle_60/raw_power_min_max_norm/color/log_pu5_su5/pus_1_sus_3_channels/models/log_vgg16/1024/best_model_lambda_0.h5\n",
      "Epoch 3/50\n",
      "64/64 - 6s - loss: 12.6221 - mse: 12.6221 - mae: 2.6461 - fp_mae: 1.3142 - val_loss: 16.0382 - val_mse: 16.0382 - val_mae: 2.9453 - val_fp_mae: 1.5181\n",
      "\n",
      "Epoch 00003: val_mae improved from 3.08825 to 2.94526, saving model to ML/data/pictures_299_299_transfer/splat/pu_circle_su_circle_60/raw_power_min_max_norm/color/log_pu5_su5/pus_1_sus_3_channels/models/log_vgg16/1024/best_model_lambda_0.h5\n",
      "Epoch 4/50\n",
      "64/64 - 6s - loss: 9.6668 - mse: 9.6668 - mae: 2.3085 - fp_mae: 1.1326 - val_loss: 16.7365 - val_mse: 16.7365 - val_mae: 3.0799 - val_fp_mae: 1.7639\n",
      "\n",
      "Epoch 00004: val_mae did not improve from 2.94526\n",
      "Epoch 5/50\n",
      "64/64 - 7s - loss: 7.9647 - mse: 7.9647 - mae: 2.0897 - fp_mae: 1.0567 - val_loss: 17.0370 - val_mse: 17.0370 - val_mae: 3.0320 - val_fp_mae: 1.5671\n",
      "\n",
      "Epoch 00005: val_mae did not improve from 2.94526\n",
      "Epoch 6/50\n",
      "64/64 - 7s - loss: 7.1683 - mse: 7.1683 - mae: 1.9803 - fp_mae: 1.0081 - val_loss: 17.8641 - val_mse: 17.8641 - val_mae: 3.3639 - val_fp_mae: 1.0548\n",
      "\n",
      "Epoch 00006: val_mae did not improve from 2.94526\n",
      "Epoch 7/50\n",
      "64/64 - 7s - loss: 6.4863 - mse: 6.4863 - mae: 1.9251 - fp_mae: 0.9346 - val_loss: 18.6487 - val_mse: 18.6487 - val_mae: 3.1371 - val_fp_mae: 2.0277\n",
      "\n",
      "Epoch 00007: val_mae did not improve from 2.94526\n",
      "Epoch 8/50\n",
      "64/64 - 7s - loss: 5.2786 - mse: 5.2786 - mae: 1.7125 - fp_mae: 0.8297 - val_loss: 18.7741 - val_mse: 18.7741 - val_mae: 3.1705 - val_fp_mae: 1.7297\n",
      "\n",
      "Epoch 00008: val_mae did not improve from 2.94526\n",
      "Epoch 9/50\n",
      "64/64 - 6s - loss: 4.4893 - mse: 4.4893 - mae: 1.5879 - fp_mae: 0.7941 - val_loss: 17.5409 - val_mse: 17.5409 - val_mae: 3.1189 - val_fp_mae: 1.5153\n",
      "\n",
      "Epoch 00009: val_mae did not improve from 2.94526\n",
      "Epoch 10/50\n",
      "64/64 - 6s - loss: 3.7418 - mse: 3.7418 - mae: 1.4552 - fp_mae: 0.7145 - val_loss: 16.9446 - val_mse: 16.9446 - val_mae: 3.0201 - val_fp_mae: 1.7877\n",
      "\n",
      "Epoch 00010: val_mae did not improve from 2.94526\n",
      "Epoch 11/50\n",
      "64/64 - 6s - loss: 3.2186 - mse: 3.2186 - mae: 1.3737 - fp_mae: 0.6819 - val_loss: 16.7514 - val_mse: 16.7514 - val_mae: 3.0573 - val_fp_mae: 1.2824\n",
      "\n",
      "Epoch 00011: val_mae did not improve from 2.94526\n",
      "Epoch 12/50\n",
      "64/64 - 6s - loss: 3.3269 - mse: 3.3269 - mae: 1.3683 - fp_mae: 0.6373 - val_loss: 15.8250 - val_mse: 15.8250 - val_mae: 2.9130 - val_fp_mae: 1.5057\n",
      "\n",
      "Epoch 00012: val_mae improved from 2.94526 to 2.91300, saving model to ML/data/pictures_299_299_transfer/splat/pu_circle_su_circle_60/raw_power_min_max_norm/color/log_pu5_su5/pus_1_sus_3_channels/models/log_vgg16/1024/best_model_lambda_0.h5\n",
      "Epoch 13/50\n",
      "64/64 - 6s - loss: 3.1523 - mse: 3.1523 - mae: 1.3279 - fp_mae: 0.6749 - val_loss: 16.7722 - val_mse: 16.7722 - val_mae: 3.0292 - val_fp_mae: 1.4204\n",
      "\n",
      "Epoch 00013: val_mae did not improve from 2.91300\n",
      "Epoch 14/50\n",
      "64/64 - 6s - loss: 3.2110 - mse: 3.2110 - mae: 1.3209 - fp_mae: 0.6254 - val_loss: 15.7550 - val_mse: 15.7550 - val_mae: 2.9514 - val_fp_mae: 1.3524\n",
      "\n",
      "Epoch 00014: val_mae did not improve from 2.91300\n",
      "Epoch 15/50\n",
      "64/64 - 6s - loss: 3.3076 - mse: 3.3076 - mae: 1.3745 - fp_mae: 0.6613 - val_loss: 15.7035 - val_mse: 15.7035 - val_mae: 2.8764 - val_fp_mae: 1.4386\n",
      "\n",
      "Epoch 00015: val_mae improved from 2.91300 to 2.87639, saving model to ML/data/pictures_299_299_transfer/splat/pu_circle_su_circle_60/raw_power_min_max_norm/color/log_pu5_su5/pus_1_sus_3_channels/models/log_vgg16/1024/best_model_lambda_0.h5\n",
      "Epoch 16/50\n",
      "64/64 - 6s - loss: 3.1137 - mse: 3.1137 - mae: 1.3034 - fp_mae: 0.6557 - val_loss: 18.3249 - val_mse: 18.3249 - val_mae: 3.2876 - val_fp_mae: 1.5514\n",
      "\n",
      "Epoch 00016: val_mae did not improve from 2.87639\n",
      "Epoch 17/50\n",
      "64/64 - 6s - loss: 2.8859 - mse: 2.8859 - mae: 1.2788 - fp_mae: 0.6085 - val_loss: 16.1169 - val_mse: 16.1169 - val_mae: 2.8998 - val_fp_mae: 1.5030\n",
      "\n",
      "Epoch 00017: val_mae did not improve from 2.87639\n",
      "Epoch 18/50\n",
      "64/64 - 6s - loss: 2.7200 - mse: 2.7200 - mae: 1.2311 - fp_mae: 0.6001 - val_loss: 16.3108 - val_mse: 16.3108 - val_mae: 2.9981 - val_fp_mae: 1.5106\n",
      "\n",
      "Epoch 00018: val_mae did not improve from 2.87639\n",
      "Epoch 19/50\n",
      "64/64 - 6s - loss: 2.6847 - mse: 2.6847 - mae: 1.2273 - fp_mae: 0.6018 - val_loss: 15.6082 - val_mse: 15.6082 - val_mae: 2.8509 - val_fp_mae: 1.6389\n",
      "\n",
      "Epoch 00019: val_mae improved from 2.87639 to 2.85089, saving model to ML/data/pictures_299_299_transfer/splat/pu_circle_su_circle_60/raw_power_min_max_norm/color/log_pu5_su5/pus_1_sus_3_channels/models/log_vgg16/1024/best_model_lambda_0.h5\n",
      "Epoch 20/50\n",
      "64/64 - 6s - loss: 2.4161 - mse: 2.4161 - mae: 1.1346 - fp_mae: 0.5408 - val_loss: 16.3540 - val_mse: 16.3540 - val_mae: 2.9347 - val_fp_mae: 1.6352\n",
      "\n",
      "Epoch 00020: val_mae did not improve from 2.85089\n",
      "Epoch 21/50\n",
      "64/64 - 6s - loss: 2.2797 - mse: 2.2797 - mae: 1.1067 - fp_mae: 0.5205 - val_loss: 17.6828 - val_mse: 17.6828 - val_mae: 3.0593 - val_fp_mae: 2.0532\n",
      "\n",
      "Epoch 00021: val_mae did not improve from 2.85089\n",
      "Epoch 22/50\n",
      "64/64 - 7s - loss: 2.3287 - mse: 2.3287 - mae: 1.1346 - fp_mae: 0.5497 - val_loss: 17.7847 - val_mse: 17.7847 - val_mae: 3.0172 - val_fp_mae: 2.2041\n",
      "\n",
      "Epoch 00022: val_mae did not improve from 2.85089\n",
      "Epoch 23/50\n",
      "64/64 - 6s - loss: 2.3777 - mse: 2.3777 - mae: 1.1314 - fp_mae: 0.5631 - val_loss: 16.2355 - val_mse: 16.2355 - val_mae: 2.9880 - val_fp_mae: 1.5663\n",
      "\n",
      "Epoch 00023: val_mae did not improve from 2.85089\n",
      "Epoch 24/50\n",
      "64/64 - 6s - loss: 2.7345 - mse: 2.7345 - mae: 1.1766 - fp_mae: 0.5612 - val_loss: 17.4020 - val_mse: 17.4020 - val_mae: 3.2073 - val_fp_mae: 1.3970\n",
      "\n",
      "Epoch 00024: val_mae did not improve from 2.85089\n",
      "Epoch 25/50\n",
      "64/64 - 6s - loss: 2.3852 - mse: 2.3852 - mae: 1.1625 - fp_mae: 0.5620 - val_loss: 15.9281 - val_mse: 15.9281 - val_mae: 2.9336 - val_fp_mae: 1.5863\n",
      "\n",
      "Epoch 00025: val_mae did not improve from 2.85089\n",
      "Epoch 26/50\n",
      "64/64 - 7s - loss: 1.8282 - mse: 1.8282 - mae: 0.9874 - fp_mae: 0.4709 - val_loss: 16.0516 - val_mse: 16.0516 - val_mae: 2.9580 - val_fp_mae: 1.5649\n",
      "\n",
      "Epoch 00026: val_mae did not improve from 2.85089\n",
      "Epoch 27/50\n",
      "64/64 - 6s - loss: 2.0584 - mse: 2.0584 - mae: 1.0830 - fp_mae: 0.5232 - val_loss: 16.0381 - val_mse: 16.0381 - val_mae: 3.0053 - val_fp_mae: 1.4600\n",
      "\n",
      "Epoch 00027: val_mae did not improve from 2.85089\n",
      "Epoch 28/50\n",
      "64/64 - 6s - loss: 1.8720 - mse: 1.8720 - mae: 1.0079 - fp_mae: 0.5080 - val_loss: 15.6330 - val_mse: 15.6330 - val_mae: 2.9444 - val_fp_mae: 1.3728\n",
      "\n",
      "Epoch 00028: val_mae did not improve from 2.85089\n",
      "Epoch 29/50\n",
      "64/64 - 6s - loss: 2.0212 - mse: 2.0212 - mae: 1.0500 - fp_mae: 0.4869 - val_loss: 16.2138 - val_mse: 16.2138 - val_mae: 2.9728 - val_fp_mae: 1.6312\n",
      "\n",
      "Epoch 00029: val_mae did not improve from 2.85089\n",
      "Epoch 30/50\n",
      "64/64 - 6s - loss: 1.7928 - mse: 1.7928 - mae: 1.0222 - fp_mae: 0.5056 - val_loss: 14.6070 - val_mse: 14.6070 - val_mae: 2.8149 - val_fp_mae: 1.4346\n",
      "\n",
      "Epoch 00030: val_mae improved from 2.85089 to 2.81486, saving model to ML/data/pictures_299_299_transfer/splat/pu_circle_su_circle_60/raw_power_min_max_norm/color/log_pu5_su5/pus_1_sus_3_channels/models/log_vgg16/1024/best_model_lambda_0.h5\n",
      "Epoch 31/50\n",
      "64/64 - 6s - loss: 1.7359 - mse: 1.7359 - mae: 0.9943 - fp_mae: 0.4769 - val_loss: 15.8024 - val_mse: 15.8024 - val_mae: 2.8499 - val_fp_mae: 1.7667\n",
      "\n",
      "Epoch 00031: val_mae did not improve from 2.81486\n",
      "Epoch 32/50\n",
      "64/64 - 6s - loss: 1.9454 - mse: 1.9454 - mae: 1.0420 - fp_mae: 0.5109 - val_loss: 15.4758 - val_mse: 15.4758 - val_mae: 2.8633 - val_fp_mae: 1.5491\n",
      "\n",
      "Epoch 00032: val_mae did not improve from 2.81486\n",
      "Epoch 33/50\n",
      "64/64 - 7s - loss: 2.1875 - mse: 2.1875 - mae: 1.0363 - fp_mae: 0.5152 - val_loss: 15.1490 - val_mse: 15.1490 - val_mae: 2.8734 - val_fp_mae: 1.4425\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00033: val_mae did not improve from 2.81486\n",
      "Epoch 34/50\n",
      "64/64 - 7s - loss: 1.8568 - mse: 1.8568 - mae: 0.9836 - fp_mae: 0.4638 - val_loss: 15.2295 - val_mse: 15.2295 - val_mae: 2.8969 - val_fp_mae: 1.3076\n",
      "\n",
      "Epoch 00034: val_mae did not improve from 2.81486\n",
      "Epoch 35/50\n",
      "64/64 - 6s - loss: 1.5155 - mse: 1.5155 - mae: 0.9004 - fp_mae: 0.4459 - val_loss: 15.4290 - val_mse: 15.4290 - val_mae: 2.8964 - val_fp_mae: 1.5734\n",
      "\n",
      "Epoch 00035: val_mae did not improve from 2.81486\n",
      "Epoch 36/50\n",
      "64/64 - 6s - loss: 1.3710 - mse: 1.3710 - mae: 0.8813 - fp_mae: 0.4162 - val_loss: 15.5610 - val_mse: 15.5610 - val_mae: 2.9002 - val_fp_mae: 1.5400\n",
      "\n",
      "Epoch 00036: val_mae did not improve from 2.81486\n",
      "Epoch 37/50\n",
      "64/64 - 6s - loss: 1.9056 - mse: 1.9056 - mae: 1.0373 - fp_mae: 0.5111 - val_loss: 15.4958 - val_mse: 15.4958 - val_mae: 2.8944 - val_fp_mae: 1.4872\n",
      "\n",
      "Epoch 00037: val_mae did not improve from 2.81486\n",
      "Epoch 38/50\n",
      "64/64 - 6s - loss: 2.0828 - mse: 2.0828 - mae: 1.0858 - fp_mae: 0.5092 - val_loss: 14.7869 - val_mse: 14.7869 - val_mae: 2.7834 - val_fp_mae: 1.5590\n",
      "\n",
      "Epoch 00038: val_mae improved from 2.81486 to 2.78342, saving model to ML/data/pictures_299_299_transfer/splat/pu_circle_su_circle_60/raw_power_min_max_norm/color/log_pu5_su5/pus_1_sus_3_channels/models/log_vgg16/1024/best_model_lambda_0.h5\n",
      "Epoch 39/50\n",
      "64/64 - 6s - loss: 1.8614 - mse: 1.8614 - mae: 0.9917 - fp_mae: 0.4847 - val_loss: 15.6408 - val_mse: 15.6408 - val_mae: 2.8313 - val_fp_mae: 1.7440\n",
      "\n",
      "Epoch 00039: val_mae did not improve from 2.78342\n",
      "Epoch 40/50\n",
      "64/64 - 6s - loss: 1.5144 - mse: 1.5144 - mae: 0.9293 - fp_mae: 0.4669 - val_loss: 15.7643 - val_mse: 15.7643 - val_mae: 2.8303 - val_fp_mae: 1.6920\n",
      "\n",
      "Epoch 00040: val_mae did not improve from 2.78342\n",
      "Epoch 41/50\n",
      "64/64 - 6s - loss: 1.4107 - mse: 1.4107 - mae: 0.8893 - fp_mae: 0.4155 - val_loss: 15.8909 - val_mse: 15.8909 - val_mae: 2.8296 - val_fp_mae: 1.7899\n",
      "\n",
      "Epoch 00041: val_mae did not improve from 2.78342\n",
      "Epoch 42/50\n",
      "64/64 - 6s - loss: 1.6169 - mse: 1.6169 - mae: 0.9680 - fp_mae: 0.4762 - val_loss: 15.2744 - val_mse: 15.2744 - val_mae: 2.8229 - val_fp_mae: 1.5449\n",
      "\n",
      "Epoch 00042: val_mae did not improve from 2.78342\n",
      "Epoch 43/50\n",
      "64/64 - 6s - loss: 1.4573 - mse: 1.4573 - mae: 0.8937 - fp_mae: 0.4277 - val_loss: 14.7819 - val_mse: 14.7819 - val_mae: 2.7626 - val_fp_mae: 1.5577\n",
      "\n",
      "Epoch 00043: val_mae improved from 2.78342 to 2.76262, saving model to ML/data/pictures_299_299_transfer/splat/pu_circle_su_circle_60/raw_power_min_max_norm/color/log_pu5_su5/pus_1_sus_3_channels/models/log_vgg16/1024/best_model_lambda_0.h5\n",
      "Epoch 44/50\n",
      "64/64 - 6s - loss: 1.5998 - mse: 1.5998 - mae: 0.9051 - fp_mae: 0.4405 - val_loss: 15.3698 - val_mse: 15.3698 - val_mae: 2.8689 - val_fp_mae: 1.3025\n",
      "\n",
      "Epoch 00044: val_mae did not improve from 2.76262\n",
      "Epoch 45/50\n",
      "64/64 - 6s - loss: 1.9239 - mse: 1.9239 - mae: 1.0084 - fp_mae: 0.4832 - val_loss: 14.9826 - val_mse: 14.9826 - val_mae: 2.8194 - val_fp_mae: 1.5470\n",
      "\n",
      "Epoch 00045: val_mae did not improve from 2.76262\n",
      "Epoch 46/50\n",
      "64/64 - 6s - loss: 1.6211 - mse: 1.6211 - mae: 0.9695 - fp_mae: 0.4713 - val_loss: 15.5530 - val_mse: 15.5530 - val_mae: 2.8888 - val_fp_mae: 1.5275\n",
      "\n",
      "Epoch 00046: val_mae did not improve from 2.76262\n",
      "Epoch 47/50\n",
      "64/64 - 7s - loss: 1.5158 - mse: 1.5158 - mae: 0.9173 - fp_mae: 0.4520 - val_loss: 15.4739 - val_mse: 15.4739 - val_mae: 2.9073 - val_fp_mae: 1.5552\n",
      "\n",
      "Epoch 00047: val_mae did not improve from 2.76262\n",
      "Epoch 48/50\n",
      "64/64 - 6s - loss: 1.6016 - mse: 1.6016 - mae: 0.9488 - fp_mae: 0.4398 - val_loss: 15.5317 - val_mse: 15.5317 - val_mae: 2.8261 - val_fp_mae: 1.7531\n",
      "\n",
      "Epoch 00048: val_mae did not improve from 2.76262\n",
      "Epoch 49/50\n",
      "64/64 - 6s - loss: 1.6057 - mse: 1.6057 - mae: 0.9620 - fp_mae: 0.4738 - val_loss: 16.1100 - val_mse: 16.1100 - val_mae: 2.8855 - val_fp_mae: 1.8952\n",
      "\n",
      "Epoch 00049: val_mae did not improve from 2.76262\n",
      "Epoch 50/50\n",
      "64/64 - 6s - loss: 1.8368 - mse: 1.8368 - mae: 1.0217 - fp_mae: 0.4953 - val_loss: 15.3980 - val_mse: 15.3980 - val_mae: 2.8403 - val_fp_mae: 1.6697\n",
      "\n",
      "Epoch 00050: val_mae did not improve from 2.76262\n",
      "\n",
      "Lambda: 0 , Time: 0:05:46\n",
      "Train Error(all epochs): 0.8812554478645325 \n",
      " [3.855, 3.051, 2.646, 2.309, 2.09, 1.98, 1.925, 1.712, 1.588, 1.455, 1.374, 1.368, 1.328, 1.321, 1.374, 1.303, 1.279, 1.231, 1.227, 1.135, 1.107, 1.135, 1.131, 1.177, 1.163, 0.987, 1.083, 1.008, 1.05, 1.022, 0.994, 1.042, 1.036, 0.984, 0.9, 0.881, 1.037, 1.086, 0.992, 0.929, 0.889, 0.968, 0.894, 0.905, 1.008, 0.97, 0.917, 0.949, 0.962, 1.022]\n",
      "Train FP Error(all epochs): 0.41552433371543884 \n",
      " [1.886, 1.55, 1.314, 1.133, 1.057, 1.008, 0.935, 0.83, 0.794, 0.715, 0.682, 0.637, 0.675, 0.625, 0.661, 0.656, 0.609, 0.6, 0.602, 0.541, 0.521, 0.55, 0.563, 0.561, 0.562, 0.471, 0.523, 0.508, 0.487, 0.506, 0.477, 0.511, 0.515, 0.464, 0.446, 0.416, 0.511, 0.509, 0.485, 0.467, 0.416, 0.476, 0.428, 0.441, 0.483, 0.471, 0.452, 0.44, 0.474, 0.495]\n",
      "Val Error(all epochs): 2.7626237869262695 \n",
      " [3.265, 3.088, 2.945, 3.08, 3.032, 3.364, 3.137, 3.17, 3.119, 3.02, 3.057, 2.913, 3.029, 2.951, 2.876, 3.288, 2.9, 2.998, 2.851, 2.935, 3.059, 3.017, 2.988, 3.207, 2.934, 2.958, 3.005, 2.944, 2.973, 2.815, 2.85, 2.863, 2.873, 2.897, 2.896, 2.9, 2.894, 2.783, 2.831, 2.83, 2.83, 2.823, 2.763, 2.869, 2.819, 2.889, 2.907, 2.826, 2.885, 2.84]\n",
      "Val FP Error(all epochs): 1.0548286437988281 \n",
      " [1.248, 1.221, 1.518, 1.764, 1.567, 1.055, 2.028, 1.73, 1.515, 1.788, 1.282, 1.506, 1.42, 1.352, 1.439, 1.551, 1.503, 1.511, 1.639, 1.635, 2.053, 2.204, 1.566, 1.397, 1.586, 1.565, 1.46, 1.373, 1.631, 1.435, 1.767, 1.549, 1.443, 1.308, 1.573, 1.54, 1.487, 1.559, 1.744, 1.692, 1.79, 1.545, 1.558, 1.303, 1.547, 1.528, 1.555, 1.753, 1.895, 1.67]\n",
      "\n",
      "Trainig set size: 1024 , Time: 0:10:41 , best_lambda: 0 , min_  error: 2.763\n",
      "Test starts:  1229 , ends:  39999\n",
      "2424/2424 [==============================] - 77s 32ms/step - loss: 16.6624 - mse: 16.6624 - mae: 3.0690 - fp_mae: 1.7103\n",
      "average_error:  3.069 , fp_average_error:  1.71\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# CNN: support batching\n",
    "AUGMENTED = False\n",
    "TEST, CONSERVE = True, False\n",
    "mini_batch = 64 if max(max_x, max_y) == 1000 else 16\n",
    "epochs = 35 if max(max_x, max_y) == 1000 else 100\n",
    "MAX_QUEUE_SIZE, WORKERS = 28, 4\n",
    "fp_penalty_coef, fn_penalty_coef = 1, 1\n",
    "hyper_metric, mode = \"val_mae\", 'min'  # the metric that hyper parameters are tuned with\n",
    "prev_sample = 0\n",
    "lambda_vec = [0] \n",
    "average_diff_power, fp_mean_power = [],[] #[7.177, 8.088, 8.183], [3.438, 3.506, 2.662]\n",
    "best_lambda = []\n",
    "average_diff_power_conserve, fp_mean_power_conserve = [], []\n",
    "all_cnns = []\n",
    "if CONSERVE: # for conservative\n",
    "    prev_number_samples = [0] + number_samples[:-1]\n",
    "\n",
    "for num_sample_idx, number_sample in enumerate(number_samples):\n",
    "    if CONSERVE:\n",
    "        data_reg[prev_number_samples[num_sample_idx]:number_sample, -1] = data_reg[\n",
    "            prev_number_samples[num_sample_idx]:number_sample, -1] - 1.5 # conserv value\n",
    "    MODEL_PATH = '/'.join(image_dir.split('/')[:-1]) + '/models/' + (\"aug/\" if AUGMENTED else \"\") \\\n",
    "    + model_name + \"/\" + (\"conservative/\" if CONSERVE else \"\") + str(number_sample)\n",
    "    if not os.path.exists(MODEL_PATH):\n",
    "        os.makedirs(MODEL_PATH)\n",
    "    MODEL_PATH += \"/best_model_lambda_\"\n",
    "    if True:\n",
    "        cnns = [cnn_model(lamb, 0) for lamb in lambda_vec]\n",
    "        for cnn in cnns:\n",
    "#             cnn.compile(loss='mean_squared_error', optimizer='adam', metrics=['mse','mae', fp_mean])\n",
    "            # =optimizers.SGD(lr=0.1, momentum=0.9, decay=0.1/epochs, nesterov=False)\n",
    "            cnn.compile(loss=custom_loss(fp_penalty_coef, fn_penalty_coef), \n",
    "                        optimizer=optimizers.Adam(), \n",
    "                        metrics=['mse', 'mae', fp_mae])\n",
    "        checkpointers = [ModelCheckpoint(filepath=MODEL_PATH + str(lamb)+ '.h5',\n",
    "                                         verbose=1, save_best_only=True, \n",
    "                                         monitor=hyper_metric,\n",
    "                                         mode=mode)\n",
    "                         for lamb in lambda_vec]\n",
    "    else:\n",
    "        cnns = []\n",
    "        cnns = [models.load_model(MODEL_PATH + str(lamb) + '.h5', \n",
    "                                  custom_objects={ 'loss': custom_loss(fp_penalty_coef, fn_penalty_coef), \n",
    "                                                  'fp_mae': fp_mae }) \n",
    "                for lamb in lambda_vec]\n",
    "    number_start = time.time()\n",
    "    train_generator = DataBatchGenerator(\n",
    "        dataset=data_reg_train[:number_sample * 4] if AUGMENTED else data_reg[:number_sample],\n",
    "        batch_size=mini_batch,\n",
    "        start_idx=prev_sample,\n",
    "        number_image_channels=number_image_channels,\n",
    "        max_x=max_x, max_y=max_y, float_memory_used=float_memory_used,\n",
    "        image_dir=image_dir + (\"/aug\" if AUGMENTED else \"\"))\n",
    "    \n",
    "\n",
    "    val_size = math.ceil(number_sample * validation_size)\n",
    "#     val_size = data_reg.shape[0] - number_sample\n",
    "    val_generator = DataBatchGenerator(dataset=data_reg[number_sample:number_sample+val_size], \n",
    "                                       batch_size=mini_batch,\n",
    "                                       start_idx=number_sample,\n",
    "                                       number_image_channels=number_image_channels,\n",
    "                                       max_x=max_x, max_y=max_y, \n",
    "                                       float_memory_used=float_memory_used,\n",
    "                                      image_dir=\"/\".join(image_dir.split(\"/\")[:-1]) + \"/images\")\n",
    "  \n",
    "    print('number_samples:', number_sample, \", New samples:\", number_sample - prev_sample)\n",
    "    print(\"Validation size:\", val_size, \", starts:\", number_sample, \", ends:\", number_sample + val_size - 1)\n",
    "    \n",
    "    for lamb_idx, lamb in enumerate(lambda_vec):\n",
    "#     for lamb_idx, lamb in enumerate(lambda_vec[:len(lambda_vec) - num_sample_idx//2]):\n",
    "#         if num_sample_idx == 3 and lamb_idx < 4:\n",
    "#             continue\n",
    "        lambda_start = time.time()\n",
    "        cnns[lamb_idx].fit(train_generator, epochs=epochs, verbose=2,\n",
    "                           validation_data=val_generator, \n",
    "                           shuffle=True, callbacks=[checkpointers[lamb_idx]], \n",
    "                           workers=WORKERS, max_queue_size=MAX_QUEUE_SIZE, \n",
    "                           use_multiprocessing=False)\n",
    "        \n",
    "        print(\"\\nLambda:\", lamb, \", Time:\", str(datetime.timedelta(seconds=int(time.time() - lambda_start))))\n",
    "        print(\"Train Error(all epochs):\", min(cnns[lamb_idx].history.history['mae']), '\\n', \n",
    "              [round(val, 3) for val in cnns[lamb_idx].history.history['mae']])\n",
    "        print(\"Train FP Error(all epochs):\", min(cnns[lamb_idx].history.history['fp_mae']), '\\n',\n",
    "              [round(val,3) for val in cnns[lamb_idx].history.history['fp_mae']])\n",
    "        print(\"Val Error(all epochs):\", min(cnns[lamb_idx].history.history['val_mae']), '\\n', \n",
    "              [round(val,3) for val in cnns[lamb_idx].history.history['val_mae']])\n",
    "        print(\"Val FP Error(all epochs):\", min(cnns[lamb_idx].history.history['val_fp_mae']), '\\n',\n",
    "              [round(val,3) for val in cnns[lamb_idx].history.history['val_fp_mae']])\n",
    "    # ******************** fine-tunning *******\n",
    "    print(\"******FINE TUNNING ******\")\n",
    "    # reloading the best\n",
    "    cnns = [models.load_model(MODEL_PATH + str(lamb) + '.h5', \n",
    "                              custom_objects={ 'loss': custom_loss(fp_penalty_coef, fn_penalty_coef), \n",
    "                                               'fp_mae': fp_mae,\n",
    "                                               'mae':'mae', 'mse':'mse'}) for lamb in lambda_vec]\n",
    "    for cnn in cnns:\n",
    "        cnn.trainable = True\n",
    "        cnn.compile(loss=custom_loss(fp_penalty_coef, fn_penalty_coef), \n",
    "                    optimizer=optimizers.Adam(1e-4), \n",
    "                    metrics=['mse', 'mae', fp_mae])\n",
    "    train_generator = DataBatchGenerator(\n",
    "        dataset=data_reg_train[:number_sample * 4] if AUGMENTED else data_reg[:number_sample],\n",
    "        batch_size=mini_batch,\n",
    "        start_idx=prev_sample,\n",
    "        number_image_channels=number_image_channels,\n",
    "        max_x=max_x, max_y=max_y, float_memory_used=float_memory_used,\n",
    "        image_dir=image_dir + (\"/aug\" if AUGMENTED else \"\"))\n",
    "    val_generator = DataBatchGenerator(dataset=data_reg[number_sample:number_sample+val_size], \n",
    "                                       batch_size=mini_batch,\n",
    "                                       start_idx=number_sample,\n",
    "                                       number_image_channels=number_image_channels,\n",
    "                                       max_x=max_x, max_y=max_y, \n",
    "                                       float_memory_used=float_memory_used,\n",
    "                                      image_dir=\"/\".join(image_dir.split(\"/\")[:-1]) + \"/images\")\n",
    "    for lamb_idx, lamb in enumerate(lambda_vec):\n",
    "#     for lamb_idx, lamb in enumerate(lambda_vec[:len(lambda_vec) - num_sample_idx//2]):\n",
    "#         if num_sample_idx == 3 and lamb_idx < 4:\n",
    "#             continue\n",
    "        lambda_start = time.time()\n",
    "        cnns[lamb_idx].fit(train_generator, epochs=int(epochs//2), verbose=2,\n",
    "                           validation_data=val_generator, \n",
    "                           shuffle=True, callbacks=[checkpointers[lamb_idx]], \n",
    "                           workers=WORKERS, max_queue_size=MAX_QUEUE_SIZE, \n",
    "                           use_multiprocessing=False)\n",
    "        \n",
    "        print(\"\\nLambda:\", lamb, \", Time:\", str(datetime.timedelta(seconds=int(time.time() - lambda_start))))\n",
    "        print(\"Train Error(all epochs):\", min(cnns[lamb_idx].history.history['mae']), '\\n', \n",
    "              [round(val, 3) for val in cnns[lamb_idx].history.history['mae']])\n",
    "        print(\"Train FP Error(all epochs):\", min(cnns[lamb_idx].history.history['fp_mae']), '\\n',\n",
    "              [round(val,3) for val in cnns[lamb_idx].history.history['fp_mae']])\n",
    "        print(\"Val Error(all epochs):\", min(cnns[lamb_idx].history.history['val_mae']), '\\n', \n",
    "              [round(val,3) for val in cnns[lamb_idx].history.history['val_mae']])\n",
    "        print(\"Val FP Error(all epochs):\", min(cnns[lamb_idx].history.history['val_fp_mae']), '\\n',\n",
    "              [round(val,3) for val in cnns[lamb_idx].history.history['val_fp_mae']])\n",
    "    \n",
    "    models_min_mae = [min(cnns[lam_idx].history.history[hyper_metric]) for\n",
    "                      lam_idx,_ in enumerate(lambda_vec)]\n",
    "    best_lamb_idx = models_min_mae.index(min(models_min_mae))\n",
    "    best_lambda.append(lambda_vec[best_lamb_idx])\n",
    "    print(\"\\nTrainig set size:\", number_sample, \", Time:\", str(datetime.timedelta(seconds=int(time.time() - \n",
    "                                                                                              number_start))),\n",
    "          \", best_lambda:\", lambda_vec[best_lamb_idx], \", min_\" , (\"fp_\" if hyper_metric == \"val_fp_mae\" else \"\"),\n",
    "          \"error:\", round(min(models_min_mae), 3))\n",
    "    all_cnns.append(cnns)\n",
    "    del cnns, train_generator, val_generator\n",
    "    \n",
    "    if TEST:\n",
    "        # evaluating test images\n",
    "        best_model = None\n",
    "        best_model = models.load_model(MODEL_PATH + str(lambda_vec[best_lamb_idx]) + '.h5', \n",
    "                                       custom_objects={ 'loss': custom_loss(fp_penalty_coef, fn_penalty_coef), \n",
    "                                                       'fp_mae': fp_mae,\n",
    "                                                      'mae':'mae', 'mse':'mse'})\n",
    "        test_generator = DataBatchGenerator(dataset=data_reg[number_sample + val_size:], \n",
    "                                            batch_size=mini_batch,\n",
    "                                            start_idx=number_sample + val_size, \n",
    "                                            number_image_channels=number_image_channels,\n",
    "                                            max_x=max_x, max_y=max_y, float_memory_used=float_memory_used)\n",
    "\n",
    "        print(\"Test starts: \", number_sample + val_size, \", ends: \", data_reg.shape[0] - 1)\n",
    "        time.sleep(1)\n",
    "        test_res = best_model.evaluate(test_generator, verbose=1, \n",
    "                                       workers=WORKERS, max_queue_size=MAX_QUEUE_SIZE, use_multiprocessing=False)\n",
    "        \n",
    "        test_mae_idx, test_fp_mae_idx = [best_model.metrics_names.index(mtrc) \n",
    "                                         for mtrc in ['mae','fp_mae']]\n",
    "        test_mae, test_fp_mae = test_res[test_mae_idx], test_res[test_fp_mae_idx]\n",
    "        average_diff_power.append(round(test_mae, 3))\n",
    "        fp_mean_power.append(round(test_fp_mae, 3))\n",
    "        print('average_error: ', average_diff_power[-1], ', fp_average_error: ', \n",
    "              fp_mean_power[-1])\n",
    "        \n",
    "        if False:\n",
    "            test_generator_conserve = DataBatchGenerator(dataset=data_reg[number_sample + val_size:], \n",
    "                                                         batch_size=mini_batch,\n",
    "                                                         start_idx=number_sample + val_size, \n",
    "                                                         number_image_channels=number_image_channels,\n",
    "                                                         max_x=max_x, max_y=max_y, \n",
    "                                                         float_memory_used=float_memory_used, \n",
    "                                                         conserve=1)\n",
    "            test_res_conserve = best_model.evaluate(test_generator_conserve, verbose=1, \n",
    "                                                    workers=WORKERS, max_queue_size=MAX_QUEUE_SIZE, \n",
    "                                                    use_multiprocessing=False)\n",
    "            test_mae_cons, test_fp_mae_cons = test_res_conserve[test_mae_idx], test_res_conserve[test_fp_mae_idx]\n",
    "            average_diff_power_conserve.append(round(test_mae_cons, 3))\n",
    "            fp_mean_power_conserve.append(round(test_fp_mae_cons, 3))\n",
    "            print('Conserve, average_error: ', average_diff_power_conserve[-1], ', fp_average_error: ',\n",
    "                 fp_mean_power_conserve[-1])\n",
    "        print(\"\\n\\n\")\n",
    "\n",
    "        \n",
    "#         var_f = open('/'.join(image_dir.split('/')[:-1]) +  '/models/' + model_name + \"/\"\n",
    "#                      + intensity_degradation + '_' + str(slope) + '_' + \n",
    "#                      dtime + \".dat\", \"wb\") # file for saving results\n",
    "#         pickle.dump([average_diff_power, fp_mean_power, number_samples, best_lambda, \n",
    "#                      dataset_name, max_dataset_name, average_diff_power_conserve, fp_mean_power_conserve,\n",
    "#                      checkpointers],\n",
    "#                     file=var_f)\n",
    "#         var_f.close()\n",
    "        del best_model, test_generator\n",
    "#     prev_sample = number_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving images once to save time\n",
    "# run this cell just for creating images\n",
    "def creating_image_max_su_tot(start, end):\n",
    "    # for image_num in range(115, data_reg.shape[0]):\n",
    "    # for image_num in range(1625, 5000):\n",
    "    for row_idx in tqdm.tqdm(range(start, end+1)):  #4463, data_reg.shape[0]\n",
    "        row_sample = data_reg[row_idx]\n",
    "        row_sample[int(row_sample[0]) * 3 + 1] = 1\n",
    "        for su_idx in range(int(data_max_su_tot[row_idx][0])):\n",
    "            row_sample[int(row_sample[0]) * 3 + 2 : \n",
    "                       int(row_sample[0]) * 3 + 4] = data_max_su_tot[row_idx][1 + su_idx * 3: \n",
    "                                                                              1 + su_idx * 3 + 2]\n",
    "            image = create_image(data=row_sample, \n",
    "                                 slope=slope, style=style, noise_floor=noise_floor,\n",
    "                                 pu_shape=pu_shape, su_shape=su_shape, su_param=su_param, \n",
    "                                 sensors_num=(sensors_num if sensors else 0), \n",
    "                                 intensity_degradation=intensity_degradation, \n",
    "                                 max_pu_power=0.0 if not sensors else -60,\n",
    "                                 max_su_power=40.0)\n",
    "            if False and style == \"image_intensity\":\n",
    "                if number_image_channels != 3:\n",
    "                    image = np.append(np.array(image[0]), np.zeros((3-number_image_channels,max_x, max_y), \n",
    "                                                                   dtype=float_memory_used), axis=0)\n",
    "                image_save = np.swapaxes(image, 0, 2)\n",
    "                plt.imsave(max_su_image_dir + '/image' + str(image_num)+'.png', image_save)\n",
    "            elif style == \"raw_power_min_max_norm\" or style == \"raw_power_zscore_norm\" or style == \"image_intensity\":\n",
    "        #         np.save(max_su_image_dir + '/image' + str(image_num), image)\n",
    "    #             np.savez_compressed(f\"{image_dir}{(600000 + image_num)//100000}/image{600000 + image_num}\",\n",
    "    #                                 a=np.expand_dims(image,0))\n",
    "                np.savez_compressed(f\"{max_su_image_dir}/image{row_idx * 5 + su_idx}\",\n",
    "                                    a=np.expand_dims(image,0))\n",
    "            del image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 2680/2680 [1:06:55<00:00,  1.50s/it]\n",
      "100%|█████████████████████████████████████| 2680/2680 [1:07:07<00:00,  1.50s/it]\n",
      "100%|█████████████████████████████████████| 2680/2680 [1:07:18<00:00,  1.51s/it]\n",
      "100%|█████████████████████████████████████| 2680/2680 [1:07:20<00:00,  1.51s/it]\n",
      "100%|█████████████████████████████████████| 2680/2680 [1:07:29<00:00,  1.51s/it]\n",
      "100%|█████████████████████████████████████| 2680/2680 [1:07:30<00:00,  1.51s/it]\n",
      "100%|█████████████████████████████████████| 2680/2680 [1:07:31<00:00,  1.51s/it]\n",
      "100%|█████████████████████████████████████| 2680/2680 [1:07:34<00:00,  1.51s/it]\n",
      "100%|█████████████████████████████████████| 2680/2680 [1:07:46<00:00,  1.52s/it]\n",
      "100%|█████████████████████████████████████| 2680/2680 [1:07:47<00:00,  1.52s/it]\n",
      "100%|█████████████████████████████████████| 2682/2682 [1:07:48<00:00,  1.52s/it]\n",
      "100%|█████████████████████████████████████| 2680/2680 [1:07:55<00:00,  1.52s/it]\n"
     ]
    }
   ],
   "source": [
    "model_name = \"log_vgg16_max_su_total\"\n",
    "max_su_image_dir = '/'.join(image_dir.split('/')[:-1]) + \"/\" + model_name + \"/images\"\n",
    "if not os.path.exists(max_su_image_dir):\n",
    "        os.makedirs(max_su_image_dir)\n",
    "jobs = []\n",
    "proc_sizes = [data_reg.shape[0]//number_of_proccessors] * (number_of_proccessors)\n",
    "proc_sizes[-1] += data_reg.shape[0]%number_of_proccessors\n",
    "proc_idx = [(sum(proc_sizes[:i]), sum(proc_sizes[:i+1])-1) for i in range(number_of_proccessors)]\n",
    "\n",
    "for i in range(number_of_proccessors):\n",
    "    p = Process(target=creating_image_max_su_tot, args=(proc_idx[i][0], proc_idx[i][1]))\n",
    "    jobs.append(p)\n",
    "    p.start()\n",
    "for i in range(number_of_proccessors):\n",
    "    jobs[i].join()\n",
    "\n",
    "for i in range(number_of_proccessors):\n",
    "    jobs[i].terminate()\n",
    "    jobs[i].close()\n",
    "del jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ML/data/pictures_299_299_transfer/splat/pu_circle_su_circle_60/raw_power_min_max_norm/color/log_pu5_su5/20pus_5sus_3channels/log_vgg16_max_su_total/images'"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_su_image_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1257/1257 [==============================] - 241s 192ms/step\n",
      "128 finished.\n",
      "1257/1257 [==============================] - 244s 194ms/step\n",
      "256 finished.\n",
      "1257/1257 [==============================] - 244s 194ms/step\n",
      "512 finished.\n",
      "1257/1257 [==============================] - 244s 194ms/step\n",
      "1024 finished.\n",
      "1257/1257 [==============================] - 244s 194ms/step\n",
      "2048 finished.\n",
      "1257/1257 [==============================] - 244s 194ms/step\n",
      "4096 finished.\n"
     ]
    }
   ],
   "source": [
    "# Multi-SU using single deep-alloc and another NN\n",
    "# Create dataset\n",
    "\n",
    "number_samples = [128, 256, 512, 1024, 2048, 4096]\n",
    "MAX_QUEUE_SIZE, WORKERS = 28, 4\n",
    "mini_batch = 128\n",
    "fp_penalty_coef, fn_penalty_coef = 1, 1\n",
    "dataset_path = '/'.join(image_dir.split('/')[:-1]) + \"/\" + model_name\n",
    "\n",
    "model_path = \"/home/shahrokh/projects/MLSpectrumAllocation/ML/data/pictures_299_299_transfer/\" + \\\n",
    "             \"splat/pu_circle_su_circle_60/raw_power_min_max_norm/color/log_pu5_su5/\" + \\\n",
    "             \"pus_1_sus_3_channels/models/log_vgg16/\"\n",
    "y = np.copy(data_max_su_tot[:,3::3])\n",
    "\n",
    "for number_sample in number_samples:\n",
    "    model = models.load_model(f\"{model_path}/{number_sample}/best_model_lambda_0.h5\",\n",
    "                              custom_objects={ 'loss': custom_loss(fp_penalty_coef, fn_penalty_coef),\n",
    "                                              'fp_mae': fp_mae,\n",
    "                                              'mae':'mae', 'mse':'mse'})\n",
    "    model.trainable = False\n",
    "    predic_batch_generator = PredictBatchGenerator(dataset_size=data_reg.shape[0] * max_sus_num,\n",
    "                                                   batch_size=mini_batch,\n",
    "                                                   image_dir=max_su_image_dir,\n",
    "                                                   max_x=max_x, max_y=max_y, \n",
    "                                                   number_image_channels=number_image_channels,\n",
    "                                                   start_idx=0, float_memory_used=float_memory_used)\n",
    "    number_dataset_path = f\"{dataset_path}/{number_sample}\"\n",
    "    if not os.path.exists(number_dataset_path):\n",
    "        os.makedirs(number_dataset_path)\n",
    "    predict_power = model.predict(predic_batch_generator, verbose=1, \n",
    "                                  workers=WORKERS, max_queue_size=MAX_QUEUE_SIZE,\n",
    "                                  use_multiprocessing=False)\n",
    "    X = np.copy(data_max_su_tot[:, 1:])\n",
    "    X[:,2::3] = predict_power.reshape(data_reg.shape[0], max_sus_num)\n",
    "    \n",
    "    np.savetxt(f\"{number_dataset_path}/X.txt\", X, delimiter=\",\")\n",
    "    np.savetxt(f\"{number_dataset_path}/y.txt\", y, delimiter=\",\")\n",
    "    print(f\"{number_sample} finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_model(n_inputs: int, n_outputs:int, kernel_lam, bias_lam, num_hidden_layers = 2, num_neurons = 100):\n",
    "    hidden_filter, last_layer_filter = 'relu', 'linear'\n",
    "    hidden_init, last_layer_init = \"lecun_normal\", \"RandomNormal\"  #he_uniform\n",
    "    model = Sequential()\n",
    "    for _ in range(num_hidden_layers):\n",
    "        model.add(layers.Dense(num_neurons,\n",
    "                              input_dim=n_inputs, \n",
    "                               kernel_initializer=hidden_init,\n",
    "                               activation=hidden_filter,\n",
    "                               kernel_regularizer=regularizers.l2(kernel_lam), \n",
    "                               bias_regularizer=regularizers.l2(bias_lam)))\n",
    "        model.add(BatchNormalization())\n",
    "    model.add(layers.Dropout(0.8))\n",
    "    model.add(layers.Dense(n_outputs, \n",
    "                           kernel_initializer=last_layer_init, \n",
    "                           activation=last_layer_filter,\n",
    "                           kernel_regularizer=regularizers.l2(kernel_lam),\n",
    "                           bias_regularizer=regularizers.l2(bias_lam)))\n",
    "    return model\n",
    "        \n",
    "    \n",
    "def custom_loss(fp_penalty_coef, fn_penalty_coef):\n",
    "    # custom loss function that penalize false positive and negative differently\n",
    "    def loss(y_true, y_pred):\n",
    "        res = y_pred - y_true\n",
    "        res = tf.where(res > 0, res * fp_penalty_coef, res * fn_penalty_coef)\n",
    "        return K.mean(K.square(res))\n",
    "    return loss\n",
    "\n",
    "def fp_mae(y_true, y_pred):\n",
    "    # custom metric that replace false negative with zero and return the mean of new vector\n",
    "    res = y_pred - y_true\n",
    "    res = tf.nn.relu(res)\n",
    "#     res = tf.where(res <= 0, 0, res)\n",
    "    return K.mean(res)\n",
    "\n",
    "def cus_mae(y_true, y_pred):\n",
    "    def log10(x):\n",
    "        numerator = K.log(x)\n",
    "        denominator = K.log(K.constant(10, dtype=numerator.dtype))\n",
    "        return numerator / denominator\n",
    "    p_true = K.sum(K.pow(10.0, y_true/10))\n",
    "    p_pred = K.sum(K.pow(10.0, y_pred/10))\n",
    "    return K.mean(K.abs(10 * log10(p_true) - 10 * log10(p_pred)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number_samples: 128 , New samples: 128\n",
      "Validation size: 26 , starts: 128 , ends: 153\n",
      "\n",
      "Lambda: 0 , Time: 0:00:07\n",
      "Train Error(all epochs): 2.7973647117614746 \n",
      " [8.269, 8.072, 8.019, 7.81, 7.861, 7.741, 7.667, 7.55, 7.487, 7.453, 7.325, 7.302, 7.136, 7.113, 6.993, 7.042, 6.847, 6.731, 6.623, 6.594, 6.462, 6.602, 6.405, 6.185, 6.027, 6.143, 6.159, 5.829, 5.802, 5.839, 5.487, 5.564, 5.495, 5.534, 5.407, 5.244, 5.009, 5.1, 4.894, 4.816, 4.918, 4.897, 4.813, 4.661, 4.636, 4.545, 4.416, 4.513, 4.339, 4.275, 4.124, 4.18, 4.233, 4.222, 4.366, 3.898, 4.122, 3.894, 4.105, 4.012, 3.83, 3.987, 4.023, 3.842, 3.839, 3.935, 3.989, 4.055, 3.54, 3.807, 3.77, 3.842, 3.797, 3.61, 3.747, 3.722, 3.787, 3.727, 3.686, 3.702, 3.831, 3.527, 3.644, 3.651, 3.569, 3.734, 3.632, 3.408, 3.475, 3.603, 3.775, 3.803, 3.567, 3.558, 3.792, 3.592, 3.419, 3.306, 3.417, 3.365, 3.461, 3.35, 3.361, 3.506, 3.401, 3.374, 3.563, 3.435, 3.372, 3.316, 3.268, 3.513, 3.704, 3.219, 3.425, 3.388, 3.441, 3.447, 3.557, 3.439, 3.26, 3.438, 3.289, 3.197, 3.478, 3.466, 3.315, 3.425, 3.401, 3.358, 3.238, 3.387, 3.257, 3.198, 3.513, 3.305, 3.634, 3.174, 3.301, 3.314, 3.068, 3.549, 3.34, 3.153, 3.441, 3.147, 3.391, 3.44, 3.254, 3.226, 3.352, 3.26, 3.368, 3.416, 3.217, 3.361, 3.346, 3.086, 3.19, 3.196, 3.228, 3.057, 3.243, 3.144, 3.309, 3.222, 3.15, 3.004, 3.046, 3.16, 3.448, 3.169, 3.031, 3.136, 3.232, 3.11, 3.12, 3.524, 3.186, 3.358, 3.108, 3.016, 3.116, 3.192, 3.352, 2.989, 3.237, 3.174, 3.22, 3.044, 3.262, 3.02, 2.923, 3.234, 3.294, 3.11, 3.324, 3.206, 3.324, 3.45, 3.351, 3.359, 3.193, 3.074, 3.366, 3.391, 2.921, 3.183, 3.168, 3.002, 3.05, 3.185, 3.158, 3.195, 3.08, 3.231, 2.904, 3.006, 2.883, 3.069, 2.978, 3.089, 3.244, 3.187, 2.956, 2.879, 3.102, 2.906, 2.911, 3.245, 2.901, 3.111, 2.974, 3.074, 3.538, 3.284, 2.917, 3.197, 3.069, 3.047, 3.186, 3.073, 3.375, 3.265, 2.881, 2.993, 3.091, 3.21, 3.179, 3.41, 3.162, 2.988, 3.117, 3.05, 3.037, 3.192, 3.114, 3.14, 3.194, 3.129, 3.026, 3.227, 3.25, 3.011, 3.012, 3.138, 2.797, 3.316, 3.123, 2.858, 3.21, 3.236, 3.013, 2.974, 3.113, 2.987, 3.207, 3.03, 3.406, 2.938, 3.264, 3.054, 3.045, 2.866, 3.301, 3.049, 2.98, 3.102, 2.998, 3.024, 3.004, 3.127, 3.125, 3.331, 3.093, 2.861, 2.946, 2.915, 2.909, 3.201]\n",
      "Val Error(all epochs): 7.008062362670898 \n",
      " [7.774, 7.715, 7.678, 7.651, 7.623, 7.588, 7.562, 7.533, 7.51, 7.48, 7.443, 7.413, 7.377, 7.345, 7.311, 7.271, 7.239, 7.206, 7.174, 7.127, 7.099, 7.083, 7.06, 7.032, 7.026, 7.026, 7.015, 7.018, 7.032, 7.025, 7.008, 7.025, 7.054, 7.064, 7.029, 7.05, 7.07, 7.091, 7.186, 7.293, 7.348, 7.364, 7.386, 7.398, 7.416, 7.47, 7.499, 7.527, 7.557, 7.552, 7.642, 7.703, 7.779, 7.911, 8.03, 8.125, 8.088, 7.952, 7.885, 7.894, 7.986, 8.154, 8.257, 8.343, 8.457, 8.558, 8.662, 8.705, 8.809, 8.879, 8.725, 8.625, 8.633, 8.694, 8.8, 8.868, 9.075, 9.321, 9.432, 9.526, 9.592, 9.587, 9.439, 9.428, 9.386, 9.312, 9.349, 9.373, 9.378, 9.418, 9.47, 9.554, 9.665, 9.832, 9.981, 10.094, 10.137, 10.201, 10.244, 10.248, 10.258, 10.328, 10.381, 10.488, 10.633, 10.715, 10.753, 10.744, 10.605, 10.545, 10.496, 10.502, 10.391, 10.236, 10.15, 10.226, 10.371, 10.522, 10.696, 10.887, 11.033, 11.14, 11.144, 11.112, 11.055, 11.056, 11.085, 11.135, 11.16, 11.128, 11.147, 11.074, 10.961, 10.885, 10.884, 10.941, 11.018, 11.108, 11.22, 11.39, 11.572, 11.741, 11.804, 11.793, 11.838, 11.768, 11.749, 11.781, 11.948, 12.075, 12.096, 12.096, 12.012, 11.979, 11.916, 11.99, 12.077, 12.15, 12.193, 12.272, 12.314, 12.261, 12.297, 12.361, 12.431, 12.417, 12.433, 12.485, 12.474, 12.449, 12.418, 12.456, 12.418, 12.44, 12.584, 12.779, 12.957, 13.031, 13.085, 13.079, 13.118, 13.163, 13.238, 13.25, 13.286, 13.305, 13.393, 13.425, 13.409, 13.301, 13.233, 13.224, 13.143, 12.994, 13.071, 13.089, 13.201, 13.404, 13.547, 13.438, 13.257, 13.107, 13.023, 13.015, 13.145, 13.172, 13.191, 13.167, 13.042, 12.846, 12.731, 12.639, 12.54, 12.472, 12.441, 12.334, 12.281, 12.334, 12.465, 12.57, 12.742, 12.874, 13.032, 13.062, 13.065, 13.088, 13.102, 13.101, 13.055, 12.998, 12.94, 12.869, 12.694, 12.546, 12.521, 12.533, 12.549, 12.727, 12.792, 12.858, 12.815, 12.745, 12.701, 12.707, 12.835, 12.98, 13.198, 13.388, 13.356, 13.468, 13.683, 13.771, 13.826, 13.862, 13.825, 13.76, 13.72, 13.487, 13.302, 13.27, 13.221, 13.137, 12.896, 12.785, 12.702, 12.728, 12.679, 12.567, 12.451, 12.424, 12.433, 12.477, 12.42, 12.386, 12.291, 12.292, 12.279, 12.171, 12.048, 11.936, 11.852, 11.846, 11.811, 11.748, 11.732, 11.78, 11.75, 11.728, 11.76, 11.742, 11.852, 12.05, 12.229, 12.41, 12.562, 12.621, 12.566, 12.393, 12.26, 12.328]\n",
      "Val custom mae Error(all epochs): 0.40540313720703125 \n",
      " [11.815, 11.897, 11.883, 11.845, 11.813, 11.753, 11.669, 11.576, 11.5, 11.418, 11.33, 11.255, 11.152, 11.064, 10.961, 10.842, 10.714, 10.542, 10.408, 10.211, 9.984, 9.731, 9.43, 9.132, 8.868, 8.587, 8.326, 8.039, 7.785, 7.537, 7.297, 7.123, 6.833, 6.363, 5.761, 5.188, 4.507, 3.798, 2.901, 2.486, 2.448, 2.237, 1.867, 1.618, 1.205, 0.622, 0.405, 1.699, 2.83, 4.191, 5.607, 6.657, 7.611, 8.401, 8.449, 8.532, 8.764, 9.32, 10.451, 11.534, 14.315, 16.953, 19.364, 21.562, 22.94, 24.849, 26.212, 27.17, 29.986, 33.299, 34.148, 35.227, 36.717, 39.753, 41.908, 41.844, 42.326, 42.352, 42.706, 43.699, 46.137, 48.185, 49.544, 51.455, 52.84, 53.663, 55.084, 56.493, 57.346, 57.466, 59.254, 60.105, 62.057, 65.79, 70.399, 73.91, 77.311, 80.487, 83.855, 86.092, 87.757, 88.143, 87.687, 88.031, 89.173, 89.764, 89.094, 88.55, 86.934, 85.16, 84.868, 84.266, 83.23, 82.439, 81.839, 82.963, 85.564, 88.075, 90.518, 89.907, 91.296, 94.651, 96.852, 97.977, 100.898, 103.282, 105.273, 105.946, 104.766, 102.954, 102.101, 100.56, 98.655, 99.616, 101.868, 103.104, 103.424, 105.267, 107.669, 110.759, 115.167, 120.197, 122.776, 122.576, 122.091, 121.921, 124.71, 126.035, 128.645, 128.838, 128.502, 129.032, 128.215, 127.821, 125.873, 125.515, 124.186, 123.511, 121.747, 121.308, 120.67, 118.74, 119.144, 120.069, 120.902, 119.754, 120.401, 123.157, 124.268, 125.816, 129.861, 132.801, 133.484, 134.523, 136.175, 136.423, 137.481, 138.425, 139.648, 139.943, 142.623, 145.351, 146.438, 146.956, 147.462, 148.261, 149.349, 146.552, 143.688, 141.363, 140.27, 140.349, 138.508, 137.711, 140.715, 142.427, 145.475, 149.985, 152.875, 153.109, 152.273, 149.724, 147.845, 147.227, 150.677, 150.811, 150.783, 150.607, 146.982, 142.194, 138.066, 136.182, 133.574, 131.606, 128.268, 124.733, 124.031, 125.757, 129.096, 133.421, 138.135, 139.995, 140.775, 141.112, 141.239, 142.612, 143.361, 144.802, 146.149, 147.586, 148.147, 145.785, 140.327, 136.405, 133.526, 131.908, 131.571, 131.804, 131.553, 131.144, 127.247, 124.204, 122.735, 122.301, 124.187, 128.194, 131.776, 135.947, 138.063, 140.926, 142.93, 143.128, 143.663, 143.562, 141.955, 140.986, 141.472, 140.107, 138.824, 140.764, 141.408, 141.572, 135.803, 129.032, 122.828, 121.63, 118.453, 116.964, 113.662, 111.965, 109.905, 109.677, 108.468, 107.927, 106.905, 108.844, 109.954, 109.472, 107.232, 105.181, 102.425, 100.053, 97.516, 94.732, 92.783, 92.592, 92.144, 91.327, 92.097, 93.037, 96.695, 100.346, 103.023, 104.192, 105.694, 106.579, 104.772, 101.537, 98.484, 100.322]\n",
      "\n",
      "Lambda: 0.01 , Time: 0:00:07\n",
      "Train Error(all epochs): 2.8536832332611084 \n",
      " [8.174, 7.977, 7.981, 7.831, 7.691, 7.594, 7.499, 7.528, 7.445, 7.262, 7.249, 7.317, 7.125, 7.064, 6.918, 6.848, 6.79, 6.701, 6.607, 6.536, 6.427, 6.403, 6.228, 6.339, 6.102, 6.108, 5.793, 5.964, 5.812, 5.569, 5.554, 5.591, 5.422, 5.335, 5.368, 5.222, 5.183, 5.011, 4.934, 5.005, 4.775, 4.922, 4.598, 4.59, 4.318, 4.545, 4.493, 4.502, 4.462, 4.337, 4.261, 4.512, 4.306, 4.278, 4.366, 4.07, 4.08, 4.133, 4.063, 4.081, 3.684, 3.839, 3.824, 3.761, 3.883, 3.519, 3.77, 3.7, 3.861, 3.864, 3.487, 3.718, 3.707, 3.678, 3.636, 3.606, 3.731, 3.698, 3.564, 3.75, 3.5, 3.577, 3.345, 3.278, 3.71, 3.477, 3.502, 3.652, 3.743, 3.645, 3.362, 3.516, 3.4, 3.355, 3.585, 3.382, 3.527, 3.637, 3.365, 3.768, 3.61, 3.365, 3.568, 3.499, 3.427, 3.423, 3.473, 3.344, 3.42, 3.585, 3.427, 3.409, 3.28, 3.329, 3.307, 3.345, 3.309, 3.383, 3.322, 3.539, 3.252, 3.279, 3.32, 3.279, 3.456, 3.348, 3.357, 3.534, 3.365, 3.496, 3.182, 3.378, 3.223, 3.415, 3.353, 3.276, 3.224, 3.373, 3.381, 3.401, 3.382, 3.277, 3.431, 3.302, 3.341, 3.281, 3.228, 3.297, 3.2, 3.458, 3.111, 3.522, 3.24, 3.215, 3.198, 3.274, 3.224, 3.111, 3.294, 3.112, 3.329, 3.419, 3.45, 3.192, 3.406, 3.284, 3.339, 3.181, 3.371, 3.2, 3.296, 3.085, 3.179, 3.112, 3.306, 3.113, 3.248, 3.222, 3.231, 3.183, 3.264, 3.209, 3.155, 3.006, 3.275, 3.124, 3.12, 3.287, 3.224, 3.122, 3.169, 3.207, 3.432, 3.276, 3.271, 3.152, 3.268, 3.109, 2.888, 3.447, 3.172, 3.249, 2.96, 3.266, 3.168, 3.167, 3.434, 3.018, 2.936, 3.121, 3.1, 3.115, 3.157, 3.135, 3.154, 3.263, 3.087, 3.042, 3.341, 3.176, 3.171, 3.053, 2.931, 3.124, 3.273, 3.004, 3.283, 3.216, 3.164, 2.981, 3.07, 3.112, 3.232, 3.34, 3.155, 2.991, 3.023, 3.128, 3.223, 3.091, 3.195, 2.992, 3.339, 3.211, 2.992, 3.182, 3.21, 3.032, 3.235, 3.176, 3.082, 3.036, 3.094, 3.117, 3.195, 3.112, 3.361, 3.109, 3.129, 3.143, 3.043, 3.189, 3.231, 3.244, 3.047, 3.181, 3.081, 3.294, 3.216, 3.435, 3.411, 3.048, 2.96, 3.151, 3.05, 3.107, 3.049, 3.147, 3.318, 2.99, 3.001, 3.167, 3.006, 3.038, 3.148, 2.971, 3.119, 3.249, 2.926, 3.18, 3.08, 3.243, 3.254, 3.084, 3.394, 3.097, 3.231, 3.357, 2.988, 2.854]\n",
      "Val Error(all epochs): 6.851119041442871 \n",
      " [7.774, 7.707, 7.657, 7.622, 7.606, 7.587, 7.563, 7.55, 7.53, 7.498, 7.462, 7.433, 7.396, 7.347, 7.283, 7.238, 7.2, 7.157, 7.121, 7.082, 7.044, 7.019, 7.003, 6.991, 6.989, 6.977, 6.972, 6.967, 6.973, 6.965, 6.95, 6.938, 6.921, 6.912, 6.906, 6.904, 6.905, 6.919, 6.905, 6.896, 6.926, 6.973, 6.997, 7.017, 6.98, 6.912, 6.857, 6.851, 6.908, 6.973, 7.074, 7.139, 7.107, 7.094, 7.067, 7.122, 7.197, 7.252, 7.261, 7.35, 7.396, 7.426, 7.344, 7.346, 7.335, 7.282, 7.17, 7.125, 7.073, 7.075, 7.1, 7.171, 7.235, 7.291, 7.359, 7.426, 7.462, 7.466, 7.522, 7.525, 7.479, 7.52, 7.65, 7.797, 7.678, 7.665, 7.615, 7.584, 7.525, 7.507, 7.596, 7.749, 7.832, 7.8, 7.622, 7.525, 7.488, 7.521, 7.571, 7.574, 7.574, 7.62, 7.646, 7.615, 7.647, 7.658, 7.663, 7.661, 7.694, 7.807, 7.84, 7.848, 7.811, 7.746, 7.624, 7.525, 7.428, 7.426, 7.489, 7.582, 7.637, 7.634, 7.624, 7.646, 7.656, 7.611, 7.59, 7.614, 7.657, 7.632, 7.614, 7.538, 7.47, 7.452, 7.5, 7.536, 7.553, 7.613, 7.668, 7.702, 7.746, 7.774, 7.843, 7.858, 7.868, 7.919, 7.898, 7.882, 7.854, 7.844, 7.858, 7.882, 7.811, 7.721, 7.709, 7.73, 7.705, 7.708, 7.714, 7.696, 7.671, 7.673, 7.654, 7.69, 7.71, 7.689, 7.665, 7.629, 7.531, 7.465, 7.488, 7.574, 7.621, 7.651, 7.666, 7.683, 7.678, 7.68, 7.685, 7.709, 7.696, 7.692, 7.67, 7.638, 7.682, 7.686, 7.638, 7.602, 7.606, 7.649, 7.734, 7.835, 7.925, 7.964, 7.949, 7.873, 7.746, 7.62, 7.581, 7.608, 7.709, 7.819, 7.922, 7.998, 7.997, 7.978, 8.011, 8.059, 8.045, 7.983, 7.909, 7.838, 7.836, 7.871, 7.911, 7.927, 7.796, 7.717, 7.678, 7.646, 7.642, 7.648, 7.66, 7.67, 7.635, 7.573, 7.536, 7.55, 7.55, 7.57, 7.637, 7.688, 7.701, 7.711, 7.709, 7.686, 7.617, 7.636, 7.696, 7.742, 7.813, 7.793, 7.767, 7.68, 7.619, 7.588, 7.551, 7.563, 7.611, 7.657, 7.696, 7.735, 7.795, 7.818, 7.816, 7.813, 7.828, 7.78, 7.709, 7.63, 7.594, 7.612, 7.677, 7.695, 7.637, 7.612, 7.567, 7.525, 7.499, 7.471, 7.441, 7.494, 7.478, 7.385, 7.313, 7.318, 7.348, 7.357, 7.361, 7.411, 7.434, 7.457, 7.5, 7.521, 7.536, 7.511, 7.489, 7.439, 7.387, 7.42, 7.476, 7.456, 7.476, 7.494, 7.513, 7.51, 7.546, 7.616, 7.686, 7.718]\n",
      "Val custom mae Error(all epochs): 0.22589492797851562 \n",
      " [10.249, 10.82, 10.915, 10.876, 10.808, 10.7, 10.571, 10.499, 10.424, 10.381, 10.363, 10.323, 10.23, 10.095, 9.932, 9.821, 9.762, 9.762, 9.721, 9.642, 9.468, 9.295, 9.172, 9.089, 9.07, 9.002, 8.893, 8.567, 8.233, 8.139, 8.045, 7.939, 7.873, 7.745, 7.429, 7.027, 6.733, 6.645, 6.672, 6.608, 6.54, 6.325, 6.257, 6.19, 6.153, 6.125, 6.453, 6.524, 6.482, 6.253, 5.83, 5.558, 5.336, 5.141, 5.13, 4.873, 4.543, 4.132, 3.812, 3.083, 2.693, 2.338, 2.113, 2.206, 2.363, 2.832, 3.316, 3.816, 4.021, 3.972, 3.752, 3.209, 2.879, 2.623, 1.981, 1.06, 0.226, 1.495, 2.548, 3.293, 4.632, 7.19, 10.1, 12.669, 10.617, 8.991, 8.864, 10.123, 11.105, 11.893, 13.818, 16.174, 18.083, 18.721, 16.922, 15.981, 16.207, 16.825, 17.923, 18.834, 20.649, 22.038, 23.239, 23.672, 22.807, 21.717, 21.633, 22.618, 23.761, 25.627, 26.897, 26.881, 25.413, 23.334, 19.674, 17.353, 17.182, 17.829, 18.665, 19.445, 19.122, 18.541, 18.081, 18.451, 18.937, 18.269, 18.105, 18.845, 19.743, 19.443, 19.069, 18.583, 17.553, 16.583, 17.088, 17.345, 16.955, 17.068, 16.55, 15.856, 15.338, 15.331, 16.465, 18.075, 20.011, 23.118, 22.735, 22.732, 23.685, 24.457, 23.742, 22.015, 19.31, 17.3, 16.884, 16.905, 17.087, 17.633, 17.627, 17.099, 15.929, 15.03, 15.039, 17.664, 21.046, 23.492, 23.646, 22.262, 19.607, 17.714, 18.268, 19.422, 21.019, 22.506, 22.7, 21.969, 20.522, 20.35, 21.006, 21.32, 21.281, 21.809, 22.851, 23.409, 24.14, 21.378, 19.052, 19.309, 19.094, 18.499, 18.339, 19.337, 20.731, 21.917, 23.166, 23.958, 24.091, 24.522, 25.084, 26.205, 27.041, 29.095, 30.959, 32.582, 32.984, 33.961, 36.603, 37.556, 37.415, 35.802, 34.975, 34.206, 33.548, 32.775, 33.522, 34.262, 30.439, 28.548, 27.414, 25.542, 24.155, 22.954, 22.466, 23.441, 24.332, 23.232, 22.683, 23.316, 24.262, 24.952, 25.786, 27.349, 29.788, 32.282, 34.268, 35.876, 35.751, 35.831, 34.213, 32.746, 31.039, 30.753, 30.247, 28.871, 28.945, 30.594, 30.956, 30.759, 30.275, 29.925, 29.87, 30.723, 31.922, 33.825, 34.676, 34.777, 34.463, 33.818, 32.799, 31.707, 31.146, 30.797, 30.611, 29.62, 27.628, 25.944, 25.006, 25.114, 24.725, 23.886, 22.749, 22.345, 20.802, 19.38, 19.074, 20.255, 21.372, 22.416, 23.311, 24.306, 24.646, 24.872, 25.367, 25.966, 25.243, 20.767, 18.124, 16.776, 15.692, 15.6, 15.821, 15.275, 15.185, 15.584, 15.826, 16.442, 17.627, 18.549, 19.79, 20.342]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Lambda: 0.1 , Time: 0:00:08\n",
      "Train Error(all epochs): 2.863245964050293 \n",
      " [8.167, 7.931, 7.912, 7.804, 7.712, 7.665, 7.528, 7.471, 7.373, 7.31, 7.212, 7.144, 7.039, 7.051, 6.865, 6.814, 6.686, 6.603, 6.504, 6.503, 6.483, 6.217, 6.129, 6.019, 6.097, 5.748, 5.857, 5.685, 5.382, 5.709, 5.425, 5.26, 5.212, 5.427, 5.191, 5.043, 4.961, 4.917, 4.911, 4.882, 4.823, 4.722, 4.644, 4.738, 4.635, 4.377, 4.634, 4.495, 4.27, 4.245, 3.944, 4.245, 4.276, 4.151, 4.008, 4.016, 3.803, 4.062, 3.823, 4.04, 4.145, 3.93, 3.972, 3.845, 3.835, 3.668, 3.787, 3.876, 3.829, 3.615, 3.636, 3.997, 3.654, 3.647, 3.606, 3.761, 3.568, 3.318, 3.47, 3.533, 3.707, 3.713, 3.302, 3.563, 3.841, 3.638, 3.437, 3.804, 3.62, 3.607, 3.62, 3.633, 3.485, 3.418, 3.639, 3.238, 3.976, 3.547, 3.462, 3.54, 3.461, 3.24, 3.391, 3.601, 3.374, 3.481, 3.361, 3.499, 3.62, 3.356, 3.374, 3.521, 3.256, 3.301, 3.723, 3.463, 3.419, 3.275, 3.424, 3.582, 3.457, 3.266, 3.418, 3.469, 3.101, 3.366, 3.434, 3.511, 3.346, 3.469, 3.378, 3.623, 3.379, 3.356, 3.597, 3.408, 3.268, 3.501, 3.424, 3.382, 3.169, 3.363, 3.236, 3.313, 3.367, 3.354, 3.297, 3.252, 3.403, 3.435, 3.343, 3.271, 3.431, 3.414, 3.168, 3.318, 3.261, 3.343, 3.239, 3.207, 3.297, 3.27, 3.425, 3.253, 3.325, 3.355, 3.173, 3.371, 3.299, 3.19, 3.125, 3.331, 3.42, 3.157, 3.191, 3.256, 3.503, 3.177, 3.341, 3.28, 3.248, 3.456, 3.262, 3.162, 3.017, 3.286, 3.184, 3.178, 3.284, 3.141, 3.061, 3.013, 3.21, 3.12, 3.266, 3.262, 3.222, 3.094, 3.342, 3.136, 3.187, 3.206, 2.863, 3.186, 3.128, 3.277, 2.958, 3.013, 3.355, 3.028, 3.141, 3.275, 3.344, 3.1, 3.22, 3.367, 3.224, 3.211, 3.139, 3.378, 3.34, 3.081, 3.325, 3.382, 3.092, 3.419, 3.113, 3.523, 3.248, 3.347, 2.922, 3.29, 3.024, 3.447, 3.001, 3.201, 3.193, 3.338, 3.188, 3.351, 3.293, 3.063, 3.311, 3.407, 3.208, 3.226, 3.071, 3.095, 3.209, 3.363, 3.149, 3.117, 3.452, 3.131, 3.328, 3.231, 3.137, 3.249, 3.094, 3.221, 3.343, 2.957, 3.339, 3.26, 3.168, 3.459, 3.001, 3.143, 3.27, 3.086, 3.115, 3.343, 3.084, 3.037, 3.066, 3.385, 3.36, 3.138, 3.082, 3.226, 3.146, 3.254, 3.264, 3.046, 3.287, 3.3, 3.561, 3.11, 3.152, 3.153, 3.001, 2.982, 3.191, 3.042, 3.189, 3.126, 3.031, 3.167, 3.323, 3.118]\n",
      "Val Error(all epochs): 6.471344470977783 \n",
      " [7.979, 7.806, 7.734, 7.678, 7.64, 7.599, 7.561, 7.531, 7.502, 7.472, 7.442, 7.423, 7.4, 7.377, 7.353, 7.331, 7.307, 7.277, 7.234, 7.195, 7.169, 7.15, 7.124, 7.097, 7.085, 7.084, 7.066, 7.044, 7.017, 6.981, 6.949, 6.932, 6.91, 6.874, 6.876, 6.885, 6.879, 6.894, 6.874, 6.846, 6.836, 6.822, 6.829, 6.842, 6.872, 6.81, 6.682, 6.604, 6.665, 6.725, 6.765, 6.711, 6.71, 6.702, 6.727, 6.768, 6.765, 6.745, 6.702, 6.714, 6.771, 6.818, 6.764, 6.774, 6.812, 6.828, 6.758, 6.72, 6.783, 6.88, 6.888, 6.825, 6.741, 6.715, 6.714, 6.748, 6.754, 6.792, 6.825, 6.857, 6.821, 6.725, 6.667, 6.564, 6.504, 6.471, 6.475, 6.545, 6.611, 6.673, 6.744, 6.803, 6.794, 6.778, 6.823, 6.884, 6.908, 6.959, 6.969, 6.962, 6.88, 6.836, 6.823, 6.806, 6.852, 6.976, 7.078, 7.095, 7.052, 6.991, 6.882, 6.826, 6.876, 6.954, 6.954, 6.885, 6.803, 6.858, 6.903, 6.913, 6.894, 6.877, 6.817, 6.808, 6.831, 6.838, 6.927, 6.974, 7.012, 7.007, 6.958, 6.896, 6.901, 6.984, 7.103, 7.175, 7.139, 7.12, 7.075, 6.969, 6.863, 6.839, 6.832, 6.853, 6.906, 6.949, 6.967, 6.931, 6.959, 7.027, 6.994, 6.928, 6.924, 6.909, 6.935, 6.992, 7.02, 7.001, 7.001, 7.021, 7.065, 7.071, 7.017, 6.958, 6.914, 6.852, 6.838, 6.859, 6.918, 7.005, 7.093, 7.379, 7.981, 7.919, 7.582, 7.273, 7.015, 6.965, 7.027, 7.085, 7.068, 7.084, 7.052, 7.045, 7.029, 7.012, 7.071, 7.137, 7.093, 7.018, 6.924, 6.872, 6.879, 6.917, 6.941, 6.934, 6.984, 7.067, 7.122, 7.118, 7.141, 7.199, 7.247, 7.218, 7.128, 7.022, 6.922, 6.892, 7.001, 7.084, 7.152, 7.134, 7.141, 7.103, 7.002, 6.928, 6.834, 6.835, 6.903, 6.997, 6.976, 6.975, 6.997, 7.043, 6.956, 6.93, 6.973, 6.915, 6.843, 6.837, 6.875, 7.017, 7.151, 7.443, 7.318, 7.203, 7.256, 7.326, 7.395, 7.289, 7.078, 7.136, 7.124, 6.982, 6.982, 7.022, 7.089, 7.098, 7.073, 7.084, 7.14, 7.177, 7.087, 7.041, 6.989, 7.033, 7.143, 7.153, 7.05, 6.998, 7.038, 7.035, 7.003, 6.962, 7.021, 7.032, 6.95, 6.854, 6.911, 7.013, 7.109, 7.088, 7.007, 6.857, 6.761, 6.771, 6.875, 6.925, 6.95, 6.854, 6.831, 6.875, 6.878, 6.868, 6.839, 6.837, 6.891, 6.928, 6.89, 6.816, 6.74, 6.67, 6.679, 6.7, 6.793, 6.878, 6.901, 6.971, 6.98, 7.022]\n",
      "Val custom mae Error(all epochs): 0.09732818603515625 \n",
      " [12.384, 12.372, 12.301, 12.185, 12.118, 12.042, 11.944, 11.868, 11.773, 11.682, 11.59, 11.52, 11.426, 11.325, 11.234, 11.161, 11.099, 11.038, 10.974, 10.891, 10.819, 10.723, 10.624, 10.538, 10.45, 10.442, 10.378, 10.326, 10.236, 10.053, 9.856, 9.728, 9.627, 9.627, 9.742, 9.753, 9.688, 9.594, 9.342, 9.117, 8.981, 9.018, 9.023, 8.97, 8.932, 8.838, 8.613, 8.571, 8.66, 8.599, 8.458, 8.195, 7.981, 7.872, 7.7, 7.554, 7.394, 7.119, 7.015, 6.992, 6.887, 6.86, 6.516, 6.241, 6.026, 6.182, 6.259, 6.225, 6.06, 6.053, 6.058, 6.123, 6.324, 6.527, 6.618, 6.389, 5.697, 5.171, 5.329, 5.612, 5.733, 6.022, 6.111, 6.335, 6.414, 6.151, 5.9, 5.882, 5.676, 5.537, 5.614, 5.631, 5.38, 5.235, 4.976, 4.714, 4.447, 4.678, 5.107, 5.31, 5.362, 5.314, 5.342, 5.442, 5.31, 5.083, 4.937, 5.061, 5.096, 5.109, 4.649, 3.909, 3.411, 2.994, 2.772, 3.187, 4.087, 4.662, 4.629, 4.723, 5.037, 5.173, 5.193, 5.04, 5.087, 5.121, 5.413, 5.834, 5.944, 5.773, 5.551, 5.394, 5.412, 5.432, 5.266, 4.952, 4.763, 4.697, 4.892, 5.067, 5.145, 4.969, 4.81, 4.671, 4.08, 3.788, 3.428, 3.18, 2.939, 2.767, 2.264, 1.789, 1.308, 1.078, 1.224, 2.005, 2.812, 3.062, 3.079, 2.78, 2.523, 2.586, 2.752, 2.493, 2.171, 2.177, 1.89, 1.015, 0.879, 1.545, 2.222, 1.066, 0.452, 0.283, 0.097, 0.952, 1.52, 2.155, 2.508, 2.191, 1.897, 1.442, 2.729, 4.095, 4.452, 4.827, 5.086, 5.105, 4.862, 4.503, 4.031, 3.239, 2.854, 2.885, 3.342, 3.67, 3.617, 3.594, 3.555, 3.294, 3.195, 3.316, 3.643, 3.747, 3.461, 3.617, 3.767, 3.816, 3.702, 3.187, 2.618, 2.332, 2.099, 2.22, 2.746, 2.864, 3.239, 3.849, 4.241, 4.292, 3.952, 3.548, 3.379, 3.271, 3.304, 3.791, 4.364, 4.757, 4.517, 4.526, 4.501, 4.314, 4.253, 4.343, 4.496, 3.713, 3.016, 2.165, 1.914, 1.866, 2.218, 2.492, 2.876, 3.291, 3.269, 2.926, 2.935, 3.458, 4.37, 4.64, 4.737, 4.681, 4.588, 4.44, 4.294, 4.53, 4.695, 4.688, 4.649, 4.783, 4.583, 4.36, 4.062, 4.2, 4.357, 4.43, 4.262, 4.369, 4.304, 4.122, 3.622, 3.362, 3.363, 3.04, 2.541, 2.349, 2.602, 3.471, 3.863, 3.819, 3.459, 3.118, 2.981, 2.991, 2.831, 2.65, 3.196, 3.423, 3.082, 2.737, 2.685, 2.725, 2.661, 2.685, 2.512, 2.646, 3.287, 3.768, 3.981, 3.712]\n",
      "\n",
      "Lambda: 1 , Time: 0:00:08\n",
      "Train Error(all epochs): 3.0061287879943848 \n",
      " [8.204, 7.977, 7.908, 7.756, 7.761, 7.611, 7.576, 7.704, 7.402, 7.31, 7.288, 7.199, 7.212, 7.06, 7.009, 6.943, 6.94, 6.757, 6.698, 6.555, 6.441, 6.389, 6.326, 6.148, 6.165, 6.119, 5.845, 5.887, 5.965, 5.903, 5.593, 5.662, 5.397, 5.556, 5.071, 5.224, 5.263, 5.06, 5.004, 4.941, 4.933, 4.961, 4.711, 4.778, 4.77, 4.63, 4.437, 4.631, 4.457, 4.606, 4.259, 4.252, 4.232, 4.296, 4.124, 4.306, 4.245, 4.31, 4.238, 4.149, 4.315, 4.269, 4.38, 4.039, 4.329, 3.966, 4.099, 3.992, 4.009, 4.013, 4.049, 3.962, 4.186, 4.111, 4.009, 3.841, 4.034, 3.709, 4.001, 3.88, 3.679, 3.984, 3.793, 4.116, 4.17, 3.881, 3.784, 3.649, 3.458, 3.946, 3.682, 3.766, 3.81, 3.823, 3.653, 3.951, 3.822, 3.858, 3.623, 3.746, 3.718, 3.847, 3.852, 3.452, 3.691, 3.694, 3.724, 3.73, 3.829, 3.701, 3.637, 3.68, 3.733, 3.792, 3.724, 3.737, 3.644, 3.778, 3.664, 3.698, 3.735, 3.428, 3.758, 3.692, 3.47, 3.708, 3.479, 3.536, 3.561, 3.561, 3.648, 3.706, 3.492, 3.639, 3.535, 3.599, 3.364, 3.7, 3.666, 3.788, 3.663, 3.63, 3.65, 3.68, 3.567, 3.556, 3.669, 3.502, 3.46, 3.665, 3.655, 3.618, 3.535, 3.323, 3.462, 3.479, 3.344, 3.353, 3.468, 3.312, 3.427, 3.479, 3.433, 3.65, 3.648, 3.585, 3.363, 3.721, 3.574, 3.5, 3.537, 3.556, 3.37, 3.607, 3.481, 3.331, 3.581, 3.458, 3.363, 3.548, 3.522, 3.226, 3.532, 3.36, 3.307, 3.439, 3.347, 3.317, 3.35, 3.205, 3.486, 3.28, 3.363, 3.399, 3.348, 3.509, 3.593, 3.5, 3.505, 3.386, 3.421, 3.293, 3.263, 3.532, 3.353, 3.483, 3.511, 3.263, 3.391, 3.494, 3.69, 3.288, 3.527, 3.585, 3.458, 3.535, 3.571, 3.473, 3.122, 3.484, 3.654, 3.396, 3.447, 3.403, 3.432, 3.18, 3.238, 3.168, 3.334, 3.356, 3.518, 3.218, 3.547, 3.447, 3.341, 3.281, 3.449, 3.412, 3.275, 3.271, 3.277, 3.353, 3.426, 3.283, 3.26, 3.227, 3.245, 3.332, 3.273, 3.343, 3.109, 3.504, 3.23, 3.54, 3.427, 3.066, 3.484, 3.176, 3.424, 3.357, 3.653, 3.522, 3.365, 3.409, 3.434, 3.368, 3.306, 3.408, 3.513, 3.562, 3.465, 3.386, 3.297, 3.668, 3.454, 3.359, 3.194, 3.485, 3.34, 3.449, 3.371, 3.405, 3.279, 3.295, 3.407, 3.485, 3.381, 3.323, 3.373, 3.416, 3.235, 3.142, 3.006, 3.447, 3.427, 3.171, 3.32, 3.414, 3.21, 3.369]\n",
      "Val Error(all epochs): 6.3778276443481445 \n",
      " [7.58, 7.537, 7.496, 7.457, 7.426, 7.412, 7.405, 7.399, 7.393, 7.385, 7.379, 7.371, 7.362, 7.347, 7.34, 7.335, 7.331, 7.324, 7.314, 7.312, 7.309, 7.308, 7.312, 7.305, 7.286, 7.273, 7.269, 7.269, 7.256, 7.26, 7.267, 7.261, 7.265, 7.252, 7.244, 7.238, 7.233, 7.229, 7.233, 7.23, 7.198, 7.185, 7.163, 7.146, 7.13, 7.128, 7.121, 7.114, 7.098, 7.081, 7.08, 7.09, 7.058, 7.022, 6.99, 7.007, 7.009, 7.005, 7.06, 7.101, 7.083, 7.034, 7.016, 7.048, 7.023, 6.934, 6.895, 6.922, 6.974, 6.972, 6.956, 6.914, 6.923, 6.968, 7.001, 6.979, 6.937, 6.943, 6.989, 7.02, 6.955, 6.939, 7.026, 7.076, 7.158, 7.178, 7.22, 7.224, 7.135, 7.024, 7.03, 7.002, 6.879, 6.934, 6.929, 6.977, 6.975, 6.892, 6.747, 6.688, 6.795, 6.939, 6.971, 6.991, 6.981, 6.852, 6.858, 6.922, 6.962, 6.957, 6.877, 6.846, 7.004, 7.35, 7.35, 7.205, 7.22, 7.268, 7.198, 7.194, 7.198, 7.227, 7.119, 7.088, 7.234, 7.296, 7.214, 7.141, 7.06, 7.027, 6.965, 6.999, 7.11, 7.14, 7.135, 7.131, 7.196, 7.2, 7.292, 7.411, 7.426, 7.316, 7.101, 7.201, 7.291, 7.375, 7.453, 7.354, 7.266, 7.251, 7.351, 7.403, 7.272, 7.291, 7.248, 7.174, 7.025, 6.893, 6.841, 6.932, 7.006, 7.182, 7.325, 7.504, 7.55, 7.445, 7.284, 7.287, 7.248, 7.221, 7.071, 6.856, 6.856, 6.983, 7.059, 7.361, 7.376, 7.207, 7.176, 7.276, 7.305, 7.433, 7.527, 7.532, 7.622, 7.706, 7.771, 7.809, 7.77, 7.546, 7.249, 7.186, 7.184, 7.29, 7.354, 7.559, 7.851, 7.877, 7.749, 7.703, 7.867, 7.676, 7.56, 7.679, 7.598, 7.553, 7.683, 7.731, 7.697, 7.576, 7.468, 7.251, 7.397, 7.485, 7.631, 7.754, 7.588, 7.337, 7.166, 7.198, 7.313, 7.366, 7.35, 7.166, 7.137, 7.495, 7.625, 7.638, 7.509, 7.223, 7.133, 7.09, 7.237, 7.434, 7.33, 7.274, 7.303, 7.292, 7.025, 6.943, 6.96, 7.113, 7.23, 7.081, 7.026, 6.978, 7.016, 7.227, 7.281, 7.129, 7.009, 6.995, 7.104, 7.277, 7.392, 7.455, 7.372, 7.194, 7.085, 6.907, 7.084, 7.114, 7.292, 7.581, 7.326, 7.241, 7.171, 7.177, 7.09, 7.032, 6.996, 7.095, 7.345, 7.507, 7.363, 7.184, 7.186, 7.131, 7.057, 7.146, 7.747, 7.263, 7.597, 7.65, 7.288, 7.163, 7.01, 6.807, 6.715, 6.378, 6.402, 6.615, 6.631, 6.511, 6.76, 6.843, 6.555, 6.462, 6.61, 6.482]\n",
      "Val custom mae Error(all epochs): 2.087949752807617 \n",
      " [11.14, 11.208, 11.183, 11.14, 11.115, 11.135, 11.158, 11.197, 11.237, 11.24, 11.242, 11.236, 11.226, 11.191, 11.187, 11.182, 11.177, 11.161, 11.144, 11.137, 11.117, 11.095, 11.118, 11.092, 11.048, 11.019, 11.048, 11.078, 11.027, 10.993, 10.953, 10.952, 10.975, 10.962, 10.923, 10.85, 10.786, 10.815, 10.827, 10.78, 10.697, 10.688, 10.684, 10.577, 10.507, 10.491, 10.451, 10.396, 10.418, 10.495, 10.504, 10.463, 10.33, 10.201, 10.178, 10.167, 10.125, 10.083, 10.158, 10.262, 10.255, 10.179, 10.169, 10.242, 10.252, 10.133, 10.009, 9.993, 10.005, 10.027, 10.011, 10.004, 10.053, 10.071, 9.934, 9.897, 9.918, 9.971, 9.83, 9.683, 9.564, 9.49, 9.592, 9.581, 9.673, 9.724, 10.055, 10.133, 9.833, 9.531, 9.468, 9.38, 9.224, 9.349, 9.32, 9.156, 9.234, 9.254, 8.857, 8.657, 8.685, 8.838, 8.8, 8.595, 8.738, 8.909, 8.806, 8.58, 8.697, 8.758, 8.719, 8.381, 8.465, 9.322, 9.9, 9.847, 9.878, 10.027, 9.932, 9.587, 9.324, 9.212, 8.902, 8.972, 9.038, 8.956, 8.643, 8.576, 8.685, 8.982, 8.752, 8.442, 8.274, 8.601, 8.781, 8.539, 8.112, 8.055, 8.24, 8.517, 8.64, 8.091, 7.975, 8.404, 8.574, 8.671, 9.024, 9.068, 8.691, 8.833, 9.29, 8.9, 9.046, 9.375, 9.14, 8.953, 8.662, 8.458, 8.412, 8.382, 8.085, 7.845, 7.698, 8.074, 8.424, 8.835, 8.695, 8.609, 8.465, 8.441, 8.821, 8.57, 8.305, 8.265, 8.467, 8.463, 8.384, 8.171, 8.428, 8.572, 8.273, 7.942, 7.679, 7.401, 7.321, 7.211, 7.002, 7.469, 7.915, 8.149, 7.784, 7.268, 6.655, 6.882, 7.29, 7.226, 6.763, 7.145, 7.271, 7.782, 8.019, 7.954, 7.677, 7.717, 7.534, 7.173, 7.531, 7.961, 7.784, 7.058, 7.092, 7.683, 8.046, 7.783, 7.746, 7.995, 8.453, 8.236, 8.257, 8.288, 7.746, 7.517, 8.074, 7.732, 7.255, 6.512, 6.572, 6.734, 7.115, 7.377, 7.434, 7.591, 7.453, 7.484, 8.097, 8.754, 8.807, 8.309, 8.153, 8.24, 8.12, 7.586, 7.666, 7.562, 7.18, 6.51, 6.634, 7.228, 7.144, 6.647, 5.952, 6.065, 6.751, 6.791, 6.722, 7.141, 7.78, 8.193, 7.821, 7.332, 7.373, 7.325, 5.573, 5.471, 6.476, 6.668, 6.871, 6.464, 6.027, 6.289, 6.238, 5.915, 6.463, 6.596, 6.397, 5.903, 5.571, 5.927, 6.44, 4.953, 2.088, 3.775, 2.831, 5.461, 6.045, 5.37, 4.726, 5.098, 5.037, 5.278, 5.353, 5.799, 5.833, 5.728, 5.472, 6.275, 5.615, 5.382, 4.512, 6.166]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Lambda: 10 , Time: 0:00:07\n",
      "Train Error(all epochs): 3.838141679763794 \n",
      " [8.18, 8.174, 8.075, 8.002, 8.078, 8.089, 8.019, 7.966, 7.884, 7.873, 7.871, 7.799, 7.773, 7.789, 7.736, 7.712, 7.677, 7.646, 7.687, 7.595, 7.59, 7.519, 7.567, 7.413, 7.453, 7.378, 7.302, 7.267, 7.214, 7.242, 7.133, 7.046, 7.016, 6.956, 7.013, 6.833, 6.829, 6.803, 6.719, 6.663, 6.69, 6.588, 6.652, 6.491, 6.413, 6.64, 6.601, 6.389, 6.377, 6.36, 6.322, 6.212, 6.356, 6.369, 6.284, 6.231, 6.177, 6.199, 6.001, 5.955, 5.992, 6.05, 6.03, 6.005, 5.93, 5.845, 6.068, 6.005, 5.915, 5.934, 5.964, 5.886, 5.871, 5.798, 5.781, 5.88, 5.765, 5.673, 5.651, 5.605, 5.684, 5.463, 5.663, 5.723, 5.627, 5.471, 5.628, 5.572, 5.452, 5.536, 5.506, 5.376, 5.353, 5.395, 5.386, 5.326, 5.152, 5.186, 5.265, 5.351, 5.317, 5.159, 5.331, 5.163, 5.151, 5.163, 4.837, 5.035, 5.109, 4.959, 4.921, 4.866, 4.905, 4.965, 5.002, 4.946, 4.939, 5.054, 4.987, 5.048, 4.97, 5.078, 4.962, 5.018, 4.921, 4.902, 4.825, 4.783, 4.662, 4.758, 4.804, 4.714, 4.729, 4.741, 4.711, 4.59, 4.758, 4.724, 4.59, 4.599, 4.716, 4.667, 4.851, 4.811, 4.573, 4.628, 4.711, 4.512, 4.609, 4.596, 4.45, 4.751, 4.616, 4.575, 4.408, 4.447, 4.53, 4.463, 4.448, 4.388, 4.437, 4.658, 4.54, 4.581, 4.594, 4.647, 4.502, 4.424, 4.41, 4.425, 4.508, 4.566, 4.572, 4.554, 4.565, 4.411, 4.263, 4.432, 4.475, 4.476, 4.453, 4.395, 4.343, 4.292, 4.304, 4.341, 4.264, 4.405, 4.436, 4.38, 4.476, 4.457, 4.496, 4.392, 4.499, 4.444, 4.38, 4.381, 4.399, 4.363, 4.374, 4.352, 4.191, 4.383, 4.269, 4.303, 4.264, 4.297, 4.199, 4.369, 4.322, 4.278, 4.31, 4.243, 4.189, 4.177, 4.307, 4.347, 4.381, 4.347, 4.193, 4.417, 4.19, 4.097, 4.313, 4.277, 4.309, 4.425, 4.164, 4.309, 4.302, 4.336, 4.203, 4.423, 4.258, 4.366, 4.162, 4.417, 4.23, 4.272, 4.138, 4.233, 4.332, 4.284, 4.366, 4.27, 4.236, 4.062, 4.197, 3.998, 4.023, 4.009, 4.154, 4.23, 4.108, 4.137, 4.1, 4.391, 4.013, 4.112, 4.287, 4.238, 4.083, 4.173, 4.329, 3.953, 4.175, 4.224, 4.255, 3.975, 4.125, 4.1, 4.112, 4.159, 4.094, 4.061, 4.069, 4.035, 4.019, 3.894, 3.903, 3.971, 3.905, 3.893, 4.045, 4.03, 3.964, 4.084, 4.192, 3.838, 4.029, 4.004, 3.857, 4.226, 3.964, 4.08, 3.876, 3.88, 3.95, 3.961]\n",
      "Val Error(all epochs): 6.066097736358643 \n",
      " [7.428, 7.439, 7.443, 7.446, 7.447, 7.447, 7.448, 7.447, 7.447, 7.448, 7.449, 7.449, 7.45, 7.449, 7.449, 7.448, 7.448, 7.448, 7.447, 7.446, 7.445, 7.444, 7.443, 7.441, 7.439, 7.435, 7.432, 7.429, 7.426, 7.422, 7.418, 7.415, 7.409, 7.402, 7.396, 7.39, 7.383, 7.378, 7.374, 7.37, 7.369, 7.368, 7.362, 7.355, 7.349, 7.34, 7.339, 7.335, 7.331, 7.327, 7.323, 7.32, 7.316, 7.314, 7.311, 7.301, 7.293, 7.296, 7.3, 7.297, 7.287, 7.27, 7.265, 7.269, 7.278, 7.283, 7.279, 7.269, 7.246, 7.243, 7.239, 7.233, 7.239, 7.233, 7.22, 7.232, 7.227, 7.224, 7.236, 7.223, 7.198, 7.203, 7.209, 7.202, 7.206, 7.211, 7.196, 7.169, 7.182, 7.182, 7.179, 7.171, 7.184, 7.142, 7.111, 7.119, 7.105, 7.135, 7.122, 7.136, 7.14, 7.094, 7.092, 7.107, 7.108, 7.116, 7.119, 7.111, 7.097, 7.097, 7.069, 7.02, 7.035, 7.055, 7.027, 6.981, 6.989, 7.025, 7.001, 7.055, 7.075, 7.078, 7.084, 7.041, 7.1, 7.097, 7.089, 7.179, 7.14, 7.117, 7.061, 6.964, 6.9, 6.89, 6.863, 6.94, 6.93, 6.837, 6.876, 6.832, 6.864, 6.982, 6.941, 6.872, 6.912, 6.98, 6.973, 6.935, 6.962, 6.954, 6.897, 6.799, 6.822, 6.852, 6.845, 6.864, 6.88, 6.705, 6.805, 6.91, 6.82, 6.759, 6.623, 6.69, 6.912, 6.801, 6.686, 6.771, 6.869, 6.818, 7.004, 7.027, 6.837, 6.695, 6.761, 6.829, 6.776, 6.814, 6.624, 6.623, 6.585, 6.551, 6.572, 6.505, 6.54, 6.685, 6.542, 6.526, 6.595, 6.369, 6.236, 6.494, 6.636, 6.386, 6.417, 6.619, 6.462, 6.496, 6.322, 6.619, 6.52, 6.513, 6.757, 6.598, 6.626, 6.538, 6.543, 6.442, 6.653, 6.725, 6.39, 6.362, 6.509, 6.596, 6.529, 6.561, 6.515, 6.54, 6.645, 6.628, 6.589, 6.649, 6.608, 6.731, 6.264, 8.07, 8.744, 7.172, 6.539, 6.572, 6.703, 6.624, 6.266, 6.529, 6.476, 6.641, 6.314, 6.399, 6.389, 6.231, 6.407, 6.478, 6.249, 6.303, 6.44, 6.229, 6.066, 6.16, 6.246, 6.176, 6.074, 6.319, 6.248, 6.147, 6.354, 7.297, 6.542, 6.471, 6.352, 6.472, 6.631, 6.652, 6.59, 6.839, 7.25, 6.977, 6.778, 6.835, 6.531, 6.452, 7.744, 7.417, 6.859, 7.262, 7.006, 6.653, 6.322, 6.524, 6.231, 6.237, 6.474, 6.42, 6.719, 6.967, 6.914, 6.672, 6.431, 6.417, 6.642, 6.948, 7.093, 7.052, 6.581, 6.3, 6.43, 6.643, 6.469, 6.387, 6.298, 6.603]\n",
      "Val custom mae Error(all epochs): 0.20201492309570312 \n",
      " [10.78, 11.167, 11.325, 11.419, 11.477, 11.513, 11.541, 11.562, 11.576, 11.587, 11.593, 11.596, 11.598, 11.598, 11.599, 11.599, 11.599, 11.598, 11.597, 11.595, 11.592, 11.59, 11.586, 11.579, 11.571, 11.557, 11.544, 11.533, 11.52, 11.504, 11.49, 11.479, 11.456, 11.43, 11.409, 11.383, 11.36, 11.334, 11.312, 11.294, 11.283, 11.267, 11.242, 11.21, 11.175, 11.138, 11.133, 11.118, 11.09, 11.07, 11.056, 11.044, 11.034, 11.007, 10.99, 10.965, 10.942, 10.946, 10.959, 10.943, 10.905, 10.848, 10.822, 10.814, 10.839, 10.852, 10.855, 10.853, 10.766, 10.715, 10.683, 10.672, 10.702, 10.647, 10.596, 10.635, 10.635, 10.671, 10.717, 10.666, 10.584, 10.57, 10.586, 10.553, 10.565, 10.6, 10.575, 10.45, 10.521, 10.543, 10.532, 10.493, 10.494, 10.332, 10.256, 10.304, 10.224, 10.314, 10.216, 10.253, 10.365, 10.276, 10.321, 10.371, 10.434, 10.37, 10.311, 10.306, 10.245, 10.245, 10.159, 9.872, 10.086, 10.205, 10.129, 10.001, 9.968, 9.982, 10.022, 10.295, 10.304, 10.278, 10.339, 10.289, 10.215, 10.152, 10.496, 10.882, 10.578, 10.537, 10.335, 10.123, 10.003, 10.025, 10.11, 10.183, 10.091, 9.881, 9.868, 9.882, 10.274, 10.496, 10.103, 10.193, 10.249, 10.435, 10.703, 10.262, 10.008, 9.957, 10.156, 10.268, 10.575, 10.677, 10.58, 10.27, 10.18, 10.028, 10.285, 10.508, 10.395, 10.412, 9.38, 9.549, 10.267, 9.325, 9.171, 9.109, 9.23, 9.282, 9.729, 10.241, 9.811, 9.843, 9.55, 9.525, 9.619, 9.834, 9.708, 8.848, 9.065, 9.205, 9.185, 8.888, 8.436, 8.54, 8.929, 8.945, 8.572, 8.768, 8.576, 9.674, 10.095, 9.501, 8.585, 9.926, 8.923, 8.843, 8.863, 9.818, 9.727, 8.924, 9.302, 9.162, 9.292, 9.182, 9.31, 9.385, 10.245, 10.281, 9.288, 9.085, 9.579, 9.676, 9.971, 10.083, 9.221, 9.806, 10.296, 10.106, 9.126, 9.549, 8.853, 8.294, 8.786, 1.715, 0.202, 3.423, 6.371, 8.42, 9.704, 8.513, 8.935, 5.183, 7.857, 8.477, 7.482, 7.538, 6.532, 6.832, 7.206, 7.652, 7.99, 6.905, 6.542, 6.762, 7.094, 7.703, 7.633, 7.613, 7.777, 7.574, 6.646, 7.103, 5.805, 1.389, 5.257, 7.293, 6.125, 6.002, 6.257, 5.319, 6.3, 5.795, 3.297, 4.159, 4.586, 6.665, 6.273, 5.94, 1.605, 4.847, 4.237, 2.878, 4.029, 4.851, 5.197, 5.834, 6.345, 6.731, 6.528, 6.691, 7.182, 7.705, 7.103, 7.814, 7.66, 7.399, 8.782, 7.523, 8.041, 8.937, 7.922, 6.851, 5.654, 5.741, 5.47, 5.55, 7.181, 8.122]\n",
      "\n",
      "Trainig set size: 128 , Time: 0:00:39 , best_lambda: 0.1 , min_  error: 0.097\n",
      "Test starts:  154 , ends:  32161\n",
      "1001/1001 [==============================] - 1s 477us/step\n",
      "total_power:  14.425347 , average_difference:  4.623081284800603\n",
      "\n",
      "\n",
      "\n",
      "number_samples: 256 , New samples: 256\n",
      "Validation size: 52 , starts: 256 , ends: 307\n",
      "\n",
      "Lambda: 0 , Time: 0:00:09\n",
      "Train Error(all epochs): 2.7548367977142334 \n",
      " [8.104, 7.886, 7.723, 7.552, 7.504, 7.444, 7.34, 7.185, 7.045, 6.918, 6.666, 6.641, 6.46, 6.341, 6.221, 5.956, 5.823, 5.815, 5.612, 5.483, 5.336, 5.143, 5.108, 4.925, 4.874, 4.895, 4.753, 4.379, 4.453, 4.368, 4.308, 4.243, 4.269, 4.157, 4.219, 4.221, 4.107, 3.915, 4.11, 4.008, 3.954, 3.863, 3.799, 3.783, 3.89, 3.686, 3.668, 3.738, 3.669, 3.586, 3.517, 3.587, 3.644, 3.748, 3.566, 3.588, 3.516, 3.577, 3.655, 3.739, 3.601, 3.508, 3.542, 3.463, 3.78, 3.411, 3.572, 3.646, 3.491, 3.593, 3.615, 3.442, 3.453, 3.37, 3.456, 3.389, 3.342, 3.426, 3.464, 3.381, 3.341, 3.368, 3.491, 3.384, 3.408, 3.265, 3.521, 3.348, 3.439, 3.471, 3.25, 3.277, 3.332, 3.443, 3.269, 3.355, 3.157, 3.361, 3.242, 3.334, 3.478, 3.208, 3.337, 3.267, 3.373, 3.179, 3.399, 3.185, 3.296, 3.208, 3.313, 3.338, 3.366, 3.238, 3.229, 3.36, 3.166, 3.273, 3.262, 3.39, 3.29, 3.126, 3.271, 3.303, 3.15, 3.181, 3.212, 3.24, 3.089, 3.256, 3.158, 3.251, 3.359, 3.155, 2.984, 3.116, 3.318, 3.13, 3.278, 3.203, 3.208, 3.255, 3.215, 3.225, 3.287, 3.356, 3.189, 3.257, 3.135, 3.074, 3.141, 3.24, 3.224, 3.175, 3.144, 3.236, 3.218, 3.105, 3.302, 3.158, 3.164, 3.081, 3.127, 3.207, 3.108, 3.063, 3.296, 3.027, 3.104, 3.091, 3.016, 3.097, 3.037, 3.053, 3.084, 3.149, 3.108, 3.08, 3.299, 3.068, 3.134, 3.213, 3.145, 3.155, 3.151, 3.106, 3.027, 2.974, 3.146, 2.995, 3.286, 2.982, 2.969, 3.128, 3.019, 3.113, 3.018, 3.143, 3.048, 3.21, 2.992, 3.187, 3.045, 3.033, 3.022, 3.052, 3.104, 3.213, 3.031, 3.197, 2.81, 3.07, 3.101, 3.156, 2.943, 2.968, 3.167, 3.024, 2.969, 3.099, 3.066, 2.855, 3.055, 3.066, 3.046, 3.058, 3.114, 3.086, 3.146, 3.057, 2.989, 2.994, 2.885, 2.885, 2.984, 2.993, 2.96, 2.92, 2.958, 2.957, 3.041, 3.012, 2.94, 3.097, 3.126, 3.115, 2.927, 2.97, 3.049, 2.95, 2.849, 3.097, 3.082, 2.992, 2.923, 3.287, 3.053, 2.973, 3.016, 2.986, 3.026, 3.057, 3.086, 2.839, 3.039, 3.143, 2.983, 2.959, 2.889, 3.009, 2.841, 2.864, 2.937, 2.972, 3.119, 2.843, 2.929, 3.036, 2.946, 3.025, 2.903, 3.062, 2.935, 2.974, 2.815, 2.886, 3.002, 2.838, 2.894, 3.001, 2.857, 3.094, 2.769, 2.755, 2.916, 2.935, 2.942, 2.856, 2.851, 3.017]\n",
      "Val Error(all epochs): 5.278482437133789 \n",
      " [7.851, 7.835, 7.814, 7.771, 7.728, 7.693, 7.642, 7.609, 7.561, 7.508, 7.438, 7.377, 7.232, 7.083, 6.997, 6.909, 6.838, 6.725, 6.645, 6.609, 6.471, 6.4, 6.395, 6.425, 6.318, 6.16, 6.032, 5.909, 5.904, 5.881, 5.894, 5.796, 5.693, 5.71, 5.737, 5.74, 5.586, 5.565, 5.59, 5.649, 5.738, 5.717, 5.555, 5.57, 5.513, 5.589, 5.545, 5.646, 5.618, 5.596, 5.642, 5.592, 5.483, 5.536, 5.536, 5.487, 5.409, 5.305, 5.458, 5.754, 5.612, 5.566, 5.499, 5.63, 5.691, 5.716, 5.577, 5.638, 5.68, 5.68, 5.694, 5.557, 5.447, 5.401, 5.468, 5.442, 5.391, 5.451, 5.556, 5.49, 5.479, 5.577, 5.519, 5.468, 5.432, 5.404, 5.356, 5.478, 5.609, 5.474, 5.453, 5.597, 5.753, 5.727, 5.613, 5.594, 5.576, 5.554, 5.513, 5.45, 5.47, 5.584, 5.559, 5.531, 5.555, 5.664, 5.568, 5.562, 5.491, 5.449, 5.386, 5.446, 5.497, 5.531, 5.545, 5.579, 5.59, 5.564, 5.545, 5.573, 5.586, 5.532, 5.509, 5.576, 5.75, 5.758, 5.624, 5.544, 5.535, 5.518, 5.456, 5.471, 5.454, 5.373, 5.353, 5.331, 5.363, 5.316, 5.389, 5.471, 5.375, 5.372, 5.376, 5.435, 5.495, 5.507, 5.473, 5.4, 5.346, 5.369, 5.401, 5.395, 5.399, 5.367, 5.389, 5.404, 5.343, 5.365, 5.41, 5.41, 5.428, 5.476, 5.424, 5.354, 5.325, 5.398, 5.454, 5.49, 5.485, 5.429, 5.371, 5.344, 5.472, 5.545, 5.587, 5.549, 5.435, 5.411, 5.379, 5.394, 5.331, 5.279, 5.424, 5.489, 5.466, 5.519, 5.467, 5.369, 5.346, 5.41, 5.44, 5.33, 5.308, 5.347, 5.411, 5.444, 5.416, 5.38, 5.38, 5.433, 5.505, 5.507, 5.472, 5.451, 5.399, 5.349, 5.388, 5.459, 5.519, 5.478, 5.396, 5.388, 5.325, 5.31, 5.361, 5.431, 5.392, 5.345, 5.38, 5.343, 5.318, 5.356, 5.384, 5.345, 5.44, 5.417, 5.386, 5.431, 5.492, 5.448, 5.375, 5.344, 5.403, 5.454, 5.561, 5.57, 5.492, 5.449, 5.401, 5.396, 5.428, 5.416, 5.419, 5.446, 5.496, 5.536, 5.486, 5.504, 5.529, 5.496, 5.481, 5.444, 5.497, 5.566, 5.513, 5.421, 5.448, 5.437, 5.461, 5.459, 5.451, 5.392, 5.333, 5.302, 5.365, 5.374, 5.402, 5.432, 5.411, 5.365, 5.337, 5.388, 5.34, 5.307, 5.394, 5.498, 5.501, 5.39, 5.372, 5.295, 5.292, 5.367, 5.431, 5.374, 5.278, 5.379, 5.434, 5.498, 5.481, 5.444, 5.424, 5.437, 5.478, 5.47, 5.455, 5.444, 5.422, 5.375, 5.419, 5.471]\n",
      "Val custom mae Error(all epochs): 2.9351730346679688 \n",
      " [12.148, 12.187, 12.181, 12.101, 12.018, 11.954, 11.832, 11.797, 11.759, 11.684, 11.586, 11.484, 11.085, 10.726, 10.573, 10.286, 10.067, 9.628, 9.306, 9.147, 8.665, 8.479, 8.491, 8.673, 8.075, 7.747, 7.764, 7.367, 7.22, 6.647, 6.708, 6.57, 6.007, 6.224, 6.363, 5.004, 4.683, 5.276, 5.326, 5.17, 5.415, 5.283, 4.866, 4.998, 4.625, 4.36, 4.637, 4.555, 4.886, 4.765, 4.059, 4.235, 4.911, 4.674, 4.218, 3.772, 4.282, 4.599, 4.388, 3.816, 3.957, 3.652, 3.664, 3.652, 3.457, 3.39, 3.366, 3.475, 3.869, 3.695, 3.472, 3.716, 3.569, 3.427, 3.762, 3.931, 3.735, 3.405, 3.447, 3.956, 3.778, 3.13, 3.386, 3.462, 3.396, 3.542, 3.478, 3.201, 2.935, 3.537, 3.632, 3.619, 3.732, 3.786, 3.709, 3.812, 4.084, 4.004, 3.959, 4.051, 4.16, 3.709, 3.784, 3.627, 3.363, 3.296, 3.749, 3.921, 3.873, 3.899, 4.028, 3.36, 3.607, 3.932, 3.92, 3.439, 3.283, 3.296, 3.784, 3.846, 3.613, 3.98, 3.883, 3.65, 3.449, 3.627, 4.27, 4.253, 4.393, 4.522, 4.334, 3.841, 3.475, 3.766, 3.622, 3.49, 3.492, 3.702, 3.624, 3.648, 4.052, 4.116, 4.214, 3.977, 3.845, 3.613, 3.221, 3.508, 4.264, 4.22, 3.68, 3.615, 3.515, 3.652, 3.511, 3.452, 3.714, 3.996, 4.019, 4.094, 4.305, 4.114, 4.096, 4.025, 4.089, 3.898, 3.961, 4.123, 3.898, 4.027, 4.0, 4.133, 4.01, 3.813, 3.601, 3.621, 3.925, 3.841, 3.952, 3.934, 3.93, 3.7, 3.656, 3.834, 4.148, 4.033, 3.905, 4.082, 4.376, 4.379, 3.927, 3.805, 4.072, 4.176, 4.182, 4.213, 4.255, 4.211, 4.525, 4.549, 4.383, 4.501, 4.369, 4.235, 4.592, 4.633, 4.417, 4.05, 3.714, 3.841, 4.074, 4.071, 4.256, 4.134, 4.053, 3.986, 3.675, 3.614, 3.579, 3.867, 4.127, 4.112, 4.031, 4.267, 4.263, 3.971, 3.823, 3.907, 4.137, 4.272, 4.34, 4.483, 4.379, 4.106, 3.857, 3.979, 4.324, 4.346, 4.265, 4.294, 4.326, 4.224, 3.905, 3.655, 3.905, 4.134, 4.092, 4.173, 4.091, 3.924, 3.875, 4.259, 4.349, 4.354, 4.255, 4.292, 4.365, 4.286, 4.256, 4.453, 4.354, 4.306, 4.233, 3.947, 3.53, 3.416, 3.572, 3.789, 3.659, 3.83, 4.032, 3.881, 3.975, 4.118, 4.008, 3.955, 3.825, 4.048, 4.116, 4.28, 4.153, 3.894, 3.657, 3.837, 4.232, 4.07, 3.515, 3.44, 3.627, 3.79, 3.949, 3.902, 3.743, 3.83, 4.173, 4.242, 4.195, 3.993, 3.644, 3.403]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Lambda: 0.01 , Time: 0:00:09\n",
      "Train Error(all epochs): 2.798269748687744 \n",
      " [8.011, 7.875, 7.789, 7.64, 7.439, 7.332, 7.295, 7.144, 6.948, 6.912, 6.727, 6.561, 6.417, 6.313, 6.067, 5.894, 5.76, 5.718, 5.544, 5.424, 5.326, 5.211, 5.161, 4.943, 4.886, 4.714, 4.704, 4.566, 4.656, 4.486, 4.337, 4.387, 4.325, 4.036, 4.061, 4.144, 4.115, 4.05, 3.988, 3.884, 3.839, 3.751, 3.914, 3.933, 3.769, 3.798, 3.812, 3.643, 3.762, 3.746, 3.655, 3.521, 3.715, 3.527, 3.546, 3.671, 3.757, 3.574, 3.484, 3.555, 3.556, 3.55, 3.513, 3.454, 3.463, 3.58, 3.557, 3.493, 3.521, 3.46, 3.335, 3.493, 3.567, 3.505, 3.416, 3.446, 3.392, 3.515, 3.423, 3.369, 3.405, 3.322, 3.258, 3.392, 3.591, 3.227, 3.356, 3.295, 3.214, 3.434, 3.454, 3.294, 3.501, 3.25, 3.303, 3.44, 3.22, 3.315, 3.228, 3.148, 3.186, 3.315, 3.189, 3.423, 3.324, 3.244, 3.441, 3.363, 3.249, 3.448, 3.37, 3.309, 3.397, 3.242, 3.428, 3.23, 3.259, 3.149, 3.369, 3.291, 3.151, 3.194, 3.303, 3.15, 3.238, 3.181, 3.359, 3.294, 3.261, 3.213, 3.297, 3.259, 3.241, 3.057, 3.155, 3.257, 3.305, 3.123, 3.143, 3.245, 3.149, 3.126, 3.21, 3.181, 3.15, 3.141, 3.156, 3.12, 3.145, 3.296, 3.158, 3.175, 3.009, 3.218, 3.131, 3.141, 3.217, 3.256, 3.051, 3.13, 3.247, 3.198, 3.183, 3.206, 3.25, 3.325, 3.266, 3.236, 3.036, 3.069, 3.205, 3.095, 2.902, 3.067, 3.24, 3.126, 3.172, 3.208, 3.194, 3.08, 3.024, 3.067, 3.051, 3.182, 3.045, 3.077, 3.069, 3.066, 3.052, 3.119, 3.045, 3.063, 2.953, 3.057, 3.179, 3.06, 3.096, 3.067, 3.045, 3.066, 3.074, 3.271, 3.056, 3.11, 3.102, 2.91, 3.066, 3.273, 3.134, 3.071, 3.168, 3.124, 3.145, 3.163, 3.209, 3.218, 2.997, 3.06, 3.018, 3.008, 3.088, 3.117, 3.122, 2.933, 3.271, 2.992, 3.118, 3.182, 2.999, 3.116, 3.225, 3.025, 3.171, 3.106, 3.115, 3.148, 3.101, 3.098, 3.15, 3.014, 2.874, 2.946, 3.106, 3.084, 3.088, 2.943, 2.949, 3.013, 3.075, 3.037, 3.192, 2.798, 2.994, 3.14, 2.944, 2.952, 3.101, 2.99, 3.007, 3.148, 2.988, 2.902, 3.122, 2.972, 3.006, 2.963, 3.052, 2.953, 3.0, 3.02, 2.914, 3.15, 3.069, 2.913, 3.041, 3.122, 2.991, 3.009, 2.984, 2.998, 3.05, 3.075, 3.073, 2.819, 2.893, 2.993, 3.046, 2.843, 2.92, 2.887, 2.818, 2.881, 2.996, 2.973, 2.929, 3.108, 3.061, 3.13, 2.955, 3.001]\n",
      "Val Error(all epochs): 5.294291019439697 \n",
      " [8.29, 8.281, 8.257, 8.234, 8.196, 8.143, 8.099, 8.066, 7.993, 7.942, 7.853, 7.75, 7.571, 7.429, 7.389, 7.321, 7.167, 7.031, 6.948, 6.838, 6.769, 6.671, 6.667, 6.511, 6.53, 6.376, 6.279, 6.27, 6.239, 6.164, 6.169, 6.099, 6.015, 5.918, 5.921, 5.854, 5.848, 5.938, 5.961, 5.958, 5.905, 5.873, 5.988, 5.992, 5.831, 5.767, 5.854, 5.884, 5.867, 5.766, 5.736, 5.806, 5.819, 5.824, 5.742, 5.815, 5.837, 5.788, 5.697, 5.636, 5.727, 5.877, 5.798, 5.726, 5.769, 5.898, 5.972, 5.831, 5.731, 5.83, 5.891, 5.818, 5.778, 5.791, 5.742, 5.776, 5.82, 5.747, 5.664, 5.593, 5.606, 5.651, 5.832, 5.892, 5.72, 5.635, 5.742, 5.768, 5.728, 5.711, 5.692, 5.686, 5.756, 5.779, 5.765, 5.763, 5.741, 5.734, 5.723, 5.801, 5.795, 5.773, 5.752, 5.756, 5.755, 5.785, 5.782, 5.781, 5.65, 5.66, 5.658, 5.632, 5.717, 5.753, 5.86, 5.835, 5.765, 5.784, 5.763, 5.705, 5.717, 5.793, 5.85, 5.775, 5.692, 5.712, 5.654, 5.638, 5.72, 5.734, 5.692, 5.633, 5.709, 5.714, 5.749, 5.683, 5.687, 5.674, 5.658, 5.684, 5.681, 5.7, 5.758, 5.749, 5.782, 5.786, 5.692, 5.692, 5.64, 5.668, 5.702, 5.742, 5.735, 5.62, 5.648, 5.656, 5.552, 5.558, 5.666, 5.703, 5.659, 5.734, 5.721, 5.677, 5.72, 5.572, 5.551, 5.505, 5.517, 5.613, 5.646, 5.658, 6.118, 5.885, 5.739, 5.664, 5.716, 5.785, 5.76, 5.592, 5.544, 5.608, 5.702, 5.703, 5.664, 5.686, 5.754, 5.724, 5.688, 5.743, 5.798, 5.77, 5.695, 5.708, 5.632, 5.518, 5.429, 5.406, 5.424, 5.441, 5.546, 5.612, 5.597, 5.644, 5.617, 5.643, 5.669, 5.566, 5.532, 5.605, 5.597, 5.589, 5.51, 5.5, 5.47, 5.524, 5.686, 5.637, 5.566, 5.51, 5.502, 5.504, 5.46, 5.421, 5.38, 5.294, 5.364, 5.484, 5.529, 5.485, 5.56, 5.542, 5.44, 5.426, 5.457, 5.419, 5.48, 5.59, 5.487, 5.452, 5.404, 5.354, 5.415, 5.428, 5.402, 5.368, 5.381, 5.448, 5.463, 5.543, 5.558, 5.516, 5.465, 5.424, 5.47, 5.504, 5.487, 5.368, 5.355, 5.4, 5.449, 5.466, 5.503, 5.41, 5.338, 5.363, 5.393, 5.381, 5.341, 5.323, 5.367, 5.435, 5.446, 5.379, 5.333, 5.385, 5.413, 5.413, 5.438, 5.383, 5.366, 5.433, 5.47, 5.48, 5.434, 5.428, 5.463, 5.531, 5.525, 5.524, 5.62, 5.638, 5.684, 5.676, 5.603, 5.577, 5.514, 5.465, 5.511, 5.529]\n",
      "Val custom mae Error(all epochs): 1.538172721862793 \n",
      " [13.106, 13.127, 13.065, 13.02, 12.92, 12.8, 12.731, 12.682, 12.559, 12.466, 12.305, 12.134, 11.784, 11.538, 11.485, 11.377, 10.992, 10.589, 10.334, 10.104, 9.869, 9.559, 9.731, 9.381, 9.181, 8.693, 8.396, 7.855, 7.623, 7.606, 7.59, 6.729, 6.602, 5.849, 5.218, 5.196, 5.626, 5.016, 5.059, 5.242, 5.296, 5.483, 4.702, 4.022, 4.174, 4.002, 3.292, 3.742, 3.981, 3.875, 3.873, 3.991, 4.019, 4.314, 4.241, 4.326, 3.958, 3.818, 3.773, 3.475, 3.442, 3.592, 3.758, 3.44, 3.551, 3.302, 3.176, 3.809, 3.911, 3.725, 3.446, 3.548, 3.978, 4.808, 4.655, 4.558, 3.914, 3.375, 3.997, 4.007, 4.081, 3.941, 3.727, 4.433, 4.528, 4.132, 3.834, 3.492, 3.012, 2.716, 2.873, 2.613, 2.251, 1.947, 2.541, 3.192, 2.975, 3.189, 2.762, 1.538, 2.187, 2.683, 2.97, 3.392, 3.481, 3.075, 3.326, 3.699, 3.761, 3.383, 3.6, 3.74, 4.012, 4.149, 4.222, 3.792, 3.116, 3.999, 4.368, 4.123, 3.683, 3.889, 4.023, 3.989, 3.676, 3.685, 3.564, 3.741, 3.902, 3.79, 4.019, 4.267, 4.294, 4.03, 3.62, 3.593, 3.874, 3.859, 3.91, 3.972, 3.929, 3.829, 3.835, 3.912, 4.068, 3.991, 3.476, 3.292, 3.723, 3.847, 3.653, 3.723, 3.892, 3.875, 3.822, 3.475, 3.601, 3.687, 3.611, 3.604, 3.595, 3.853, 3.616, 3.342, 3.269, 3.476, 3.822, 3.986, 3.707, 3.438, 3.391, 3.795, 1.827, 2.238, 2.86, 3.404, 3.835, 4.053, 4.054, 3.971, 3.957, 3.77, 3.584, 3.61, 4.103, 4.402, 4.028, 3.686, 3.87, 4.223, 4.196, 3.976, 4.035, 4.155, 4.017, 4.093, 4.156, 4.032, 3.751, 3.59, 3.241, 3.384, 3.761, 3.784, 3.728, 3.55, 3.551, 3.981, 4.411, 4.397, 4.33, 4.454, 4.31, 4.192, 3.794, 4.125, 4.12, 3.978, 3.886, 3.976, 3.992, 4.042, 4.08, 4.143, 3.919, 3.731, 3.783, 4.104, 4.079, 3.796, 3.818, 4.222, 4.344, 4.161, 4.107, 4.239, 4.154, 4.384, 4.463, 4.339, 4.113, 4.026, 3.972, 4.121, 4.086, 3.944, 4.12, 4.48, 4.518, 4.059, 3.86, 4.177, 4.363, 4.25, 3.788, 3.867, 4.189, 4.341, 4.242, 4.018, 3.67, 3.51, 3.48, 3.993, 4.254, 4.111, 4.021, 3.848, 3.595, 3.481, 3.678, 4.153, 4.256, 4.242, 4.359, 4.198, 3.835, 4.105, 4.254, 4.212, 4.11, 3.896, 3.997, 4.187, 4.258, 4.357, 4.543, 4.225, 3.974, 3.711, 3.802, 3.953, 4.144, 4.149, 3.799, 3.689, 3.962, 4.131, 4.146, 4.09]\n",
      "\n",
      "Lambda: 0.1 , Time: 0:00:10\n",
      "Train Error(all epochs): 2.968651294708252 \n",
      " [7.974, 7.824, 7.742, 7.677, 7.54, 7.47, 7.372, 7.179, 7.002, 6.92, 6.751, 6.619, 6.465, 6.337, 6.146, 6.042, 5.911, 5.756, 5.682, 5.324, 5.282, 5.123, 5.105, 5.038, 4.887, 4.712, 4.647, 4.476, 4.518, 4.51, 4.471, 4.258, 4.121, 4.315, 4.14, 4.119, 4.28, 3.995, 4.107, 3.824, 4.084, 3.935, 3.809, 3.899, 3.995, 3.756, 3.731, 3.838, 3.768, 3.854, 3.82, 3.629, 3.654, 3.823, 3.679, 3.583, 3.711, 3.808, 3.793, 3.643, 3.502, 3.747, 3.682, 3.648, 3.604, 3.644, 3.463, 3.615, 3.486, 3.58, 3.545, 3.586, 3.507, 3.599, 3.494, 3.375, 3.492, 3.468, 3.646, 3.453, 3.408, 3.524, 3.493, 3.406, 3.348, 3.369, 3.554, 3.397, 3.389, 3.374, 3.402, 3.466, 3.235, 3.366, 3.396, 3.324, 3.254, 3.445, 3.533, 3.368, 3.464, 3.422, 3.328, 3.444, 3.516, 3.385, 3.325, 3.328, 3.263, 3.319, 3.241, 3.226, 3.306, 3.477, 3.347, 3.307, 3.465, 3.448, 3.456, 3.273, 3.442, 3.359, 3.53, 3.223, 3.211, 3.324, 3.361, 3.308, 3.311, 3.21, 3.194, 3.118, 3.427, 3.363, 3.155, 3.279, 3.377, 3.276, 3.26, 3.258, 3.357, 3.307, 3.368, 3.319, 3.288, 3.341, 3.443, 3.272, 3.238, 3.295, 3.162, 3.236, 3.369, 3.275, 3.173, 3.169, 3.257, 3.258, 3.165, 3.115, 3.276, 3.155, 3.256, 3.212, 3.007, 3.232, 3.244, 3.311, 3.073, 3.234, 3.211, 3.289, 3.113, 3.215, 3.287, 3.3, 3.355, 3.507, 3.238, 3.086, 3.323, 3.095, 3.318, 3.226, 3.264, 3.14, 3.268, 3.349, 3.124, 3.148, 3.275, 3.145, 3.141, 3.278, 3.314, 3.402, 3.164, 3.188, 3.244, 3.164, 3.116, 3.274, 3.219, 3.217, 3.329, 3.252, 3.114, 3.035, 3.212, 3.15, 3.116, 3.129, 3.266, 3.291, 3.262, 3.191, 3.023, 3.301, 3.108, 3.186, 3.219, 3.221, 3.149, 3.119, 3.244, 3.213, 3.232, 3.128, 3.224, 3.165, 3.218, 3.148, 3.224, 3.056, 3.03, 3.108, 3.165, 3.078, 3.165, 3.185, 3.21, 3.119, 3.142, 3.202, 3.232, 3.178, 3.029, 3.291, 3.252, 3.235, 3.109, 3.088, 3.143, 3.128, 3.174, 3.22, 3.117, 3.072, 3.031, 3.159, 3.128, 3.224, 3.271, 3.224, 3.209, 3.015, 3.256, 3.185, 3.145, 3.098, 3.15, 3.168, 3.122, 3.106, 3.267, 3.121, 3.221, 3.247, 3.054, 3.083, 3.263, 3.065, 3.08, 3.155, 2.988, 3.058, 3.049, 3.216, 3.25, 3.223, 3.319, 3.16, 3.092, 3.176, 3.165, 3.029, 3.171, 2.98, 2.969, 3.142]\n",
      "Val Error(all epochs): 5.518578052520752 \n",
      " [8.887, 8.573, 8.43, 8.345, 8.26, 8.155, 8.081, 7.999, 7.909, 7.81, 7.68, 7.563, 7.405, 7.304, 7.214, 7.131, 7.101, 6.967, 6.874, 6.802, 6.825, 6.756, 6.741, 6.73, 6.694, 6.606, 6.573, 6.508, 6.401, 6.478, 6.474, 6.385, 6.364, 6.364, 6.206, 6.214, 6.221, 6.078, 6.055, 5.968, 6.002, 6.001, 5.93, 6.005, 6.061, 6.015, 5.959, 5.967, 5.975, 5.965, 5.946, 5.965, 5.941, 5.882, 6.032, 6.033, 6.011, 6.08, 5.999, 5.906, 5.983, 5.964, 6.073, 6.101, 6.032, 5.889, 5.814, 6.118, 6.031, 5.958, 5.885, 5.855, 5.971, 5.903, 5.978, 5.946, 5.969, 5.972, 5.959, 5.975, 5.849, 5.94, 5.984, 6.0, 5.845, 5.946, 6.057, 5.98, 5.852, 5.824, 5.944, 5.962, 5.895, 6.002, 5.925, 5.774, 5.902, 5.914, 5.89, 5.8, 5.872, 5.884, 5.938, 5.824, 5.915, 5.943, 5.913, 5.928, 5.908, 5.977, 6.089, 5.995, 5.935, 5.851, 5.958, 6.026, 6.082, 6.014, 6.003, 5.977, 6.076, 5.992, 6.002, 6.068, 5.927, 6.003, 6.103, 6.035, 5.903, 5.847, 5.831, 5.849, 5.955, 5.766, 5.659, 5.871, 5.93, 5.767, 5.807, 5.856, 5.764, 5.858, 5.935, 6.063, 5.897, 5.942, 6.019, 6.018, 5.814, 5.84, 5.975, 6.023, 5.847, 5.827, 5.859, 5.88, 5.867, 6.019, 5.903, 5.922, 5.978, 5.924, 5.871, 5.933, 5.856, 5.749, 5.894, 6.069, 5.833, 5.771, 5.956, 5.907, 5.81, 5.827, 5.853, 5.83, 5.815, 5.854, 5.888, 5.884, 5.739, 5.676, 5.824, 5.988, 5.906, 5.788, 5.9, 5.908, 5.834, 5.862, 5.74, 7.498, 6.965, 6.437, 6.044, 5.753, 5.933, 5.975, 6.008, 5.902, 5.732, 5.748, 5.826, 5.774, 5.884, 5.882, 5.852, 5.902, 5.909, 5.955, 5.805, 5.783, 5.758, 5.663, 5.729, 5.591, 5.598, 5.712, 5.925, 5.912, 5.899, 5.888, 5.857, 5.916, 5.787, 5.74, 5.627, 5.876, 5.89, 5.786, 5.532, 5.693, 5.625, 5.59, 5.785, 5.635, 5.755, 5.833, 5.859, 5.85, 5.811, 5.678, 5.804, 5.745, 5.958, 5.811, 5.744, 5.993, 5.865, 5.968, 5.958, 6.011, 6.048, 5.879, 5.789, 5.877, 5.899, 5.748, 5.703, 5.809, 5.69, 5.812, 5.879, 5.695, 5.829, 5.974, 5.77, 5.805, 5.843, 5.797, 5.735, 5.718, 5.797, 5.885, 5.895, 5.871, 5.886, 5.726, 5.58, 5.519, 5.663, 5.618, 5.66, 5.817, 5.924, 5.866, 5.94, 5.939, 5.838, 5.713, 5.821, 5.635, 5.666, 5.727, 5.83, 5.655, 5.651, 5.702, 5.66, 5.773]\n",
      "Val custom mae Error(all epochs): 4.590409278869629 \n",
      " [13.859, 13.6, 13.404, 13.248, 13.083, 12.89, 12.745, 12.559, 12.408, 12.239, 12.002, 11.791, 11.477, 11.295, 11.119, 10.94, 10.87, 10.576, 10.43, 10.182, 10.224, 9.92, 9.934, 9.877, 9.658, 9.393, 9.017, 9.034, 9.014, 8.666, 8.56, 8.475, 8.39, 8.141, 7.716, 7.561, 7.577, 7.385, 7.737, 7.396, 7.117, 6.988, 7.184, 7.736, 7.372, 7.373, 6.999, 6.747, 7.073, 7.519, 7.481, 7.049, 7.019, 6.779, 6.551, 6.269, 6.286, 6.166, 6.081, 6.001, 6.113, 6.501, 6.555, 6.5, 6.779, 6.421, 6.369, 5.982, 6.292, 6.677, 6.337, 6.078, 6.141, 5.832, 5.887, 6.408, 6.413, 6.01, 6.382, 6.606, 6.244, 6.122, 6.059, 6.036, 5.909, 5.862, 5.946, 5.888, 6.135, 6.358, 6.091, 5.922, 5.686, 5.867, 5.91, 5.519, 5.213, 5.623, 6.011, 5.731, 5.436, 5.334, 5.758, 5.546, 5.59, 5.462, 5.319, 5.093, 5.544, 6.209, 6.315, 5.889, 5.916, 6.039, 5.914, 6.03, 5.754, 5.979, 6.04, 5.696, 5.771, 6.127, 5.969, 5.91, 6.121, 6.114, 5.731, 5.955, 6.108, 5.777, 5.703, 5.884, 5.921, 5.963, 5.958, 5.786, 5.84, 5.916, 6.197, 5.803, 5.476, 5.397, 5.867, 5.683, 6.195, 6.189, 6.12, 5.811, 6.208, 6.264, 5.859, 5.981, 5.925, 5.679, 5.88, 6.057, 5.721, 5.928, 6.222, 6.083, 5.784, 6.147, 6.125, 5.711, 6.027, 6.173, 6.191, 6.194, 5.73, 5.854, 6.002, 5.617, 5.463, 5.665, 5.681, 5.901, 6.313, 5.975, 5.747, 5.217, 5.158, 5.135, 5.292, 5.499, 5.332, 5.744, 6.04, 5.523, 5.72, 6.135, 5.707, 11.118, 9.359, 7.28, 6.684, 6.272, 6.082, 5.879, 6.095, 6.144, 5.918, 5.815, 6.28, 5.972, 5.74, 6.154, 6.163, 6.116, 5.977, 6.206, 6.057, 5.703, 5.554, 5.825, 5.705, 5.491, 5.261, 5.67, 5.93, 5.731, 5.934, 6.008, 5.522, 5.66, 6.081, 6.227, 5.989, 5.649, 5.868, 6.128, 5.711, 5.559, 5.703, 5.626, 5.443, 5.433, 5.867, 5.742, 5.27, 5.69, 5.91, 5.899, 5.881, 5.827, 6.134, 5.776, 5.632, 5.823, 6.373, 6.812, 6.185, 6.273, 5.943, 5.454, 5.83, 6.095, 6.018, 5.45, 5.76, 5.75, 5.169, 5.922, 5.593, 4.848, 5.896, 6.205, 5.911, 5.377, 6.169, 6.218, 5.462, 5.696, 6.05, 5.056, 5.275, 5.625, 5.406, 4.746, 4.988, 4.79, 5.098, 5.228, 5.321, 5.949, 5.348, 5.303, 5.715, 5.311, 5.225, 5.393, 5.626, 4.718, 4.591, 4.994, 4.59, 4.934, 5.721, 5.271, 4.674, 5.725]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Lambda: 1 , Time: 0:00:09\n",
      "Train Error(all epochs): 3.113250732421875 \n",
      " [8.087, 7.894, 7.759, 7.63, 7.639, 7.443, 7.361, 7.276, 7.038, 6.932, 6.83, 6.71, 6.466, 6.251, 6.053, 6.02, 5.891, 5.733, 5.531, 5.442, 5.296, 5.267, 4.996, 5.025, 4.971, 4.806, 4.656, 4.741, 4.627, 4.6, 4.691, 4.542, 4.459, 4.52, 4.62, 4.371, 4.479, 4.391, 4.31, 4.342, 4.336, 4.341, 4.12, 4.123, 4.093, 4.002, 4.233, 3.96, 4.127, 4.014, 4.085, 4.11, 4.18, 4.1, 4.058, 4.108, 4.027, 4.069, 3.913, 3.889, 3.935, 3.99, 3.929, 3.862, 3.913, 4.063, 3.824, 3.913, 3.906, 3.809, 3.726, 3.985, 3.797, 3.719, 3.975, 3.769, 3.609, 3.717, 3.764, 3.724, 3.782, 3.859, 3.801, 3.892, 3.876, 3.652, 3.706, 3.622, 3.571, 3.58, 3.725, 3.555, 3.806, 3.787, 3.909, 3.873, 3.898, 3.834, 3.83, 3.707, 3.517, 3.587, 3.53, 3.487, 3.623, 3.659, 3.56, 3.728, 3.811, 3.719, 3.496, 3.59, 3.567, 3.659, 3.489, 3.884, 3.697, 3.685, 3.518, 3.669, 3.517, 3.671, 3.655, 3.601, 3.606, 3.712, 3.6, 3.535, 3.527, 3.455, 3.553, 3.511, 3.659, 3.721, 3.458, 3.614, 3.615, 3.496, 3.582, 3.476, 3.453, 3.446, 3.404, 3.576, 3.64, 3.568, 3.462, 3.527, 3.579, 3.644, 3.58, 3.62, 3.664, 3.606, 3.568, 3.538, 3.506, 3.535, 3.56, 3.433, 3.453, 3.457, 3.557, 3.427, 3.453, 3.462, 3.47, 3.313, 3.48, 3.538, 3.566, 3.549, 3.524, 3.493, 3.566, 3.65, 3.256, 3.391, 3.527, 3.531, 3.619, 3.491, 3.553, 3.409, 3.53, 3.534, 3.688, 3.691, 3.522, 3.454, 3.447, 3.515, 3.52, 3.471, 3.533, 3.319, 3.505, 3.449, 3.695, 3.517, 3.461, 3.578, 3.546, 3.465, 3.461, 3.589, 3.618, 3.349, 3.532, 3.441, 3.51, 3.324, 3.267, 3.415, 3.576, 3.722, 3.48, 3.524, 3.538, 3.547, 3.4, 3.604, 3.469, 3.499, 3.348, 3.533, 3.468, 3.419, 3.43, 3.416, 3.523, 3.516, 3.5, 3.469, 3.525, 3.494, 3.437, 3.52, 3.588, 3.691, 3.418, 3.484, 3.438, 3.37, 3.438, 3.411, 3.226, 3.433, 3.399, 3.475, 3.39, 3.384, 3.551, 3.357, 3.423, 3.591, 3.47, 3.466, 3.517, 3.549, 3.429, 3.365, 3.428, 3.545, 3.318, 3.53, 3.565, 3.408, 3.447, 3.342, 3.274, 3.555, 3.533, 3.469, 3.345, 3.438, 3.36, 3.429, 3.323, 3.318, 3.502, 3.382, 3.485, 3.416, 3.455, 3.486, 3.655, 3.489, 3.479, 3.593, 3.341, 3.374, 3.113, 3.494, 3.289, 3.252, 3.31, 3.337, 3.439, 3.506]\n",
      "Val Error(all epochs): 5.3567633628845215 \n",
      " [8.109, 8.072, 8.054, 8.032, 7.999, 7.963, 7.916, 7.89, 7.874, 7.862, 7.867, 7.849, 7.814, 7.75, 7.758, 7.754, 7.707, 7.687, 7.698, 7.609, 7.617, 7.545, 7.514, 7.474, 7.478, 7.487, 7.409, 7.4, 7.384, 7.363, 7.322, 7.259, 7.138, 7.178, 7.159, 7.101, 7.067, 7.089, 7.13, 7.021, 7.009, 6.98, 6.931, 6.888, 6.904, 6.792, 6.724, 6.714, 6.659, 6.746, 6.783, 6.732, 6.645, 6.65, 6.566, 6.688, 6.612, 6.51, 6.676, 6.834, 6.581, 6.608, 6.768, 6.64, 6.578, 6.652, 6.617, 6.23, 6.452, 6.659, 6.448, 6.535, 6.414, 6.567, 6.483, 6.489, 6.485, 6.45, 6.424, 6.226, 6.39, 6.289, 6.187, 6.299, 6.523, 6.424, 6.381, 6.5, 6.189, 6.319, 6.313, 6.286, 6.415, 6.483, 6.419, 6.12, 6.088, 6.084, 5.985, 6.055, 6.102, 6.066, 6.047, 6.3, 6.252, 6.279, 6.317, 6.135, 6.286, 6.038, 6.028, 6.066, 6.226, 6.036, 6.035, 5.759, 5.696, 5.929, 6.21, 5.993, 5.945, 6.118, 5.964, 6.052, 6.284, 6.204, 5.881, 5.823, 5.896, 5.928, 6.098, 6.012, 5.895, 5.944, 5.972, 5.936, 5.815, 5.829, 5.971, 5.868, 5.854, 5.714, 5.734, 5.777, 5.88, 5.794, 6.801, 8.951, 6.34, 5.547, 5.828, 5.951, 5.752, 6.065, 5.805, 5.637, 5.99, 5.693, 5.818, 5.966, 6.006, 6.073, 5.875, 5.928, 5.908, 6.106, 5.657, 5.676, 5.575, 5.516, 5.811, 5.906, 5.651, 5.772, 5.777, 5.892, 6.169, 6.022, 5.615, 5.935, 6.003, 5.854, 5.817, 5.975, 5.927, 6.158, 5.91, 5.5, 5.684, 6.324, 6.182, 5.79, 5.657, 6.277, 5.86, 5.818, 5.802, 5.75, 5.617, 5.669, 5.636, 5.684, 5.831, 5.706, 5.477, 5.42, 5.515, 5.671, 5.646, 5.634, 5.593, 5.648, 5.733, 5.582, 5.722, 5.66, 5.87, 5.827, 5.788, 5.473, 5.357, 5.993, 5.876, 5.964, 5.792, 5.831, 5.972, 5.931, 5.796, 5.886, 5.977, 5.636, 5.639, 5.383, 5.959, 6.011, 5.661, 5.975, 5.909, 5.556, 5.67, 5.613, 5.519, 5.738, 5.749, 5.729, 5.826, 5.86, 5.777, 6.795, 6.836, 6.082, 6.168, 6.101, 5.889, 5.658, 5.43, 5.534, 5.653, 5.849, 5.761, 5.995, 5.71, 5.904, 5.933, 5.84, 5.48, 5.537, 5.716, 5.465, 5.464, 5.71, 5.643, 5.804, 5.841, 5.959, 5.575, 5.744, 5.682, 5.638, 6.128, 5.905, 5.53, 5.466, 5.433, 5.535, 5.415, 5.416, 5.464, 5.596, 5.566, 5.782, 6.248, 6.784, 5.624, 5.839, 5.475, 5.69, 5.456, 5.636]\n",
      "Val custom mae Error(all epochs): 0.8119468688964844 \n",
      " [12.374, 12.451, 12.483, 12.482, 12.442, 12.384, 12.293, 12.262, 12.266, 12.266, 12.296, 12.268, 12.225, 12.105, 12.129, 12.159, 12.078, 12.055, 12.056, 11.872, 11.915, 11.755, 11.724, 11.721, 11.709, 11.721, 11.576, 11.489, 11.455, 11.444, 11.437, 11.278, 11.093, 11.173, 11.09, 11.025, 10.9, 10.915, 10.895, 10.648, 10.649, 10.609, 10.598, 10.629, 10.652, 10.239, 10.054, 9.846, 9.669, 10.124, 10.139, 9.658, 9.59, 9.712, 9.606, 9.746, 9.41, 9.491, 9.928, 9.675, 9.255, 9.511, 9.727, 9.33, 9.278, 9.058, 6.039, 7.472, 8.031, 9.004, 8.732, 9.097, 8.811, 9.361, 8.378, 8.624, 8.749, 8.837, 8.235, 7.637, 8.507, 7.746, 7.883, 8.255, 8.156, 8.655, 8.969, 8.596, 8.031, 8.082, 8.236, 8.472, 8.525, 9.115, 8.191, 7.141, 7.857, 7.235, 6.839, 7.277, 6.869, 7.154, 7.599, 8.171, 8.04, 6.929, 7.614, 7.784, 7.425, 7.622, 6.891, 6.814, 7.606, 7.172, 7.029, 6.714, 6.456, 7.055, 7.678, 5.279, 7.095, 6.986, 7.025, 8.525, 8.012, 7.487, 7.043, 6.594, 6.857, 7.094, 6.476, 6.579, 6.473, 6.832, 7.583, 7.781, 7.029, 6.806, 6.89, 7.158, 7.004, 6.633, 6.778, 6.401, 6.159, 6.928, 1.065, 5.746, 0.834, 3.564, 4.405, 6.016, 4.893, 5.143, 6.811, 5.588, 5.683, 5.713, 5.79, 5.308, 5.507, 6.152, 5.37, 5.81, 6.3, 5.587, 5.961, 5.971, 5.16, 4.566, 4.483, 4.896, 5.425, 5.226, 5.638, 6.021, 6.018, 6.364, 6.027, 5.697, 6.522, 6.275, 5.977, 6.24, 5.389, 5.213, 5.878, 3.969, 5.262, 4.679, 5.595, 4.973, 3.682, 6.039, 5.135, 3.464, 4.423, 4.886, 4.637, 2.976, 2.87, 3.351, 3.697, 3.872, 5.218, 4.496, 4.506, 5.214, 4.833, 4.052, 4.18, 4.572, 4.733, 3.9, 5.129, 4.544, 3.658, 3.899, 4.924, 3.764, 4.925, 4.184, 3.65, 5.555, 4.51, 4.627, 5.612, 5.022, 3.899, 3.474, 4.671, 5.243, 4.731, 5.03, 5.213, 5.607, 4.681, 6.467, 4.987, 4.894, 4.804, 5.164, 4.923, 4.282, 3.69, 4.318, 4.76, 3.82, 4.54, 5.389, 6.17, 4.374, 5.435, 4.009, 5.475, 4.68, 4.642, 4.677, 3.348, 3.981, 4.768, 5.198, 5.681, 5.413, 5.767, 5.68, 5.257, 3.788, 4.743, 4.336, 3.752, 4.947, 5.771, 5.224, 5.457, 6.402, 4.952, 4.491, 3.855, 4.091, 3.004, 2.082, 3.796, 4.386, 4.449, 4.689, 4.51, 4.95, 4.394, 5.184, 5.079, 5.097, 7.216, 3.616, 0.812, 1.256, 2.984, 4.076, 3.683, 3.95]\n",
      "\n",
      "Lambda: 10 , Time: 0:00:09\n",
      "Train Error(all epochs): 3.972332000732422 \n",
      " [8.082, 7.894, 7.894, 7.881, 7.835, 7.816, 7.758, 7.726, 7.684, 7.639, 7.584, 7.545, 7.502, 7.467, 7.345, 7.238, 7.193, 7.108, 7.001, 6.93, 6.933, 6.841, 6.791, 6.804, 6.622, 6.718, 6.652, 6.54, 6.585, 6.542, 6.487, 6.5, 6.471, 6.466, 6.435, 6.291, 6.338, 6.367, 6.218, 6.319, 6.203, 6.142, 6.088, 6.106, 6.128, 6.091, 6.068, 5.843, 5.95, 5.895, 5.834, 5.831, 5.867, 5.78, 5.725, 5.851, 5.785, 5.793, 5.734, 5.643, 5.654, 5.7, 5.64, 5.58, 5.669, 5.66, 5.595, 5.533, 5.486, 5.646, 5.529, 5.44, 5.449, 5.449, 5.553, 5.58, 5.501, 5.518, 5.401, 5.463, 5.553, 5.36, 5.409, 5.385, 5.525, 5.505, 5.538, 5.423, 5.563, 5.412, 5.405, 5.307, 5.285, 5.36, 5.29, 5.496, 5.463, 5.37, 5.436, 5.432, 5.437, 5.308, 5.488, 5.38, 5.343, 5.341, 5.347, 5.285, 5.317, 5.338, 5.416, 5.407, 5.233, 5.279, 5.359, 5.337, 5.316, 5.264, 5.161, 5.266, 5.305, 5.308, 5.283, 5.261, 5.132, 5.275, 5.245, 5.265, 5.165, 5.134, 5.132, 5.215, 5.274, 5.214, 5.177, 5.192, 5.151, 5.297, 5.249, 5.213, 5.103, 5.065, 5.12, 5.06, 4.989, 5.076, 5.136, 5.136, 5.129, 4.977, 4.998, 4.941, 4.957, 4.85, 4.826, 4.972, 5.002, 4.965, 4.869, 5.049, 4.984, 4.886, 4.91, 4.896, 4.962, 5.047, 4.931, 4.986, 4.942, 4.954, 5.08, 5.034, 4.808, 4.959, 5.067, 4.91, 5.033, 4.976, 4.923, 4.715, 4.892, 4.836, 4.891, 4.835, 4.852, 4.813, 4.75, 4.792, 4.877, 4.819, 4.951, 4.68, 4.805, 4.819, 4.843, 4.893, 4.924, 4.838, 4.737, 4.8, 4.825, 4.824, 4.761, 4.738, 4.613, 4.705, 4.65, 4.674, 4.713, 4.68, 4.642, 4.568, 4.538, 4.449, 4.363, 4.401, 4.387, 4.564, 4.392, 4.381, 4.426, 4.297, 4.382, 4.312, 4.372, 4.326, 4.349, 4.561, 4.423, 4.439, 4.266, 4.28, 4.324, 4.364, 4.329, 4.166, 4.376, 4.37, 4.324, 4.319, 4.355, 4.308, 4.301, 4.32, 4.2, 4.26, 4.164, 4.101, 4.093, 4.174, 4.311, 4.247, 4.036, 4.292, 4.117, 4.249, 4.255, 4.256, 4.242, 4.162, 4.099, 4.217, 4.162, 4.25, 4.233, 4.064, 4.161, 4.157, 4.227, 4.093, 4.191, 4.208, 4.159, 4.16, 4.151, 4.091, 4.063, 4.209, 4.375, 4.195, 4.218, 4.108, 4.122, 4.112, 4.15, 4.135, 4.055, 4.194, 4.173, 4.221, 4.02, 3.972, 4.049, 4.006, 4.124, 4.035, 4.129, 4.227, 4.091, 4.066]\n",
      "Val Error(all epochs): 5.286808967590332 \n",
      " [8.17, 8.109, 8.092, 8.081, 8.074, 8.069, 8.065, 8.063, 8.06, 8.057, 8.055, 8.051, 8.047, 8.043, 8.032, 8.025, 8.016, 8.005, 7.986, 7.968, 7.958, 7.948, 7.937, 7.931, 7.918, 7.908, 7.899, 7.89, 7.869, 7.847, 7.842, 7.812, 7.781, 7.781, 7.743, 7.75, 7.711, 7.74, 7.715, 7.694, 7.681, 7.647, 7.599, 7.585, 7.59, 7.574, 7.551, 7.654, 7.503, 7.511, 7.539, 7.574, 7.589, 7.411, 7.387, 7.559, 7.35, 7.335, 7.426, 7.529, 7.497, 7.279, 7.557, 7.327, 7.242, 7.188, 7.114, 7.294, 7.532, 7.15, 7.27, 7.304, 7.291, 7.233, 7.32, 7.026, 7.053, 7.127, 7.088, 7.113, 6.767, 6.712, 7.014, 7.207, 7.239, 7.188, 6.922, 7.146, 6.566, 6.816, 6.603, 6.768, 6.759, 6.824, 6.943, 7.141, 6.862, 6.517, 6.476, 7.025, 6.77, 6.739, 6.646, 6.573, 6.639, 6.77, 6.59, 6.326, 6.166, 6.468, 6.401, 6.836, 6.323, 6.58, 6.505, 6.531, 6.307, 6.359, 6.625, 6.402, 6.301, 6.472, 6.121, 6.501, 6.335, 6.142, 6.244, 5.989, 5.963, 6.081, 6.122, 6.347, 6.306, 6.867, 6.895, 6.087, 6.294, 6.444, 6.033, 6.159, 6.082, 6.225, 6.253, 6.426, 5.807, 6.029, 6.641, 6.145, 5.963, 6.128, 6.295, 5.963, 6.246, 6.124, 6.019, 6.221, 5.794, 6.018, 6.141, 6.265, 6.263, 6.948, 6.233, 6.869, 6.273, 6.47, 6.504, 6.516, 6.006, 7.065, 6.525, 6.154, 6.126, 5.96, 6.258, 5.977, 6.091, 5.937, 5.972, 5.938, 5.885, 5.958, 6.385, 6.059, 5.976, 6.024, 6.427, 6.068, 6.171, 6.213, 6.154, 5.974, 5.987, 6.298, 6.702, 6.242, 6.854, 6.001, 5.98, 5.733, 5.969, 6.3, 6.16, 5.593, 5.72, 6.242, 6.196, 5.896, 6.003, 5.613, 5.642, 5.781, 5.767, 5.519, 5.564, 5.98, 5.774, 7.337, 6.613, 9.16, 8.649, 5.682, 5.74, 5.696, 7.303, 5.927, 6.187, 7.928, 5.659, 7.829, 6.382, 6.138, 5.723, 5.92, 5.773, 5.779, 5.919, 5.852, 9.401, 8.05, 5.994, 6.3, 5.655, 5.894, 6.113, 5.735, 5.669, 5.827, 5.783, 5.287, 5.396, 5.585, 5.554, 5.584, 6.324, 6.829, 6.801, 6.749, 5.681, 5.769, 5.694, 5.972, 6.68, 6.4, 5.727, 5.535, 5.855, 5.604, 5.615, 5.995, 5.528, 6.163, 5.854, 6.192, 5.935, 7.933, 6.548, 6.531, 6.241, 5.812, 7.334, 6.017, 5.859, 5.427, 5.73, 5.927, 6.566, 6.335, 5.352, 5.557, 5.68, 5.297, 5.712, 5.326, 5.552, 6.586, 5.716, 7.035, 7.218, 5.797]\n",
      "Val custom mae Error(all epochs): 0.2978363037109375 \n",
      " [12.764, 12.662, 12.635, 12.613, 12.598, 12.587, 12.578, 12.571, 12.566, 12.556, 12.55, 12.54, 12.532, 12.521, 12.496, 12.48, 12.461, 12.433, 12.385, 12.339, 12.312, 12.291, 12.266, 12.245, 12.215, 12.197, 12.176, 12.154, 12.089, 12.04, 12.031, 11.957, 11.886, 11.905, 11.802, 11.839, 11.734, 11.806, 11.755, 11.699, 11.675, 11.609, 11.526, 11.51, 11.534, 11.56, 11.52, 11.83, 11.361, 11.417, 11.563, 11.6, 11.712, 11.256, 11.32, 11.789, 11.339, 11.453, 11.557, 11.913, 11.835, 11.181, 12.018, 11.339, 11.332, 11.398, 10.991, 11.531, 12.24, 11.268, 11.525, 11.6, 11.424, 11.23, 11.675, 10.826, 11.229, 11.439, 11.185, 11.261, 10.074, 9.892, 11.243, 11.235, 11.283, 11.438, 10.418, 11.141, 9.579, 10.579, 9.701, 10.078, 9.994, 10.328, 10.844, 11.157, 10.771, 9.653, 8.718, 10.527, 8.398, 8.751, 7.902, 9.31, 9.433, 10.118, 9.233, 9.328, 7.942, 9.289, 9.168, 10.412, 8.383, 8.96, 9.322, 8.588, 9.19, 8.452, 9.813, 9.727, 9.152, 8.653, 7.997, 7.218, 6.741, 6.353, 5.75, 5.014, 7.155, 6.597, 7.404, 8.656, 8.155, 10.138, 8.362, 6.89, 9.004, 5.736, 6.122, 5.096, 5.827, 7.604, 7.184, 7.892, 6.314, 7.535, 9.037, 6.635, 7.014, 7.811, 3.702, 5.682, 5.793, 5.712, 6.343, 5.977, 5.76, 7.933, 7.141, 7.836, 6.907, 9.321, 4.982, 6.046, 7.077, 8.051, 7.673, 8.723, 4.755, 7.709, 7.684, 6.227, 6.064, 6.157, 7.865, 6.711, 7.908, 4.407, 6.912, 4.508, 5.847, 6.061, 5.85, 6.102, 6.393, 6.212, 7.354, 4.702, 6.705, 5.575, 6.266, 6.087, 6.272, 5.05, 6.721, 4.527, 7.862, 4.817, 6.749, 6.389, 7.623, 5.884, 6.96, 5.188, 6.745, 7.707, 7.303, 6.018, 7.003, 4.721, 6.626, 4.92, 5.558, 5.176, 4.619, 7.359, 5.111, 7.702, 1.642, 4.769, 3.77, 3.951, 4.058, 6.022, 2.997, 4.291, 4.006, 1.122, 5.984, 5.166, 1.007, 4.462, 2.092, 2.163, 2.009, 3.614, 6.033, 5.15, 3.169, 3.346, 4.468, 0.298, 3.542, 4.581, 2.106, 4.189, 5.827, 5.338, 4.102, 3.547, 4.794, 6.094, 5.288, 5.948, 3.799, 0.591, 3.349, 1.035, 3.0, 1.712, 5.236, 5.72, 0.748, 3.604, 3.919, 5.936, 5.862, 1.876, 6.679, 5.156, 3.783, 6.863, 4.44, 3.855, 2.091, 7.858, 2.915, 4.538, 5.522, 3.556, 4.885, 5.391, 4.636, 3.844, 5.005, 5.81, 5.921, 4.086, 3.679, 4.069, 2.9, 5.036, 6.383, 4.919, 4.899, 5.879, 2.188, 1.355, 1.298, 2.495]\n",
      "\n",
      "Trainig set size: 256 , Time: 0:00:48 , best_lambda: 10 , min_  error: 0.298\n",
      "Test starts:  308 , ends:  32161\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "996/996 [==============================] - 1s 490us/step\n",
      "total_power:  16.938116 , average_difference:  3.784753627462606\n",
      "\n",
      "\n",
      "\n",
      "number_samples: 512 , New samples: 512\n",
      "Validation size: 103 , starts: 512 , ends: 614\n",
      "\n",
      "Lambda: 0 , Time: 0:00:13\n",
      "Train Error(all epochs): 2.8363325595855713 \n",
      " [8.083, 7.894, 7.716, 7.542, 7.325, 7.08, 6.792, 6.596, 6.351, 6.081, 5.984, 5.673, 5.538, 5.315, 5.185, 5.115, 4.902, 4.723, 4.674, 4.526, 4.483, 4.412, 4.366, 4.308, 4.278, 4.231, 4.121, 3.993, 4.123, 4.055, 4.162, 3.982, 3.994, 3.895, 3.981, 3.998, 3.896, 4.004, 3.926, 3.886, 3.744, 3.949, 3.768, 3.785, 3.862, 3.78, 3.738, 3.702, 3.73, 3.711, 3.74, 3.67, 3.646, 3.576, 3.73, 3.568, 3.589, 3.628, 3.655, 3.532, 3.573, 3.648, 3.549, 3.631, 3.555, 3.44, 3.502, 3.559, 3.456, 3.588, 3.457, 3.495, 3.586, 3.543, 3.534, 3.506, 3.448, 3.481, 3.433, 3.438, 3.314, 3.327, 3.385, 3.521, 3.48, 3.398, 3.389, 3.338, 3.528, 3.459, 3.395, 3.582, 3.357, 3.516, 3.368, 3.345, 3.368, 3.34, 3.397, 3.372, 3.366, 3.377, 3.267, 3.391, 3.29, 3.303, 3.35, 3.339, 3.274, 3.375, 3.335, 3.228, 3.273, 3.299, 3.206, 3.222, 3.234, 3.246, 3.368, 3.186, 3.323, 3.243, 3.187, 3.206, 3.308, 3.29, 3.333, 3.252, 3.28, 3.28, 3.21, 3.29, 3.335, 3.223, 3.194, 3.189, 3.253, 3.217, 3.227, 3.25, 3.257, 3.251, 3.293, 3.161, 3.132, 3.191, 3.224, 3.225, 3.302, 3.198, 3.076, 3.225, 3.238, 3.127, 3.26, 3.205, 3.189, 3.128, 3.207, 3.204, 3.144, 3.147, 3.204, 3.176, 3.285, 3.141, 3.149, 3.064, 3.173, 3.106, 3.176, 3.113, 3.007, 3.075, 3.162, 3.114, 3.031, 3.059, 3.093, 3.145, 3.151, 3.042, 3.129, 3.135, 3.089, 3.046, 2.983, 3.048, 3.066, 3.085, 3.114, 3.158, 3.028, 3.113, 2.996, 2.963, 2.96, 3.174, 3.061, 3.098, 3.053, 3.062, 3.062, 3.266, 3.068, 3.024, 3.133, 3.141, 3.146, 3.131, 3.102, 3.142, 3.083, 3.178, 3.134, 2.938, 3.049, 3.02, 3.067, 3.016, 3.161, 3.008, 2.975, 3.119, 3.088, 3.044, 3.025, 2.981, 3.067, 3.049, 3.043, 2.995, 3.095, 3.018, 2.96, 3.025, 3.1, 3.117, 3.055, 2.891, 2.898, 3.064, 3.029, 2.962, 2.973, 2.943, 2.993, 3.045, 3.204, 2.979, 2.911, 3.073, 2.989, 3.041, 2.889, 3.095, 3.051, 2.941, 3.052, 3.054, 3.106, 3.079, 2.937, 2.914, 2.873, 3.064, 3.023, 2.836, 3.086, 2.988, 2.936, 2.925, 2.917, 3.012, 3.08, 2.937, 3.01, 2.999, 3.115, 3.028, 2.9, 3.018, 3.006, 2.891, 2.96, 2.913, 3.005, 2.983, 2.927, 2.853, 3.005, 3.003, 2.945, 2.847, 2.962, 2.861, 2.966, 2.972, 2.995, 2.899]\n",
      "Val Error(all epochs): 5.5302534103393555 \n",
      " [8.423, 8.264, 8.099, 7.861, 7.667, 7.479, 7.126, 6.813, 6.622, 6.389, 6.194, 6.026, 5.886, 5.919, 5.805, 5.807, 5.722, 5.617, 5.567, 5.542, 5.603, 5.651, 5.602, 5.577, 5.558, 5.53, 5.743, 5.694, 5.671, 5.627, 5.609, 5.622, 5.709, 5.715, 5.762, 5.785, 5.679, 5.72, 5.825, 5.762, 5.626, 5.681, 5.728, 5.582, 5.643, 5.664, 5.657, 5.692, 5.716, 5.701, 5.742, 5.706, 5.586, 5.59, 5.78, 5.78, 5.752, 5.8, 5.776, 5.834, 5.916, 5.836, 5.966, 5.977, 5.963, 5.847, 5.83, 5.834, 5.82, 5.847, 5.849, 5.805, 5.829, 5.831, 5.912, 5.882, 5.893, 5.874, 5.916, 5.945, 5.938, 5.83, 5.86, 5.975, 5.931, 5.928, 5.922, 5.859, 5.893, 5.803, 5.861, 5.948, 5.925, 5.901, 5.908, 5.878, 5.932, 5.949, 5.907, 5.893, 5.805, 5.766, 5.796, 5.924, 5.807, 5.856, 5.99, 5.912, 6.001, 5.916, 5.829, 5.814, 5.842, 5.945, 5.904, 5.905, 5.906, 5.969, 5.971, 5.938, 5.925, 5.923, 5.916, 5.904, 5.903, 6.015, 6.058, 6.019, 5.932, 5.899, 5.884, 5.942, 5.864, 5.819, 5.969, 5.979, 5.98, 5.979, 5.936, 5.959, 5.868, 5.909, 5.856, 5.86, 5.963, 5.951, 5.836, 5.901, 5.914, 5.917, 5.857, 5.875, 5.88, 5.855, 5.878, 5.932, 5.9, 5.829, 5.879, 5.87, 5.812, 5.839, 5.856, 5.889, 5.912, 5.988, 5.921, 5.9, 5.75, 5.834, 5.838, 5.884, 5.898, 5.874, 5.871, 5.883, 5.852, 5.86, 5.822, 5.961, 5.939, 5.872, 5.842, 5.884, 5.748, 5.747, 5.792, 5.787, 5.778, 5.814, 5.845, 5.817, 5.781, 5.76, 5.824, 5.856, 5.868, 5.831, 5.849, 5.906, 5.84, 5.856, 5.833, 5.8, 5.871, 5.842, 5.807, 5.769, 5.769, 5.798, 5.769, 5.783, 5.735, 5.736, 5.701, 5.814, 5.827, 5.754, 5.834, 5.73, 5.746, 5.811, 5.778, 5.775, 5.762, 5.843, 5.849, 5.845, 5.838, 5.879, 5.904, 5.776, 5.759, 5.727, 5.701, 5.787, 5.685, 5.664, 5.719, 5.765, 5.731, 5.695, 5.656, 5.694, 5.698, 5.66, 5.73, 5.816, 5.792, 5.785, 5.757, 5.781, 5.826, 5.872, 5.914, 5.843, 5.843, 5.882, 5.842, 5.908, 5.889, 5.86, 5.906, 5.963, 5.985, 5.954, 5.885, 5.94, 5.912, 6.004, 6.001, 5.956, 5.928, 5.97, 5.912, 5.937, 5.915, 5.967, 6.025, 5.946, 5.964, 5.905, 5.853, 5.94, 5.917, 5.916, 5.974, 6.001, 5.909, 5.943, 5.982, 5.943, 5.973, 5.927, 5.933, 5.94, 5.963, 5.908, 5.889, 5.9]\n",
      "Val custom mae Error(all epochs): 2.186072826385498 \n",
      " [12.16, 11.984, 11.706, 11.22, 10.879, 10.677, 9.939, 9.158, 9.104, 8.587, 7.61, 7.566, 6.777, 7.167, 6.472, 6.485, 4.975, 5.211, 5.192, 4.673, 4.52, 3.935, 3.782, 3.719, 3.812, 3.648, 3.208, 3.291, 3.218, 2.933, 2.927, 2.951, 2.783, 3.246, 2.792, 2.671, 3.221, 2.891, 2.818, 2.957, 2.989, 2.544, 2.539, 3.036, 3.624, 3.029, 3.174, 3.31, 3.585, 3.263, 3.428, 4.046, 3.525, 3.324, 3.451, 2.993, 3.158, 3.524, 3.22, 3.089, 3.154, 2.627, 2.954, 2.972, 2.653, 3.127, 3.145, 3.145, 2.959, 2.912, 2.674, 2.636, 2.679, 2.521, 2.93, 2.95, 2.449, 2.966, 2.704, 3.15, 3.452, 2.844, 2.753, 3.074, 3.017, 3.431, 3.397, 3.318, 3.361, 3.609, 2.879, 3.344, 3.212, 3.275, 2.965, 3.377, 3.402, 3.233, 2.962, 3.278, 3.059, 3.036, 3.231, 3.032, 3.053, 2.926, 2.935, 2.773, 3.013, 2.771, 2.796, 3.001, 3.329, 3.268, 3.209, 2.783, 2.838, 2.673, 2.757, 2.883, 3.066, 2.846, 2.783, 2.503, 2.818, 3.049, 2.63, 3.083, 2.666, 2.728, 2.959, 2.69, 3.009, 3.141, 2.942, 2.932, 2.998, 3.161, 3.018, 3.294, 3.137, 3.504, 3.048, 3.18, 2.937, 2.836, 2.903, 2.848, 2.777, 3.133, 2.804, 3.461, 3.39, 3.343, 3.157, 3.455, 3.17, 3.243, 3.391, 3.06, 2.907, 2.734, 2.735, 2.77, 3.059, 2.776, 3.035, 2.911, 3.056, 2.936, 3.204, 3.208, 3.249, 3.06, 2.824, 2.945, 2.968, 2.739, 2.735, 2.944, 2.992, 2.732, 3.006, 3.369, 2.9, 3.017, 3.369, 3.222, 2.981, 2.775, 2.64, 2.954, 3.114, 3.008, 3.111, 2.991, 3.331, 3.255, 2.922, 2.67, 2.805, 2.866, 2.925, 2.811, 2.625, 2.832, 2.579, 2.524, 3.134, 2.853, 2.949, 2.772, 2.961, 2.667, 2.779, 2.947, 2.74, 2.724, 2.45, 2.784, 3.193, 2.996, 3.064, 2.748, 2.541, 2.865, 2.873, 2.618, 3.007, 2.755, 2.445, 2.526, 2.266, 2.426, 2.774, 2.856, 2.86, 2.63, 3.002, 2.873, 3.067, 2.871, 2.614, 2.544, 2.59, 2.751, 2.617, 2.34, 2.481, 2.376, 2.653, 2.904, 2.561, 2.525, 2.226, 2.193, 2.186, 2.247, 2.481, 2.737, 2.528, 2.775, 2.511, 2.535, 2.521, 2.477, 2.578, 2.459, 2.598, 2.4, 2.712, 2.905, 2.851, 2.515, 2.865, 2.994, 2.865, 2.7, 2.768, 2.869, 2.959, 2.816, 2.676, 3.003, 2.803, 2.49, 2.634, 2.96, 2.695, 2.778, 2.788, 2.716, 2.459, 2.792, 2.673, 2.775, 2.498, 2.451, 2.524, 2.725]\n",
      "\n",
      "Lambda: 0.01 , Time: 0:00:13\n",
      "Train Error(all epochs): 2.842485189437866 \n",
      " [8.048, 7.896, 7.704, 7.513, 7.264, 7.071, 6.724, 6.517, 6.305, 6.081, 5.813, 5.545, 5.42, 5.258, 5.097, 4.876, 4.737, 4.582, 4.466, 4.422, 4.472, 4.262, 4.359, 4.057, 4.066, 4.136, 4.069, 4.076, 4.147, 4.015, 4.095, 3.968, 3.902, 3.892, 3.988, 3.971, 3.897, 3.891, 3.848, 3.755, 3.765, 3.787, 3.792, 3.813, 3.926, 3.813, 3.803, 3.801, 3.612, 3.678, 3.665, 3.767, 3.625, 3.63, 3.526, 3.653, 3.623, 3.586, 3.556, 3.462, 3.65, 3.655, 3.507, 3.531, 3.5, 3.568, 3.477, 3.472, 3.522, 3.392, 3.484, 3.601, 3.52, 3.442, 3.343, 3.431, 3.316, 3.62, 3.515, 3.399, 3.504, 3.35, 3.424, 3.393, 3.312, 3.419, 3.285, 3.347, 3.389, 3.352, 3.431, 3.48, 3.315, 3.422, 3.425, 3.384, 3.363, 3.432, 3.291, 3.384, 3.35, 3.303, 3.296, 3.394, 3.357, 3.322, 3.224, 3.244, 3.345, 3.341, 3.289, 3.281, 3.295, 3.34, 3.244, 3.272, 3.296, 3.259, 3.209, 3.207, 3.272, 3.283, 3.307, 3.216, 3.327, 3.21, 3.189, 3.26, 3.252, 3.281, 3.239, 3.223, 3.15, 3.3, 3.181, 3.122, 3.274, 3.141, 3.161, 3.17, 3.243, 3.238, 3.293, 3.184, 3.105, 3.168, 3.167, 3.153, 3.189, 3.281, 3.235, 3.207, 3.114, 3.241, 3.156, 3.17, 3.188, 3.147, 3.185, 3.206, 3.174, 3.221, 3.167, 3.121, 3.104, 3.133, 3.188, 3.202, 3.12, 3.104, 3.087, 3.045, 3.175, 3.196, 3.096, 3.197, 3.143, 3.147, 3.214, 3.155, 3.221, 3.032, 3.162, 3.126, 3.074, 3.223, 3.106, 3.023, 3.139, 3.176, 3.159, 3.105, 3.131, 3.207, 3.122, 3.02, 3.104, 3.037, 3.083, 3.144, 3.089, 3.152, 3.113, 3.059, 3.064, 3.065, 3.128, 3.055, 3.098, 3.082, 3.071, 3.084, 3.187, 3.041, 3.13, 3.141, 3.095, 3.078, 3.021, 3.074, 3.064, 2.973, 3.12, 3.137, 3.044, 2.968, 2.966, 3.137, 2.963, 3.055, 3.03, 3.02, 2.951, 3.096, 2.966, 3.021, 3.034, 3.016, 3.062, 3.126, 2.96, 3.063, 2.967, 2.943, 3.114, 3.057, 3.053, 3.137, 3.052, 3.027, 3.014, 3.062, 2.978, 2.907, 3.11, 2.953, 2.988, 3.065, 3.007, 3.058, 2.895, 2.956, 2.938, 3.176, 2.939, 2.989, 3.054, 3.127, 3.003, 3.056, 2.985, 2.88, 3.128, 3.057, 3.011, 2.968, 3.096, 3.027, 3.128, 2.992, 2.974, 3.042, 2.955, 2.962, 2.999, 3.036, 3.008, 3.144, 3.06, 2.992, 2.935, 2.972, 3.066, 3.037, 2.842, 3.074, 2.938, 2.863, 3.019, 2.899]\n",
      "Val Error(all epochs): 5.53865385055542 \n",
      " [8.329, 8.217, 8.122, 7.989, 7.792, 7.541, 7.312, 7.01, 6.667, 6.485, 6.254, 6.144, 6.115, 6.09, 5.92, 5.782, 5.77, 5.833, 5.837, 5.826, 5.764, 5.803, 5.681, 5.806, 5.712, 5.765, 5.814, 5.873, 5.911, 5.964, 5.943, 6.024, 5.892, 6.026, 6.029, 6.043, 6.033, 6.05, 6.206, 6.205, 6.145, 6.079, 6.154, 6.316, 6.303, 6.338, 6.339, 6.257, 6.372, 6.426, 6.46, 6.405, 6.44, 6.459, 6.59, 6.439, 6.392, 6.521, 6.572, 6.529, 6.602, 6.641, 6.623, 6.584, 6.509, 6.531, 6.544, 6.511, 6.496, 6.449, 6.455, 6.522, 6.422, 6.395, 6.314, 6.294, 6.178, 6.44, 6.151, 6.123, 6.144, 6.169, 6.301, 6.165, 6.218, 6.265, 6.173, 6.191, 6.115, 6.155, 6.203, 6.183, 6.063, 6.049, 6.103, 6.169, 6.147, 6.174, 6.172, 6.073, 6.183, 6.18, 6.02, 5.971, 6.014, 6.082, 5.863, 5.781, 5.627, 5.738, 5.694, 5.701, 5.689, 5.665, 5.697, 5.758, 5.795, 5.644, 5.888, 5.902, 5.827, 5.772, 5.774, 5.701, 5.819, 5.896, 5.913, 5.902, 5.979, 6.043, 5.895, 6.061, 6.084, 6.029, 5.998, 5.963, 6.07, 6.035, 6.11, 6.094, 6.264, 6.309, 6.364, 6.285, 6.254, 6.214, 6.221, 6.146, 6.159, 6.224, 6.199, 6.199, 6.309, 6.315, 6.324, 6.267, 6.217, 6.158, 6.164, 6.194, 6.204, 6.134, 6.144, 6.203, 6.146, 6.171, 6.166, 6.021, 6.065, 5.94, 6.034, 6.056, 6.074, 6.151, 6.12, 6.12, 6.06, 5.976, 5.975, 6.088, 6.153, 6.08, 6.062, 6.249, 5.856, 5.827, 5.901, 6.013, 5.953, 6.062, 5.952, 6.047, 5.876, 5.883, 5.901, 6.012, 5.994, 6.039, 6.048, 6.063, 6.1, 6.033, 5.98, 6.101, 5.961, 5.883, 5.857, 5.918, 6.036, 5.987, 5.946, 6.011, 5.889, 5.951, 5.988, 6.045, 6.052, 6.079, 6.014, 5.889, 6.08, 5.993, 6.055, 5.925, 5.972, 5.879, 5.985, 5.951, 5.981, 5.933, 5.966, 5.899, 5.812, 5.828, 5.86, 5.906, 5.815, 5.817, 5.633, 5.73, 5.991, 5.663, 5.733, 5.832, 5.811, 5.736, 5.694, 5.655, 5.678, 5.671, 5.738, 5.73, 5.754, 5.792, 5.715, 5.555, 5.613, 5.586, 5.754, 5.682, 5.598, 5.657, 5.779, 5.763, 5.683, 5.593, 5.652, 5.643, 5.602, 5.613, 5.648, 5.663, 5.681, 5.676, 5.619, 5.693, 5.6, 5.625, 5.543, 5.599, 5.626, 5.63, 5.542, 5.547, 5.666, 5.584, 5.671, 5.653, 5.63, 5.662, 5.675, 5.58, 5.596, 5.643, 5.638, 5.611, 5.539, 5.546, 5.588, 5.544]\n",
      "Val custom mae Error(all epochs): 3.8938708305358887 \n",
      " [12.425, 12.179, 12.03, 11.778, 11.428, 10.929, 10.689, 9.965, 9.153, 8.868, 8.439, 7.928, 7.213, 7.071, 6.158, 7.328, 7.157, 9.068, 8.905, 9.817, 11.42, 11.771, 13.851, 14.562, 14.701, 17.661, 17.978, 17.264, 20.006, 22.482, 21.035, 24.957, 26.012, 29.704, 32.038, 33.21, 39.548, 39.165, 45.482, 50.199, 53.312, 55.146, 56.76, 60.425, 61.738, 58.074, 59.986, 63.998, 67.523, 70.45, 67.216, 69.127, 73.103, 69.187, 74.897, 74.353, 70.076, 76.66, 80.23, 77.168, 81.327, 78.684, 75.321, 73.297, 70.521, 68.423, 66.101, 66.078, 63.844, 65.432, 63.708, 64.668, 63.373, 58.593, 60.324, 63.385, 58.648, 52.974, 50.107, 50.369, 50.536, 48.945, 43.733, 45.6, 46.584, 46.03, 47.071, 48.528, 47.418, 47.788, 52.776, 52.527, 49.856, 46.812, 45.088, 42.03, 41.644, 43.225, 46.48, 48.381, 46.418, 41.918, 41.115, 39.721, 36.951, 37.829, 17.688, 13.932, 8.743, 10.808, 12.343, 13.847, 13.365, 12.55, 13.681, 16.262, 18.476, 19.906, 21.731, 21.701, 22.578, 23.588, 25.028, 26.984, 30.106, 29.73, 34.458, 36.12, 36.575, 38.013, 39.831, 42.993, 44.724, 43.8, 44.24, 48.197, 50.033, 48.656, 45.614, 49.565, 51.599, 57.209, 62.218, 59.943, 61.029, 52.321, 51.188, 47.853, 47.251, 47.212, 51.034, 53.228, 54.803, 55.53, 57.892, 46.964, 47.446, 46.727, 46.45, 48.645, 50.829, 47.876, 51.256, 52.03, 50.721, 52.457, 48.853, 43.035, 43.631, 43.458, 43.817, 44.038, 40.619, 41.517, 44.77, 43.308, 38.922, 35.527, 34.146, 36.399, 40.138, 36.232, 33.221, 29.509, 25.254, 26.103, 27.603, 29.313, 27.239, 30.417, 29.416, 30.189, 24.534, 25.413, 24.723, 26.692, 25.155, 24.759, 25.27, 26.334, 24.815, 22.415, 21.334, 19.219, 19.628, 20.901, 21.624, 23.801, 26.228, 23.863, 22.067, 23.694, 22.684, 22.55, 23.229, 20.99, 18.865, 21.024, 18.592, 15.889, 18.886, 19.971, 18.468, 16.493, 15.896, 14.843, 16.801, 17.69, 18.489, 18.861, 15.819, 14.34, 13.045, 13.879, 12.771, 12.524, 11.062, 10.053, 10.21, 9.088, 10.815, 11.908, 14.511, 13.471, 14.688, 13.433, 12.949, 13.033, 12.19, 12.487, 14.699, 13.051, 10.745, 10.237, 9.817, 9.508, 11.332, 12.041, 12.532, 9.541, 6.312, 4.477, 6.431, 6.84, 5.065, 6.316, 7.165, 7.045, 7.189, 7.639, 7.358, 8.875, 10.841, 10.272, 12.176, 11.667, 11.24, 9.018, 7.033, 7.085, 6.686, 6.28, 4.739, 5.752, 5.178, 4.251, 4.123, 4.926, 6.565, 5.921, 5.727, 5.521, 4.509, 5.241, 5.677, 4.963, 5.067, 4.982, 3.894, 4.243]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Lambda: 0.1 , Time: 0:00:13\n",
      "Train Error(all epochs): 3.1136972904205322 \n",
      " [8.086, 7.865, 7.698, 7.535, 7.389, 7.129, 6.91, 6.57, 6.384, 6.127, 5.842, 5.625, 5.419, 5.152, 5.099, 5.017, 4.804, 4.648, 4.536, 4.542, 4.436, 4.431, 4.306, 4.338, 4.14, 4.24, 4.362, 4.216, 4.205, 4.178, 4.094, 4.041, 4.048, 3.835, 4.02, 4.091, 3.786, 3.942, 3.892, 3.895, 3.873, 3.761, 3.886, 3.823, 3.83, 3.786, 3.819, 3.78, 3.643, 3.862, 3.821, 3.666, 3.749, 3.719, 3.703, 3.784, 3.715, 3.648, 3.545, 3.735, 3.635, 3.57, 3.655, 3.619, 3.682, 3.586, 3.613, 3.643, 3.578, 3.612, 3.659, 3.575, 3.635, 3.612, 3.506, 3.616, 3.539, 3.637, 3.414, 3.669, 3.461, 3.57, 3.535, 3.465, 3.6, 3.514, 3.5, 3.544, 3.494, 3.62, 3.509, 3.553, 3.449, 3.519, 3.501, 3.436, 3.482, 3.5, 3.39, 3.437, 3.511, 3.459, 3.46, 3.437, 3.509, 3.457, 3.443, 3.399, 3.4, 3.358, 3.486, 3.526, 3.458, 3.467, 3.418, 3.466, 3.416, 3.498, 3.452, 3.385, 3.426, 3.451, 3.357, 3.384, 3.407, 3.453, 3.483, 3.319, 3.36, 3.319, 3.415, 3.452, 3.515, 3.356, 3.415, 3.369, 3.558, 3.434, 3.531, 3.423, 3.314, 3.431, 3.335, 3.334, 3.393, 3.336, 3.311, 3.391, 3.481, 3.426, 3.569, 3.322, 3.333, 3.421, 3.399, 3.394, 3.314, 3.427, 3.313, 3.343, 3.435, 3.291, 3.275, 3.39, 3.373, 3.436, 3.199, 3.233, 3.351, 3.359, 3.313, 3.327, 3.379, 3.255, 3.164, 3.32, 3.263, 3.286, 3.251, 3.201, 3.376, 3.313, 3.357, 3.39, 3.318, 3.268, 3.345, 3.433, 3.34, 3.343, 3.216, 3.302, 3.243, 3.269, 3.333, 3.207, 3.271, 3.383, 3.296, 3.298, 3.327, 3.266, 3.193, 3.303, 3.238, 3.361, 3.3, 3.284, 3.321, 3.344, 3.337, 3.283, 3.34, 3.283, 3.299, 3.37, 3.33, 3.236, 3.146, 3.239, 3.179, 3.245, 3.344, 3.201, 3.24, 3.227, 3.232, 3.243, 3.258, 3.255, 3.319, 3.317, 3.348, 3.272, 3.269, 3.212, 3.281, 3.244, 3.257, 3.372, 3.303, 3.187, 3.165, 3.227, 3.161, 3.162, 3.221, 3.187, 3.181, 3.254, 3.114, 3.272, 3.292, 3.225, 3.218, 3.166, 3.199, 3.187, 3.176, 3.291, 3.28, 3.205, 3.214, 3.153, 3.144, 3.24, 3.372, 3.238, 3.184, 3.164, 3.217, 3.196, 3.317, 3.205, 3.28, 3.189, 3.258, 3.153, 3.234, 3.262, 3.342, 3.117, 3.215, 3.267, 3.288, 3.226, 3.314, 3.18, 3.28, 3.223, 3.183, 3.192, 3.169, 3.258, 3.292, 3.202, 3.201, 3.168, 3.251, 3.228]\n",
      "Val Error(all epochs): 5.368030548095703 \n",
      " [8.744, 8.515, 8.384, 8.214, 8.011, 7.778, 7.535, 7.248, 6.991, 6.657, 6.49, 6.384, 6.303, 6.129, 6.149, 6.076, 5.996, 5.94, 5.926, 5.848, 5.866, 5.893, 5.829, 5.835, 5.662, 5.559, 5.624, 5.504, 5.764, 5.544, 5.471, 5.578, 5.612, 5.495, 5.516, 5.613, 5.556, 5.606, 5.672, 5.604, 5.795, 5.608, 5.62, 5.707, 5.772, 5.815, 5.69, 5.678, 5.711, 5.669, 5.758, 5.718, 5.673, 5.791, 5.608, 5.655, 5.636, 5.681, 5.635, 5.728, 5.666, 5.71, 5.757, 5.783, 5.743, 5.794, 5.624, 5.743, 5.634, 5.787, 5.668, 5.673, 5.584, 5.55, 5.607, 5.739, 5.621, 5.537, 5.467, 5.769, 5.656, 5.73, 5.384, 5.449, 5.496, 5.599, 5.445, 5.589, 5.605, 5.623, 5.673, 5.599, 5.741, 5.648, 5.583, 5.552, 5.603, 5.658, 5.603, 5.652, 5.629, 5.625, 5.621, 5.693, 5.594, 5.624, 5.626, 5.45, 5.527, 5.57, 5.562, 5.586, 5.689, 5.535, 5.562, 5.62, 5.689, 5.662, 5.681, 5.712, 5.663, 5.604, 5.649, 5.591, 5.806, 5.753, 5.552, 5.651, 5.706, 5.788, 5.564, 5.553, 5.568, 5.401, 5.58, 5.529, 5.545, 5.371, 5.571, 5.459, 5.755, 5.692, 5.524, 5.44, 5.378, 5.49, 5.552, 5.478, 5.638, 5.611, 5.555, 5.657, 5.559, 5.588, 5.634, 5.677, 5.603, 5.539, 5.703, 5.514, 5.562, 5.616, 5.699, 5.743, 5.619, 5.721, 5.661, 5.689, 5.524, 5.737, 5.709, 5.607, 5.558, 5.536, 5.581, 5.601, 5.673, 5.551, 5.636, 5.569, 5.673, 5.599, 5.597, 5.622, 5.779, 5.774, 5.669, 5.527, 5.368, 5.513, 5.646, 5.63, 5.602, 5.526, 5.651, 5.528, 5.579, 5.54, 5.593, 5.689, 5.694, 5.595, 5.577, 5.624, 5.765, 5.675, 5.548, 5.605, 5.739, 5.692, 5.726, 5.692, 5.693, 5.697, 5.563, 5.573, 5.476, 5.528, 5.496, 5.51, 5.533, 5.649, 5.635, 5.533, 5.623, 5.706, 5.656, 5.68, 5.77, 5.617, 5.547, 5.549, 5.703, 5.619, 5.827, 5.599, 5.692, 5.56, 5.527, 5.688, 5.609, 5.693, 5.608, 5.615, 5.591, 5.673, 5.719, 5.625, 5.743, 5.556, 5.469, 5.416, 5.626, 5.488, 5.679, 5.638, 5.609, 5.722, 5.535, 5.664, 5.725, 5.765, 5.626, 5.704, 5.666, 5.849, 5.65, 5.715, 5.646, 5.634, 5.489, 5.724, 5.663, 5.555, 5.641, 5.605, 5.708, 5.516, 5.605, 5.625, 5.655, 5.627, 5.503, 5.595, 5.617, 5.722, 5.652, 5.615, 5.684, 5.525, 5.623, 5.579, 5.6, 5.642, 5.671, 5.425, 5.479, 5.514, 5.527, 5.603]\n",
      "Val custom mae Error(all epochs): 2.502406120300293 \n",
      " [13.443, 13.003, 12.693, 12.321, 11.905, 11.439, 11.141, 10.645, 10.29, 9.479, 9.358, 8.675, 8.937, 8.142, 8.412, 7.413, 7.152, 6.913, 6.673, 6.248, 6.503, 6.455, 6.446, 5.641, 5.868, 5.704, 5.871, 5.125, 5.49, 5.061, 5.189, 5.405, 5.166, 4.956, 4.68, 5.048, 4.757, 4.86, 4.678, 5.226, 5.078, 5.003, 4.476, 5.094, 4.254, 4.512, 4.445, 4.6, 3.66, 3.854, 4.308, 4.673, 4.45, 5.13, 4.499, 4.891, 4.624, 4.324, 4.626, 4.551, 4.793, 4.81, 5.111, 4.665, 4.826, 4.253, 4.328, 4.48, 4.237, 4.795, 4.745, 5.253, 5.03, 4.737, 4.544, 4.419, 4.085, 4.144, 4.517, 4.802, 4.756, 4.279, 4.21, 3.743, 3.957, 4.425, 4.134, 3.981, 4.176, 4.411, 4.389, 3.624, 4.147, 4.215, 4.029, 4.332, 4.165, 4.075, 4.276, 4.109, 4.617, 4.175, 4.139, 3.945, 4.399, 4.35, 4.464, 4.501, 4.331, 4.005, 4.152, 3.855, 3.814, 3.988, 3.913, 3.806, 3.649, 3.702, 3.707, 3.524, 4.034, 3.939, 3.505, 3.62, 3.597, 4.22, 4.034, 4.384, 4.26, 4.315, 3.832, 4.359, 4.133, 3.743, 2.667, 3.54, 3.643, 3.12, 2.927, 3.43, 4.063, 4.629, 4.47, 3.901, 4.392, 3.58, 4.111, 4.158, 4.456, 4.219, 4.111, 4.215, 4.27, 3.829, 3.576, 4.195, 4.814, 4.135, 4.39, 3.426, 4.046, 4.379, 4.736, 4.488, 4.226, 4.429, 3.716, 4.814, 4.08, 4.739, 3.853, 2.765, 3.935, 3.418, 3.721, 4.001, 4.086, 4.043, 3.527, 3.695, 4.037, 3.768, 3.605, 3.369, 3.818, 3.602, 4.067, 3.92, 4.146, 3.93, 3.97, 3.983, 3.683, 3.181, 3.541, 3.496, 3.841, 3.53, 3.971, 4.767, 3.762, 4.443, 3.761, 3.743, 4.151, 4.407, 3.299, 3.873, 3.588, 3.619, 3.556, 2.692, 3.406, 3.473, 3.387, 3.082, 3.339, 3.32, 3.439, 3.38, 4.188, 4.869, 4.301, 4.868, 4.493, 5.022, 4.142, 4.703, 4.595, 2.916, 3.298, 3.71, 4.055, 3.669, 4.034, 3.061, 3.201, 3.573, 3.754, 3.589, 4.351, 3.65, 3.535, 3.922, 3.67, 4.446, 3.963, 3.588, 3.468, 3.885, 3.688, 2.947, 3.159, 3.948, 3.419, 3.957, 3.377, 3.62, 3.213, 3.437, 3.605, 4.175, 3.52, 3.962, 3.577, 3.629, 3.711, 3.501, 2.502, 3.368, 3.585, 3.621, 3.142, 3.619, 3.502, 3.796, 3.974, 3.557, 4.01, 3.798, 3.351, 3.125, 3.457, 3.261, 4.152, 3.593, 3.35, 3.159, 2.989, 4.002, 3.356, 3.678, 4.498, 3.9, 4.507, 3.912, 3.448, 3.715, 3.403, 4.196]\n",
      "\n",
      "Lambda: 1 , Time: 0:00:13\n",
      "Train Error(all epochs): 3.41062068939209 \n",
      " [8.063, 7.848, 7.675, 7.471, 7.184, 6.98, 6.74, 6.451, 6.236, 5.993, 5.873, 5.718, 5.477, 5.402, 5.256, 5.193, 5.099, 4.984, 4.958, 4.869, 4.768, 4.76, 4.712, 4.614, 4.553, 4.648, 4.644, 4.569, 4.441, 4.54, 4.448, 4.402, 4.496, 4.35, 4.345, 4.347, 4.399, 4.393, 4.254, 4.29, 4.378, 4.119, 4.307, 4.371, 4.404, 4.296, 4.318, 4.25, 4.098, 4.243, 4.133, 4.128, 4.198, 4.174, 4.191, 4.021, 4.183, 4.068, 4.084, 4.143, 4.142, 4.302, 4.255, 4.192, 4.185, 4.217, 4.18, 4.07, 4.004, 3.984, 4.265, 3.996, 4.0, 4.017, 4.132, 4.07, 4.167, 3.962, 3.958, 3.976, 4.078, 4.071, 3.977, 4.042, 3.934, 4.006, 3.949, 4.137, 3.996, 4.076, 4.012, 4.063, 3.971, 3.887, 4.081, 4.12, 4.016, 4.01, 3.981, 3.933, 3.904, 3.917, 3.93, 3.919, 4.029, 4.041, 3.951, 3.971, 3.927, 3.842, 3.913, 3.883, 3.964, 4.016, 4.024, 3.905, 3.951, 3.961, 4.136, 4.048, 4.007, 4.044, 3.957, 4.036, 3.82, 3.982, 3.902, 3.855, 3.861, 3.896, 3.847, 3.736, 3.781, 3.926, 3.88, 3.84, 3.887, 3.921, 3.868, 3.743, 3.924, 3.998, 3.852, 3.819, 3.847, 3.835, 3.75, 3.824, 3.755, 3.893, 3.854, 3.884, 3.664, 3.74, 3.655, 3.911, 3.826, 3.888, 3.838, 3.761, 3.834, 3.872, 3.879, 3.847, 3.705, 3.888, 3.915, 3.953, 3.925, 3.78, 3.664, 3.77, 3.866, 3.819, 3.889, 3.806, 3.827, 3.805, 3.798, 3.879, 3.731, 3.788, 3.766, 3.695, 3.695, 3.767, 3.827, 3.734, 3.721, 3.751, 3.715, 3.697, 3.88, 3.693, 3.758, 3.763, 3.751, 3.811, 3.773, 3.886, 3.788, 3.706, 3.778, 3.743, 3.636, 3.772, 3.698, 3.843, 3.685, 3.677, 3.662, 3.714, 3.584, 3.707, 3.759, 3.853, 3.737, 3.651, 3.657, 3.676, 3.551, 3.529, 3.709, 3.571, 3.632, 3.676, 3.713, 3.692, 3.788, 3.735, 3.78, 3.849, 3.647, 3.691, 3.673, 3.589, 3.74, 3.748, 3.691, 3.628, 3.596, 3.746, 3.68, 3.662, 3.745, 3.628, 3.763, 3.763, 3.648, 3.519, 3.542, 3.755, 3.616, 3.639, 3.568, 3.612, 3.714, 3.704, 3.744, 3.684, 3.616, 3.549, 3.683, 3.618, 3.607, 3.55, 3.457, 3.661, 3.559, 3.604, 3.543, 3.516, 3.496, 3.594, 3.531, 3.655, 3.598, 3.558, 3.578, 3.59, 3.611, 3.686, 3.584, 3.626, 3.574, 3.428, 3.573, 3.549, 3.585, 3.54, 3.575, 3.487, 3.63, 3.66, 3.688, 3.685, 3.544, 3.458, 3.497, 3.411]\n",
      "Val Error(all epochs): 5.323427200317383 \n",
      " [8.209, 8.203, 8.156, 8.098, 8.013, 7.917, 7.876, 7.813, 7.763, 7.715, 7.683, 7.607, 7.532, 7.488, 7.263, 7.281, 7.185, 7.03, 6.977, 6.893, 6.791, 6.773, 6.81, 6.64, 6.615, 6.566, 6.604, 6.442, 6.382, 6.503, 6.535, 6.275, 6.305, 6.26, 6.126, 6.103, 6.174, 6.106, 6.062, 6.323, 6.088, 5.983, 6.165, 6.241, 6.033, 5.943, 5.937, 6.01, 6.052, 5.973, 6.813, 6.14, 5.906, 6.095, 6.106, 6.021, 5.99, 5.896, 6.054, 6.027, 5.868, 6.063, 5.745, 5.782, 5.939, 6.311, 6.041, 5.916, 5.954, 5.961, 5.876, 5.714, 5.915, 5.853, 5.952, 6.262, 6.041, 6.047, 5.998, 5.711, 5.914, 6.108, 5.868, 5.665, 5.711, 5.951, 5.904, 5.73, 5.652, 5.691, 5.598, 5.566, 5.722, 5.561, 5.697, 5.852, 5.791, 5.495, 5.614, 5.68, 6.077, 5.539, 5.653, 5.552, 5.628, 5.703, 5.665, 5.916, 5.577, 5.683, 5.595, 5.621, 5.828, 5.944, 5.865, 5.703, 5.977, 5.562, 5.571, 5.542, 5.795, 5.751, 5.739, 5.609, 5.847, 5.785, 5.766, 5.655, 5.534, 5.718, 5.802, 5.732, 5.808, 5.786, 5.495, 5.769, 5.825, 5.682, 5.506, 5.995, 5.794, 5.749, 5.689, 5.541, 5.966, 5.702, 5.619, 5.688, 6.01, 5.63, 5.57, 5.592, 5.661, 5.622, 5.562, 5.635, 5.684, 5.375, 5.587, 5.443, 5.549, 5.542, 5.511, 5.905, 6.179, 5.784, 5.58, 5.619, 5.57, 5.632, 5.738, 6.057, 5.63, 5.323, 5.734, 5.729, 5.496, 5.488, 5.771, 5.399, 5.771, 5.659, 5.748, 5.824, 6.134, 5.63, 5.799, 5.667, 5.602, 5.534, 5.704, 5.635, 5.802, 5.911, 5.701, 6.001, 5.974, 5.649, 5.799, 5.733, 5.847, 5.691, 5.988, 5.65, 5.863, 5.665, 5.752, 5.708, 5.45, 5.59, 5.606, 5.606, 5.676, 5.736, 5.709, 5.584, 5.638, 5.615, 5.636, 5.795, 5.688, 5.715, 5.544, 6.132, 5.591, 5.682, 5.721, 5.528, 5.682, 5.622, 5.689, 5.707, 5.792, 5.67, 5.547, 5.586, 5.537, 6.993, 6.39, 5.768, 5.656, 5.809, 5.946, 6.132, 6.013, 5.815, 5.732, 5.668, 5.649, 5.606, 5.672, 5.751, 5.81, 5.798, 5.894, 5.891, 5.882, 5.865, 5.84, 5.908, 5.62, 5.679, 5.589, 5.603, 5.89, 5.76, 5.616, 5.475, 5.952, 5.582, 5.508, 5.663, 6.01, 5.627, 5.518, 5.589, 5.57, 5.525, 5.729, 5.447, 5.65, 5.95, 5.813, 5.712, 5.677, 5.681, 5.601, 5.556, 5.445, 5.776, 5.6, 5.73, 5.707, 5.815, 5.615, 5.639, 5.76, 5.608, 5.58, 5.59]\n",
      "Val custom mae Error(all epochs): 0.860774040222168 \n",
      " [12.185, 12.187, 12.095, 11.991, 11.835, 11.657, 11.648, 11.53, 11.457, 11.31, 11.351, 11.179, 11.073, 11.084, 10.671, 10.612, 10.515, 10.491, 10.236, 9.83, 9.832, 10.084, 9.833, 9.703, 9.453, 9.372, 9.527, 9.42, 9.113, 9.303, 8.953, 8.486, 8.655, 8.778, 8.582, 8.015, 7.741, 7.88, 7.894, 8.224, 7.795, 7.664, 7.862, 7.73, 6.894, 7.428, 7.231, 6.883, 7.432, 7.063, 9.32, 6.248, 6.242, 7.166, 7.202, 6.327, 6.419, 7.104, 7.167, 6.353, 5.799, 6.178, 5.723, 5.731, 5.839, 6.05, 6.659, 6.525, 7.298, 6.331, 5.42, 5.989, 6.854, 6.858, 5.776, 6.203, 5.705, 6.255, 5.524, 4.925, 5.515, 3.679, 5.175, 4.751, 4.66, 5.444, 4.469, 4.563, 3.827, 4.818, 4.154, 3.526, 4.945, 5.64, 4.705, 5.581, 3.295, 4.949, 3.781, 3.448, 5.795, 5.095, 4.445, 3.705, 3.69, 5.061, 4.365, 4.533, 3.373, 3.805, 4.454, 3.798, 4.234, 5.534, 2.546, 3.579, 3.975, 2.742, 2.687, 2.456, 3.732, 3.687, 3.332, 3.316, 3.836, 3.748, 3.632, 3.064, 3.43, 4.116, 4.842, 4.773, 3.93, 4.08, 3.514, 4.018, 2.476, 4.054, 3.291, 3.899, 5.327, 4.292, 4.861, 4.07, 4.047, 3.226, 3.892, 3.119, 2.381, 2.909, 3.267, 4.248, 4.374, 3.256, 2.171, 4.283, 3.732, 4.404, 3.573, 4.42, 3.915, 3.836, 3.628, 2.812, 4.404, 4.384, 3.715, 4.133, 4.385, 5.634, 3.204, 4.676, 4.429, 2.658, 3.594, 4.289, 2.837, 4.314, 4.297, 3.561, 4.387, 4.02, 2.987, 4.515, 2.635, 3.594, 3.984, 4.168, 3.791, 3.495, 4.506, 3.135, 3.564, 1.523, 3.336, 1.105, 3.478, 1.42, 4.657, 4.038, 4.102, 4.063, 1.784, 2.894, 1.56, 2.066, 3.519, 3.647, 2.866, 4.169, 3.871, 4.009, 3.481, 5.026, 3.49, 2.706, 3.719, 3.275, 4.029, 3.295, 3.469, 3.392, 1.683, 3.302, 4.308, 4.215, 4.415, 3.67, 2.828, 3.384, 3.798, 2.339, 4.887, 3.463, 3.945, 4.522, 3.982, 4.1, 4.235, 3.726, 2.96, 3.887, 0.861, 3.895, 2.184, 2.799, 3.166, 3.884, 2.408, 3.832, 3.715, 3.708, 1.808, 3.411, 2.204, 3.007, 2.391, 3.801, 1.781, 2.963, 3.177, 3.163, 2.654, 2.431, 4.169, 2.66, 4.033, 3.441, 1.786, 2.771, 2.262, 2.951, 2.006, 2.107, 3.568, 3.473, 3.621, 3.286, 2.687, 3.398, 3.017, 1.943, 2.144, 4.329, 4.192, 3.076, 2.31, 3.673, 3.418, 2.2, 2.322, 4.94, 4.275, 3.335, 1.464, 3.583, 3.493, 3.81, 3.792, 2.008]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Lambda: 10 , Time: 0:00:13\n",
      "Train Error(all epochs): 5.308837413787842 \n",
      " [8.12, 7.985, 7.954, 7.895, 7.855, 7.741, 7.663, 7.499, 7.363, 7.284, 7.15, 7.07, 6.942, 6.891, 6.918, 6.865, 6.839, 6.789, 6.667, 6.706, 6.687, 6.661, 6.662, 6.532, 6.497, 6.544, 6.403, 6.433, 6.274, 6.31, 6.331, 6.223, 6.213, 6.117, 6.116, 6.097, 6.136, 6.144, 6.093, 6.13, 6.104, 6.059, 6.01, 6.031, 6.041, 5.974, 5.923, 5.95, 6.003, 5.972, 5.953, 6.04, 5.987, 6.033, 5.958, 6.078, 5.981, 5.85, 5.996, 6.023, 6.018, 5.972, 5.91, 5.861, 5.877, 5.997, 5.944, 5.96, 5.935, 5.864, 5.874, 5.798, 5.883, 5.883, 5.896, 5.874, 5.859, 5.836, 5.916, 5.848, 5.793, 5.875, 5.865, 5.833, 5.771, 5.801, 5.793, 5.895, 5.811, 5.799, 5.755, 5.793, 5.743, 5.833, 5.724, 5.672, 5.628, 5.723, 5.785, 5.776, 5.759, 5.665, 5.666, 5.753, 5.729, 5.758, 5.732, 5.67, 5.682, 5.7, 5.709, 5.714, 5.74, 5.649, 5.783, 5.758, 5.688, 5.745, 5.741, 5.653, 5.571, 5.739, 5.713, 5.66, 5.671, 5.667, 5.659, 5.686, 5.648, 5.667, 5.779, 5.637, 5.659, 5.68, 5.679, 5.635, 5.685, 5.712, 5.644, 5.643, 5.692, 5.695, 5.683, 5.615, 5.646, 5.581, 5.653, 5.711, 5.616, 5.569, 5.621, 5.634, 5.599, 5.606, 5.62, 5.61, 5.593, 5.627, 5.593, 5.622, 5.673, 5.585, 5.59, 5.582, 5.61, 5.591, 5.65, 5.629, 5.541, 5.562, 5.589, 5.55, 5.575, 5.591, 5.502, 5.595, 5.519, 5.651, 5.475, 5.49, 5.578, 5.656, 5.507, 5.472, 5.598, 5.608, 5.549, 5.64, 5.58, 5.544, 5.546, 5.531, 5.546, 5.497, 5.489, 5.482, 5.479, 5.601, 5.555, 5.531, 5.552, 5.524, 5.614, 5.604, 5.59, 5.506, 5.561, 5.487, 5.461, 5.521, 5.499, 5.46, 5.599, 5.459, 5.513, 5.472, 5.51, 5.446, 5.523, 5.513, 5.505, 5.555, 5.522, 5.46, 5.5, 5.525, 5.594, 5.461, 5.464, 5.509, 5.464, 5.484, 5.522, 5.497, 5.59, 5.447, 5.455, 5.462, 5.464, 5.536, 5.59, 5.505, 5.563, 5.49, 5.454, 5.434, 5.424, 5.525, 5.457, 5.417, 5.544, 5.569, 5.529, 5.481, 5.489, 5.433, 5.418, 5.522, 5.416, 5.41, 5.451, 5.532, 5.483, 5.518, 5.5, 5.366, 5.513, 5.413, 5.422, 5.444, 5.45, 5.435, 5.494, 5.413, 5.428, 5.459, 5.433, 5.445, 5.388, 5.419, 5.553, 5.478, 5.432, 5.404, 5.438, 5.405, 5.412, 5.509, 5.491, 5.424, 5.43, 5.458, 5.349, 5.353, 5.456, 5.402, 5.445, 5.372, 5.348, 5.309]\n",
      "Val Error(all epochs): 5.665439128875732 \n",
      " [8.411, 8.328, 8.302, 8.295, 8.285, 8.272, 8.256, 8.235, 8.211, 8.187, 8.163, 8.139, 8.103, 8.078, 8.066, 8.04, 8.028, 7.974, 7.965, 7.918, 7.86, 7.891, 7.856, 7.859, 7.807, 7.876, 7.757, 7.621, 7.672, 7.474, 7.328, 8.076, 7.848, 7.528, 7.555, 7.302, 7.088, 6.823, 6.934, 6.903, 6.865, 6.798, 6.674, 7.032, 7.135, 6.721, 7.091, 7.71, 6.821, 6.904, 6.608, 6.672, 6.552, 6.665, 7.33, 6.738, 6.649, 6.393, 6.554, 6.238, 6.444, 6.301, 6.947, 6.42, 6.555, 6.662, 6.863, 6.38, 6.288, 6.564, 6.164, 6.474, 6.47, 6.276, 6.281, 6.778, 6.285, 8.51, 6.243, 6.459, 6.39, 6.616, 6.142, 6.722, 6.817, 6.559, 6.408, 6.359, 7.019, 6.763, 6.446, 6.379, 6.438, 6.27, 6.497, 6.284, 6.253, 6.59, 6.328, 6.215, 6.138, 6.261, 6.314, 6.392, 6.994, 6.266, 6.072, 6.181, 7.053, 6.607, 5.987, 6.512, 6.357, 6.131, 6.226, 6.911, 6.161, 6.156, 6.031, 6.172, 6.199, 6.42, 7.119, 6.367, 5.918, 6.114, 6.11, 7.497, 6.66, 9.189, 6.117, 5.961, 6.414, 6.729, 5.949, 6.116, 5.895, 6.264, 6.339, 6.459, 5.958, 6.149, 5.962, 5.994, 6.135, 6.394, 6.072, 5.938, 6.348, 6.14, 6.114, 5.929, 5.925, 6.037, 6.04, 6.211, 6.395, 7.824, 7.133, 8.076, 6.324, 5.908, 6.335, 5.905, 5.981, 8.031, 6.224, 5.978, 6.055, 6.351, 6.134, 6.059, 5.939, 6.061, 5.84, 6.003, 6.058, 5.88, 5.961, 6.5, 7.722, 6.152, 5.928, 6.163, 6.372, 6.953, 6.72, 7.491, 6.599, 6.487, 5.879, 5.972, 6.066, 5.904, 5.92, 6.117, 6.273, 6.081, 5.898, 6.305, 5.872, 5.95, 5.826, 5.853, 6.001, 5.854, 6.136, 5.866, 6.325, 6.46, 6.964, 5.962, 6.384, 5.933, 5.909, 6.294, 8.147, 6.097, 6.114, 5.95, 8.153, 7.661, 6.7, 5.973, 5.956, 6.085, 6.484, 6.262, 6.131, 6.052, 5.903, 6.058, 6.331, 6.417, 5.9, 7.001, 5.963, 6.017, 9.433, 6.224, 5.919, 6.004, 8.516, 6.011, 5.963, 5.923, 5.96, 6.008, 5.768, 5.953, 5.906, 6.331, 6.274, 5.867, 5.897, 5.969, 5.975, 5.863, 5.958, 5.887, 6.124, 5.93, 5.892, 7.177, 6.37, 6.136, 5.862, 5.95, 7.047, 6.263, 6.015, 6.368, 6.121, 6.059, 5.829, 7.589, 6.049, 5.665, 5.909, 6.657, 6.854, 5.721, 5.909, 6.113, 5.939, 5.963, 6.078, 6.141, 6.033, 6.0, 6.117, 5.95, 5.899, 6.123, 6.067, 5.981, 5.767, 5.794, 5.841, 5.799]\n",
      "Val custom mae Error(all epochs): 0.34788942337036133 \n",
      " [12.54, 12.398, 12.346, 12.327, 12.298, 12.262, 12.22, 12.163, 12.099, 12.03, 11.957, 11.884, 11.778, 11.7, 11.669, 11.628, 11.583, 11.45, 11.436, 11.335, 11.261, 11.454, 11.471, 11.304, 11.18, 11.481, 11.282, 11.002, 10.974, 10.544, 10.31, 12.381, 11.771, 10.624, 10.991, 10.715, 9.741, 9.731, 9.922, 9.527, 9.822, 9.707, 9.404, 10.329, 10.122, 9.122, 10.402, 11.653, 8.03, 9.583, 9.025, 9.326, 8.962, 8.828, 11.004, 8.209, 8.287, 8.509, 8.703, 6.573, 7.599, 8.213, 10.157, 7.313, 7.781, 7.518, 9.402, 7.146, 8.243, 8.96, 7.434, 8.118, 8.585, 5.682, 6.537, 9.293, 8.425, 10.794, 7.165, 6.946, 7.733, 8.034, 7.098, 7.91, 9.186, 7.926, 8.535, 4.6, 4.059, 7.69, 8.165, 8.026, 6.891, 8.045, 8.766, 7.916, 6.759, 8.336, 7.639, 5.503, 6.302, 5.07, 7.613, 7.151, 7.596, 3.948, 6.063, 6.028, 9.901, 3.733, 5.355, 3.977, 8.533, 6.655, 5.414, 2.809, 7.251, 6.965, 6.57, 6.022, 5.237, 7.09, 7.919, 6.978, 6.614, 6.485, 6.529, 4.678, 7.426, 6.587, 6.605, 6.101, 8.386, 4.362, 6.856, 5.82, 5.013, 7.764, 6.766, 3.577, 5.926, 6.047, 6.652, 6.518, 4.352, 7.467, 4.366, 5.628, 6.313, 6.918, 5.867, 6.373, 5.665, 6.154, 6.463, 6.104, 7.428, 7.25, 7.239, 0.789, 3.962, 5.279, 7.363, 6.092, 5.067, 0.876, 3.754, 6.949, 6.31, 6.783, 6.013, 6.19, 6.205, 4.602, 5.517, 6.77, 6.78, 5.684, 6.24, 8.389, 1.752, 6.545, 4.423, 5.329, 7.878, 4.644, 6.898, 4.771, 1.943, 7.608, 4.48, 3.989, 4.488, 5.555, 6.994, 5.826, 7.978, 6.702, 6.228, 2.478, 4.643, 4.824, 5.32, 6.297, 4.541, 4.686, 6.501, 5.235, 5.666, 8.405, 3.977, 6.546, 4.931, 5.717, 5.996, 3.505, 0.555, 3.577, 4.654, 3.807, 1.369, 8.931, 8.691, 5.816, 4.858, 4.356, 6.369, 2.95, 3.842, 4.266, 5.906, 5.653, 7.159, 3.839, 4.975, 1.355, 6.253, 7.059, 1.047, 6.998, 6.445, 4.891, 11.289, 4.263, 6.263, 4.908, 5.418, 4.704, 4.162, 4.698, 3.804, 7.942, 2.88, 4.68, 5.437, 4.22, 5.877, 4.868, 5.864, 5.937, 3.406, 5.616, 4.422, 6.085, 2.831, 3.107, 4.338, 4.271, 1.987, 7.14, 6.247, 3.605, 4.191, 2.972, 4.896, 5.376, 5.161, 5.034, 6.736, 5.424, 0.348, 4.579, 5.596, 5.885, 4.265, 5.281, 6.038, 3.263, 5.017, 5.718, 6.259, 5.198, 5.494, 5.909, 6.316, 5.444, 4.98, 4.62, 4.933, 4.116]\n",
      "\n",
      "Trainig set size: 512 , Time: 0:01:07 , best_lambda: 10 , min_  error: 0.348\n",
      "Test starts:  615 , ends:  32161\n",
      "986/986 [==============================] - 1s 481us/step\n",
      "total_power:  16.020887 , average_difference:  3.571933018213715\n",
      "\n",
      "\n",
      "\n",
      "number_samples: 1024 , New samples: 1024\n",
      "Validation size: 205 , starts: 1024 , ends: 1228\n",
      "\n",
      "Lambda: 0 , Time: 0:00:20\n",
      "Train Error(all epochs): 3.0105392932891846 \n",
      " [8.167, 7.909, 7.49, 6.975, 6.554, 6.18, 5.821, 5.503, 5.265, 5.103, 4.932, 4.852, 4.849, 4.818, 4.689, 4.641, 4.644, 4.561, 4.551, 4.475, 4.47, 4.389, 4.395, 4.363, 4.3, 4.282, 4.313, 4.242, 4.202, 4.241, 4.144, 4.252, 4.211, 4.248, 4.064, 4.118, 4.087, 4.041, 4.042, 3.996, 4.015, 4.007, 4.008, 4.081, 3.955, 4.006, 3.939, 3.959, 3.966, 3.824, 3.801, 3.882, 3.817, 3.836, 3.874, 3.777, 3.779, 3.829, 3.798, 3.835, 3.868, 3.754, 3.867, 3.838, 3.663, 3.708, 3.766, 3.708, 3.736, 3.662, 3.685, 3.663, 3.66, 3.657, 3.673, 3.616, 3.614, 3.609, 3.638, 3.652, 3.549, 3.794, 3.662, 3.559, 3.624, 3.568, 3.553, 3.557, 3.625, 3.55, 3.468, 3.605, 3.489, 3.52, 3.507, 3.567, 3.583, 3.481, 3.452, 3.526, 3.527, 3.531, 3.513, 3.489, 3.491, 3.473, 3.557, 3.46, 3.495, 3.396, 3.384, 3.442, 3.48, 3.471, 3.366, 3.464, 3.416, 3.382, 3.435, 3.494, 3.47, 3.446, 3.296, 3.331, 3.283, 3.43, 3.34, 3.402, 3.412, 3.329, 3.384, 3.412, 3.395, 3.411, 3.27, 3.305, 3.402, 3.307, 3.377, 3.456, 3.288, 3.32, 3.305, 3.296, 3.377, 3.22, 3.269, 3.359, 3.256, 3.298, 3.247, 3.353, 3.303, 3.214, 3.341, 3.285, 3.22, 3.313, 3.251, 3.357, 3.243, 3.309, 3.311, 3.226, 3.287, 3.198, 3.219, 3.265, 3.238, 3.234, 3.262, 3.266, 3.282, 3.235, 3.229, 3.289, 3.22, 3.077, 3.208, 3.234, 3.312, 3.229, 3.161, 3.222, 3.22, 3.199, 3.226, 3.19, 3.209, 3.276, 3.185, 3.303, 3.236, 3.181, 3.221, 3.23, 3.207, 3.275, 3.191, 3.223, 3.142, 3.221, 3.233, 3.195, 3.193, 3.14, 3.177, 3.136, 3.231, 3.131, 3.123, 3.106, 3.156, 3.206, 3.15, 3.212, 3.167, 3.184, 3.233, 3.168, 3.187, 3.132, 3.173, 3.155, 3.146, 3.159, 3.117, 3.144, 3.158, 3.123, 3.113, 3.153, 3.106, 3.168, 3.143, 3.133, 3.137, 3.176, 3.17, 3.159, 3.082, 3.15, 3.215, 3.16, 3.14, 3.2, 3.187, 3.065, 3.069, 3.11, 3.217, 3.144, 3.056, 3.135, 3.177, 3.149, 3.08, 3.192, 3.075, 3.075, 3.157, 3.118, 3.117, 3.174, 3.143, 3.036, 3.089, 3.029, 3.091, 3.037, 3.139, 3.051, 3.037, 3.085, 3.141, 3.145, 3.093, 3.126, 3.147, 3.185, 3.049, 3.118, 3.094, 3.154, 3.076, 3.049, 3.109, 3.067, 3.104, 3.139, 3.067, 3.023, 3.08, 3.127, 3.058, 3.136, 3.066, 3.011, 3.032, 3.065]\n",
      "Val Error(all epochs): 4.958527565002441 \n",
      " [8.361, 8.002, 7.458, 6.894, 6.456, 5.898, 5.614, 5.349, 5.279, 5.147, 5.148, 5.015, 4.959, 5.106, 5.105, 5.083, 5.121, 5.173, 5.134, 5.184, 5.21, 5.253, 5.347, 5.299, 5.393, 5.428, 5.474, 5.519, 5.527, 5.533, 5.433, 5.516, 5.493, 5.655, 5.571, 5.593, 5.636, 5.53, 5.667, 5.676, 5.614, 5.694, 5.645, 5.765, 5.764, 5.661, 5.626, 5.617, 5.593, 5.791, 5.73, 5.697, 5.627, 5.738, 5.681, 5.749, 5.737, 5.779, 5.776, 5.791, 5.805, 5.755, 5.777, 5.715, 5.774, 5.673, 5.75, 5.829, 5.786, 5.736, 5.916, 5.881, 5.849, 5.936, 5.777, 5.861, 5.8, 5.8, 5.812, 5.86, 5.85, 5.809, 5.879, 5.8, 5.811, 5.904, 5.872, 5.785, 5.893, 5.796, 5.869, 5.77, 5.797, 5.715, 5.722, 5.606, 5.69, 5.628, 5.71, 5.633, 5.756, 5.771, 5.778, 5.803, 5.79, 5.673, 5.666, 5.669, 5.683, 5.681, 5.694, 5.627, 5.682, 5.638, 5.631, 5.648, 5.764, 5.679, 5.694, 5.653, 5.68, 5.742, 5.693, 5.708, 5.733, 5.765, 5.686, 5.817, 5.769, 5.819, 5.824, 5.822, 5.728, 5.72, 5.679, 5.702, 5.741, 5.604, 5.719, 5.62, 5.715, 5.643, 5.705, 5.65, 5.72, 5.687, 5.629, 5.65, 5.64, 5.685, 5.658, 5.613, 5.645, 5.684, 5.693, 5.649, 5.665, 5.701, 5.594, 5.599, 5.719, 5.599, 5.66, 5.552, 5.589, 5.545, 5.526, 5.6, 5.631, 5.634, 5.627, 5.577, 5.526, 5.561, 5.552, 5.547, 5.53, 5.435, 5.503, 5.501, 5.528, 5.53, 5.568, 5.527, 5.48, 5.441, 5.42, 5.511, 5.458, 5.463, 5.577, 5.616, 5.595, 5.537, 5.584, 5.545, 5.585, 5.581, 5.653, 5.528, 5.595, 5.574, 5.51, 5.538, 5.511, 5.478, 5.458, 5.462, 5.541, 5.495, 5.486, 5.526, 5.587, 5.571, 5.558, 5.576, 5.515, 5.463, 5.544, 5.517, 5.601, 5.472, 5.481, 5.609, 5.449, 5.526, 5.562, 5.64, 5.651, 5.532, 5.499, 5.529, 5.543, 5.481, 5.591, 5.597, 5.66, 5.612, 5.6, 5.609, 5.594, 5.565, 5.524, 5.564, 5.562, 5.525, 5.608, 5.538, 5.596, 5.56, 5.563, 5.507, 5.552, 5.478, 5.575, 5.583, 5.576, 5.55, 5.459, 5.492, 5.481, 5.522, 5.458, 5.546, 5.485, 5.585, 5.552, 5.508, 5.584, 5.536, 5.525, 5.585, 5.541, 5.585, 5.518, 5.625, 5.57, 5.56, 5.604, 5.688, 5.612, 5.596, 5.516, 5.59, 5.509, 5.51, 5.545, 5.475, 5.571, 5.526, 5.517, 5.531, 5.544, 5.569, 5.503, 5.446, 5.482, 5.593, 5.657, 5.538]\n",
      "Val custom mae Error(all epochs): 2.069472074508667 \n",
      " [11.537, 10.942, 10.013, 8.88, 8.349, 6.709, 6.08, 5.207, 3.688, 2.591, 2.414, 2.819, 2.498, 2.422, 2.7, 2.807, 2.282, 2.571, 2.069, 3.891, 5.854, 8.112, 11.919, 11.194, 12.137, 15.085, 16.868, 14.992, 14.622, 16.101, 15.625, 13.613, 15.175, 14.979, 14.764, 15.592, 11.432, 6.983, 10.83, 11.855, 6.489, 10.212, 13.239, 13.362, 13.908, 12.991, 9.186, 8.864, 10.254, 9.677, 11.89, 10.195, 15.621, 14.59, 16.605, 15.744, 15.161, 16.676, 12.173, 13.135, 16.04, 17.463, 12.635, 14.06, 13.194, 13.0, 11.141, 14.066, 12.661, 14.848, 15.844, 16.804, 18.274, 21.03, 20.972, 20.105, 19.96, 24.587, 23.96, 24.606, 21.087, 20.953, 20.927, 21.449, 19.316, 22.716, 17.652, 16.535, 16.095, 16.902, 17.37, 17.417, 17.986, 10.484, 13.669, 12.274, 10.086, 10.544, 14.277, 12.132, 14.589, 13.46, 16.237, 17.562, 17.867, 17.572, 17.645, 19.138, 17.898, 20.026, 18.901, 13.575, 14.911, 12.715, 13.313, 15.615, 16.822, 16.473, 15.82, 15.451, 17.756, 18.595, 17.455, 19.052, 16.737, 21.447, 16.719, 13.287, 14.79, 13.805, 12.711, 13.087, 9.852, 10.318, 8.088, 8.942, 11.228, 9.743, 11.036, 11.56, 12.943, 14.744, 12.258, 13.482, 13.242, 13.623, 10.841, 11.241, 8.992, 10.195, 11.122, 10.395, 11.023, 10.107, 10.323, 10.471, 9.404, 10.952, 9.785, 8.8, 9.261, 10.227, 10.336, 8.891, 8.424, 7.881, 7.611, 7.127, 6.579, 8.173, 7.476, 5.439, 5.195, 5.284, 4.24, 3.538, 3.959, 2.195, 3.208, 4.245, 5.027, 4.537, 4.762, 5.731, 5.99, 3.72, 4.822, 6.047, 6.095, 5.956, 7.574, 7.519, 8.445, 8.791, 9.202, 9.678, 9.059, 8.047, 7.89, 7.592, 6.532, 6.572, 7.153, 6.363, 5.435, 5.118, 5.111, 5.519, 5.409, 5.616, 4.955, 5.151, 6.084, 7.306, 6.689, 6.385, 7.306, 5.6, 6.385, 7.091, 8.277, 6.997, 8.061, 9.745, 9.385, 8.596, 10.718, 11.597, 12.506, 9.392, 7.502, 6.373, 8.014, 7.43, 7.491, 6.554, 6.593, 6.644, 8.591, 8.478, 7.75, 6.512, 7.887, 7.738, 8.32, 9.153, 8.579, 7.73, 6.923, 5.728, 7.13, 7.309, 9.898, 8.751, 10.474, 9.97, 9.113, 8.218, 9.022, 9.055, 7.072, 5.958, 6.075, 6.811, 5.746, 5.079, 6.449, 7.066, 6.258, 7.042, 7.3, 7.794, 6.896, 8.33, 9.47, 9.723, 9.006, 10.212, 9.233, 10.576, 9.141, 9.339, 9.487, 8.935, 7.747, 7.318, 7.956, 7.627, 8.484, 8.696, 8.37, 9.337, 8.547, 9.096, 9.036, 8.898, 7.627, 9.294, 8.636, 7.722]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Lambda: 0.01 , Time: 0:00:20\n",
      "Train Error(all epochs): 3.096693277359009 \n",
      " [8.169, 7.833, 7.344, 6.874, 6.432, 6.161, 5.733, 5.585, 5.367, 5.038, 5.008, 4.92, 4.761, 4.766, 4.71, 4.719, 4.655, 4.543, 4.573, 4.54, 4.557, 4.47, 4.391, 4.425, 4.382, 4.315, 4.207, 4.321, 4.26, 4.308, 4.258, 4.138, 4.179, 4.159, 4.072, 4.107, 4.106, 4.066, 4.115, 4.124, 4.074, 4.004, 3.978, 4.021, 4.017, 3.99, 3.938, 3.928, 3.952, 3.961, 3.934, 3.912, 3.952, 3.851, 3.91, 3.88, 3.79, 3.768, 3.778, 3.729, 3.703, 3.803, 3.776, 3.782, 3.782, 3.675, 3.702, 3.74, 3.669, 3.67, 3.623, 3.719, 3.71, 3.729, 3.647, 3.681, 3.643, 3.677, 3.575, 3.684, 3.647, 3.551, 3.623, 3.62, 3.623, 3.657, 3.554, 3.61, 3.517, 3.594, 3.577, 3.645, 3.516, 3.563, 3.529, 3.512, 3.525, 3.601, 3.545, 3.527, 3.514, 3.539, 3.518, 3.509, 3.514, 3.433, 3.543, 3.573, 3.446, 3.377, 3.433, 3.428, 3.365, 3.432, 3.447, 3.454, 3.441, 3.455, 3.38, 3.367, 3.39, 3.453, 3.35, 3.439, 3.41, 3.353, 3.396, 3.375, 3.327, 3.408, 3.446, 3.37, 3.348, 3.295, 3.345, 3.331, 3.272, 3.316, 3.435, 3.366, 3.416, 3.347, 3.368, 3.371, 3.324, 3.364, 3.299, 3.382, 3.371, 3.292, 3.376, 3.292, 3.239, 3.328, 3.394, 3.319, 3.328, 3.315, 3.276, 3.293, 3.283, 3.146, 3.27, 3.257, 3.328, 3.348, 3.38, 3.403, 3.309, 3.305, 3.347, 3.335, 3.225, 3.266, 3.263, 3.232, 3.356, 3.365, 3.288, 3.194, 3.33, 3.329, 3.295, 3.35, 3.222, 3.362, 3.284, 3.355, 3.28, 3.35, 3.266, 3.212, 3.227, 3.226, 3.328, 3.247, 3.28, 3.349, 3.305, 3.265, 3.211, 3.216, 3.207, 3.157, 3.265, 3.307, 3.281, 3.204, 3.147, 3.187, 3.257, 3.278, 3.225, 3.235, 3.193, 3.333, 3.246, 3.241, 3.213, 3.283, 3.239, 3.284, 3.154, 3.224, 3.275, 3.263, 3.169, 3.276, 3.219, 3.262, 3.24, 3.234, 3.21, 3.182, 3.204, 3.229, 3.243, 3.228, 3.248, 3.215, 3.273, 3.262, 3.267, 3.18, 3.243, 3.187, 3.266, 3.19, 3.221, 3.212, 3.195, 3.245, 3.168, 3.194, 3.252, 3.327, 3.203, 3.243, 3.237, 3.193, 3.202, 3.212, 3.24, 3.187, 3.247, 3.197, 3.163, 3.246, 3.165, 3.153, 3.221, 3.187, 3.233, 3.185, 3.22, 3.262, 3.292, 3.137, 3.217, 3.22, 3.171, 3.154, 3.097, 3.135, 3.176, 3.129, 3.195, 3.174, 3.14, 3.134, 3.2, 3.263, 3.291, 3.167, 3.181, 3.216, 3.178, 3.18, 3.12, 3.189]\n",
      "Val Error(all epochs): 4.899156093597412 \n",
      " [8.43, 8.041, 7.418, 6.962, 6.472, 6.036, 5.775, 5.496, 5.474, 5.266, 5.111, 4.99, 5.11, 5.048, 4.962, 5.055, 5.086, 4.899, 4.974, 5.075, 4.977, 4.99, 5.252, 5.034, 5.027, 4.993, 4.953, 5.054, 5.083, 5.093, 5.07, 5.158, 5.16, 5.034, 5.023, 5.15, 5.157, 5.162, 5.191, 5.054, 5.076, 5.216, 5.193, 5.241, 5.115, 5.207, 5.269, 5.204, 5.259, 5.256, 5.202, 5.316, 5.334, 5.284, 5.296, 5.305, 5.274, 5.26, 5.274, 5.289, 5.228, 5.259, 5.263, 5.257, 5.201, 5.226, 5.197, 5.329, 5.258, 5.125, 5.285, 5.294, 5.297, 5.265, 5.268, 5.365, 5.248, 5.34, 5.429, 5.343, 5.421, 5.451, 5.327, 5.379, 5.31, 5.336, 5.286, 5.36, 5.4, 5.397, 5.358, 5.38, 5.355, 5.349, 5.276, 5.351, 5.371, 5.282, 5.331, 5.352, 5.447, 5.537, 5.364, 5.29, 5.225, 5.337, 5.314, 5.298, 5.33, 5.34, 5.303, 5.312, 5.263, 5.252, 5.456, 5.316, 5.269, 5.278, 5.181, 5.223, 5.339, 5.307, 5.254, 5.255, 5.311, 5.28, 5.327, 5.254, 5.286, 5.355, 5.28, 5.453, 5.248, 5.25, 5.264, 5.22, 5.268, 5.263, 5.208, 5.253, 5.181, 5.255, 5.224, 5.299, 5.298, 5.247, 5.316, 5.305, 5.28, 5.297, 5.319, 5.263, 5.226, 5.322, 5.237, 5.204, 5.29, 5.329, 5.22, 5.349, 5.288, 5.318, 5.348, 5.32, 5.269, 5.309, 5.384, 5.252, 5.368, 5.339, 5.255, 5.261, 5.282, 5.26, 5.246, 5.388, 5.195, 5.201, 5.304, 5.237, 5.27, 5.253, 5.254, 5.374, 5.319, 5.371, 5.351, 5.385, 5.364, 5.366, 5.31, 5.276, 5.34, 5.244, 5.282, 5.292, 5.312, 5.237, 5.275, 5.236, 5.177, 5.232, 5.178, 5.189, 5.196, 5.189, 5.144, 5.153, 5.22, 5.346, 5.254, 5.285, 5.315, 5.297, 5.242, 5.252, 5.251, 5.25, 5.266, 5.258, 5.215, 5.294, 5.217, 5.234, 5.281, 5.275, 5.201, 5.288, 5.307, 5.374, 5.267, 5.282, 5.322, 5.287, 5.255, 5.298, 5.288, 5.178, 5.228, 5.215, 5.094, 5.216, 5.282, 5.327, 5.29, 5.23, 5.231, 5.194, 5.318, 5.183, 5.105, 5.109, 5.147, 5.15, 5.182, 5.182, 5.2, 5.152, 5.205, 5.186, 5.139, 5.229, 5.232, 5.222, 5.201, 5.197, 5.165, 5.147, 5.134, 5.149, 5.241, 5.203, 5.151, 5.134, 5.185, 5.206, 5.239, 5.132, 5.179, 5.104, 5.216, 5.175, 5.287, 5.217, 5.23, 5.249, 5.08, 5.174, 5.217, 5.081, 5.099, 5.066, 5.269, 5.214, 5.128, 5.082, 5.1, 5.269, 5.178, 5.191]\n",
      "Val custom mae Error(all epochs): 1.9453920125961304 \n",
      " [11.82, 11.096, 9.963, 9.493, 8.367, 7.32, 6.901, 6.036, 6.01, 4.622, 3.629, 3.409, 3.443, 3.287, 2.693, 3.384, 3.12, 2.896, 3.366, 2.57, 2.98, 3.157, 1.945, 2.677, 3.227, 3.347, 3.013, 3.222, 3.212, 3.654, 3.564, 3.37, 3.557, 3.532, 3.077, 3.314, 3.378, 3.392, 3.431, 3.434, 3.105, 3.632, 3.589, 3.474, 3.449, 3.212, 3.321, 3.156, 3.379, 3.388, 3.552, 3.001, 3.344, 2.579, 3.454, 3.18, 3.141, 3.316, 3.447, 3.484, 3.226, 3.736, 3.295, 3.241, 3.331, 3.194, 3.863, 3.744, 3.715, 3.766, 3.294, 2.803, 3.315, 3.294, 3.292, 3.247, 3.097, 3.482, 3.069, 3.465, 2.887, 4.021, 3.645, 3.734, 3.641, 3.427, 3.395, 3.298, 3.68, 3.277, 3.587, 3.371, 3.413, 3.255, 3.234, 3.204, 3.114, 3.161, 3.222, 3.722, 3.16, 3.151, 3.296, 3.439, 3.584, 3.5, 3.143, 3.094, 2.856, 3.341, 3.384, 3.561, 3.203, 3.416, 3.068, 2.88, 3.122, 3.107, 3.141, 2.769, 3.032, 2.959, 3.276, 3.154, 3.526, 3.236, 3.984, 3.346, 2.997, 2.987, 3.456, 3.643, 3.33, 3.386, 2.536, 2.631, 3.152, 3.197, 2.85, 3.129, 3.2, 3.286, 3.198, 3.433, 3.315, 2.903, 2.684, 3.043, 2.969, 3.248, 3.414, 3.405, 3.282, 3.445, 3.408, 3.242, 3.376, 3.491, 3.21, 2.78, 3.373, 3.453, 3.522, 3.428, 3.133, 3.93, 4.479, 4.698, 3.881, 3.944, 3.904, 3.712, 3.443, 3.766, 3.129, 3.489, 2.984, 3.788, 3.193, 4.313, 3.515, 3.69, 3.764, 3.798, 3.331, 3.036, 3.262, 3.289, 3.556, 3.775, 3.225, 2.718, 3.354, 3.233, 3.289, 3.675, 3.396, 3.39, 3.347, 3.121, 3.241, 3.23, 3.071, 3.28, 3.035, 2.9, 2.966, 3.072, 3.114, 3.567, 3.138, 3.398, 3.375, 3.03, 3.318, 3.129, 3.372, 3.662, 3.146, 3.547, 2.879, 3.013, 3.08, 2.937, 3.101, 3.375, 2.765, 2.798, 3.179, 3.379, 3.69, 3.586, 3.59, 3.568, 3.196, 3.481, 3.688, 3.486, 3.452, 3.206, 3.126, 3.39, 2.973, 3.165, 3.103, 3.008, 3.427, 2.979, 2.952, 3.285, 3.205, 3.104, 3.362, 3.702, 3.443, 3.552, 3.539, 3.501, 3.389, 2.862, 3.141, 3.541, 3.735, 3.388, 3.431, 3.239, 3.252, 3.207, 2.773, 3.178, 3.226, 2.686, 3.369, 2.827, 2.923, 3.424, 3.533, 2.915, 3.354, 3.255, 3.083, 3.45, 3.33, 3.231, 3.308, 3.275, 3.496, 3.486, 3.537, 3.327, 3.618, 3.008, 3.551, 3.179, 3.35, 2.819, 3.012, 3.695, 3.182, 3.217]\n",
      "\n",
      "Lambda: 0.1 , Time: 0:00:20\n",
      "Train Error(all epochs): 3.420200824737549 \n",
      " [8.191, 7.85, 7.548, 7.073, 6.613, 6.187, 5.864, 5.584, 5.303, 5.154, 5.046, 4.911, 4.854, 4.755, 4.717, 4.701, 4.627, 4.57, 4.569, 4.588, 4.493, 4.507, 4.481, 4.405, 4.453, 4.422, 4.494, 4.439, 4.473, 4.296, 4.289, 4.374, 4.299, 4.285, 4.28, 4.324, 4.264, 4.272, 4.262, 4.195, 4.194, 4.231, 4.189, 4.205, 4.154, 4.06, 4.151, 4.106, 4.176, 4.136, 4.143, 4.12, 4.085, 4.054, 4.075, 4.019, 4.032, 3.987, 4.019, 4.02, 4.002, 3.958, 3.942, 3.929, 4.093, 3.983, 3.99, 4.103, 4.032, 3.883, 4.066, 4.076, 3.892, 3.887, 3.932, 3.967, 3.969, 3.935, 3.92, 3.902, 3.856, 3.841, 3.867, 3.875, 3.853, 3.803, 3.835, 3.861, 3.832, 3.786, 3.798, 3.791, 3.901, 3.888, 3.836, 3.777, 3.771, 3.953, 3.866, 3.765, 3.735, 3.666, 3.743, 3.633, 3.704, 3.719, 3.72, 3.864, 3.803, 3.79, 3.725, 3.708, 3.662, 3.778, 3.74, 3.747, 3.786, 3.765, 3.729, 3.817, 3.719, 3.667, 3.679, 3.72, 3.627, 3.58, 3.687, 3.674, 3.684, 3.726, 3.682, 3.708, 3.725, 3.683, 3.669, 3.758, 3.688, 3.695, 3.641, 3.674, 3.791, 3.64, 3.566, 3.552, 3.604, 3.58, 3.639, 3.589, 3.759, 3.767, 3.694, 3.644, 3.689, 3.689, 3.646, 3.615, 3.632, 3.554, 3.572, 3.627, 3.636, 3.647, 3.598, 3.685, 3.581, 3.672, 3.72, 3.593, 3.69, 3.639, 3.575, 3.612, 3.649, 3.659, 3.618, 3.652, 3.607, 3.639, 3.628, 3.617, 3.592, 3.573, 3.495, 3.556, 3.583, 3.562, 3.538, 3.577, 3.678, 3.651, 3.653, 3.573, 3.607, 3.585, 3.553, 3.616, 3.693, 3.631, 3.635, 3.569, 3.575, 3.674, 3.587, 3.601, 3.619, 3.576, 3.589, 3.575, 3.581, 3.564, 3.663, 3.67, 3.595, 3.583, 3.585, 3.559, 3.537, 3.532, 3.533, 3.554, 3.587, 3.628, 3.587, 3.548, 3.58, 3.571, 3.557, 3.551, 3.554, 3.644, 3.678, 3.692, 3.649, 3.591, 3.578, 3.643, 3.625, 3.627, 3.59, 3.51, 3.554, 3.503, 3.657, 3.575, 3.526, 3.693, 3.576, 3.621, 3.597, 3.464, 3.574, 3.584, 3.564, 3.537, 3.525, 3.59, 3.645, 3.502, 3.615, 3.622, 3.593, 3.487, 3.602, 3.55, 3.595, 3.589, 3.615, 3.602, 3.563, 3.549, 3.481, 3.444, 3.42, 3.618, 3.531, 3.641, 3.561, 3.485, 3.579, 3.593, 3.597, 3.56, 3.552, 3.6, 3.61, 3.513, 3.485, 3.59, 3.568, 3.603, 3.572, 3.468, 3.542, 3.564, 3.506, 3.548, 3.59, 3.536, 3.553, 3.56]\n",
      "Val Error(all epochs): 5.13791036605835 \n",
      " [8.346, 7.982, 7.622, 7.138, 6.818, 6.578, 6.175, 5.895, 5.759, 5.572, 5.524, 5.498, 5.425, 5.397, 5.312, 5.333, 5.36, 5.404, 5.405, 5.444, 5.366, 5.354, 5.445, 5.462, 5.348, 5.53, 5.482, 5.45, 5.476, 5.366, 5.335, 5.44, 5.398, 5.346, 5.428, 5.452, 5.347, 5.391, 5.454, 5.399, 5.504, 5.486, 5.61, 5.517, 5.462, 5.366, 5.424, 5.441, 5.354, 5.419, 5.427, 5.387, 5.501, 5.311, 5.558, 5.377, 5.505, 5.499, 5.484, 5.461, 5.513, 5.569, 5.427, 5.507, 5.489, 5.597, 5.579, 5.385, 5.425, 5.391, 5.802, 5.477, 5.47, 5.472, 5.461, 5.398, 5.438, 5.414, 5.458, 5.372, 5.367, 5.538, 5.474, 5.449, 5.473, 5.372, 5.539, 5.409, 5.456, 5.445, 5.466, 5.533, 5.446, 5.419, 5.543, 5.535, 5.462, 5.432, 5.422, 5.518, 5.415, 5.417, 5.419, 5.324, 5.456, 5.502, 5.436, 5.388, 5.364, 5.442, 5.466, 5.48, 5.499, 5.381, 5.531, 5.485, 5.414, 5.465, 5.468, 5.52, 5.412, 5.597, 5.424, 5.397, 5.558, 5.513, 5.548, 5.526, 5.706, 5.613, 5.403, 5.459, 5.415, 5.476, 5.398, 5.359, 5.425, 5.389, 5.418, 5.466, 5.516, 5.472, 5.457, 5.493, 5.574, 5.476, 5.381, 5.427, 5.303, 5.411, 5.376, 5.214, 5.421, 5.509, 5.426, 5.477, 5.502, 5.534, 5.487, 5.345, 5.542, 5.41, 5.497, 5.342, 5.526, 5.534, 5.469, 5.505, 5.42, 5.408, 5.395, 5.5, 5.368, 5.411, 5.43, 5.392, 5.379, 5.404, 5.406, 5.37, 5.317, 5.343, 5.519, 5.441, 5.42, 5.306, 5.433, 5.398, 5.416, 5.374, 5.316, 5.634, 5.571, 5.415, 5.327, 5.4, 5.418, 5.322, 5.424, 5.339, 5.264, 5.374, 5.413, 5.273, 5.34, 5.268, 5.402, 5.244, 5.358, 5.33, 5.288, 5.23, 5.152, 5.307, 5.287, 5.267, 5.202, 5.279, 5.294, 5.368, 5.444, 5.239, 5.138, 5.39, 5.291, 5.291, 5.315, 5.297, 5.274, 5.367, 5.277, 5.192, 5.327, 5.336, 5.32, 5.363, 5.369, 5.336, 5.365, 5.384, 5.44, 5.377, 5.376, 5.388, 5.385, 5.351, 5.466, 5.531, 5.431, 5.533, 5.501, 5.384, 5.404, 5.38, 5.426, 5.46, 5.477, 5.361, 5.371, 5.364, 5.474, 5.45, 5.479, 5.441, 5.289, 5.614, 5.471, 5.352, 5.471, 5.341, 5.392, 5.457, 5.396, 5.466, 5.447, 5.438, 5.408, 5.502, 5.379, 5.399, 5.461, 5.361, 5.58, 5.405, 5.422, 5.465, 5.506, 5.516, 5.317, 5.506, 5.499, 5.59, 5.487, 5.452, 5.502, 5.467, 5.42, 5.584, 5.446, 5.421]\n",
      "Val custom mae Error(all epochs): 1.6719400882720947 \n",
      " [11.672, 10.958, 10.55, 9.719, 9.479, 9.2, 8.527, 7.743, 7.3, 6.722, 6.379, 5.692, 5.773, 5.628, 5.305, 5.323, 5.733, 5.216, 5.416, 5.018, 4.716, 4.955, 4.843, 4.837, 4.276, 4.961, 4.818, 4.695, 4.772, 4.578, 4.507, 4.708, 4.573, 3.886, 4.729, 4.496, 4.139, 3.788, 4.138, 4.625, 4.218, 4.365, 4.271, 3.546, 4.212, 4.152, 4.334, 4.438, 4.218, 4.458, 3.888, 3.929, 3.479, 3.523, 4.548, 4.068, 4.201, 4.47, 3.903, 3.743, 3.99, 3.196, 3.161, 2.697, 3.469, 3.663, 3.807, 3.911, 3.422, 3.584, 1.898, 4.11, 3.682, 3.591, 4.088, 3.434, 3.677, 3.971, 4.039, 3.741, 3.529, 3.863, 3.784, 3.501, 4.076, 3.864, 4.014, 3.857, 3.794, 3.499, 3.983, 4.367, 3.926, 3.204, 3.705, 3.35, 3.971, 3.081, 3.243, 3.465, 3.866, 4.1, 2.723, 3.533, 3.268, 3.126, 3.367, 3.121, 3.075, 3.91, 4.395, 4.368, 3.8, 3.981, 3.599, 3.268, 3.471, 2.616, 3.303, 4.09, 3.574, 3.501, 3.649, 3.553, 3.842, 3.919, 3.299, 3.752, 4.733, 3.786, 3.81, 4.12, 3.64, 3.359, 3.454, 4.276, 3.774, 3.339, 3.683, 3.964, 3.923, 3.802, 3.739, 4.122, 4.363, 3.923, 3.696, 3.753, 2.735, 2.623, 3.017, 3.245, 2.9, 4.079, 3.5, 3.635, 3.735, 4.162, 3.901, 3.679, 3.811, 3.971, 3.65, 2.734, 3.534, 3.343, 2.943, 3.588, 3.312, 2.318, 3.131, 3.986, 3.621, 2.742, 3.481, 4.172, 4.106, 3.524, 3.235, 3.298, 3.024, 3.93, 3.96, 2.907, 2.49, 2.955, 3.797, 3.842, 4.0, 3.845, 3.48, 4.939, 4.202, 3.573, 3.493, 2.837, 2.939, 3.193, 3.149, 3.845, 3.633, 3.425, 3.397, 3.259, 3.873, 3.378, 3.869, 3.626, 3.899, 3.79, 2.742, 3.227, 2.856, 3.294, 3.391, 2.743, 2.985, 3.229, 3.226, 3.354, 3.909, 3.491, 3.552, 2.836, 3.099, 3.525, 4.027, 3.352, 2.951, 2.81, 3.721, 2.573, 2.39, 3.166, 2.932, 3.855, 3.706, 3.573, 2.83, 2.933, 3.239, 3.296, 3.169, 2.647, 2.673, 2.126, 2.553, 3.421, 2.474, 2.225, 3.742, 2.493, 2.079, 3.701, 4.021, 2.997, 3.896, 3.367, 2.916, 3.044, 3.318, 3.729, 4.217, 2.127, 3.354, 3.219, 3.648, 3.313, 3.701, 3.184, 2.871, 3.924, 3.805, 3.456, 2.965, 2.753, 2.829, 1.929, 3.782, 3.01, 2.946, 3.328, 3.62, 2.867, 2.996, 2.633, 3.307, 1.672, 3.358, 3.101, 3.452, 4.177, 3.676, 3.353, 3.265, 3.339, 2.846, 3.416, 3.23, 3.839]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Lambda: 1 , Time: 0:00:22\n",
      "Train Error(all epochs): 3.9574944972991943 \n",
      " [8.261, 7.943, 7.589, 7.095, 6.627, 6.29, 6.0, 5.82, 5.542, 5.544, 5.421, 5.303, 5.275, 5.262, 5.176, 5.138, 5.141, 5.054, 5.019, 5.03, 4.954, 5.002, 4.971, 4.994, 4.898, 4.854, 4.871, 4.919, 4.999, 4.97, 4.881, 5.001, 4.954, 4.821, 4.875, 4.815, 4.912, 4.813, 4.944, 4.882, 4.8, 4.892, 4.843, 4.712, 4.848, 4.85, 4.817, 4.883, 4.798, 4.747, 4.814, 4.774, 4.663, 4.814, 4.848, 4.717, 4.755, 4.782, 4.774, 4.821, 4.782, 4.743, 4.74, 4.758, 4.775, 4.683, 4.765, 4.623, 4.701, 4.781, 4.69, 4.731, 4.689, 4.691, 4.748, 4.644, 4.6, 4.708, 4.673, 4.628, 4.6, 4.655, 4.69, 4.626, 4.705, 4.699, 4.685, 4.571, 4.656, 4.672, 4.678, 4.603, 4.62, 4.644, 4.593, 4.645, 4.684, 4.593, 4.565, 4.575, 4.64, 4.481, 4.609, 4.613, 4.599, 4.498, 4.55, 4.537, 4.457, 4.565, 4.542, 4.557, 4.455, 4.514, 4.548, 4.574, 4.513, 4.51, 4.519, 4.419, 4.389, 4.461, 4.475, 4.52, 4.555, 4.417, 4.546, 4.53, 4.417, 4.435, 4.399, 4.368, 4.481, 4.492, 4.457, 4.41, 4.414, 4.385, 4.415, 4.38, 4.32, 4.341, 4.379, 4.33, 4.504, 4.38, 4.389, 4.391, 4.389, 4.312, 4.363, 4.348, 4.409, 4.377, 4.402, 4.302, 4.355, 4.377, 4.31, 4.334, 4.496, 4.347, 4.31, 4.317, 4.42, 4.31, 4.232, 4.337, 4.358, 4.424, 4.297, 4.28, 4.249, 4.299, 4.269, 4.382, 4.425, 4.347, 4.257, 4.288, 4.359, 4.259, 4.199, 4.417, 4.32, 4.289, 4.245, 4.256, 4.356, 4.332, 4.354, 4.303, 4.252, 4.173, 4.203, 4.283, 4.259, 4.281, 4.222, 4.176, 4.317, 4.367, 4.366, 4.231, 4.25, 4.244, 4.202, 4.213, 4.209, 4.18, 4.11, 4.186, 4.191, 4.16, 4.093, 4.156, 4.275, 4.217, 4.183, 4.253, 4.225, 4.138, 4.197, 4.245, 4.243, 4.267, 4.193, 4.176, 4.233, 4.273, 4.193, 4.228, 4.126, 4.166, 4.145, 4.162, 4.178, 4.066, 4.11, 4.128, 4.215, 4.21, 4.159, 4.215, 4.127, 4.186, 4.247, 4.195, 4.171, 4.133, 4.01, 4.09, 4.176, 4.134, 4.164, 4.107, 4.098, 4.096, 4.187, 4.149, 4.148, 4.107, 4.155, 4.062, 4.178, 4.07, 4.184, 4.048, 4.126, 4.118, 4.128, 4.167, 4.137, 4.142, 4.188, 4.094, 4.11, 4.024, 4.108, 4.086, 4.115, 4.108, 4.072, 4.054, 4.199, 4.11, 4.236, 4.168, 4.036, 4.077, 4.124, 4.274, 4.108, 4.075, 3.957, 4.037, 4.079, 4.09, 4.091, 4.122]\n",
      "Val Error(all epochs): 5.0158491134643555 \n",
      " [8.521, 8.451, 8.278, 8.14, 7.99, 7.832, 7.583, 7.395, 7.277, 6.98, 6.823, 6.643, 6.359, 6.343, 6.03, 6.131, 5.931, 5.861, 5.81, 5.925, 5.623, 5.467, 5.627, 5.814, 5.624, 5.886, 5.387, 5.7, 5.8, 5.606, 5.437, 5.605, 5.651, 5.372, 5.472, 5.479, 5.477, 5.572, 5.337, 5.392, 5.393, 5.558, 5.389, 5.821, 5.442, 5.632, 5.183, 5.26, 5.446, 5.654, 5.333, 5.251, 5.298, 5.377, 5.243, 5.737, 5.343, 5.231, 5.418, 5.241, 5.318, 5.373, 5.467, 5.266, 5.179, 5.341, 5.685, 5.18, 5.268, 5.374, 5.206, 5.356, 5.231, 5.561, 5.361, 5.432, 5.722, 5.268, 5.152, 5.371, 5.404, 5.493, 5.409, 5.303, 5.366, 5.299, 5.41, 5.412, 5.151, 5.12, 5.179, 5.13, 5.228, 5.348, 5.406, 5.264, 5.255, 5.016, 5.777, 5.604, 5.227, 5.274, 5.232, 5.7, 5.136, 5.232, 5.377, 5.421, 5.648, 5.336, 5.474, 5.087, 5.342, 5.313, 5.352, 5.188, 5.564, 6.895, 5.46, 5.384, 5.156, 5.316, 5.374, 5.195, 5.178, 5.986, 5.459, 5.18, 5.475, 5.37, 5.208, 5.694, 5.975, 5.358, 5.376, 5.253, 5.313, 5.223, 5.371, 5.297, 5.58, 5.349, 5.599, 5.246, 5.463, 5.128, 5.422, 5.322, 5.357, 5.032, 5.333, 5.216, 5.236, 5.075, 5.188, 5.239, 5.939, 5.396, 5.221, 5.613, 5.153, 5.505, 5.095, 5.168, 5.36, 5.24, 5.286, 5.182, 5.284, 5.391, 5.142, 5.232, 5.497, 5.311, 5.579, 5.345, 5.196, 5.227, 5.505, 5.299, 5.445, 5.387, 5.307, 5.938, 5.463, 5.064, 5.444, 5.122, 5.428, 5.906, 5.292, 5.403, 5.292, 5.253, 5.403, 5.372, 5.06, 5.193, 5.483, 5.578, 5.299, 5.382, 5.205, 5.274, 5.185, 5.385, 5.308, 5.366, 5.492, 5.358, 5.436, 5.381, 5.215, 5.212, 5.242, 5.288, 5.4, 5.561, 5.365, 5.175, 5.359, 5.217, 5.371, 5.363, 5.454, 5.311, 6.481, 5.33, 5.694, 5.42, 5.173, 5.323, 5.602, 5.44, 5.482, 5.156, 5.223, 5.406, 5.129, 5.27, 5.585, 5.237, 5.404, 5.287, 5.343, 5.238, 5.358, 5.483, 5.454, 5.149, 5.443, 5.392, 5.214, 5.208, 5.3, 5.617, 5.428, 5.398, 5.47, 5.528, 5.147, 5.295, 5.454, 5.491, 5.628, 5.385, 5.441, 5.422, 5.404, 5.499, 5.293, 5.601, 5.386, 5.534, 5.227, 5.815, 5.455, 5.409, 5.296, 5.644, 5.378, 5.554, 5.595, 5.646, 5.441, 5.411, 5.235, 5.333, 5.481, 5.406, 5.459, 5.156, 5.211, 5.511, 5.487, 5.4, 5.193, 5.363, 5.133, 5.366]\n",
      "Val custom mae Error(all epochs): 0.977864682674408 \n",
      " [12.12, 12.001, 11.699, 11.542, 11.311, 11.092, 10.614, 10.402, 10.385, 9.843, 9.587, 9.14, 8.825, 8.925, 8.036, 8.186, 7.837, 7.503, 7.436, 7.552, 6.448, 6.284, 7.018, 7.606, 6.762, 5.902, 5.507, 6.08, 7.041, 6.509, 5.471, 6.039, 6.487, 5.543, 5.658, 5.407, 6.492, 6.28, 5.779, 5.91, 4.56, 5.855, 5.855, 6.871, 5.361, 5.04, 3.769, 3.507, 5.463, 5.028, 4.656, 4.487, 4.875, 2.404, 3.82, 6.492, 3.915, 4.223, 3.807, 2.829, 4.335, 3.239, 4.296, 2.704, 2.676, 2.987, 5.401, 3.969, 4.671, 3.67, 4.369, 3.938, 2.997, 4.007, 2.622, 3.563, 3.487, 3.819, 2.536, 3.499, 1.515, 4.667, 4.896, 2.537, 2.545, 3.935, 4.511, 4.36, 3.172, 3.654, 4.115, 3.586, 4.943, 4.376, 5.277, 4.952, 4.121, 3.263, 6.068, 5.096, 4.438, 4.834, 3.043, 0.978, 4.136, 4.205, 5.387, 4.767, 5.53, 4.715, 4.061, 3.649, 4.47, 3.45, 3.663, 4.406, 4.846, 1.006, 1.161, 4.994, 2.858, 4.948, 4.729, 3.239, 4.166, 3.075, 3.504, 3.119, 4.003, 4.371, 4.397, 5.89, 4.46, 3.453, 2.587, 3.11, 4.638, 3.603, 4.608, 3.235, 5.189, 3.702, 1.044, 2.908, 3.415, 3.055, 4.043, 2.981, 3.34, 2.935, 3.889, 3.068, 3.31, 2.812, 3.602, 4.298, 6.045, 4.624, 2.803, 5.25, 4.317, 4.569, 3.375, 4.266, 3.79, 3.064, 2.961, 3.636, 3.19, 4.472, 3.266, 4.851, 4.333, 4.203, 4.082, 4.037, 1.956, 2.407, 1.977, 3.421, 3.63, 2.794, 3.862, 4.692, 4.709, 3.142, 4.024, 3.638, 3.845, 4.933, 2.24, 4.284, 4.058, 3.736, 4.421, 3.992, 3.478, 3.125, 4.573, 4.767, 3.535, 3.842, 3.249, 2.788, 2.577, 4.804, 4.073, 3.214, 2.7, 3.473, 3.737, 3.487, 3.819, 2.808, 3.717, 3.579, 1.77, 3.689, 2.196, 2.07, 3.998, 3.124, 3.442, 4.084, 3.086, 2.801, 2.816, 2.696, 2.569, 3.626, 2.855, 3.08, 5.402, 3.998, 2.733, 2.884, 2.714, 3.508, 3.916, 3.888, 3.929, 2.959, 2.744, 2.371, 2.258, 2.666, 2.828, 3.669, 4.445, 3.874, 1.594, 2.307, 2.728, 3.098, 2.977, 3.028, 2.59, 4.184, 2.848, 3.511, 2.347, 1.48, 2.5, 2.917, 3.615, 3.409, 2.551, 3.414, 2.995, 2.867, 4.157, 3.84, 3.548, 4.403, 2.213, 4.1, 4.675, 3.572, 4.106, 4.249, 3.422, 4.886, 2.828, 2.937, 3.749, 3.437, 2.57, 2.939, 3.889, 2.653, 3.178, 3.112, 2.962, 3.659, 3.386, 3.247, 3.346, 2.941, 2.915, 4.288]\n",
      "\n",
      "Lambda: 10 , Time: 0:00:20\n",
      "Train Error(all epochs): 5.606147289276123 \n",
      " [8.212, 8.116, 8.01, 7.8, 7.481, 7.331, 7.128, 7.039, 6.98, 6.789, 6.778, 6.682, 6.638, 6.564, 6.577, 6.493, 6.458, 6.411, 6.398, 6.375, 6.335, 6.344, 6.33, 6.337, 6.246, 6.271, 6.284, 6.267, 6.217, 6.219, 6.18, 6.206, 6.218, 6.161, 6.198, 6.161, 6.204, 6.098, 6.082, 6.068, 6.095, 6.014, 6.104, 6.085, 6.044, 5.989, 6.007, 6.042, 6.033, 5.985, 5.976, 6.013, 5.99, 5.964, 5.942, 5.977, 5.945, 5.989, 5.96, 5.987, 5.965, 5.961, 5.897, 5.919, 5.865, 5.927, 5.97, 5.949, 5.911, 5.907, 5.93, 5.877, 5.902, 5.896, 5.907, 5.911, 5.914, 5.887, 5.862, 5.989, 5.893, 5.912, 5.806, 5.881, 5.932, 5.864, 5.865, 5.867, 5.852, 5.873, 5.866, 5.83, 5.904, 5.819, 5.86, 5.791, 5.855, 5.861, 5.808, 5.821, 5.826, 5.845, 5.805, 5.813, 5.828, 5.821, 5.771, 5.795, 5.788, 5.884, 5.827, 5.819, 5.883, 5.864, 5.808, 5.823, 5.787, 5.803, 5.781, 5.811, 5.833, 5.795, 5.773, 5.819, 5.805, 5.816, 5.782, 5.841, 5.746, 5.742, 5.767, 5.777, 5.841, 5.882, 5.766, 5.783, 5.809, 5.745, 5.766, 5.737, 5.778, 5.804, 5.755, 5.824, 5.825, 5.793, 5.805, 5.787, 5.757, 5.76, 5.839, 5.795, 5.761, 5.828, 5.759, 5.761, 5.798, 5.739, 5.793, 5.769, 5.76, 5.76, 5.769, 5.743, 5.738, 5.789, 5.712, 5.736, 5.785, 5.811, 5.791, 5.734, 5.724, 5.748, 5.716, 5.761, 5.739, 5.709, 5.727, 5.744, 5.707, 5.757, 5.76, 5.77, 5.77, 5.696, 5.717, 5.732, 5.722, 5.684, 5.745, 5.753, 5.724, 5.718, 5.72, 5.744, 5.704, 5.761, 5.669, 5.761, 5.734, 5.699, 5.708, 5.691, 5.721, 5.708, 5.72, 5.772, 5.716, 5.744, 5.713, 5.716, 5.709, 5.719, 5.77, 5.77, 5.711, 5.755, 5.743, 5.744, 5.7, 5.676, 5.654, 5.787, 5.666, 5.727, 5.747, 5.742, 5.656, 5.732, 5.665, 5.739, 5.694, 5.726, 5.679, 5.614, 5.689, 5.714, 5.721, 5.696, 5.726, 5.717, 5.742, 5.69, 5.66, 5.642, 5.648, 5.681, 5.708, 5.718, 5.73, 5.703, 5.68, 5.699, 5.671, 5.674, 5.738, 5.694, 5.697, 5.698, 5.713, 5.69, 5.669, 5.737, 5.673, 5.696, 5.713, 5.706, 5.682, 5.724, 5.674, 5.674, 5.712, 5.723, 5.734, 5.741, 5.663, 5.685, 5.704, 5.696, 5.67, 5.663, 5.677, 5.695, 5.696, 5.674, 5.692, 5.715, 5.738, 5.606, 5.709, 5.688, 5.783, 5.631, 5.639, 5.688, 5.714, 5.673, 5.691, 5.644]\n",
      "Val Error(all epochs): 5.961977958679199 \n",
      " [8.6, 8.597, 8.586, 8.544, 8.469, 8.374, 8.349, 8.275, 8.18, 8.068, 7.987, 7.952, 7.662, 7.53, 7.552, 7.515, 7.717, 7.597, 7.766, 7.213, 7.503, 7.51, 7.335, 7.264, 7.381, 7.157, 6.853, 6.788, 6.955, 7.199, 6.79, 7.83, 6.807, 6.463, 6.688, 6.584, 7.023, 6.49, 6.487, 6.616, 6.848, 6.622, 6.642, 6.758, 6.756, 6.506, 6.825, 6.668, 7.181, 7.376, 6.963, 6.73, 6.508, 6.638, 6.49, 6.396, 6.389, 10.171, 6.477, 6.252, 6.573, 6.284, 6.686, 6.44, 6.499, 6.472, 7.113, 6.3, 8.101, 6.612, 6.319, 6.566, 6.443, 6.239, 6.485, 7.124, 6.511, 6.551, 8.878, 6.754, 6.124, 6.269, 6.492, 7.507, 6.238, 7.14, 6.429, 6.439, 6.342, 6.459, 6.358, 6.885, 6.679, 6.16, 6.336, 6.589, 6.423, 6.287, 6.331, 6.235, 6.469, 6.247, 6.344, 6.884, 6.202, 6.403, 6.354, 6.388, 6.541, 6.221, 6.057, 6.253, 6.353, 6.402, 6.254, 6.593, 6.639, 5.962, 6.27, 6.173, 6.168, 6.209, 6.322, 6.064, 6.371, 6.656, 7.136, 6.55, 6.252, 6.828, 6.592, 6.61, 6.428, 6.48, 6.189, 6.766, 6.374, 6.67, 6.189, 6.678, 6.409, 7.911, 6.481, 6.256, 6.451, 6.209, 6.217, 6.178, 6.201, 6.352, 6.244, 6.355, 6.234, 6.411, 6.375, 6.247, 6.513, 6.353, 6.357, 6.074, 6.121, 6.176, 6.288, 6.256, 10.041, 6.372, 6.454, 6.36, 6.208, 6.225, 6.201, 6.607, 6.079, 6.19, 6.19, 6.258, 6.025, 6.219, 6.125, 6.264, 6.244, 6.109, 6.127, 6.26, 6.112, 6.243, 6.092, 6.223, 6.086, 6.186, 6.118, 6.119, 5.986, 6.148, 6.2, 6.135, 6.384, 6.123, 7.142, 6.396, 6.16, 6.488, 6.096, 6.235, 6.141, 6.036, 6.063, 6.096, 6.225, 6.051, 6.243, 6.261, 5.974, 6.16, 6.396, 6.166, 6.376, 6.3, 6.16, 6.066, 6.041, 6.268, 6.097, 6.057, 6.258, 6.055, 6.039, 6.197, 6.123, 6.241, 6.278, 6.223, 6.102, 6.048, 6.121, 6.263, 6.12, 6.202, 6.17, 6.322, 6.007, 6.115, 6.078, 6.282, 6.186, 6.393, 6.248, 6.099, 6.201, 6.205, 5.989, 6.644, 6.414, 8.742, 6.261, 6.203, 6.238, 6.273, 6.231, 6.057, 6.062, 6.248, 6.059, 6.164, 6.256, 6.172, 6.065, 6.311, 6.144, 5.997, 6.048, 6.101, 6.158, 6.09, 6.103, 6.035, 6.241, 6.168, 6.153, 6.085, 6.119, 6.129, 6.206, 6.181, 6.212, 6.099, 6.191, 6.048, 6.361, 6.056, 6.086, 6.011, 6.085, 6.297, 6.144, 6.198, 6.266, 6.445, 6.14, 6.073]\n",
      "Val custom mae Error(all epochs): 1.3773653507232666 \n",
      " [12.222, 12.236, 12.21, 12.108, 11.912, 11.659, 11.622, 11.431, 11.311, 11.122, 11.026, 11.149, 10.39, 10.363, 10.531, 10.787, 10.97, 10.909, 11.132, 9.849, 10.746, 10.742, 10.373, 10.102, 10.705, 9.608, 8.968, 8.426, 9.293, 9.526, 9.171, 11.437, 8.771, 6.028, 7.479, 7.891, 9.487, 7.118, 7.08, 8.552, 8.576, 6.948, 6.681, 8.704, 4.044, 7.936, 9.014, 8.523, 8.727, 9.398, 9.396, 8.01, 8.086, 6.316, 7.578, 7.005, 7.679, 12.898, 5.34, 6.613, 7.607, 5.864, 6.204, 6.73, 5.788, 5.721, 7.299, 6.222, 1.94, 5.28, 6.46, 8.059, 6.133, 4.887, 6.676, 3.555, 8.194, 7.512, 1.377, 4.259, 6.169, 7.202, 7.948, 2.529, 6.671, 7.709, 5.554, 5.075, 6.514, 7.996, 7.347, 6.634, 6.668, 5.176, 5.58, 5.26, 7.054, 5.082, 6.147, 6.65, 6.445, 6.138, 5.587, 8.767, 5.81, 7.562, 6.994, 6.454, 6.837, 5.353, 4.97, 6.601, 5.942, 7.396, 6.342, 7.859, 8.579, 5.253, 5.479, 6.393, 5.86, 4.492, 7.202, 5.108, 5.129, 4.198, 9.451, 3.308, 4.775, 7.346, 7.964, 8.412, 6.607, 7.658, 6.082, 8.293, 6.953, 8.767, 6.672, 8.576, 7.288, 2.603, 7.845, 5.385, 7.246, 6.227, 5.68, 6.27, 6.741, 5.709, 4.621, 6.433, 3.852, 4.289, 6.577, 6.411, 7.696, 3.676, 5.291, 4.333, 5.425, 5.886, 5.192, 6.234, 12.365, 5.907, 5.329, 6.688, 6.513, 6.453, 2.553, 7.832, 5.174, 7.079, 5.803, 5.946, 5.298, 5.972, 6.284, 7.41, 6.958, 6.178, 5.893, 5.291, 5.685, 7.31, 5.855, 5.489, 5.422, 6.499, 4.841, 6.135, 5.032, 5.993, 5.767, 6.213, 6.913, 6.347, 8.991, 5.183, 5.975, 5.407, 5.936, 4.413, 5.08, 5.972, 5.305, 5.515, 6.287, 5.242, 6.378, 7.123, 4.36, 5.199, 3.602, 3.727, 6.997, 6.315, 6.082, 5.484, 5.891, 6.831, 4.546, 5.33, 5.08, 5.299, 5.131, 6.016, 6.414, 6.356, 4.768, 3.646, 5.391, 5.153, 6.004, 6.342, 5.836, 6.544, 5.654, 7.229, 5.19, 5.148, 5.722, 5.64, 6.095, 6.408, 4.629, 5.687, 6.73, 6.082, 4.991, 2.4, 5.911, 10.034, 6.848, 6.826, 5.119, 3.893, 4.859, 5.615, 5.708, 4.304, 5.49, 6.287, 6.131, 6.135, 4.611, 3.726, 5.893, 5.311, 5.184, 5.911, 6.406, 5.427, 5.338, 5.083, 5.711, 5.124, 5.499, 5.508, 5.825, 5.902, 4.345, 6.439, 4.851, 5.826, 4.789, 5.974, 7.419, 5.243, 5.641, 5.046, 5.167, 7.278, 6.488, 6.073, 6.386, 5.495, 4.983, 5.724]\n",
      "\n",
      "Trainig set size: 1024 , Time: 0:01:45 , best_lambda: 1 , min_  error: 0.978\n",
      "Test starts:  1229 , ends:  32161\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "967/967 [==============================] - 1s 483us/step\n",
      "total_power:  16.206432 , average_difference:  3.548662208542994\n",
      "\n",
      "\n",
      "\n",
      "number_samples: 2048 , New samples: 2048\n",
      "Validation size: 410 , starts: 2048 , ends: 2457\n",
      "\n",
      "Lambda: 0 , Time: 0:00:34\n",
      "Train Error(all epochs): 3.139981269836426 \n",
      " [8.034, 7.415, 6.613, 5.945, 5.433, 5.194, 5.129, 4.988, 4.933, 4.873, 4.849, 4.788, 4.742, 4.714, 4.71, 4.653, 4.626, 4.614, 4.573, 4.521, 4.531, 4.447, 4.486, 4.494, 4.452, 4.479, 4.463, 4.358, 4.34, 4.298, 4.282, 4.365, 4.347, 4.268, 4.299, 4.306, 4.285, 4.176, 4.157, 4.247, 4.202, 4.158, 4.146, 4.165, 4.113, 4.086, 4.1, 4.072, 4.028, 3.998, 4.026, 4.012, 3.95, 3.961, 4.02, 3.96, 3.933, 3.88, 3.936, 3.979, 3.93, 3.938, 3.896, 3.88, 3.83, 3.848, 3.782, 3.766, 3.803, 3.806, 3.826, 3.795, 3.841, 3.765, 3.798, 3.771, 3.729, 3.726, 3.748, 3.701, 3.767, 3.765, 3.691, 3.703, 3.649, 3.657, 3.669, 3.72, 3.651, 3.669, 3.65, 3.641, 3.689, 3.678, 3.616, 3.68, 3.655, 3.583, 3.572, 3.65, 3.588, 3.588, 3.598, 3.531, 3.602, 3.586, 3.557, 3.558, 3.545, 3.587, 3.525, 3.616, 3.52, 3.591, 3.546, 3.579, 3.48, 3.517, 3.54, 3.545, 3.506, 3.552, 3.458, 3.516, 3.538, 3.457, 3.512, 3.534, 3.436, 3.465, 3.496, 3.464, 3.448, 3.397, 3.415, 3.465, 3.502, 3.406, 3.39, 3.401, 3.475, 3.418, 3.465, 3.385, 3.425, 3.417, 3.385, 3.437, 3.39, 3.393, 3.394, 3.434, 3.411, 3.359, 3.397, 3.441, 3.372, 3.376, 3.393, 3.439, 3.369, 3.34, 3.366, 3.283, 3.387, 3.388, 3.382, 3.416, 3.326, 3.319, 3.378, 3.308, 3.362, 3.348, 3.338, 3.383, 3.321, 3.327, 3.293, 3.384, 3.36, 3.346, 3.374, 3.312, 3.306, 3.387, 3.36, 3.363, 3.274, 3.346, 3.338, 3.283, 3.278, 3.332, 3.324, 3.329, 3.339, 3.284, 3.291, 3.317, 3.308, 3.221, 3.327, 3.278, 3.262, 3.242, 3.322, 3.333, 3.294, 3.309, 3.3, 3.292, 3.309, 3.238, 3.327, 3.306, 3.224, 3.188, 3.245, 3.24, 3.244, 3.216, 3.235, 3.237, 3.188, 3.308, 3.266, 3.269, 3.281, 3.248, 3.233, 3.227, 3.282, 3.142, 3.265, 3.267, 3.204, 3.247, 3.269, 3.268, 3.234, 3.169, 3.231, 3.28, 3.191, 3.218, 3.234, 3.226, 3.239, 3.231, 3.265, 3.196, 3.192, 3.178, 3.213, 3.259, 3.236, 3.24, 3.224, 3.204, 3.245, 3.171, 3.181, 3.203, 3.288, 3.228, 3.194, 3.205, 3.215, 3.183, 3.201, 3.221, 3.175, 3.21, 3.202, 3.196, 3.194, 3.168, 3.218, 3.177, 3.243, 3.261, 3.273, 3.216, 3.2, 3.175, 3.227, 3.212, 3.152, 3.188, 3.14, 3.172, 3.237, 3.189, 3.205, 3.165, 3.173, 3.215, 3.189, 3.181]\n",
      "Val Error(all epochs): 4.412785053253174 \n",
      " [7.835, 6.604, 5.828, 5.182, 4.859, 4.616, 4.622, 4.466, 4.499, 4.479, 4.499, 4.413, 4.538, 4.481, 4.451, 4.472, 4.567, 4.591, 4.597, 4.687, 4.518, 4.623, 4.57, 4.586, 4.587, 4.625, 4.58, 4.665, 4.645, 4.658, 4.755, 4.728, 4.722, 4.751, 4.722, 4.767, 4.78, 4.778, 4.785, 4.902, 4.935, 4.884, 4.777, 4.808, 4.935, 4.84, 4.925, 4.853, 4.838, 4.878, 4.914, 4.865, 4.862, 4.84, 4.849, 4.814, 4.856, 4.876, 4.971, 4.876, 4.857, 4.97, 4.915, 4.918, 4.955, 4.956, 4.878, 4.992, 4.939, 4.92, 4.888, 4.914, 4.968, 4.897, 4.979, 5.018, 5.04, 4.983, 5.019, 4.934, 4.882, 4.995, 4.932, 4.937, 4.964, 4.916, 4.934, 4.908, 5.002, 5.085, 5.045, 5.006, 5.066, 5.074, 5.035, 5.001, 5.071, 5.054, 5.029, 4.949, 5.045, 5.023, 4.999, 5.039, 4.975, 5.05, 5.029, 4.96, 4.94, 5.003, 4.995, 5.025, 4.959, 5.063, 5.046, 4.874, 4.892, 4.997, 4.902, 4.937, 4.886, 4.966, 5.03, 4.981, 5.041, 5.014, 5.057, 4.973, 4.97, 5.021, 4.97, 4.929, 4.935, 4.978, 4.927, 4.914, 4.928, 4.965, 4.953, 4.978, 5.001, 4.998, 4.916, 4.927, 4.954, 5.025, 4.919, 5.027, 4.902, 4.936, 4.935, 4.925, 4.968, 4.968, 4.934, 4.952, 4.856, 4.882, 4.829, 4.879, 4.843, 4.859, 4.862, 4.816, 4.917, 4.904, 4.865, 4.821, 4.889, 4.828, 4.886, 4.855, 4.817, 4.822, 4.798, 4.859, 4.845, 4.894, 4.875, 4.996, 4.857, 4.881, 4.865, 4.853, 4.929, 4.89, 4.931, 4.976, 4.923, 4.952, 4.921, 4.949, 4.86, 4.899, 4.873, 4.804, 4.808, 4.82, 4.829, 4.823, 4.883, 4.845, 4.958, 4.975, 4.907, 4.933, 4.892, 4.942, 4.951, 4.886, 4.925, 4.867, 4.878, 4.88, 4.845, 4.87, 4.912, 4.883, 4.953, 4.851, 4.856, 4.908, 4.907, 4.888, 4.849, 4.836, 4.836, 4.806, 4.845, 4.826, 4.873, 4.883, 4.846, 4.843, 4.884, 4.869, 4.921, 4.887, 4.888, 4.854, 4.809, 4.846, 4.861, 4.92, 4.925, 4.901, 4.831, 4.834, 4.868, 4.917, 4.866, 4.889, 4.914, 4.894, 4.815, 4.919, 4.864, 4.871, 4.969, 4.895, 4.875, 4.89, 4.816, 4.826, 4.823, 4.829, 4.802, 4.803, 4.818, 4.852, 4.836, 4.818, 4.86, 4.843, 4.858, 4.812, 4.8, 4.844, 4.834, 4.849, 4.819, 4.84, 4.804, 4.82, 4.828, 4.909, 4.873, 4.832, 4.863, 4.846, 4.849, 4.861, 4.927, 4.86, 4.819, 4.86, 4.876, 4.901, 4.826, 4.916]\n",
      "Val custom mae Error(all epochs): 1.6885530948638916 \n",
      " [12.104, 9.654, 8.066, 5.833, 4.319, 3.176, 2.744, 2.853, 3.23, 2.423, 3.641, 2.837, 3.57, 2.763, 3.145, 3.346, 3.398, 3.719, 3.15, 3.537, 2.692, 3.193, 3.29, 3.411, 3.344, 3.367, 2.906, 3.045, 2.889, 2.866, 3.458, 2.963, 3.01, 2.526, 3.34, 2.699, 2.913, 2.383, 2.641, 2.706, 2.467, 2.661, 2.995, 2.801, 3.308, 2.867, 3.173, 2.675, 2.851, 2.757, 3.102, 2.767, 2.713, 2.857, 2.427, 2.801, 3.154, 2.575, 2.685, 3.303, 2.932, 3.062, 2.911, 3.083, 2.967, 2.527, 2.707, 3.015, 3.145, 2.496, 2.817, 2.861, 2.944, 2.691, 2.529, 3.051, 3.051, 2.945, 2.498, 2.301, 2.55, 3.106, 2.96, 2.896, 2.765, 2.501, 2.626, 2.72, 3.146, 3.048, 2.834, 2.13, 1.983, 2.89, 2.447, 2.745, 2.931, 2.91, 2.686, 2.714, 2.375, 2.763, 2.438, 2.232, 2.578, 2.091, 2.657, 2.168, 2.486, 2.946, 2.577, 3.006, 2.673, 2.885, 2.155, 2.645, 2.9, 3.003, 2.559, 2.474, 2.445, 2.327, 2.596, 2.175, 2.699, 2.591, 2.763, 2.782, 2.8, 2.052, 2.911, 2.546, 2.678, 2.543, 2.458, 2.695, 2.747, 2.793, 2.757, 3.329, 2.656, 2.79, 2.785, 1.808, 2.737, 2.713, 2.826, 2.634, 2.471, 2.834, 2.35, 2.419, 1.689, 2.742, 2.184, 2.523, 2.623, 2.529, 2.708, 2.984, 2.613, 2.549, 2.777, 2.336, 2.251, 2.888, 2.496, 2.37, 2.62, 2.684, 2.705, 2.564, 2.432, 2.15, 2.151, 2.787, 2.739, 2.7, 2.772, 3.569, 2.75, 2.62, 2.822, 2.449, 2.48, 2.6, 2.463, 2.791, 2.423, 2.463, 2.697, 2.553, 2.441, 2.628, 2.571, 2.264, 2.497, 2.528, 2.241, 2.612, 2.858, 2.689, 3.021, 3.089, 2.631, 2.622, 2.684, 2.949, 2.749, 2.644, 3.024, 2.516, 2.842, 2.465, 2.908, 2.76, 2.615, 2.681, 2.718, 2.774, 2.319, 2.514, 2.495, 2.632, 2.524, 2.541, 2.504, 2.204, 2.593, 2.649, 2.347, 2.196, 2.402, 2.509, 3.154, 2.632, 2.978, 2.959, 2.728, 2.612, 2.403, 2.396, 2.689, 3.102, 2.812, 2.813, 2.44, 2.429, 2.663, 2.751, 2.602, 2.261, 2.665, 2.975, 2.334, 2.445, 2.537, 2.6, 2.949, 2.894, 2.627, 2.711, 2.333, 2.324, 2.533, 2.619, 2.853, 2.688, 3.045, 2.573, 2.804, 2.386, 2.648, 2.512, 2.593, 2.177, 2.384, 2.656, 2.413, 2.367, 2.367, 2.435, 2.673, 2.757, 2.4, 3.052, 2.753, 2.571, 2.639, 3.095, 3.216, 2.908, 2.877, 2.61, 2.481, 3.172, 3.036, 2.846, 2.755, 2.733]\n",
      "\n",
      "Lambda: 0.01 , Time: 0:00:36\n",
      "Train Error(all epochs): 3.350234031677246 \n",
      " [8.049, 7.416, 6.584, 5.865, 5.421, 5.202, 5.097, 4.962, 4.928, 4.902, 4.805, 4.784, 4.703, 4.752, 4.652, 4.674, 4.663, 4.597, 4.568, 4.564, 4.614, 4.507, 4.422, 4.482, 4.434, 4.471, 4.476, 4.367, 4.417, 4.4, 4.317, 4.374, 4.32, 4.321, 4.242, 4.306, 4.284, 4.242, 4.228, 4.293, 4.181, 4.166, 4.228, 4.128, 4.164, 4.073, 4.059, 4.06, 4.094, 4.099, 4.116, 4.117, 4.063, 3.956, 3.977, 3.982, 3.992, 3.98, 3.945, 3.922, 3.935, 3.902, 3.99, 3.914, 3.893, 3.884, 3.869, 3.879, 3.885, 3.891, 3.851, 3.846, 3.801, 3.813, 3.83, 3.838, 3.819, 3.803, 3.84, 3.835, 3.731, 3.782, 3.754, 3.731, 3.772, 3.714, 3.711, 3.81, 3.702, 3.739, 3.724, 3.734, 3.648, 3.688, 3.675, 3.629, 3.662, 3.71, 3.644, 3.66, 3.644, 3.721, 3.647, 3.652, 3.651, 3.652, 3.628, 3.631, 3.62, 3.632, 3.625, 3.638, 3.582, 3.664, 3.607, 3.561, 3.641, 3.591, 3.657, 3.629, 3.596, 3.556, 3.647, 3.588, 3.637, 3.543, 3.564, 3.568, 3.573, 3.557, 3.557, 3.527, 3.625, 3.531, 3.563, 3.501, 3.594, 3.549, 3.563, 3.525, 3.493, 3.543, 3.539, 3.568, 3.477, 3.532, 3.564, 3.541, 3.522, 3.586, 3.534, 3.525, 3.544, 3.491, 3.427, 3.521, 3.499, 3.496, 3.56, 3.506, 3.448, 3.519, 3.51, 3.525, 3.536, 3.542, 3.468, 3.497, 3.596, 3.522, 3.542, 3.479, 3.506, 3.53, 3.472, 3.5, 3.519, 3.467, 3.493, 3.5, 3.495, 3.499, 3.505, 3.489, 3.572, 3.52, 3.412, 3.484, 3.484, 3.495, 3.526, 3.442, 3.449, 3.478, 3.455, 3.456, 3.479, 3.433, 3.411, 3.45, 3.492, 3.443, 3.46, 3.456, 3.479, 3.484, 3.455, 3.449, 3.519, 3.517, 3.508, 3.368, 3.459, 3.462, 3.468, 3.453, 3.484, 3.504, 3.436, 3.391, 3.47, 3.448, 3.496, 3.508, 3.44, 3.494, 3.489, 3.462, 3.391, 3.477, 3.456, 3.416, 3.402, 3.485, 3.45, 3.419, 3.468, 3.474, 3.467, 3.419, 3.455, 3.414, 3.47, 3.442, 3.493, 3.463, 3.489, 3.376, 3.389, 3.453, 3.411, 3.469, 3.436, 3.429, 3.464, 3.494, 3.365, 3.429, 3.474, 3.383, 3.368, 3.412, 3.383, 3.363, 3.383, 3.449, 3.414, 3.437, 3.39, 3.504, 3.382, 3.432, 3.418, 3.406, 3.373, 3.426, 3.392, 3.385, 3.373, 3.443, 3.385, 3.39, 3.415, 3.387, 3.46, 3.438, 3.423, 3.388, 3.35, 3.356, 3.424, 3.425, 3.425, 3.396, 3.356, 3.467, 3.431, 3.414, 3.436, 3.438]\n",
      "Val Error(all epochs): 4.51619291305542 \n",
      " [7.879, 6.762, 5.831, 5.355, 4.92, 4.815, 4.795, 4.803, 4.678, 4.634, 4.609, 4.536, 4.58, 4.661, 4.582, 4.576, 4.648, 4.661, 4.645, 4.516, 4.649, 4.656, 4.704, 4.634, 4.679, 4.656, 4.631, 4.702, 4.64, 4.715, 4.662, 4.713, 4.749, 4.671, 4.696, 4.783, 4.662, 4.742, 4.689, 4.658, 4.763, 4.754, 4.811, 4.806, 4.755, 4.729, 4.872, 4.727, 4.814, 4.87, 4.834, 4.924, 4.862, 4.832, 4.889, 5.159, 4.86, 4.914, 4.917, 4.891, 4.879, 4.823, 5.406, 4.787, 4.823, 4.858, 4.911, 4.914, 4.871, 4.802, 4.849, 4.959, 4.985, 4.794, 4.828, 4.874, 4.893, 4.851, 4.842, 4.902, 4.982, 4.864, 4.88, 4.884, 4.836, 4.848, 4.875, 4.962, 4.918, 5.017, 4.801, 4.93, 4.844, 4.911, 4.975, 4.881, 4.817, 4.941, 4.871, 4.926, 4.853, 4.831, 4.809, 4.831, 4.852, 5.066, 4.941, 4.903, 4.86, 4.901, 4.916, 4.993, 4.826, 4.971, 4.988, 4.959, 4.882, 4.971, 4.974, 5.088, 5.004, 4.941, 4.921, 4.816, 4.859, 5.018, 5.007, 4.83, 4.879, 4.862, 4.92, 4.955, 4.91, 4.955, 4.887, 4.97, 4.967, 4.927, 4.935, 4.974, 4.978, 4.956, 4.969, 4.958, 4.923, 4.976, 4.953, 4.925, 4.903, 4.876, 4.941, 5.011, 5.006, 4.98, 4.978, 5.027, 4.86, 5.015, 5.021, 5.055, 4.968, 4.92, 4.961, 4.839, 4.855, 4.968, 4.89, 5.027, 4.859, 4.854, 4.836, 4.957, 4.877, 4.845, 4.949, 4.869, 4.909, 4.965, 4.863, 4.889, 4.983, 4.888, 4.84, 4.866, 4.85, 4.887, 4.836, 4.906, 4.907, 4.928, 4.918, 5.046, 4.895, 4.847, 4.837, 4.821, 4.776, 4.863, 5.104, 4.935, 4.836, 4.89, 4.981, 4.984, 4.885, 4.998, 4.978, 5.01, 4.944, 4.9, 5.038, 4.975, 5.064, 4.985, 4.976, 4.906, 4.953, 4.912, 4.976, 4.891, 5.031, 4.975, 4.82, 4.991, 4.967, 4.982, 4.958, 4.976, 4.909, 4.845, 4.85, 4.955, 4.812, 4.912, 4.922, 4.873, 4.967, 4.962, 4.94, 4.849, 4.909, 4.928, 4.949, 4.887, 4.899, 5.015, 4.905, 4.989, 4.908, 5.007, 4.99, 4.934, 4.912, 4.886, 4.868, 4.954, 4.952, 4.872, 4.884, 4.847, 4.858, 5.015, 4.935, 5.037, 4.889, 4.947, 4.946, 4.999, 4.951, 4.95, 5.002, 4.992, 5.036, 4.953, 4.893, 4.935, 4.972, 5.047, 4.993, 4.953, 5.018, 4.947, 4.951, 4.918, 4.92, 4.993, 4.972, 5.016, 5.065, 5.195, 5.029, 5.016, 4.988, 4.979, 5.038, 5.099, 5.032, 4.888, 4.93, 4.973]\n",
      "Val custom mae Error(all epochs): 1.5911554098129272 \n",
      " [12.249, 10.275, 7.999, 6.87, 4.52, 4.018, 3.844, 3.538, 3.704, 3.663, 3.48, 3.321, 3.956, 3.924, 2.726, 3.262, 3.82, 3.616, 3.535, 2.754, 3.32, 3.593, 3.96, 3.355, 3.221, 3.648, 3.347, 3.11, 2.985, 2.958, 2.815, 3.388, 3.234, 3.039, 3.275, 3.342, 3.034, 3.073, 3.089, 3.382, 3.168, 3.501, 3.798, 3.495, 3.495, 3.3, 3.547, 2.705, 3.081, 3.051, 2.934, 3.374, 2.851, 2.948, 2.933, 3.261, 2.824, 3.362, 3.138, 4.269, 3.872, 3.041, 1.591, 2.078, 3.322, 3.122, 3.299, 3.311, 3.019, 3.143, 3.079, 3.132, 3.599, 2.841, 2.903, 3.04, 3.631, 2.689, 2.931, 3.133, 3.079, 3.215, 3.248, 3.179, 2.666, 2.923, 3.042, 3.01, 3.117, 3.073, 3.179, 3.048, 2.608, 3.057, 2.996, 2.981, 2.694, 3.063, 2.624, 3.128, 2.867, 2.457, 2.692, 2.497, 3.118, 3.267, 3.087, 2.37, 2.862, 2.792, 3.216, 3.528, 2.54, 3.363, 3.212, 3.043, 2.771, 2.842, 3.164, 3.68, 3.264, 3.387, 2.94, 3.247, 2.966, 3.842, 3.41, 2.386, 2.996, 2.6, 2.596, 3.281, 3.237, 3.556, 2.862, 3.349, 3.259, 3.033, 3.104, 3.795, 3.04, 2.875, 3.551, 3.048, 3.384, 2.767, 2.733, 3.386, 2.921, 2.754, 3.004, 3.399, 2.935, 3.084, 2.964, 2.69, 2.263, 2.921, 3.08, 3.002, 2.88, 2.785, 3.343, 2.432, 2.813, 3.491, 2.751, 2.049, 2.757, 2.234, 3.156, 3.588, 2.866, 3.404, 2.965, 2.78, 3.196, 3.087, 2.631, 2.622, 3.779, 2.891, 3.175, 2.409, 2.66, 3.405, 2.898, 3.105, 3.156, 3.38, 3.199, 3.561, 3.025, 2.832, 2.503, 2.835, 2.737, 3.265, 3.372, 2.774, 2.269, 2.438, 3.336, 3.063, 2.629, 3.642, 3.346, 3.769, 3.324, 3.129, 3.453, 3.3, 3.585, 2.923, 2.926, 2.426, 3.135, 2.862, 3.542, 2.677, 3.448, 3.44, 2.572, 3.251, 3.226, 2.98, 3.137, 3.001, 2.552, 2.811, 3.42, 3.268, 2.608, 2.922, 3.3, 3.123, 3.701, 3.355, 3.251, 2.78, 2.924, 3.388, 3.016, 3.362, 2.782, 2.654, 2.792, 2.812, 2.938, 3.855, 3.332, 3.06, 3.119, 2.765, 3.169, 2.932, 3.552, 3.094, 2.98, 2.805, 2.385, 3.128, 3.308, 3.5, 2.581, 3.245, 2.451, 3.178, 3.351, 2.946, 2.778, 2.758, 3.052, 3.214, 3.159, 3.42, 3.306, 3.394, 2.67, 2.018, 3.029, 3.361, 3.353, 3.271, 2.651, 3.084, 2.805, 2.933, 3.185, 3.242, 2.455, 3.01, 2.983, 3.103, 2.362, 2.806, 3.004, 3.135, 2.657, 2.986]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Lambda: 0.1 , Time: 0:00:36\n",
      "Train Error(all epochs): 3.821125030517578 \n",
      " [7.983, 7.317, 6.525, 5.898, 5.476, 5.232, 5.099, 5.106, 4.995, 4.888, 4.881, 4.898, 4.85, 4.85, 4.825, 4.795, 4.749, 4.75, 4.764, 4.703, 4.791, 4.706, 4.621, 4.684, 4.601, 4.719, 4.623, 4.668, 4.624, 4.656, 4.566, 4.511, 4.566, 4.539, 4.594, 4.573, 4.57, 4.513, 4.496, 4.56, 4.477, 4.499, 4.484, 4.429, 4.383, 4.452, 4.377, 4.357, 4.411, 4.304, 4.405, 4.355, 4.321, 4.369, 4.367, 4.365, 4.329, 4.324, 4.254, 4.306, 4.29, 4.334, 4.343, 4.293, 4.363, 4.285, 4.301, 4.255, 4.214, 4.193, 4.191, 4.249, 4.181, 4.208, 4.236, 4.255, 4.136, 4.171, 4.195, 4.168, 4.255, 4.202, 4.135, 4.189, 4.162, 4.247, 4.094, 4.192, 4.157, 4.097, 4.198, 4.185, 4.181, 4.153, 4.195, 4.095, 4.2, 4.083, 4.187, 4.166, 4.112, 4.2, 4.103, 4.091, 4.093, 4.106, 4.073, 4.055, 4.047, 4.122, 4.063, 4.134, 4.177, 4.05, 4.093, 4.011, 4.026, 4.077, 4.129, 4.048, 4.027, 4.047, 4.042, 4.098, 4.053, 4.034, 4.064, 4.079, 4.05, 4.095, 4.124, 4.039, 4.051, 4.043, 4.021, 4.091, 4.065, 4.055, 4.05, 4.014, 4.02, 4.118, 4.048, 4.084, 4.093, 4.06, 4.028, 4.011, 4.107, 3.993, 4.033, 3.975, 4.055, 4.008, 3.973, 3.986, 3.96, 4.028, 3.943, 3.995, 3.988, 4.003, 4.036, 4.004, 4.019, 4.012, 3.985, 4.009, 3.996, 3.948, 3.962, 3.959, 3.935, 3.986, 3.98, 4.015, 3.957, 3.989, 3.965, 3.995, 4.025, 4.027, 3.939, 3.984, 3.966, 3.994, 3.997, 3.939, 3.981, 3.931, 3.927, 4.002, 3.941, 3.968, 3.971, 3.919, 3.909, 3.973, 3.922, 4.026, 4.047, 4.04, 3.941, 3.919, 3.964, 3.973, 3.98, 3.893, 3.928, 3.913, 3.928, 3.923, 3.96, 3.903, 3.95, 3.99, 3.975, 3.892, 3.901, 3.961, 4.004, 3.958, 3.929, 3.997, 3.94, 4.02, 4.004, 4.002, 3.926, 3.959, 3.915, 3.881, 3.914, 3.896, 3.957, 3.923, 3.891, 3.936, 3.954, 3.902, 3.914, 3.96, 3.997, 3.921, 3.953, 3.949, 3.934, 3.969, 3.954, 3.87, 3.927, 3.945, 3.896, 3.921, 3.912, 3.996, 3.941, 3.957, 3.97, 3.939, 3.954, 3.95, 3.893, 3.876, 3.891, 3.944, 3.842, 3.957, 3.941, 3.959, 3.857, 3.897, 3.863, 3.833, 3.821, 3.834, 3.93, 3.852, 3.939, 3.856, 3.898, 3.956, 3.914, 3.889, 3.888, 3.903, 3.877, 3.926, 3.921, 3.858, 3.933, 3.946, 3.888, 3.906, 3.866, 3.85, 3.877, 3.912, 3.857, 3.95]\n",
      "Val Error(all epochs): 4.650946617126465 \n",
      " [7.669, 6.767, 6.302, 5.388, 5.148, 5.116, 4.946, 4.886, 4.801, 4.807, 4.915, 4.77, 4.736, 4.829, 4.813, 4.702, 4.776, 4.738, 5.102, 4.862, 4.858, 4.834, 4.748, 4.934, 4.852, 5.305, 4.806, 4.826, 4.706, 4.853, 4.856, 4.761, 4.74, 4.796, 4.873, 4.712, 4.863, 4.727, 4.918, 4.86, 5.114, 4.779, 4.823, 4.881, 4.776, 4.895, 4.818, 4.83, 4.694, 4.761, 4.651, 4.849, 4.894, 4.81, 4.889, 4.95, 4.773, 4.88, 4.943, 4.83, 4.819, 4.835, 4.99, 4.903, 4.841, 4.871, 4.973, 4.909, 4.84, 4.873, 5.044, 4.864, 4.87, 4.849, 4.821, 4.791, 4.823, 4.852, 4.936, 4.843, 4.803, 4.901, 4.819, 4.721, 4.855, 4.825, 4.782, 4.817, 4.707, 4.907, 4.901, 4.85, 4.916, 5.068, 4.802, 4.839, 4.906, 4.804, 4.884, 4.752, 4.81, 4.94, 4.771, 5.001, 4.88, 4.934, 4.853, 4.902, 4.866, 4.678, 4.818, 4.965, 4.816, 4.875, 4.8, 4.843, 4.709, 5.206, 4.853, 4.907, 4.743, 4.905, 4.86, 5.09, 4.848, 4.813, 4.803, 4.783, 4.806, 4.774, 4.969, 4.684, 4.83, 4.754, 4.803, 5.02, 5.118, 4.718, 4.682, 4.69, 4.679, 4.853, 4.797, 4.786, 5.102, 4.878, 4.904, 4.947, 4.816, 4.892, 4.835, 4.796, 4.786, 4.776, 4.782, 4.86, 4.985, 4.763, 4.847, 4.845, 5.061, 4.934, 5.168, 4.917, 4.837, 4.931, 4.78, 4.913, 4.89, 4.949, 4.883, 4.866, 4.777, 4.844, 5.047, 4.921, 4.869, 4.983, 4.975, 4.886, 5.148, 5.033, 4.998, 4.908, 4.903, 4.982, 4.855, 4.803, 4.826, 4.897, 4.798, 4.909, 4.896, 4.92, 5.007, 4.832, 4.846, 4.947, 4.903, 4.882, 4.887, 4.723, 4.904, 5.115, 4.909, 4.862, 5.0, 5.016, 4.929, 4.855, 4.84, 5.088, 5.126, 4.834, 5.061, 5.064, 5.097, 4.962, 4.882, 4.861, 4.849, 4.892, 4.865, 4.933, 4.844, 4.915, 5.138, 4.792, 4.913, 4.855, 4.827, 4.943, 4.828, 4.81, 4.9, 4.971, 4.89, 4.884, 5.022, 4.995, 4.892, 4.928, 4.942, 5.041, 5.052, 4.949, 5.007, 4.851, 4.946, 4.869, 4.91, 4.902, 4.892, 4.869, 4.918, 4.868, 4.939, 4.892, 4.947, 5.114, 5.213, 4.967, 4.994, 5.113, 5.315, 4.962, 5.051, 5.04, 5.009, 5.079, 4.8, 4.976, 4.976, 5.0, 4.883, 4.884, 5.082, 4.977, 5.052, 5.159, 4.945, 4.793, 4.907, 4.965, 4.871, 4.855, 4.922, 4.867, 4.879, 4.994, 4.812, 4.766, 5.002, 4.922, 4.962, 4.854, 4.997, 4.929, 4.964, 4.904]\n",
      "Val custom mae Error(all epochs): 1.3377420902252197 \n",
      " [11.688, 10.509, 9.665, 7.424, 6.196, 5.691, 5.147, 5.535, 5.214, 5.21, 5.252, 5.255, 4.842, 5.221, 5.425, 4.684, 5.199, 4.841, 5.558, 4.709, 4.9, 4.488, 4.407, 4.803, 4.636, 5.69, 4.081, 3.837, 4.231, 3.522, 4.698, 3.684, 3.687, 4.431, 4.25, 4.287, 4.026, 3.505, 4.653, 3.953, 4.408, 3.874, 3.576, 4.246, 2.819, 3.819, 4.007, 3.376, 3.098, 4.15, 2.767, 3.916, 3.627, 3.129, 3.429, 4.403, 3.366, 3.547, 3.402, 3.598, 3.448, 2.882, 4.25, 4.207, 3.687, 2.89, 3.049, 2.396, 3.183, 3.761, 4.544, 3.582, 3.533, 3.509, 3.225, 3.021, 3.084, 2.802, 3.815, 3.431, 3.007, 3.699, 2.847, 3.212, 1.849, 2.77, 2.109, 2.697, 3.219, 3.142, 3.662, 3.65, 3.594, 3.755, 2.785, 3.483, 3.127, 3.969, 3.04, 3.442, 2.901, 3.844, 3.463, 3.927, 2.706, 3.765, 3.057, 2.635, 2.977, 2.62, 3.173, 3.158, 2.618, 3.143, 3.155, 2.304, 3.3, 4.304, 2.839, 3.315, 2.528, 2.968, 3.067, 3.664, 3.197, 2.806, 2.844, 2.595, 3.049, 2.807, 3.279, 3.118, 2.739, 3.263, 3.182, 4.342, 4.118, 2.047, 2.063, 1.912, 2.845, 3.647, 2.8, 2.333, 3.08, 3.362, 3.825, 3.124, 1.896, 3.563, 2.822, 2.206, 2.809, 3.434, 2.002, 2.927, 3.195, 2.908, 2.926, 2.221, 3.519, 2.041, 4.328, 3.453, 2.551, 3.899, 2.372, 3.658, 2.717, 1.894, 2.868, 3.169, 2.598, 3.168, 3.511, 2.915, 2.793, 3.835, 2.811, 3.064, 3.28, 3.671, 3.394, 2.393, 2.979, 2.183, 2.552, 1.78, 3.444, 2.823, 2.966, 2.558, 1.691, 3.303, 3.406, 2.435, 2.864, 3.268, 3.235, 3.745, 3.224, 1.957, 2.681, 3.065, 2.951, 2.052, 4.054, 3.153, 1.952, 3.725, 2.725, 2.887, 3.985, 1.974, 3.06, 3.524, 3.388, 3.65, 2.373, 2.78, 3.672, 3.375, 2.679, 3.273, 2.842, 3.208, 1.338, 1.837, 2.94, 2.592, 2.381, 1.844, 3.065, 2.859, 2.836, 3.347, 2.498, 2.548, 1.6, 2.441, 2.305, 2.56, 1.502, 3.318, 2.299, 2.709, 3.363, 2.53, 2.246, 2.671, 2.003, 2.207, 1.963, 3.446, 2.093, 2.486, 2.603, 2.24, 3.039, 2.74, 2.682, 1.503, 2.28, 2.662, 3.338, 2.293, 2.617, 3.16, 1.676, 3.088, 2.565, 3.396, 3.833, 2.557, 2.978, 2.837, 3.282, 2.673, 3.456, 3.028, 2.809, 2.313, 3.526, 2.444, 2.43, 2.468, 3.038, 2.393, 2.832, 3.186, 2.863, 2.777, 2.412, 1.653, 2.541, 1.8, 2.514, 2.668, 2.312, 2.977]\n",
      "\n",
      "Lambda: 1 , Time: 0:00:36\n",
      "Train Error(all epochs): 4.543496608734131 \n",
      " [8.023, 7.348, 6.591, 6.107, 5.793, 5.649, 5.595, 5.458, 5.365, 5.315, 5.283, 5.25, 5.198, 5.195, 5.173, 5.186, 5.177, 5.19, 5.196, 5.211, 5.201, 5.185, 5.148, 5.188, 5.137, 5.191, 5.151, 5.117, 5.16, 5.122, 5.115, 5.105, 5.129, 5.173, 5.123, 5.13, 5.115, 5.137, 5.064, 5.135, 5.133, 5.119, 5.091, 5.131, 5.075, 5.041, 5.002, 5.026, 5.161, 5.045, 5.084, 5.041, 5.002, 5.077, 5.069, 5.061, 5.063, 5.08, 5.08, 5.029, 5.016, 5.056, 4.982, 5.044, 5.088, 5.078, 5.023, 5.027, 4.984, 5.039, 5.065, 5.072, 4.982, 4.978, 4.984, 4.967, 4.973, 5.011, 4.917, 4.973, 4.972, 4.999, 4.948, 4.954, 4.951, 4.92, 4.954, 4.98, 4.894, 4.891, 4.946, 4.901, 4.89, 4.903, 4.883, 4.951, 4.901, 4.941, 4.907, 4.9, 4.908, 4.828, 4.877, 4.877, 4.869, 4.873, 4.843, 4.888, 4.872, 4.891, 4.886, 4.833, 4.816, 4.912, 4.851, 4.855, 4.819, 4.842, 4.833, 4.851, 4.874, 4.844, 4.847, 4.812, 4.809, 4.772, 4.804, 4.779, 4.771, 4.811, 4.779, 4.755, 4.795, 4.779, 4.817, 4.74, 4.75, 4.777, 4.775, 4.797, 4.736, 4.759, 4.734, 4.744, 4.766, 4.736, 4.76, 4.762, 4.769, 4.793, 4.741, 4.784, 4.735, 4.754, 4.737, 4.717, 4.777, 4.714, 4.732, 4.684, 4.748, 4.747, 4.774, 4.771, 4.769, 4.748, 4.729, 4.735, 4.708, 4.674, 4.694, 4.669, 4.731, 4.715, 4.716, 4.699, 4.671, 4.686, 4.714, 4.764, 4.678, 4.682, 4.683, 4.752, 4.743, 4.647, 4.694, 4.678, 4.662, 4.684, 4.735, 4.725, 4.633, 4.713, 4.711, 4.723, 4.716, 4.666, 4.609, 4.702, 4.675, 4.695, 4.652, 4.697, 4.746, 4.692, 4.751, 4.705, 4.661, 4.646, 4.654, 4.683, 4.712, 4.631, 4.698, 4.662, 4.669, 4.632, 4.687, 4.656, 4.664, 4.651, 4.688, 4.625, 4.617, 4.668, 4.649, 4.681, 4.658, 4.683, 4.7, 4.7, 4.674, 4.663, 4.736, 4.629, 4.602, 4.678, 4.672, 4.662, 4.663, 4.66, 4.667, 4.642, 4.659, 4.704, 4.63, 4.686, 4.645, 4.59, 4.654, 4.575, 4.636, 4.692, 4.635, 4.66, 4.659, 4.599, 4.571, 4.6, 4.683, 4.625, 4.665, 4.646, 4.643, 4.601, 4.543, 4.598, 4.655, 4.601, 4.695, 4.663, 4.588, 4.604, 4.671, 4.58, 4.592, 4.638, 4.627, 4.685, 4.618, 4.627, 4.672, 4.67, 4.639, 4.653, 4.627, 4.612, 4.647, 4.646, 4.6, 4.629, 4.631, 4.628, 4.639, 4.607, 4.632, 4.655, 4.622, 4.607]\n",
      "Val Error(all epochs): 4.450349807739258 \n",
      " [8.001, 7.757, 7.454, 7.067, 6.617, 6.262, 6.041, 5.776, 5.739, 5.571, 5.385, 5.41, 5.526, 5.476, 5.205, 5.256, 5.321, 5.079, 5.122, 4.884, 4.982, 5.121, 4.998, 5.163, 4.902, 4.826, 5.281, 4.809, 5.443, 4.764, 5.044, 4.935, 4.857, 4.803, 4.995, 4.834, 4.747, 4.797, 5.498, 5.058, 4.865, 5.348, 4.913, 4.724, 4.996, 5.491, 4.792, 5.0, 4.838, 4.854, 5.001, 5.016, 4.914, 4.903, 5.021, 4.918, 5.118, 4.963, 5.084, 5.411, 5.341, 4.583, 4.658, 5.032, 4.814, 4.884, 4.563, 4.615, 4.955, 4.584, 4.724, 4.821, 4.891, 4.729, 4.649, 4.824, 4.88, 4.648, 4.797, 5.198, 4.941, 4.642, 4.747, 4.599, 4.703, 4.653, 4.777, 4.99, 4.748, 4.602, 4.667, 5.832, 4.719, 4.507, 4.62, 4.756, 6.006, 4.873, 4.813, 5.182, 4.734, 4.911, 4.725, 4.669, 4.841, 4.547, 4.575, 4.779, 4.664, 4.909, 5.507, 4.658, 4.856, 4.725, 4.794, 4.607, 4.512, 4.64, 4.948, 5.188, 4.717, 4.806, 4.71, 4.519, 4.566, 4.565, 4.665, 5.0, 4.529, 4.785, 4.661, 4.515, 4.658, 4.765, 4.584, 5.185, 4.857, 4.497, 4.685, 4.606, 4.609, 4.721, 4.651, 4.677, 5.296, 4.893, 4.515, 4.617, 4.77, 4.581, 4.789, 4.473, 4.686, 4.777, 4.657, 4.516, 4.603, 4.589, 4.494, 4.582, 4.609, 4.52, 5.23, 4.743, 4.666, 4.567, 4.738, 4.523, 4.58, 4.583, 4.545, 4.675, 4.611, 4.507, 4.596, 4.602, 4.545, 4.45, 4.741, 4.677, 4.683, 4.524, 4.685, 4.635, 4.649, 4.739, 4.614, 4.559, 4.615, 4.523, 4.958, 4.63, 5.027, 4.769, 4.549, 4.818, 7.67, 4.639, 4.676, 4.626, 4.616, 4.643, 4.897, 4.736, 4.66, 4.677, 4.761, 4.563, 4.7, 4.674, 4.558, 4.549, 4.642, 4.744, 4.551, 4.656, 4.652, 4.992, 4.591, 4.557, 4.623, 4.678, 4.576, 4.652, 4.571, 4.658, 4.545, 4.735, 4.627, 4.655, 4.601, 4.537, 4.805, 4.816, 4.654, 4.491, 4.614, 4.65, 4.847, 4.625, 4.704, 4.632, 4.827, 4.833, 4.565, 4.54, 4.674, 4.801, 4.743, 4.625, 4.934, 4.92, 4.838, 4.694, 4.803, 4.709, 4.747, 4.773, 4.6, 4.789, 4.588, 4.692, 4.744, 4.961, 4.531, 4.797, 4.774, 4.689, 4.638, 4.662, 4.957, 4.623, 4.489, 4.653, 4.766, 4.805, 4.569, 4.748, 4.648, 4.713, 4.648, 4.669, 4.892, 4.662, 4.87, 4.775, 4.679, 4.772, 4.588, 4.604, 4.6, 4.661, 4.715, 4.795, 4.558, 4.603, 4.736, 4.692, 4.697, 4.671]\n",
      "Val custom mae Error(all epochs): 0.8011600971221924 \n",
      " [12.381, 12.115, 11.704, 11.125, 10.627, 9.849, 9.305, 8.613, 8.35, 8.265, 7.519, 7.659, 8.333, 7.844, 6.887, 6.914, 7.424, 6.065, 7.093, 5.895, 5.794, 6.38, 6.56, 6.389, 6.008, 4.699, 7.133, 3.939, 7.44, 5.26, 5.687, 4.176, 6.042, 5.702, 6.06, 5.131, 4.546, 2.731, 7.334, 5.469, 5.455, 2.336, 5.188, 4.85, 4.595, 6.624, 4.669, 4.181, 3.24, 5.821, 5.527, 5.469, 5.005, 5.225, 6.366, 6.077, 3.921, 5.565, 5.666, 6.339, 2.749, 4.154, 4.255, 5.843, 5.272, 5.141, 4.576, 4.489, 5.59, 5.143, 4.327, 4.996, 4.855, 3.981, 4.858, 5.549, 3.311, 4.107, 4.098, 4.864, 5.562, 4.6, 4.974, 4.197, 3.747, 4.062, 5.051, 5.249, 4.531, 4.69, 4.319, 5.785, 3.903, 3.848, 4.622, 4.379, 6.586, 4.338, 3.491, 6.439, 5.046, 5.618, 5.047, 3.688, 4.82, 3.927, 4.418, 4.671, 4.841, 5.359, 5.377, 5.192, 4.193, 4.368, 4.502, 3.678, 3.592, 3.857, 4.295, 4.827, 3.655, 4.936, 3.746, 3.684, 4.227, 2.953, 4.7, 5.936, 3.852, 4.829, 4.427, 3.346, 3.951, 4.52, 3.203, 5.789, 4.652, 3.47, 4.632, 4.143, 3.377, 4.299, 3.801, 4.653, 3.119, 4.165, 2.775, 1.937, 4.641, 3.701, 4.554, 3.476, 4.691, 4.181, 3.182, 3.325, 3.078, 3.872, 2.758, 2.744, 3.682, 3.262, 1.486, 4.868, 4.354, 2.641, 4.441, 2.681, 3.818, 3.957, 3.879, 3.017, 3.903, 3.276, 4.196, 4.311, 3.316, 3.485, 4.812, 4.042, 3.439, 3.349, 3.993, 3.906, 3.651, 3.518, 3.929, 3.448, 4.188, 3.314, 5.195, 4.25, 4.456, 0.947, 3.743, 4.161, 3.668, 4.486, 4.61, 3.013, 3.616, 3.42, 4.702, 4.227, 2.455, 3.55, 4.555, 3.321, 4.412, 4.662, 3.228, 3.354, 2.802, 4.23, 3.163, 4.664, 4.659, 5.25, 2.696, 2.448, 3.683, 3.669, 3.812, 3.821, 3.547, 3.106, 3.793, 3.9, 3.568, 4.019, 3.735, 3.555, 4.972, 2.375, 4.396, 3.239, 4.053, 3.855, 4.878, 3.384, 2.731, 3.824, 4.18, 3.648, 3.236, 3.099, 4.439, 3.537, 3.869, 2.496, 0.801, 4.018, 4.59, 3.243, 3.832, 3.798, 3.843, 4.686, 3.311, 3.954, 3.696, 2.982, 2.003, 4.705, 3.201, 4.482, 2.492, 3.316, 3.812, 4.336, 4.756, 3.075, 3.278, 3.17, 4.48, 4.175, 3.685, 4.318, 2.689, 3.286, 3.533, 3.987, 4.903, 3.557, 4.409, 3.204, 2.351, 4.273, 3.419, 3.276, 4.082, 4.042, 4.501, 4.148, 2.462, 3.088, 3.665, 3.585, 2.713, 3.35]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Lambda: 10 , Time: 0:00:36\n",
      "Train Error(all epochs): 5.68642520904541 \n",
      " [8.107, 7.82, 7.384, 7.117, 6.981, 6.823, 6.677, 6.579, 6.519, 6.428, 6.397, 6.391, 6.356, 6.308, 6.286, 6.266, 6.257, 6.23, 6.217, 6.264, 6.204, 6.154, 6.12, 6.106, 6.107, 6.129, 6.081, 6.038, 6.048, 6.067, 6.055, 6.041, 6.032, 6.03, 6.01, 6.003, 6.013, 6.016, 5.944, 6.008, 5.986, 5.992, 5.945, 5.956, 5.942, 5.917, 5.933, 5.925, 5.989, 5.937, 5.962, 5.961, 5.949, 5.925, 5.917, 5.903, 5.913, 5.882, 5.874, 5.892, 5.902, 5.857, 5.888, 5.878, 5.879, 5.862, 5.891, 5.849, 5.852, 5.855, 5.904, 5.839, 5.877, 5.818, 5.836, 5.852, 5.829, 5.863, 5.828, 5.841, 5.866, 5.845, 5.813, 5.814, 5.846, 5.825, 5.821, 5.82, 5.841, 5.797, 5.836, 5.793, 5.813, 5.828, 5.81, 5.832, 5.804, 5.82, 5.822, 5.799, 5.82, 5.799, 5.817, 5.799, 5.788, 5.795, 5.806, 5.8, 5.786, 5.816, 5.761, 5.807, 5.752, 5.771, 5.803, 5.758, 5.807, 5.795, 5.79, 5.733, 5.79, 5.786, 5.784, 5.803, 5.745, 5.786, 5.773, 5.774, 5.745, 5.743, 5.749, 5.77, 5.733, 5.761, 5.793, 5.79, 5.771, 5.745, 5.769, 5.751, 5.768, 5.762, 5.777, 5.758, 5.755, 5.784, 5.764, 5.722, 5.778, 5.733, 5.815, 5.736, 5.76, 5.748, 5.742, 5.769, 5.74, 5.743, 5.735, 5.74, 5.786, 5.741, 5.737, 5.788, 5.777, 5.736, 5.72, 5.752, 5.78, 5.712, 5.766, 5.744, 5.754, 5.765, 5.722, 5.732, 5.739, 5.788, 5.747, 5.747, 5.732, 5.77, 5.738, 5.74, 5.752, 5.726, 5.764, 5.741, 5.722, 5.749, 5.698, 5.775, 5.758, 5.738, 5.7, 5.742, 5.741, 5.761, 5.74, 5.766, 5.73, 5.711, 5.747, 5.764, 5.742, 5.741, 5.711, 5.731, 5.751, 5.781, 5.727, 5.754, 5.707, 5.746, 5.74, 5.761, 5.719, 5.736, 5.774, 5.709, 5.713, 5.755, 5.759, 5.733, 5.735, 5.743, 5.723, 5.753, 5.716, 5.711, 5.725, 5.71, 5.75, 5.72, 5.71, 5.739, 5.731, 5.704, 5.725, 5.752, 5.718, 5.731, 5.713, 5.752, 5.761, 5.741, 5.751, 5.762, 5.77, 5.722, 5.733, 5.742, 5.782, 5.725, 5.722, 5.715, 5.744, 5.712, 5.719, 5.724, 5.733, 5.736, 5.743, 5.76, 5.765, 5.73, 5.702, 5.735, 5.694, 5.737, 5.751, 5.76, 5.741, 5.758, 5.7, 5.721, 5.697, 5.751, 5.699, 5.725, 5.721, 5.76, 5.686, 5.765, 5.726, 5.746, 5.721, 5.75, 5.726, 5.72, 5.723, 5.709, 5.761, 5.699, 5.731, 5.739, 5.721, 5.749, 5.739, 5.723]\n",
      "Val Error(all epochs): 5.411403656005859 \n",
      " [8.227, 8.154, 8.069, 7.936, 7.757, 7.595, 7.611, 7.068, 7.106, 6.814, 6.962, 6.858, 6.576, 6.525, 6.602, 6.348, 5.999, 6.123, 9.037, 6.132, 6.201, 6.38, 6.316, 6.371, 6.179, 5.974, 6.08, 6.561, 6.17, 6.243, 6.084, 6.646, 6.089, 6.024, 6.135, 6.026, 6.371, 7.497, 5.99, 6.035, 6.124, 6.904, 6.265, 5.998, 6.116, 5.838, 6.1, 6.064, 6.178, 6.055, 6.14, 5.919, 5.94, 5.697, 6.02, 5.702, 5.855, 5.701, 5.97, 7.149, 5.888, 5.71, 6.287, 5.845, 5.676, 5.823, 5.621, 6.034, 5.774, 5.618, 6.159, 5.704, 5.961, 5.559, 6.065, 5.968, 5.708, 5.864, 5.634, 5.786, 9.065, 5.697, 5.733, 5.705, 5.668, 5.593, 5.834, 5.667, 5.736, 5.735, 5.895, 6.559, 5.551, 5.567, 5.622, 5.742, 5.648, 6.027, 5.667, 5.652, 5.867, 5.704, 5.659, 5.62, 5.639, 5.676, 5.798, 5.76, 5.932, 5.63, 6.193, 6.134, 5.922, 5.773, 5.553, 5.489, 5.669, 5.712, 5.776, 5.584, 5.831, 5.521, 5.686, 5.8, 5.778, 5.495, 5.78, 5.767, 5.773, 5.591, 5.682, 5.618, 5.625, 5.7, 5.633, 5.591, 5.713, 5.865, 5.535, 5.596, 5.89, 5.645, 5.452, 5.628, 5.567, 5.596, 5.548, 5.645, 5.587, 5.573, 5.508, 5.519, 6.138, 5.539, 5.609, 5.641, 5.62, 5.716, 5.79, 5.564, 5.815, 5.635, 5.772, 5.53, 5.505, 5.469, 5.582, 5.727, 5.76, 5.674, 5.457, 5.739, 5.746, 5.537, 5.617, 5.461, 5.577, 5.42, 5.575, 5.525, 5.667, 5.47, 5.632, 5.578, 5.769, 5.963, 5.531, 5.538, 5.857, 5.511, 5.666, 5.489, 5.827, 5.671, 5.554, 5.662, 5.699, 5.477, 5.655, 5.51, 5.712, 5.604, 5.514, 5.56, 5.531, 5.601, 5.564, 5.803, 5.538, 5.619, 5.537, 5.543, 5.873, 5.627, 5.559, 5.522, 5.823, 5.651, 5.613, 5.539, 5.534, 5.529, 5.63, 5.66, 5.555, 5.674, 5.879, 5.609, 5.562, 5.475, 5.666, 5.604, 5.846, 5.737, 5.536, 5.475, 5.733, 5.702, 5.62, 5.516, 5.634, 5.697, 5.806, 5.82, 5.62, 5.526, 5.799, 5.627, 5.581, 5.597, 5.597, 5.607, 5.627, 5.711, 5.831, 5.637, 5.684, 5.576, 5.594, 5.634, 5.667, 5.59, 5.651, 5.716, 5.564, 5.621, 5.703, 5.445, 5.673, 5.638, 5.558, 5.649, 5.62, 5.578, 5.718, 5.646, 5.411, 5.64, 5.801, 5.418, 5.513, 5.494, 5.626, 5.498, 5.511, 5.806, 5.664, 5.593, 5.618, 5.588, 5.472, 5.44, 5.478, 5.568, 5.621, 5.736, 5.764, 5.509, 5.681, 5.625]\n",
      "Val custom mae Error(all epochs): 0.7620289325714111 \n",
      " [12.808, 12.638, 12.436, 12.12, 11.759, 11.63, 11.998, 10.886, 11.004, 10.507, 10.842, 10.384, 9.623, 9.818, 9.827, 8.887, 6.848, 5.989, 11.647, 8.444, 8.551, 8.811, 8.232, 9.476, 6.775, 6.257, 8.232, 9.065, 9.002, 8.157, 6.596, 10.085, 7.077, 8.108, 7.607, 6.203, 4.421, 0.762, 7.25, 5.94, 5.095, 4.221, 8.166, 8.114, 4.581, 6.852, 3.813, 5.665, 8.428, 4.172, 5.57, 5.038, 7.595, 5.705, 7.602, 5.799, 7.74, 5.896, 7.791, 5.309, 5.563, 7.03, 3.958, 6.027, 6.204, 7.43, 5.909, 4.143, 7.065, 6.268, 8.868, 6.441, 7.814, 5.368, 8.485, 8.057, 3.891, 7.102, 6.314, 7.105, 11.204, 5.931, 6.735, 5.763, 6.946, 6.198, 6.545, 6.655, 6.899, 7.088, 6.677, 3.098, 5.275, 4.982, 5.367, 6.996, 6.446, 7.897, 6.204, 6.754, 5.879, 5.611, 6.106, 5.737, 5.986, 5.446, 7.885, 6.243, 7.815, 6.608, 2.363, 2.617, 6.686, 4.627, 6.046, 5.057, 4.43, 5.403, 6.759, 5.385, 5.68, 4.902, 4.482, 5.235, 6.889, 5.505, 7.358, 7.575, 7.66, 3.303, 6.811, 5.558, 6.901, 5.578, 4.036, 6.07, 5.515, 6.101, 5.202, 4.389, 3.005, 6.797, 4.853, 4.786, 6.288, 4.641, 5.753, 6.017, 4.01, 5.463, 6.113, 5.926, 7.146, 5.097, 3.863, 6.913, 6.073, 7.223, 6.803, 5.421, 1.714, 6.98, 7.096, 4.781, 4.347, 5.52, 4.758, 6.783, 7.368, 6.847, 4.358, 6.712, 7.322, 6.51, 6.501, 4.238, 6.559, 4.405, 4.805, 4.295, 5.652, 4.141, 6.784, 6.643, 6.078, 7.875, 5.854, 5.587, 7.498, 3.449, 7.174, 5.275, 3.217, 7.402, 3.997, 6.649, 6.788, 5.036, 6.525, 5.002, 6.315, 4.944, 5.439, 4.933, 5.271, 5.928, 4.98, 5.102, 3.892, 6.482, 4.64, 5.793, 7.497, 6.462, 4.735, 5.103, 7.351, 5.285, 4.3, 5.688, 5.371, 3.847, 6.347, 6.07, 4.97, 6.943, 7.929, 2.836, 6.191, 4.168, 4.237, 5.236, 6.452, 6.783, 4.231, 5.091, 1.834, 6.8, 3.32, 5.87, 5.956, 3.106, 3.311, 6.715, 5.744, 6.042, 6.814, 6.977, 6.317, 4.247, 5.269, 4.974, 3.395, 6.775, 6.472, 5.603, 4.869, 5.071, 6.166, 2.748, 5.314, 4.55, 5.928, 7.33, 5.48, 5.932, 5.688, 4.945, 4.452, 5.156, 5.697, 6.422, 6.346, 5.039, 6.105, 5.496, 4.481, 2.125, 7.241, 4.432, 5.579, 3.442, 5.195, 3.415, 5.764, 7.226, 5.948, 5.785, 5.902, 3.623, 5.453, 3.578, 4.908, 4.066, 5.515, 7.09, 5.114, 2.234, 6.365, 6.963]\n",
      "\n",
      "Trainig set size: 2048 , Time: 0:03:00 , best_lambda: 10 , min_  error: 0.762\n",
      "Test starts:  2458 , ends:  32161\n",
      "929/929 [==============================] - 1s 477us/step\n",
      "total_power:  17.342045 , average_difference:  3.656685618471818\n",
      "\n",
      "\n",
      "\n",
      "number_samples: 4096 , New samples: 4096\n",
      "Validation size: 820 , starts: 4096 , ends: 4915\n",
      "\n",
      "Lambda: 0 , Time: 0:01:05\n",
      "Train Error(all epochs): 3.3284974098205566 \n",
      " [7.683, 6.291, 5.418, 5.114, 5.027, 4.941, 4.877, 4.819, 4.855, 4.802, 4.718, 4.736, 4.723, 4.726, 4.71, 4.698, 4.624, 4.662, 4.602, 4.579, 4.572, 4.604, 4.592, 4.515, 4.509, 4.495, 4.482, 4.486, 4.515, 4.487, 4.44, 4.451, 4.414, 4.4, 4.385, 4.361, 4.356, 4.331, 4.291, 4.319, 4.278, 4.251, 4.236, 4.225, 4.204, 4.262, 4.2, 4.172, 4.207, 4.197, 4.167, 4.192, 4.162, 4.144, 4.066, 4.115, 4.08, 4.132, 4.05, 4.068, 4.045, 4.093, 4.041, 4.031, 4.005, 4.027, 4.029, 4.058, 4.003, 3.987, 3.998, 3.988, 3.954, 3.927, 3.944, 3.932, 3.969, 3.944, 3.904, 3.89, 3.933, 3.884, 3.893, 3.867, 3.902, 3.871, 3.866, 3.831, 3.92, 3.832, 3.829, 3.83, 3.834, 3.82, 3.798, 3.786, 3.839, 3.799, 3.809, 3.783, 3.791, 3.796, 3.849, 3.755, 3.741, 3.779, 3.77, 3.781, 3.773, 3.757, 3.719, 3.754, 3.717, 3.671, 3.711, 3.682, 3.673, 3.774, 3.745, 3.708, 3.702, 3.687, 3.673, 3.686, 3.726, 3.701, 3.673, 3.69, 3.69, 3.665, 3.657, 3.647, 3.671, 3.632, 3.638, 3.657, 3.684, 3.618, 3.687, 3.678, 3.654, 3.661, 3.636, 3.647, 3.598, 3.629, 3.641, 3.607, 3.616, 3.654, 3.626, 3.619, 3.637, 3.579, 3.647, 3.621, 3.6, 3.598, 3.627, 3.566, 3.573, 3.626, 3.577, 3.598, 3.536, 3.57, 3.561, 3.559, 3.56, 3.542, 3.531, 3.534, 3.574, 3.536, 3.538, 3.513, 3.544, 3.523, 3.519, 3.551, 3.569, 3.58, 3.516, 3.536, 3.552, 3.563, 3.544, 3.55, 3.492, 3.503, 3.513, 3.509, 3.516, 3.545, 3.499, 3.514, 3.495, 3.527, 3.488, 3.501, 3.488, 3.481, 3.478, 3.494, 3.527, 3.526, 3.489, 3.529, 3.483, 3.443, 3.489, 3.462, 3.488, 3.473, 3.478, 3.451, 3.507, 3.465, 3.454, 3.478, 3.463, 3.472, 3.458, 3.49, 3.485, 3.491, 3.457, 3.46, 3.425, 3.432, 3.505, 3.462, 3.462, 3.456, 3.451, 3.41, 3.467, 3.451, 3.463, 3.422, 3.476, 3.422, 3.435, 3.433, 3.425, 3.411, 3.41, 3.441, 3.428, 3.442, 3.43, 3.41, 3.42, 3.43, 3.402, 3.474, 3.46, 3.401, 3.44, 3.389, 3.416, 3.413, 3.413, 3.402, 3.444, 3.418, 3.4, 3.436, 3.406, 3.44, 3.406, 3.441, 3.394, 3.368, 3.405, 3.416, 3.381, 3.43, 3.48, 3.443, 3.472, 3.404, 3.402, 3.422, 3.349, 3.375, 3.388, 3.419, 3.36, 3.328, 3.376, 3.361, 3.399, 3.389, 3.416, 3.371, 3.386, 3.348, 3.354, 3.397]\n",
      "Val Error(all epochs): 4.233791828155518 \n",
      " [6.738, 5.104, 4.551, 4.498, 4.317, 4.317, 4.304, 4.287, 4.323, 4.351, 4.272, 4.234, 4.332, 4.342, 4.267, 4.28, 4.38, 4.38, 4.404, 4.331, 4.372, 4.4, 4.343, 4.382, 4.371, 4.466, 4.395, 4.452, 4.441, 4.425, 4.394, 4.436, 4.453, 4.448, 4.549, 4.432, 4.444, 4.483, 4.453, 4.52, 4.57, 4.469, 4.645, 4.577, 4.536, 4.575, 4.539, 4.672, 4.546, 4.634, 4.66, 4.587, 4.546, 4.604, 4.557, 4.61, 4.651, 4.689, 4.729, 4.667, 4.59, 4.589, 4.676, 4.588, 4.653, 4.651, 4.652, 4.664, 4.715, 4.701, 4.745, 4.656, 4.687, 4.675, 4.646, 4.7, 4.677, 4.83, 4.719, 4.752, 4.676, 4.666, 4.715, 4.714, 4.654, 4.689, 4.678, 4.735, 4.694, 4.653, 4.711, 4.739, 4.676, 4.7, 4.759, 4.709, 4.706, 4.786, 4.757, 4.709, 4.749, 4.849, 4.686, 4.732, 4.809, 4.801, 4.769, 4.775, 4.792, 4.744, 4.761, 4.798, 4.724, 4.815, 4.724, 4.763, 4.782, 4.74, 4.772, 4.773, 4.694, 4.847, 4.776, 4.799, 4.805, 4.813, 4.75, 4.786, 4.748, 4.743, 4.762, 4.804, 4.734, 4.747, 4.852, 4.781, 4.804, 4.837, 4.912, 4.804, 4.79, 4.814, 4.84, 4.755, 4.804, 4.916, 4.772, 4.758, 4.771, 4.787, 4.794, 4.809, 4.869, 4.83, 4.821, 4.871, 4.818, 4.831, 4.949, 4.832, 4.804, 4.82, 4.794, 4.853, 4.846, 4.845, 4.819, 4.844, 4.872, 4.864, 4.785, 4.784, 4.753, 4.808, 4.868, 4.807, 4.827, 4.886, 4.784, 4.876, 4.876, 4.893, 4.848, 4.833, 4.876, 4.831, 4.859, 4.882, 4.766, 4.833, 4.813, 4.828, 4.811, 4.803, 4.858, 4.832, 4.802, 4.784, 4.846, 4.827, 4.839, 4.843, 4.898, 4.939, 4.848, 4.89, 4.842, 4.857, 4.858, 4.892, 4.888, 4.848, 4.863, 4.854, 4.918, 4.869, 4.827, 4.825, 4.864, 4.844, 4.858, 4.845, 4.933, 4.954, 4.836, 4.899, 4.852, 4.84, 4.89, 4.898, 4.868, 4.902, 4.853, 4.92, 4.887, 4.856, 4.891, 4.878, 4.926, 4.978, 4.917, 4.801, 4.916, 4.88, 4.851, 4.865, 4.917, 4.854, 4.886, 4.882, 4.892, 4.833, 4.842, 4.919, 4.836, 4.914, 4.848, 4.865, 4.828, 4.824, 4.898, 4.866, 4.796, 4.897, 4.886, 4.809, 4.866, 4.839, 4.814, 4.914, 4.869, 4.814, 4.828, 4.863, 4.876, 4.831, 4.832, 4.914, 4.808, 4.877, 4.83, 4.822, 4.816, 4.869, 4.826, 4.873, 4.849, 4.823, 4.883, 4.871, 4.902, 4.838, 4.832, 4.859, 4.868, 4.86, 4.879, 4.861, 4.853, 4.884]\n",
      "Val custom mae Error(all epochs): 2.052670955657959 \n",
      " [9.669, 5.837, 2.917, 3.46, 2.513, 3.357, 3.032, 3.091, 3.556, 3.213, 2.876, 2.982, 3.231, 3.646, 3.2, 3.183, 3.254, 3.627, 3.444, 3.155, 3.192, 3.315, 2.998, 3.078, 2.529, 3.304, 3.092, 3.212, 3.374, 3.18, 2.884, 2.989, 2.655, 2.997, 2.944, 2.97, 2.866, 2.812, 2.581, 2.818, 2.849, 2.613, 4.184, 3.462, 2.899, 2.751, 3.126, 3.365, 3.537, 3.058, 2.858, 2.403, 2.286, 2.955, 2.534, 2.306, 2.897, 2.959, 2.775, 3.008, 2.619, 2.683, 3.185, 2.911, 2.813, 2.729, 2.911, 3.108, 2.871, 2.861, 3.34, 2.581, 2.715, 2.674, 2.54, 2.63, 2.621, 3.521, 2.86, 2.808, 2.604, 2.647, 2.49, 2.633, 2.778, 2.697, 2.458, 2.901, 2.4, 2.745, 2.59, 2.878, 2.695, 2.585, 2.974, 2.473, 2.443, 2.719, 2.786, 2.626, 3.172, 4.463, 3.408, 3.069, 3.949, 3.84, 2.783, 2.664, 3.199, 2.666, 2.741, 2.745, 2.761, 2.475, 2.757, 2.894, 2.96, 2.665, 2.991, 2.815, 2.603, 3.227, 2.915, 3.206, 3.465, 3.226, 2.758, 2.519, 2.252, 2.752, 3.188, 2.576, 2.839, 2.704, 2.97, 3.265, 2.853, 3.166, 2.74, 2.92, 2.573, 2.444, 2.671, 2.2, 2.317, 2.955, 2.194, 2.485, 2.45, 2.405, 2.511, 2.497, 2.499, 2.588, 2.573, 2.225, 2.566, 2.552, 2.981, 2.736, 2.499, 2.79, 2.201, 2.824, 2.625, 2.865, 2.515, 2.719, 2.654, 2.976, 2.789, 2.332, 2.184, 2.345, 2.765, 2.634, 2.595, 2.622, 2.488, 2.52, 2.869, 3.023, 2.553, 2.827, 2.795, 2.053, 2.231, 2.496, 2.28, 2.731, 2.75, 2.686, 2.505, 2.559, 2.533, 2.482, 2.306, 2.251, 2.199, 2.531, 2.519, 2.679, 2.744, 2.359, 2.482, 2.626, 2.542, 2.673, 2.648, 2.963, 2.76, 2.885, 2.682, 2.38, 2.627, 2.375, 2.283, 2.385, 2.432, 2.515, 2.32, 2.393, 2.778, 2.847, 2.195, 2.738, 2.51, 2.374, 2.723, 2.578, 2.666, 2.654, 2.304, 2.627, 2.953, 2.539, 2.652, 2.549, 2.541, 2.512, 2.522, 2.276, 2.241, 2.36, 2.704, 2.8, 2.355, 2.3, 2.589, 2.659, 2.666, 2.339, 2.691, 3.058, 2.926, 3.039, 2.598, 2.659, 2.593, 2.494, 2.234, 2.463, 2.493, 2.719, 2.524, 2.49, 2.697, 2.822, 2.523, 3.214, 2.607, 2.556, 2.723, 3.114, 2.852, 3.109, 2.869, 3.271, 3.219, 2.698, 3.018, 3.094, 3.059, 3.007, 2.706, 2.855, 3.275, 3.408, 3.13, 3.648, 3.552, 3.071, 3.348, 3.557, 3.596, 3.701, 3.799, 3.548, 3.86, 3.636]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Lambda: 0.01 , Time: 0:01:07\n",
      "Train Error(all epochs): 3.683232307434082 \n",
      " [7.62, 6.208, 5.368, 5.082, 4.988, 4.926, 4.886, 4.825, 4.838, 4.767, 4.746, 4.729, 4.729, 4.696, 4.684, 4.665, 4.655, 4.645, 4.608, 4.631, 4.623, 4.559, 4.574, 4.576, 4.513, 4.514, 4.548, 4.505, 4.49, 4.482, 4.468, 4.427, 4.413, 4.478, 4.423, 4.387, 4.422, 4.355, 4.389, 4.326, 4.324, 4.316, 4.327, 4.296, 4.315, 4.268, 4.283, 4.255, 4.251, 4.267, 4.261, 4.204, 4.244, 4.2, 4.198, 4.199, 4.205, 4.162, 4.141, 4.167, 4.14, 4.136, 4.117, 4.146, 4.109, 4.132, 4.103, 4.08, 4.119, 4.09, 4.076, 4.103, 4.086, 4.058, 4.104, 4.1, 4.076, 4.043, 4.059, 4.074, 4.029, 4.042, 4.025, 4.012, 3.984, 4.059, 3.969, 4.023, 3.969, 4.053, 3.969, 3.977, 3.963, 3.999, 3.976, 3.937, 4.029, 3.96, 3.941, 3.98, 3.953, 3.92, 3.967, 3.898, 3.937, 3.925, 3.932, 3.948, 3.907, 3.964, 3.938, 3.934, 3.925, 3.937, 3.956, 3.848, 3.915, 3.865, 3.923, 3.889, 3.919, 3.885, 3.848, 3.879, 3.871, 3.886, 3.887, 3.9, 3.913, 3.9, 3.862, 3.894, 3.903, 3.899, 3.839, 3.813, 3.863, 3.884, 3.856, 3.867, 3.868, 3.893, 3.862, 3.875, 3.814, 3.828, 3.859, 3.83, 3.846, 3.813, 3.863, 3.818, 3.852, 3.83, 3.807, 3.857, 3.881, 3.809, 3.877, 3.854, 3.838, 3.84, 3.813, 3.866, 3.838, 3.81, 3.839, 3.839, 3.796, 3.792, 3.789, 3.815, 3.808, 3.819, 3.856, 3.803, 3.776, 3.849, 3.774, 3.816, 3.786, 3.834, 3.802, 3.772, 3.851, 3.776, 3.787, 3.81, 3.832, 3.791, 3.831, 3.847, 3.855, 3.769, 3.794, 3.84, 3.794, 3.813, 3.796, 3.797, 3.792, 3.785, 3.779, 3.76, 3.736, 3.794, 3.755, 3.771, 3.78, 3.759, 3.802, 3.736, 3.813, 3.777, 3.772, 3.751, 3.794, 3.806, 3.764, 3.753, 3.753, 3.786, 3.775, 3.759, 3.737, 3.729, 3.757, 3.745, 3.736, 3.789, 3.789, 3.77, 3.723, 3.759, 3.76, 3.755, 3.744, 3.71, 3.739, 3.714, 3.759, 3.761, 3.775, 3.789, 3.741, 3.75, 3.782, 3.773, 3.776, 3.759, 3.735, 3.745, 3.729, 3.725, 3.744, 3.787, 3.778, 3.733, 3.785, 3.739, 3.7, 3.731, 3.716, 3.721, 3.781, 3.743, 3.747, 3.755, 3.699, 3.747, 3.712, 3.716, 3.748, 3.693, 3.708, 3.759, 3.719, 3.744, 3.73, 3.683, 3.726, 3.753, 3.739, 3.706, 3.718, 3.71, 3.704, 3.716, 3.743, 3.701, 3.715, 3.779, 3.729, 3.752, 3.76, 3.721, 3.761, 3.717, 3.764, 3.704]\n",
      "Val Error(all epochs): 4.235969066619873 \n",
      " [6.51, 5.079, 4.63, 4.37, 4.3, 4.277, 4.3, 4.236, 4.359, 4.343, 4.295, 4.293, 4.284, 4.372, 4.375, 4.336, 4.349, 4.365, 4.506, 4.371, 4.369, 4.346, 4.443, 4.365, 4.35, 4.485, 4.445, 4.455, 4.451, 4.407, 4.54, 4.417, 4.464, 4.5, 4.477, 4.458, 4.441, 4.507, 4.459, 4.479, 4.399, 4.495, 4.499, 4.453, 4.458, 4.557, 4.499, 4.484, 4.491, 4.516, 4.54, 4.528, 4.51, 4.543, 4.614, 4.509, 4.537, 4.542, 4.555, 4.558, 4.501, 4.647, 4.625, 4.552, 4.596, 4.586, 4.573, 4.6, 4.59, 4.573, 4.631, 4.673, 4.716, 4.659, 4.614, 4.657, 4.57, 4.69, 4.601, 4.667, 4.724, 4.601, 4.692, 4.59, 4.685, 4.712, 4.711, 4.678, 4.727, 4.73, 4.61, 4.677, 4.721, 4.683, 4.603, 4.745, 4.694, 4.734, 4.593, 4.609, 4.656, 4.822, 4.695, 4.66, 4.601, 4.725, 4.619, 4.623, 4.695, 4.792, 4.685, 4.615, 4.69, 4.663, 4.723, 4.718, 4.616, 4.656, 4.734, 4.699, 4.705, 4.632, 4.658, 4.664, 4.671, 4.64, 4.735, 4.666, 4.674, 4.675, 4.644, 4.777, 4.665, 4.716, 4.625, 4.646, 4.61, 4.821, 4.678, 4.743, 4.738, 4.66, 4.812, 4.597, 4.76, 4.713, 4.648, 4.702, 4.742, 4.73, 4.691, 4.764, 4.714, 4.77, 4.694, 4.679, 4.738, 4.676, 4.803, 4.69, 4.742, 4.695, 4.616, 4.667, 4.735, 4.691, 4.75, 4.726, 4.651, 4.708, 4.696, 4.854, 4.723, 4.714, 4.741, 4.68, 4.743, 4.787, 4.754, 4.739, 4.75, 4.692, 4.669, 4.695, 4.624, 4.709, 4.651, 4.588, 4.72, 4.724, 4.649, 4.669, 4.684, 4.743, 4.607, 4.641, 4.718, 4.738, 4.763, 4.757, 4.745, 4.719, 4.643, 4.732, 4.747, 4.699, 4.68, 4.639, 4.713, 4.785, 4.668, 4.681, 4.733, 4.662, 4.72, 5.125, 4.716, 4.651, 4.679, 4.646, 4.83, 4.687, 4.712, 4.813, 4.735, 4.806, 4.705, 4.734, 4.716, 4.772, 4.723, 4.767, 4.744, 4.673, 4.72, 4.727, 4.814, 4.657, 4.66, 4.688, 4.76, 4.714, 4.712, 4.811, 4.755, 4.735, 4.694, 4.728, 4.858, 4.729, 4.638, 4.706, 4.706, 4.703, 4.708, 4.757, 4.741, 4.855, 4.681, 4.752, 4.729, 4.765, 4.718, 4.733, 4.791, 4.728, 4.766, 4.68, 4.736, 4.763, 4.654, 4.76, 4.66, 4.788, 4.753, 4.733, 4.712, 4.788, 4.933, 4.726, 4.734, 4.702, 4.693, 4.736, 4.694, 4.741, 4.726, 4.726, 4.683, 4.742, 4.728, 4.701, 4.744, 4.747, 4.723, 4.799, 4.918, 4.711, 4.723, 4.788]\n",
      "Val custom mae Error(all epochs): 1.7454893589019775 \n",
      " [8.848, 5.892, 3.249, 2.808, 3.137, 2.724, 3.526, 3.453, 3.949, 3.514, 3.602, 3.657, 3.424, 3.913, 3.679, 3.367, 3.63, 3.82, 3.812, 3.767, 3.299, 3.444, 3.787, 3.19, 3.454, 3.603, 3.61, 2.919, 3.378, 3.309, 3.137, 3.272, 2.84, 3.483, 3.601, 3.255, 3.014, 3.428, 3.421, 3.159, 2.908, 3.353, 3.423, 3.289, 3.038, 3.44, 3.366, 2.805, 2.791, 3.244, 3.02, 2.859, 2.163, 3.038, 2.99, 2.809, 2.601, 3.109, 2.953, 2.837, 2.646, 3.212, 3.201, 2.859, 2.703, 2.807, 2.808, 3.11, 2.919, 2.999, 3.191, 2.895, 3.337, 3.089, 2.957, 2.829, 2.292, 3.514, 2.854, 2.879, 2.747, 2.648, 2.911, 2.596, 3.027, 3.215, 3.062, 3.134, 2.96, 3.077, 2.575, 3.029, 2.649, 3.193, 2.995, 3.116, 2.824, 2.43, 2.69, 2.544, 2.635, 3.439, 2.943, 2.442, 2.279, 3.189, 2.603, 2.421, 2.94, 2.745, 2.492, 2.776, 2.625, 2.927, 3.141, 2.731, 2.429, 2.854, 2.564, 2.853, 2.858, 2.642, 2.449, 2.694, 3.013, 2.376, 3.368, 2.321, 2.672, 2.679, 2.736, 3.075, 2.288, 2.963, 2.345, 2.156, 2.72, 3.408, 2.463, 2.931, 2.239, 2.655, 2.741, 2.436, 2.764, 2.428, 2.367, 2.219, 2.605, 2.879, 2.918, 2.375, 2.44, 2.139, 2.47, 2.77, 2.964, 2.73, 2.787, 2.054, 2.499, 2.574, 1.966, 2.821, 2.825, 2.248, 3.303, 3.092, 2.139, 3.157, 2.66, 2.992, 2.632, 3.25, 2.266, 2.469, 2.858, 2.512, 2.437, 2.682, 2.919, 2.355, 2.262, 2.716, 2.266, 3.077, 2.561, 2.523, 3.039, 3.073, 2.37, 2.527, 2.782, 2.718, 2.176, 2.189, 2.33, 2.929, 2.537, 2.19, 2.567, 2.745, 2.456, 1.745, 3.123, 2.31, 2.524, 2.703, 3.157, 2.722, 2.359, 2.665, 2.616, 2.584, 2.135, 4.068, 2.89, 2.074, 2.121, 2.025, 3.01, 2.655, 2.389, 3.275, 2.458, 2.575, 1.879, 1.778, 2.605, 3.035, 2.706, 3.046, 2.573, 1.938, 2.009, 2.441, 3.271, 2.328, 2.447, 2.745, 2.184, 2.723, 1.966, 2.058, 2.634, 2.99, 2.643, 2.657, 3.466, 2.95, 2.285, 2.435, 2.649, 2.702, 2.425, 2.605, 2.579, 2.35, 1.924, 2.801, 3.057, 2.967, 2.572, 2.911, 2.421, 2.75, 2.544, 2.251, 2.687, 2.637, 1.907, 2.669, 2.536, 3.381, 2.2, 2.756, 2.056, 2.479, 3.508, 2.315, 2.012, 2.771, 2.594, 2.83, 2.296, 2.47, 3.051, 2.214, 2.687, 2.829, 2.679, 2.151, 2.398, 3.082, 2.294, 2.967, 2.82, 2.614, 2.73, 3.082]\n",
      "\n",
      "Lambda: 0.1 , Time: 0:01:07\n",
      "Train Error(all epochs): 4.322777271270752 \n",
      " [7.724, 6.383, 5.481, 5.106, 5.051, 4.999, 4.94, 4.926, 4.914, 4.874, 4.883, 4.886, 4.857, 4.876, 4.822, 4.824, 4.832, 4.818, 4.842, 4.814, 4.822, 4.817, 4.772, 4.786, 4.792, 4.755, 4.78, 4.764, 4.766, 4.776, 4.742, 4.743, 4.699, 4.711, 4.723, 4.732, 4.72, 4.688, 4.715, 4.679, 4.662, 4.716, 4.681, 4.711, 4.66, 4.689, 4.635, 4.66, 4.653, 4.623, 4.681, 4.616, 4.669, 4.648, 4.638, 4.643, 4.64, 4.612, 4.622, 4.647, 4.624, 4.6, 4.607, 4.65, 4.625, 4.57, 4.581, 4.609, 4.603, 4.602, 4.607, 4.599, 4.639, 4.615, 4.604, 4.578, 4.585, 4.569, 4.571, 4.554, 4.552, 4.584, 4.582, 4.552, 4.561, 4.559, 4.562, 4.56, 4.534, 4.536, 4.541, 4.533, 4.571, 4.594, 4.514, 4.535, 4.56, 4.555, 4.512, 4.544, 4.524, 4.558, 4.519, 4.559, 4.526, 4.5, 4.54, 4.497, 4.538, 4.508, 4.516, 4.512, 4.534, 4.535, 4.485, 4.512, 4.537, 4.526, 4.518, 4.54, 4.506, 4.508, 4.476, 4.52, 4.505, 4.475, 4.503, 4.477, 4.506, 4.409, 4.452, 4.494, 4.448, 4.489, 4.494, 4.499, 4.504, 4.47, 4.463, 4.481, 4.445, 4.441, 4.448, 4.466, 4.486, 4.468, 4.465, 4.446, 4.496, 4.44, 4.418, 4.439, 4.47, 4.463, 4.436, 4.429, 4.425, 4.463, 4.387, 4.469, 4.441, 4.438, 4.434, 4.429, 4.411, 4.438, 4.434, 4.401, 4.402, 4.459, 4.404, 4.431, 4.397, 4.421, 4.435, 4.428, 4.422, 4.425, 4.403, 4.407, 4.441, 4.363, 4.39, 4.376, 4.425, 4.422, 4.395, 4.366, 4.378, 4.388, 4.38, 4.404, 4.373, 4.416, 4.394, 4.387, 4.455, 4.407, 4.383, 4.431, 4.372, 4.359, 4.423, 4.404, 4.403, 4.38, 4.393, 4.391, 4.345, 4.404, 4.408, 4.404, 4.373, 4.35, 4.402, 4.429, 4.392, 4.402, 4.431, 4.417, 4.404, 4.382, 4.366, 4.414, 4.402, 4.37, 4.433, 4.396, 4.365, 4.353, 4.413, 4.378, 4.337, 4.394, 4.345, 4.4, 4.389, 4.406, 4.368, 4.362, 4.344, 4.355, 4.411, 4.41, 4.383, 4.395, 4.35, 4.387, 4.352, 4.357, 4.373, 4.34, 4.359, 4.366, 4.419, 4.383, 4.345, 4.373, 4.365, 4.385, 4.384, 4.372, 4.356, 4.386, 4.402, 4.382, 4.332, 4.37, 4.33, 4.323, 4.385, 4.354, 4.362, 4.399, 4.404, 4.375, 4.365, 4.376, 4.336, 4.407, 4.37, 4.329, 4.326, 4.357, 4.364, 4.361, 4.353, 4.394, 4.363, 4.377, 4.347, 4.328, 4.39, 4.327, 4.336, 4.391, 4.35, 4.351, 4.352, 4.392]\n",
      "Val Error(all epochs): 4.3025102615356445 \n",
      " [6.757, 5.538, 4.807, 4.578, 4.625, 4.614, 4.691, 4.604, 4.5, 4.508, 4.561, 4.55, 4.52, 4.557, 4.553, 4.479, 4.447, 4.379, 4.429, 4.485, 4.508, 4.565, 4.505, 4.508, 4.503, 4.52, 4.435, 4.448, 4.438, 4.523, 4.369, 4.557, 4.393, 4.402, 4.477, 4.439, 4.547, 4.384, 4.445, 4.435, 4.374, 4.449, 4.399, 4.422, 4.535, 4.35, 4.492, 4.502, 4.591, 4.572, 4.458, 4.303, 4.432, 4.363, 4.444, 4.474, 4.562, 4.607, 4.499, 4.39, 4.449, 4.521, 4.316, 4.397, 4.335, 4.561, 4.459, 4.498, 4.461, 4.394, 4.397, 4.419, 4.37, 4.569, 4.626, 4.765, 4.435, 4.53, 4.495, 4.554, 4.62, 4.46, 4.524, 4.439, 4.417, 4.449, 4.536, 4.569, 4.414, 4.746, 4.489, 4.655, 4.395, 4.51, 4.448, 4.626, 4.555, 4.489, 4.382, 4.505, 4.488, 4.508, 4.494, 4.373, 4.428, 4.829, 4.525, 4.47, 4.468, 4.524, 4.759, 4.587, 4.442, 4.464, 4.55, 4.557, 4.455, 4.433, 4.412, 4.488, 4.378, 4.527, 4.473, 4.562, 4.46, 4.523, 4.726, 4.513, 4.467, 4.482, 4.507, 4.461, 4.466, 4.468, 4.549, 4.488, 4.759, 4.722, 4.709, 4.593, 4.489, 4.747, 4.532, 4.724, 4.509, 4.685, 4.427, 4.592, 4.486, 4.756, 4.514, 4.526, 4.458, 4.481, 4.627, 4.685, 4.545, 4.565, 4.453, 4.479, 4.643, 4.587, 4.55, 4.648, 4.467, 4.6, 4.533, 4.428, 4.573, 4.518, 4.593, 4.503, 4.543, 4.537, 4.563, 4.449, 4.597, 4.647, 4.515, 4.427, 4.755, 4.472, 4.537, 4.469, 4.481, 4.51, 4.588, 4.789, 4.533, 4.634, 4.572, 4.438, 4.591, 4.524, 4.576, 4.56, 4.521, 4.501, 4.493, 4.545, 4.561, 4.57, 4.476, 4.471, 4.515, 4.663, 4.509, 4.639, 4.604, 4.611, 4.459, 4.569, 4.475, 4.538, 4.581, 4.62, 4.512, 4.437, 4.463, 4.471, 4.513, 4.455, 4.561, 4.615, 4.458, 4.63, 4.577, 4.569, 4.559, 4.452, 4.534, 4.679, 4.606, 4.572, 4.595, 4.493, 4.535, 4.729, 4.379, 4.598, 4.582, 4.494, 4.518, 4.529, 4.551, 4.528, 4.645, 4.482, 4.583, 4.543, 4.532, 4.6, 4.551, 4.586, 4.524, 4.644, 4.596, 4.584, 4.547, 4.568, 4.641, 4.572, 4.515, 4.498, 4.503, 4.475, 4.499, 4.496, 4.434, 4.538, 4.6, 4.565, 4.47, 4.613, 4.636, 4.655, 4.507, 4.507, 4.6, 4.649, 4.569, 4.502, 4.456, 4.584, 4.542, 4.446, 4.459, 4.519, 4.547, 4.527, 4.506, 4.585, 4.488, 4.606, 4.636, 4.571, 4.639, 4.623, 4.441, 4.554]\n",
      "Val custom mae Error(all epochs): 1.0143762826919556 \n",
      " [9.223, 7.541, 5.611, 4.906, 4.675, 4.42, 5.156, 4.951, 3.92, 4.769, 4.471, 4.858, 4.659, 4.374, 5.009, 4.576, 4.331, 3.832, 4.54, 4.3, 4.179, 3.951, 4.325, 4.665, 4.624, 4.286, 4.239, 4.049, 3.752, 4.336, 3.261, 4.353, 3.849, 3.606, 4.188, 3.643, 4.715, 3.502, 3.757, 3.748, 3.501, 2.9, 3.056, 3.965, 4.294, 3.016, 3.653, 3.988, 4.002, 3.997, 4.179, 2.704, 3.672, 3.355, 4.08, 3.744, 4.206, 4.415, 3.219, 3.778, 3.397, 4.283, 2.96, 3.573, 2.966, 4.136, 2.934, 3.989, 3.734, 3.234, 3.204, 3.67, 3.437, 3.363, 4.055, 4.141, 3.318, 4.18, 4.017, 2.634, 4.154, 3.629, 3.792, 3.239, 2.69, 3.059, 3.826, 3.291, 3.554, 4.587, 3.51, 4.357, 2.839, 3.383, 3.241, 3.95, 2.815, 1.944, 2.411, 2.946, 3.538, 1.715, 2.599, 3.264, 3.049, 1.014, 3.683, 3.278, 2.946, 2.893, 4.279, 3.534, 1.846, 3.418, 3.143, 1.844, 3.508, 3.256, 2.376, 3.682, 2.803, 3.446, 2.993, 2.774, 3.29, 3.052, 4.699, 3.557, 2.593, 2.635, 2.703, 3.098, 1.915, 3.446, 2.573, 2.952, 3.945, 2.802, 3.322, 3.555, 2.672, 4.369, 3.152, 1.174, 2.96, 3.535, 2.709, 2.89, 3.131, 2.074, 3.431, 3.726, 3.392, 2.278, 3.304, 3.452, 2.773, 3.623, 2.823, 3.467, 3.815, 3.562, 3.085, 4.043, 3.179, 3.265, 3.356, 2.985, 2.282, 2.914, 3.701, 3.332, 4.043, 3.005, 3.695, 2.062, 3.389, 3.901, 2.204, 2.342, 4.54, 3.065, 3.505, 3.018, 3.157, 2.737, 3.759, 4.37, 3.176, 3.704, 2.77, 2.259, 3.554, 3.0, 3.444, 2.516, 3.257, 2.612, 3.119, 1.801, 3.072, 3.296, 3.029, 2.446, 3.221, 3.629, 3.286, 3.006, 2.04, 3.648, 2.812, 2.507, 2.977, 3.568, 3.841, 3.494, 2.665, 2.533, 3.286, 3.474, 2.842, 3.175, 3.744, 3.38, 2.23, 2.778, 2.551, 3.839, 3.178, 2.232, 3.539, 1.885, 3.351, 2.903, 3.556, 3.156, 3.156, 2.755, 2.394, 3.313, 3.589, 3.042, 3.34, 2.58, 2.848, 3.231, 3.517, 2.73, 2.157, 2.768, 3.035, 3.299, 3.868, 2.974, 2.955, 3.758, 3.887, 3.108, 2.33, 3.259, 2.383, 3.642, 3.149, 2.727, 2.778, 3.639, 2.786, 3.028, 2.492, 3.209, 2.531, 2.865, 2.378, 3.775, 3.595, 3.451, 2.422, 2.908, 2.887, 2.882, 2.955, 2.734, 2.695, 3.722, 2.967, 2.657, 2.791, 2.422, 1.907, 3.188, 2.267, 2.216, 3.25, 3.235, 3.153, 2.978, 3.612, 3.287, 2.427, 3.277]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Lambda: 1 , Time: 0:01:06\n",
      "Train Error(all epochs): 4.7012739181518555 \n",
      " [7.634, 6.268, 5.672, 5.462, 5.324, 5.313, 5.257, 5.202, 5.198, 5.186, 5.119, 5.172, 5.146, 5.15, 5.135, 5.166, 5.094, 5.115, 5.104, 5.127, 5.078, 5.069, 5.113, 5.083, 5.081, 5.065, 5.099, 5.064, 5.094, 5.099, 5.076, 5.034, 5.031, 5.055, 5.025, 5.012, 5.027, 5.02, 5.022, 4.996, 5.025, 4.972, 5.001, 4.998, 4.994, 4.971, 5.0, 4.967, 4.995, 4.966, 4.959, 4.972, 4.935, 4.943, 4.976, 4.897, 4.927, 4.967, 4.919, 4.931, 4.906, 4.951, 4.906, 4.911, 4.923, 4.864, 4.876, 4.922, 4.882, 4.892, 4.861, 4.917, 4.876, 4.877, 4.862, 4.889, 4.895, 4.889, 4.853, 4.817, 4.868, 4.869, 4.857, 4.862, 4.871, 4.876, 4.814, 4.885, 4.863, 4.8, 4.83, 4.811, 4.836, 4.831, 4.841, 4.817, 4.823, 4.816, 4.81, 4.848, 4.813, 4.84, 4.812, 4.851, 4.819, 4.823, 4.799, 4.832, 4.821, 4.811, 4.813, 4.774, 4.803, 4.825, 4.815, 4.799, 4.831, 4.783, 4.777, 4.804, 4.815, 4.815, 4.814, 4.775, 4.792, 4.773, 4.804, 4.753, 4.803, 4.799, 4.788, 4.76, 4.799, 4.794, 4.774, 4.757, 4.774, 4.776, 4.763, 4.796, 4.764, 4.766, 4.781, 4.784, 4.745, 4.781, 4.8, 4.786, 4.803, 4.745, 4.77, 4.786, 4.783, 4.786, 4.765, 4.762, 4.758, 4.793, 4.773, 4.794, 4.776, 4.761, 4.768, 4.763, 4.764, 4.792, 4.787, 4.774, 4.763, 4.757, 4.77, 4.753, 4.793, 4.773, 4.777, 4.758, 4.761, 4.747, 4.731, 4.775, 4.794, 4.77, 4.754, 4.764, 4.786, 4.776, 4.746, 4.726, 4.818, 4.791, 4.744, 4.769, 4.755, 4.759, 4.794, 4.777, 4.728, 4.775, 4.72, 4.755, 4.745, 4.771, 4.741, 4.722, 4.736, 4.758, 4.752, 4.758, 4.769, 4.758, 4.751, 4.726, 4.711, 4.755, 4.745, 4.732, 4.764, 4.769, 4.763, 4.748, 4.743, 4.762, 4.746, 4.739, 4.751, 4.75, 4.759, 4.747, 4.745, 4.743, 4.731, 4.773, 4.762, 4.756, 4.776, 4.749, 4.743, 4.751, 4.756, 4.771, 4.758, 4.701, 4.762, 4.754, 4.773, 4.758, 4.755, 4.735, 4.752, 4.749, 4.738, 4.751, 4.723, 4.781, 4.754, 4.723, 4.773, 4.734, 4.732, 4.735, 4.748, 4.766, 4.73, 4.737, 4.75, 4.77, 4.745, 4.726, 4.779, 4.765, 4.723, 4.756, 4.727, 4.728, 4.728, 4.741, 4.753, 4.737, 4.719, 4.745, 4.746, 4.733, 4.765, 4.727, 4.73, 4.716, 4.724, 4.757, 4.747, 4.751, 4.746, 4.746, 4.767, 4.731, 4.76, 4.752, 4.731, 4.769, 4.738, 4.762]\n",
      "Val Error(all epochs): 4.0915350914001465 \n",
      " [7.806, 7.093, 6.348, 5.758, 5.511, 5.402, 5.072, 5.103, 5.318, 4.938, 5.019, 4.95, 5.634, 4.813, 4.725, 4.685, 4.809, 4.7, 4.799, 4.942, 4.649, 4.708, 4.997, 4.626, 4.639, 5.015, 4.824, 4.858, 4.601, 4.727, 4.615, 4.645, 4.859, 4.577, 5.057, 4.641, 4.405, 4.661, 5.741, 4.679, 4.53, 4.605, 5.232, 4.578, 4.596, 4.63, 4.522, 4.562, 4.479, 4.638, 4.705, 4.449, 4.598, 4.734, 4.615, 4.616, 4.589, 4.448, 4.624, 4.586, 4.678, 4.607, 4.429, 4.763, 4.445, 4.563, 4.622, 4.92, 4.565, 4.419, 4.604, 4.639, 4.518, 4.328, 4.384, 4.669, 4.327, 4.569, 4.531, 4.401, 4.664, 4.315, 4.544, 4.878, 4.412, 4.587, 4.391, 4.38, 4.519, 4.432, 4.386, 5.146, 4.402, 4.618, 4.355, 4.386, 4.519, 4.343, 4.247, 4.228, 4.411, 4.841, 4.382, 4.497, 4.281, 4.262, 4.491, 4.355, 4.251, 4.443, 4.294, 4.303, 4.473, 4.28, 4.839, 4.327, 4.311, 4.646, 4.3, 4.353, 4.546, 4.524, 4.341, 4.383, 4.55, 4.395, 4.276, 4.409, 4.749, 4.373, 4.414, 4.221, 4.245, 4.509, 4.469, 4.525, 4.28, 4.465, 4.241, 4.696, 4.445, 4.591, 4.272, 4.536, 4.299, 4.235, 4.297, 4.244, 4.185, 4.475, 4.27, 4.239, 4.229, 4.364, 4.317, 4.322, 4.285, 4.245, 4.238, 4.222, 4.302, 4.23, 4.266, 4.204, 4.221, 4.353, 4.359, 4.163, 4.438, 4.374, 4.191, 4.258, 4.348, 4.334, 4.381, 4.31, 4.45, 4.161, 4.211, 4.296, 4.431, 4.186, 4.425, 4.214, 4.298, 4.358, 5.319, 4.444, 4.171, 4.313, 4.501, 4.405, 4.267, 4.333, 4.305, 4.303, 4.336, 4.232, 4.514, 4.31, 4.111, 4.365, 4.326, 4.265, 4.361, 4.375, 4.284, 4.203, 4.295, 4.16, 4.308, 4.377, 4.333, 4.306, 4.363, 4.318, 4.24, 4.206, 4.251, 4.313, 4.232, 4.395, 4.249, 4.266, 4.165, 4.33, 4.273, 4.385, 4.333, 4.204, 4.155, 4.274, 4.208, 4.395, 4.207, 4.606, 4.465, 4.15, 4.224, 4.274, 4.248, 4.432, 4.22, 4.317, 4.377, 4.218, 4.274, 4.392, 4.291, 4.197, 4.331, 4.293, 4.382, 4.184, 4.261, 4.348, 4.126, 4.234, 4.134, 4.515, 4.292, 4.358, 4.455, 4.226, 4.168, 4.211, 4.286, 4.347, 4.129, 4.239, 4.146, 4.12, 4.386, 4.211, 4.383, 4.178, 4.11, 4.183, 4.16, 4.293, 4.227, 4.332, 4.212, 4.262, 4.425, 4.247, 4.253, 4.112, 4.325, 4.169, 4.16, 4.27, 4.174, 4.218, 4.251, 4.225, 4.322, 4.092, 4.202, 4.319]\n",
      "Val custom mae Error(all epochs): 1.2644765377044678 \n",
      " [11.71, 10.668, 9.553, 8.437, 7.607, 7.621, 6.21, 6.606, 6.768, 6.013, 5.827, 5.844, 7.611, 5.719, 3.798, 4.202, 5.165, 5.243, 5.187, 5.744, 4.111, 4.885, 5.228, 4.602, 4.878, 6.453, 4.117, 5.319, 3.94, 4.094, 4.208, 4.032, 5.3, 4.512, 4.71, 4.456, 3.887, 4.313, 5.99, 5.151, 4.187, 4.857, 2.91, 4.338, 4.379, 4.727, 4.773, 3.789, 4.731, 5.179, 4.698, 4.946, 4.742, 5.268, 4.033, 4.912, 4.508, 4.319, 4.722, 5.242, 4.838, 4.699, 3.502, 4.185, 4.631, 2.819, 4.621, 4.799, 5.189, 3.663, 5.133, 3.201, 4.736, 4.534, 4.348, 5.302, 4.098, 4.977, 4.603, 3.954, 5.069, 3.906, 3.735, 5.666, 3.63, 4.833, 3.22, 3.936, 4.074, 4.179, 4.341, 3.343, 4.437, 5.482, 4.442, 2.917, 5.12, 3.788, 3.767, 3.571, 3.83, 4.651, 4.571, 3.753, 3.307, 4.091, 5.161, 3.639, 3.779, 4.422, 3.967, 3.493, 4.94, 3.666, 5.874, 4.257, 3.568, 4.933, 3.981, 2.787, 5.111, 4.927, 2.685, 4.224, 4.782, 4.284, 4.135, 4.83, 1.264, 4.153, 3.401, 2.933, 3.709, 4.606, 4.009, 3.918, 3.571, 4.856, 3.037, 4.508, 4.507, 5.418, 3.46, 4.617, 3.559, 3.854, 3.719, 2.695, 3.207, 4.725, 3.692, 2.913, 3.947, 3.824, 4.055, 4.563, 3.455, 3.93, 3.463, 3.546, 3.988, 3.818, 3.529, 3.377, 2.996, 4.154, 4.492, 3.702, 4.701, 3.128, 3.745, 2.765, 4.409, 4.331, 3.864, 2.914, 4.867, 2.863, 2.865, 4.411, 4.013, 3.918, 4.139, 3.709, 3.683, 3.967, 6.232, 3.817, 3.264, 3.265, 4.534, 4.312, 3.284, 3.981, 4.47, 4.251, 3.686, 3.901, 4.512, 3.289, 3.132, 4.267, 3.051, 3.696, 4.633, 4.394, 3.802, 2.962, 3.865, 3.492, 4.223, 4.333, 4.168, 4.018, 4.138, 3.913, 3.076, 3.508, 3.108, 3.566, 3.416, 3.252, 3.354, 3.883, 3.704, 4.082, 3.866, 3.679, 3.734, 3.438, 3.167, 4.164, 3.017, 3.408, 3.733, 4.782, 4.295, 2.577, 4.053, 2.604, 3.207, 3.734, 3.445, 3.716, 4.487, 2.579, 4.154, 4.705, 3.808, 2.869, 4.071, 3.701, 3.761, 2.537, 2.965, 4.179, 3.07, 4.006, 2.93, 4.112, 3.907, 2.55, 4.124, 2.574, 3.079, 3.905, 4.376, 3.163, 3.508, 3.311, 3.313, 3.376, 4.379, 3.453, 4.801, 3.632, 3.33, 2.038, 3.301, 2.518, 3.12, 4.26, 2.648, 4.127, 4.259, 2.799, 3.755, 2.512, 4.067, 3.2, 3.753, 3.561, 3.261, 3.285, 3.221, 3.465, 4.058, 3.083, 3.567, 4.284]\n",
      "\n",
      "Lambda: 10 , Time: 0:01:07\n",
      "Train Error(all epochs): 5.673269271850586 \n",
      " [8.034, 7.395, 6.962, 6.641, 6.471, 6.385, 6.327, 6.284, 6.22, 6.156, 6.083, 6.102, 6.093, 6.066, 6.036, 6.01, 5.986, 5.993, 5.949, 5.988, 5.958, 5.936, 5.941, 5.953, 5.922, 5.923, 5.924, 5.935, 5.864, 5.881, 5.863, 5.856, 5.855, 5.873, 5.86, 5.849, 5.81, 5.842, 5.856, 5.827, 5.819, 5.823, 5.799, 5.825, 5.821, 5.793, 5.808, 5.761, 5.777, 5.801, 5.8, 5.785, 5.776, 5.759, 5.764, 5.792, 5.778, 5.785, 5.795, 5.789, 5.771, 5.776, 5.736, 5.731, 5.753, 5.74, 5.757, 5.752, 5.76, 5.748, 5.749, 5.763, 5.746, 5.748, 5.749, 5.717, 5.746, 5.729, 5.748, 5.743, 5.756, 5.756, 5.741, 5.791, 5.742, 5.749, 5.727, 5.702, 5.734, 5.75, 5.76, 5.745, 5.762, 5.727, 5.74, 5.748, 5.725, 5.743, 5.72, 5.716, 5.738, 5.722, 5.729, 5.71, 5.73, 5.755, 5.735, 5.754, 5.775, 5.75, 5.74, 5.739, 5.745, 5.706, 5.722, 5.743, 5.7, 5.74, 5.707, 5.724, 5.724, 5.734, 5.729, 5.744, 5.727, 5.722, 5.705, 5.709, 5.69, 5.737, 5.698, 5.719, 5.728, 5.698, 5.684, 5.702, 5.71, 5.72, 5.711, 5.735, 5.712, 5.734, 5.733, 5.712, 5.711, 5.694, 5.712, 5.706, 5.682, 5.705, 5.735, 5.69, 5.722, 5.71, 5.707, 5.707, 5.702, 5.735, 5.699, 5.683, 5.718, 5.713, 5.706, 5.71, 5.676, 5.717, 5.706, 5.703, 5.705, 5.709, 5.734, 5.7, 5.693, 5.725, 5.684, 5.734, 5.704, 5.731, 5.709, 5.707, 5.724, 5.703, 5.703, 5.712, 5.726, 5.716, 5.685, 5.717, 5.696, 5.737, 5.701, 5.729, 5.741, 5.698, 5.721, 5.742, 5.707, 5.709, 5.713, 5.712, 5.719, 5.699, 5.696, 5.72, 5.706, 5.714, 5.714, 5.733, 5.729, 5.717, 5.737, 5.727, 5.719, 5.743, 5.707, 5.73, 5.731, 5.705, 5.746, 5.709, 5.738, 5.725, 5.685, 5.711, 5.7, 5.724, 5.707, 5.705, 5.707, 5.692, 5.71, 5.688, 5.705, 5.708, 5.73, 5.673, 5.716, 5.729, 5.713, 5.723, 5.711, 5.709, 5.685, 5.743, 5.711, 5.727, 5.714, 5.704, 5.721, 5.705, 5.709, 5.714, 5.71, 5.705, 5.708, 5.706, 5.708, 5.736, 5.69, 5.674, 5.699, 5.706, 5.709, 5.702, 5.7, 5.716, 5.725, 5.704, 5.696, 5.717, 5.743, 5.716, 5.696, 5.708, 5.696, 5.714, 5.709, 5.703, 5.736, 5.704, 5.7, 5.704, 5.716, 5.721, 5.691, 5.72, 5.725, 5.711, 5.702, 5.704, 5.7, 5.705, 5.712, 5.689, 5.711, 5.724, 5.725, 5.712, 5.681, 5.721]\n",
      "Val Error(all epochs): 5.490114212036133 \n",
      " [8.185, 7.965, 7.672, 7.292, 7.11, 6.854, 7.07, 6.433, 7.06, 6.388, 6.263, 8.361, 7.088, 6.68, 6.575, 6.205, 6.872, 5.983, 6.724, 6.129, 5.941, 6.09, 6.211, 5.912, 5.951, 6.5, 6.31, 6.079, 6.4, 5.997, 6.241, 5.879, 6.156, 5.839, 5.784, 5.822, 5.991, 6.008, 7.506, 5.948, 5.641, 5.759, 5.808, 5.873, 6.008, 5.756, 5.929, 5.878, 5.805, 5.624, 6.036, 5.889, 5.819, 5.689, 5.79, 5.755, 5.863, 5.905, 5.865, 5.895, 5.925, 5.878, 5.784, 5.737, 5.738, 5.731, 5.728, 5.837, 5.927, 5.615, 5.813, 5.72, 5.68, 5.666, 5.661, 5.768, 5.79, 6.106, 5.946, 5.819, 5.785, 5.819, 5.852, 5.735, 5.665, 5.644, 5.654, 5.746, 5.705, 5.681, 5.913, 5.844, 5.731, 5.653, 5.753, 5.797, 5.649, 5.63, 5.666, 5.649, 5.6, 5.686, 5.606, 5.795, 5.551, 5.666, 5.684, 5.724, 5.822, 5.555, 5.895, 5.856, 5.81, 5.578, 5.785, 5.772, 5.653, 5.65, 5.578, 5.724, 5.585, 5.625, 5.57, 5.716, 5.693, 5.75, 5.7, 5.836, 5.578, 5.562, 5.746, 5.567, 5.563, 5.84, 5.588, 5.526, 5.63, 5.573, 5.646, 5.702, 5.601, 5.642, 5.641, 5.49, 5.679, 5.601, 5.92, 5.626, 5.562, 5.649, 5.849, 5.933, 5.821, 5.693, 5.584, 5.569, 5.554, 5.697, 5.729, 5.675, 5.974, 5.617, 5.756, 6.18, 5.739, 5.781, 5.576, 5.579, 5.598, 5.571, 5.779, 5.637, 5.63, 5.681, 5.554, 5.565, 5.584, 5.583, 5.737, 5.562, 5.728, 5.802, 5.629, 5.585, 5.735, 5.564, 5.878, 5.645, 5.907, 5.644, 5.612, 5.679, 5.655, 5.69, 5.661, 5.688, 5.757, 5.606, 5.676, 5.802, 5.558, 5.551, 5.554, 5.814, 5.56, 5.678, 5.744, 5.853, 5.634, 5.736, 5.651, 5.655, 5.65, 5.721, 5.584, 5.594, 5.691, 5.603, 5.64, 5.639, 5.868, 5.6, 5.774, 5.664, 5.974, 5.597, 6.024, 5.708, 5.861, 5.652, 5.555, 5.695, 5.578, 5.684, 5.575, 5.703, 5.645, 5.612, 5.602, 5.599, 5.72, 5.872, 5.564, 5.967, 5.865, 5.617, 5.673, 5.737, 5.528, 5.572, 5.608, 5.788, 5.638, 5.592, 5.682, 5.58, 5.734, 5.6, 5.659, 5.569, 5.584, 5.755, 5.606, 5.639, 5.71, 5.636, 5.674, 5.587, 5.62, 5.614, 5.628, 5.655, 5.553, 5.732, 5.594, 6.006, 5.595, 5.56, 5.64, 5.669, 5.648, 5.73, 5.717, 5.59, 5.566, 5.624, 5.642, 5.587, 5.712, 5.781, 5.591, 5.616, 5.717, 5.689, 5.646, 5.627, 5.551, 5.643, 5.721, 5.825]\n",
      "Val custom mae Error(all epochs): 1.0010743141174316 \n",
      " [12.157, 11.582, 11.184, 10.752, 10.585, 9.94, 10.125, 7.01, 9.662, 6.636, 8.499, 6.823, 6.538, 6.467, 7.444, 7.908, 3.676, 6.517, 8.026, 3.306, 5.82, 5.21, 6.71, 6.403, 6.34, 6.424, 8.588, 7.418, 3.166, 7.089, 7.556, 5.026, 4.463, 6.532, 5.789, 5.7, 5.569, 6.886, 9.103, 6.843, 4.933, 4.896, 6.469, 6.162, 6.538, 6.092, 5.538, 6.371, 5.548, 4.939, 5.283, 5.726, 5.449, 2.694, 5.897, 4.904, 6.453, 5.332, 6.547, 5.793, 5.309, 5.975, 5.358, 5.784, 4.759, 5.936, 3.466, 6.64, 6.241, 4.741, 5.72, 3.08, 6.128, 5.099, 5.151, 5.894, 4.228, 6.542, 5.749, 5.261, 6.478, 2.439, 6.28, 5.181, 5.429, 4.531, 5.422, 3.712, 5.4, 4.826, 7.11, 4.978, 5.145, 5.262, 4.78, 2.859, 5.409, 5.218, 5.179, 5.386, 3.669, 6.178, 4.3, 5.9, 3.434, 4.528, 4.628, 5.673, 3.835, 4.684, 4.987, 3.582, 6.831, 4.233, 6.441, 5.99, 5.608, 4.291, 3.877, 5.798, 4.339, 4.291, 4.1, 4.203, 5.101, 6.152, 4.382, 6.499, 4.139, 5.049, 3.757, 4.578, 5.042, 6.322, 4.32, 3.399, 5.039, 3.819, 5.344, 5.384, 4.922, 5.205, 3.973, 4.362, 3.926, 3.542, 7.096, 5.244, 4.564, 5.192, 6.128, 4.834, 6.227, 5.961, 3.433, 3.69, 4.504, 3.904, 4.815, 3.028, 6.976, 4.133, 5.571, 1.097, 5.506, 5.612, 4.808, 4.102, 4.925, 3.537, 5.849, 5.639, 5.786, 2.171, 4.193, 3.341, 3.69, 4.527, 5.342, 4.708, 5.971, 5.166, 3.659, 3.489, 5.592, 3.347, 2.564, 4.828, 5.868, 5.043, 5.08, 5.227, 3.197, 3.777, 4.921, 4.553, 6.708, 4.89, 3.501, 6.062, 5.131, 3.555, 4.143, 2.992, 3.335, 5.387, 5.947, 1.001, 4.334, 5.022, 4.056, 5.183, 4.905, 5.7, 2.932, 4.116, 3.879, 3.06, 5.123, 4.886, 5.807, 5.13, 6.328, 4.628, 7.21, 3.927, 6.433, 5.424, 6.66, 3.26, 3.518, 4.128, 4.061, 3.495, 4.476, 4.735, 5.715, 4.044, 4.6, 4.672, 5.656, 5.308, 4.247, 5.427, 6.587, 4.544, 4.996, 3.015, 3.918, 3.591, 4.71, 5.627, 5.425, 4.591, 2.635, 4.39, 5.377, 3.589, 5.5, 4.892, 3.066, 5.695, 4.906, 2.943, 5.755, 4.544, 4.929, 4.428, 4.648, 3.876, 5.254, 5.011, 5.329, 4.73, 5.653, 5.829, 3.062, 3.986, 5.692, 2.143, 5.612, 4.924, 3.069, 3.592, 4.386, 5.195, 5.591, 4.286, 3.02, 5.046, 5.079, 3.312, 5.842, 2.511, 5.769, 5.431, 4.794, 3.491, 5.705, 5.179]\n",
      "\n",
      "Trainig set size: 4096 , Time: 0:05:35 , best_lambda: 10 , min_  error: 1.001\n",
      "Test starts:  4916 , ends:  32161\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "852/852 [==============================] - 0s 466us/step\n",
      "total_power:  14.704907 , average_difference:  3.5641578196004184\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# CNN + NN(multi-SUs): support batching\n",
    "TEST, CONSERVE = True, False\n",
    "mini_batch = 64 if max(max_x, max_y) == 1000 else 16\n",
    "epochs = 35 if max(max_x, max_y) == 1000 else 300\n",
    "MAX_QUEUE_SIZE, WORKERS = 28, 4\n",
    "fp_penalty_coef, fn_penalty_coef = 1, 1\n",
    "hyper_metric, mode = \"val_cus_mae\", 'min'  # the metric that hyper parameters are tuned with\n",
    "lambda_vec = [0, 0.01, 0.1, 1, 10] \n",
    "prev_sample = 0\n",
    "average_diff_power, fp_mean_power = [],[] #[7.177, 8.088, 8.183], [3.438, 3.506, 2.662]\n",
    "best_lambda = []\n",
    "average_diff_power_conserve, fp_mean_power_conserve = [], []\n",
    "total_power = []\n",
    "number_samples = [128, 256, 512, 1024, 2048, 4096]\n",
    "\n",
    "for num_sample_idx, number_sample in enumerate(number_samples):\n",
    "    MODEL_PATH = f\"{dataset_path}/{number_sample}/models\"\n",
    "    X = np.loadtxt(f\"{dataset_path}/{number_sample}/X.txt\", delimiter=\",\")\n",
    "    y = np.loadtxt(f\"{dataset_path}/{number_sample}/y.txt\", delimiter=\",\")\n",
    "    if not os.path.exists(MODEL_PATH):\n",
    "        os.makedirs(MODEL_PATH)\n",
    "    MODEL_PATH += \"/best_model_lambda_\"\n",
    "    if True:\n",
    "        nns = [nn_model(max_sus_num * 3, max_sus_num, kernel_lam=lamb, bias_lam=0,\n",
    "                        num_hidden_layers=4, num_neurons=200)\n",
    "               for lamb in lambda_vec]\n",
    "        for nn in nns:\n",
    "#             cnn.compile(loss='mean_squared_error', optimizer='adam', metrics=['mse','mae', fp_mean])\n",
    "            # =optimizers.SGD(lr=0.1, momentum=0.9, decay=0.1/epochs, nesterov=False)\n",
    "            nn.compile(loss=\"mse\", \n",
    "                        optimizer=optimizers.Adam(), \n",
    "                        metrics=['mse', 'mae', cus_mae])\n",
    "        checkpointers = [ModelCheckpoint(filepath=MODEL_PATH + str(lamb)+ '.h5',\n",
    "                                         verbose=0, save_best_only=True, \n",
    "                                         monitor=hyper_metric,\n",
    "                                         mode=mode)\n",
    "                         for lamb in lambda_vec]\n",
    "    else:\n",
    "        cnns = []\n",
    "        cnns = [models.load_model(MODEL_PATH + str(lamb) + '.h5', \n",
    "                                  custom_objects={ 'loss': custom_loss(fp_penalty_coef, fn_penalty_coef), \n",
    "                                                  'fp_mae': fp_mae }) \n",
    "                for lamb in lambda_vec]\n",
    "    number_start = time.time()\n",
    "    val_size = math.ceil(number_sample * validation_size)\n",
    "  \n",
    "    print('number_samples:', number_sample, \", New samples:\", number_sample - prev_sample)\n",
    "    print(\"Validation size:\", val_size, \", starts:\", number_sample, \", ends:\", number_sample + val_size - 1)\n",
    "    \n",
    "    for lamb_idx, lamb in enumerate(lambda_vec):\n",
    "#     for lamb_idx, lamb in enumerate(lambda_vec[:len(lambda_vec) - num_sample_idx//2]):\n",
    "#         if num_sample_idx == 3 and lamb_idx < 4:\n",
    "#             continue\n",
    "        lambda_start = time.time()\n",
    "        nns[lamb_idx].fit(x=X[:number_sample+val_size,:],\n",
    "                           y=y[:number_sample+val_size,:],\n",
    "                           epochs=epochs, verbose=0,\n",
    "                           validation_split=validation_size/(1 + validation_size), \n",
    "                           shuffle=True, callbacks=[checkpointers[lamb_idx]], \n",
    "                           workers=WORKERS, max_queue_size=MAX_QUEUE_SIZE, \n",
    "                           use_multiprocessing=False)\n",
    "        \n",
    "        print(\"\\nLambda:\", lamb, \", Time:\", str(datetime.timedelta(seconds=int(time.time() - lambda_start))))\n",
    "        print(\"Train Error(all epochs):\", min(nns[lamb_idx].history.history['mae']), '\\n', \n",
    "              [round(val, 3) for val in nns[lamb_idx].history.history['mae']])\n",
    "#         print(\"Train FP Error(all epochs):\", min(cnns[lamb_idx].history.history['fp_mae']), '\\n',\n",
    "#               [round(val,3) for val in cnns[lamb_idx].history.history['fp_mae']])\n",
    "        print(\"Val Error(all epochs):\", min(nns[lamb_idx].history.history['val_mae']), '\\n', \n",
    "              [round(val,3) for val in nns[lamb_idx].history.history['val_mae']])\n",
    "        print(\"Val custom mae Error(all epochs):\", min(nns[lamb_idx].history.history['val_cus_mae']), '\\n',\n",
    "              [round(val,3) for val in nns[lamb_idx].history.history['val_cus_mae']])\n",
    "    \n",
    "    \n",
    "    models_min_mae = [min(nns[lam_idx].history.history[hyper_metric]) for\n",
    "                      lam_idx,_ in enumerate(lambda_vec)]\n",
    "    best_lamb_idx = models_min_mae.index(min(models_min_mae))\n",
    "    best_lambda.append(lambda_vec[best_lamb_idx])\n",
    "    print(\"\\nTrainig set size:\", number_sample, \", Time:\", str(datetime.timedelta(seconds=int(time.time() - \n",
    "                                                                                              number_start))),\n",
    "          \", best_lambda:\", lambda_vec[best_lamb_idx], \", min_\" , (\"fp_\" if hyper_metric == \"val_fp_mae\" else \"\"),\n",
    "          \"error:\", round(min(models_min_mae), 3))\n",
    "    all_cnns.append(nns)\n",
    "    del nns\n",
    "    \n",
    "    if TEST:\n",
    "        # evaluating test images\n",
    "        best_model = None\n",
    "        best_model = models.load_model(MODEL_PATH + str(lambda_vec[best_lamb_idx]) + '.h5', \n",
    "                                       custom_objects={'mae': 'mae', 'mse': 'mse', 'cus_mae': cus_mae})\n",
    "#                                        custom_objects={ 'loss': custom_loss(fp_penalty_coef, fn_penalty_coef), \n",
    "#                                                        'fp_mae': fp_mae,\n",
    "#                                                       'mae':'mae', 'mse':'mse'})\n",
    "\n",
    "        print(\"Test starts: \", number_sample + val_size, \", ends: \", data_reg.shape[0] - 1)\n",
    "        time.sleep(1)\n",
    "#         test_res = best_model.evaluate(test_generator, verbose=1, \n",
    "#                                        workers=WORKERS, max_queue_size=MAX_QUEUE_SIZE, use_multiprocessing=False)\n",
    "        y_hat = best_model.predict(X[number_sample + val_size:, :], verbose=1, \n",
    "                                       workers=WORKERS, max_queue_size=MAX_QUEUE_SIZE, use_multiprocessing=False)\n",
    "        total_power.append(np.mean(10 * np.log10(np.sum(10**(y_hat/10), axis=1))))\n",
    "        average_diff_power.append(np.mean(np.abs(\n",
    "            10 * np.log10(np.sum(10 ** (y_hat/10), axis=1)) -\n",
    "            10 * np.log10(np.sum(10 ** (y[number_sample + val_size:]/10), axis=1)))))\n",
    "        \n",
    "#         test_mae_idx, test_fp_mae_idx = [best_model.metrics_names.index(mtrc) \n",
    "#                                          for mtrc in ['mae','fp_mae']]\n",
    "#         test_mae, test_fp_mae = test_res[test_mae_idx], test_res[test_fp_mae_idx]\n",
    "#         average_diff_power.append(round(test_mae, 3))\n",
    "#         fp_mean_power.append(round(test_fp_mae, 3))\n",
    "        print('total_power: ', total_power[-1], ', average_difference: ', average_diff_power[-1])\n",
    "        \n",
    "        \n",
    "        print(\"\\n\\n\")\n",
    "\n",
    "        \n",
    "#         var_f = open('/'.join(image_dir.split('/')[:-1]) +  '/models/' + model_name + \"/\"\n",
    "#                      + intensity_degradation + '_' + str(slope) + '_' + \n",
    "#                      dtime + \".dat\", \"wb\") # file for saving results\n",
    "#         pickle.dump([average_diff_power, fp_mean_power, number_samples, best_lambda, \n",
    "#                      dataset_name, max_dataset_name, average_diff_power_conserve, fp_mean_power_conserve,\n",
    "#                      checkpointers],\n",
    "#                     file=var_f)\n",
    "#         var_f.close()\n",
    "        del best_model\n",
    "#     prev_sample = number_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.623081284800603, 3.784753627462606, 3.571933018213715, 3.548662208542994, 3.656685618471818, 3.5641578196004184]\n",
      "[14.425347, 16.938116, 16.020887, 16.206432, 17.342045, 14.704907]\n",
      "[0.1, 10, 10, 1, 10, 10]\n"
     ]
    }
   ],
   "source": [
    "print(average_diff_power)\n",
    "print(total_power)\n",
    "print(best_lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multi-su simulatnous\n",
    "# minimized by min cus_mae\n",
    "[4.623081284800603, 3.784753627462606, 3.571933018213715, 3.548662208542994, 3.656685618471818, 3.5641578196004184]\n",
    "[14.425347, 16.938116, 16.020887, 16.206432, 17.342045, 14.704907]\n",
    "[0.1, 10, 10, 1, 10, 10]\n",
    "# minimized by min mae\n",
    "[4.607942885709469, 4.195705268601129, 3.6685070016249486, 3.639623614150089, 3.3137235937382084, 3.2712268522580983]\n",
    "[12.675369, 14.197315, 14.086546, 14.06105, 14.185148, 13.945412]\n",
    "[10, 0, 0.01, 1, 1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.layers.core.Dense at 0x7f931238c2b0>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f9312378e50>,\n",
       " <keras.layers.core.Dense at 0x7f9210692d30>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f9312378a30>,\n",
       " <keras.layers.core.Dropout at 0x7f92a041fd00>,\n",
       " <keras.layers.core.Dense at 0x7f9300042640>]"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_cnns[0][0].layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number_samples: 128 , New samples: 128\n",
      "Validation size: 26 , starts: 128 , ends: 153\n",
      "Epoch 1/120\n",
      "4/4 - 29s - loss: 91.7543 - mse: 91.7543 - mae: 7.5380 - cus_mae: 11.7201 - val_loss: 75.7279 - val_mse: 75.7279 - val_mae: 6.9419 - val_cus_mae: 7.3532\n",
      "\n",
      "Epoch 00001: val_cus_mae improved from inf to 7.35322, saving model to ML/data/pictures_299_299_transfer/splat/pu_circle_su_circle_60/raw_power_min_max_norm/color/log_pu5_su5/20pus_5sus_3channels/log_vgg16_max_su_total/bilstm/128_units/best_model_128.h5\n",
      "Epoch 2/120\n",
      "4/4 - 20s - loss: 83.1838 - mse: 83.1839 - mae: 7.0353 - cus_mae: 9.1834 - val_loss: 73.9549 - val_mse: 73.9549 - val_mae: 6.8919 - val_cus_mae: 7.1588\n",
      "\n",
      "Epoch 00002: val_cus_mae improved from 7.35322 to 7.15881, saving model to ML/data/pictures_299_299_transfer/splat/pu_circle_su_circle_60/raw_power_min_max_norm/color/log_pu5_su5/20pus_5sus_3channels/log_vgg16_max_su_total/bilstm/128_units/best_model_128.h5\n",
      "Epoch 3/120\n",
      "4/4 - 21s - loss: 77.4193 - mse: 77.4193 - mae: 6.8546 - cus_mae: 9.4899 - val_loss: 72.0148 - val_mse: 72.0148 - val_mae: 6.8240 - val_cus_mae: 7.6650\n",
      "\n",
      "Epoch 00003: val_cus_mae did not improve from 7.15881\n",
      "Epoch 4/120\n",
      "4/4 - 23s - loss: 73.5943 - mse: 73.5943 - mae: 6.7537 - cus_mae: 9.4671 - val_loss: 74.3508 - val_mse: 74.3508 - val_mae: 6.9965 - val_cus_mae: 6.3017\n",
      "\n",
      "Epoch 00004: val_cus_mae improved from 7.15881 to 6.30173, saving model to ML/data/pictures_299_299_transfer/splat/pu_circle_su_circle_60/raw_power_min_max_norm/color/log_pu5_su5/20pus_5sus_3channels/log_vgg16_max_su_total/bilstm/128_units/best_model_128.h5\n",
      "Epoch 5/120\n",
      "4/4 - 21s - loss: 71.1035 - mse: 71.1035 - mae: 6.6708 - cus_mae: 8.0588 - val_loss: 73.7061 - val_mse: 73.7061 - val_mae: 6.9665 - val_cus_mae: 6.2612\n",
      "\n",
      "Epoch 00005: val_cus_mae improved from 6.30173 to 6.26124, saving model to ML/data/pictures_299_299_transfer/splat/pu_circle_su_circle_60/raw_power_min_max_norm/color/log_pu5_su5/20pus_5sus_3channels/log_vgg16_max_su_total/bilstm/128_units/best_model_128.h5\n",
      "Epoch 6/120\n",
      "4/4 - 23s - loss: 69.5926 - mse: 69.5925 - mae: 6.6654 - cus_mae: 8.6131 - val_loss: 71.4519 - val_mse: 71.4519 - val_mae: 6.8583 - val_cus_mae: 6.4646\n",
      "\n",
      "Epoch 00006: val_cus_mae did not improve from 6.26124\n",
      "Epoch 7/120\n",
      "4/4 - 22s - loss: 67.6020 - mse: 67.6020 - mae: 6.5445 - cus_mae: 8.0478 - val_loss: 71.2186 - val_mse: 71.2186 - val_mae: 6.8454 - val_cus_mae: 5.7744\n",
      "\n",
      "Epoch 00007: val_cus_mae improved from 6.26124 to 5.77443, saving model to ML/data/pictures_299_299_transfer/splat/pu_circle_su_circle_60/raw_power_min_max_norm/color/log_pu5_su5/20pus_5sus_3channels/log_vgg16_max_su_total/bilstm/128_units/best_model_128.h5\n",
      "Epoch 8/120\n",
      "4/4 - 21s - loss: 65.8325 - mse: 65.8325 - mae: 6.4562 - cus_mae: 7.9642 - val_loss: 69.5945 - val_mse: 69.5945 - val_mae: 6.7519 - val_cus_mae: 6.1466\n",
      "\n",
      "Epoch 00008: val_cus_mae did not improve from 5.77443\n",
      "Epoch 9/120\n",
      "4/4 - 23s - loss: 64.0374 - mse: 64.0374 - mae: 6.3822 - cus_mae: 8.2670 - val_loss: 69.9525 - val_mse: 69.9525 - val_mae: 6.7643 - val_cus_mae: 5.4670\n",
      "\n",
      "Epoch 00009: val_cus_mae improved from 5.77443 to 5.46697, saving model to ML/data/pictures_299_299_transfer/splat/pu_circle_su_circle_60/raw_power_min_max_norm/color/log_pu5_su5/20pus_5sus_3channels/log_vgg16_max_su_total/bilstm/128_units/best_model_128.h5\n",
      "Epoch 10/120\n",
      "4/4 - 22s - loss: 62.3221 - mse: 62.3221 - mae: 6.2402 - cus_mae: 7.6549 - val_loss: 68.0398 - val_mse: 68.0398 - val_mae: 6.6205 - val_cus_mae: 5.6510\n",
      "\n",
      "Epoch 00010: val_cus_mae did not improve from 5.46697\n",
      "Epoch 11/120\n",
      "4/4 - 22s - loss: 60.2849 - mse: 60.2849 - mae: 6.1786 - cus_mae: 7.6585 - val_loss: 68.0192 - val_mse: 68.0192 - val_mae: 6.6160 - val_cus_mae: 5.4688\n",
      "\n",
      "Epoch 00011: val_cus_mae did not improve from 5.46697\n",
      "Epoch 12/120\n",
      "4/4 - 23s - loss: 58.3290 - mse: 58.3290 - mae: 6.0569 - cus_mae: 7.4436 - val_loss: 65.3708 - val_mse: 65.3708 - val_mae: 6.4313 - val_cus_mae: 5.5297\n",
      "\n",
      "Epoch 00012: val_cus_mae did not improve from 5.46697\n",
      "Epoch 13/120\n",
      "4/4 - 21s - loss: 56.5693 - mse: 56.5693 - mae: 5.9765 - cus_mae: 7.2545 - val_loss: 66.6468 - val_mse: 66.6468 - val_mae: 6.5459 - val_cus_mae: 4.9501\n",
      "\n",
      "Epoch 00013: val_cus_mae improved from 5.46697 to 4.95007, saving model to ML/data/pictures_299_299_transfer/splat/pu_circle_su_circle_60/raw_power_min_max_norm/color/log_pu5_su5/20pus_5sus_3channels/log_vgg16_max_su_total/bilstm/128_units/best_model_128.h5\n",
      "Epoch 14/120\n",
      "4/4 - 23s - loss: 53.3355 - mse: 53.3355 - mae: 5.8162 - cus_mae: 7.0952 - val_loss: 66.2821 - val_mse: 66.2821 - val_mae: 6.4782 - val_cus_mae: 4.2311\n",
      "\n",
      "Epoch 00014: val_cus_mae improved from 4.95007 to 4.23112, saving model to ML/data/pictures_299_299_transfer/splat/pu_circle_su_circle_60/raw_power_min_max_norm/color/log_pu5_su5/20pus_5sus_3channels/log_vgg16_max_su_total/bilstm/128_units/best_model_128.h5\n",
      "Epoch 15/120\n",
      "4/4 - 22s - loss: 52.4400 - mse: 52.4400 - mae: 5.6691 - cus_mae: 6.9874 - val_loss: 64.5276 - val_mse: 64.5276 - val_mae: 6.3707 - val_cus_mae: 4.5863\n",
      "\n",
      "Epoch 00015: val_cus_mae did not improve from 4.23112\n",
      "Epoch 16/120\n",
      "4/4 - 21s - loss: 50.4258 - mse: 50.4258 - mae: 5.6132 - cus_mae: 6.2717 - val_loss: 65.4964 - val_mse: 65.4964 - val_mae: 6.4339 - val_cus_mae: 4.4468\n",
      "\n",
      "Epoch 00016: val_cus_mae did not improve from 4.23112\n",
      "Epoch 17/120\n",
      "4/4 - 23s - loss: 49.2732 - mse: 49.2732 - mae: 5.5322 - cus_mae: 6.5933 - val_loss: 66.2255 - val_mse: 66.2255 - val_mae: 6.3067 - val_cus_mae: 3.3348\n",
      "\n",
      "Epoch 00017: val_cus_mae improved from 4.23112 to 3.33480, saving model to ML/data/pictures_299_299_transfer/splat/pu_circle_su_circle_60/raw_power_min_max_norm/color/log_pu5_su5/20pus_5sus_3channels/log_vgg16_max_su_total/bilstm/128_units/best_model_128.h5\n",
      "Epoch 18/120\n",
      "4/4 - 21s - loss: 46.5775 - mse: 46.5775 - mae: 5.3938 - cus_mae: 6.8213 - val_loss: 65.8930 - val_mse: 65.8930 - val_mae: 6.2709 - val_cus_mae: 3.4788\n",
      "\n",
      "Epoch 00018: val_cus_mae did not improve from 3.33480\n",
      "Epoch 19/120\n",
      "4/4 - 23s - loss: 44.3679 - mse: 44.3679 - mae: 5.3049 - cus_mae: 5.4583 - val_loss: 64.5345 - val_mse: 64.5345 - val_mae: 6.4233 - val_cus_mae: 4.2188\n",
      "\n",
      "Epoch 00019: val_cus_mae did not improve from 3.33480\n",
      "Epoch 20/120\n",
      "4/4 - 22s - loss: 42.9931 - mse: 42.9931 - mae: 5.1905 - cus_mae: 5.4829 - val_loss: 61.7379 - val_mse: 61.7379 - val_mae: 6.0799 - val_cus_mae: 3.7571\n",
      "\n",
      "Epoch 00020: val_cus_mae did not improve from 3.33480\n",
      "Epoch 21/120\n",
      "4/4 - 21s - loss: 42.2249 - mse: 42.2249 - mae: 5.0666 - cus_mae: 6.5506 - val_loss: 71.2486 - val_mse: 71.2486 - val_mae: 6.4900 - val_cus_mae: 1.9418\n",
      "\n",
      "Epoch 00021: val_cus_mae improved from 3.33480 to 1.94178, saving model to ML/data/pictures_299_299_transfer/splat/pu_circle_su_circle_60/raw_power_min_max_norm/color/log_pu5_su5/20pus_5sus_3channels/log_vgg16_max_su_total/bilstm/128_units/best_model_128.h5\n",
      "Epoch 22/120\n",
      "4/4 - 23s - loss: 39.6399 - mse: 39.6399 - mae: 4.9129 - cus_mae: 4.9085 - val_loss: 62.3542 - val_mse: 62.3542 - val_mae: 6.2236 - val_cus_mae: 4.1005\n",
      "\n",
      "Epoch 00022: val_cus_mae did not improve from 1.94178\n",
      "Epoch 23/120\n",
      "4/4 - 21s - loss: 36.3695 - mse: 36.3695 - mae: 4.7574 - cus_mae: 5.7843 - val_loss: 61.7629 - val_mse: 61.7629 - val_mae: 6.0086 - val_cus_mae: 2.6982\n",
      "\n",
      "Epoch 00023: val_cus_mae did not improve from 1.94178\n",
      "Epoch 24/120\n",
      "4/4 - 24s - loss: 33.8423 - mse: 33.8423 - mae: 4.5584 - cus_mae: 5.5120 - val_loss: 63.0356 - val_mse: 63.0356 - val_mae: 6.1357 - val_cus_mae: 2.8772\n",
      "\n",
      "Epoch 00024: val_cus_mae did not improve from 1.94178\n",
      "Epoch 25/120\n"
     ]
    }
   ],
   "source": [
    "# Multi-SU using single deep-alloc and Bi-LSTM\n",
    "# DataBatchGenerator\n",
    "\n",
    "class LSTMBatchGenerator(Sequence):\n",
    "    def __init__(self, dataset, batch_size: int, start_idx: int, float_memory_used,\n",
    "                 image_dir: str, cnn_feature_model, num_sus, cnn_output_size):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size, self.start_idx = batch_size, start_idx\n",
    "#         self.number_image_channels, self.max_x, self.max_y = number_image_channels, max_x, max_y\n",
    "        self.float_memory_used = float_memory_used\n",
    "        self.image_dir = image_dir\n",
    "        self.cnn_feature_model = cnn_feature_model\n",
    "        self.num_sus = num_sus\n",
    "        self.cnn_output_size = cnn_output_size\n",
    "    \n",
    "    def __len__(self):\n",
    "        return np.ceil(self.dataset.shape[0] / self.batch_size).astype(int)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        size = min(self.dataset.shape[0] - idx * self.batch_size, self.batch_size)\n",
    "        batch_x = np.empty((size, self.num_sus, self.cnn_output_size),\n",
    "                           dtype=self.float_memory_used)\n",
    "        batch_y = np.copy(self.dataset[idx * self.batch_size: idx * self.batch_size + size, 3::3])\n",
    "        batch_x2 = np.copy(batch_y)\n",
    "        batch_x2[:, 1:] = batch_x2[:, :-1]\n",
    "        batch_x2[:,0] = 0\n",
    "        for i in range(size):\n",
    "            for su_idx in range(self.num_sus):\n",
    "                imm = read_image((self.start_idx + idx * self.batch_size + i) * self.num_sus + su_idx,\n",
    "                                 self.image_dir)\n",
    "                batch_x[i][su_idx] = self.cnn_feature_model.predict(imm)[0]\n",
    "        return ([batch_x, batch_x2.reshape(size, self.num_sus, 1)], batch_y.reshape(size, self.num_sus, 1))\n",
    "\n",
    "def create_bilstm_model(units, num_sus, cnn_output_size):\n",
    "    model = Sequential()\n",
    "    model.add(layers.Bidirectional(layers.LSTM(units, return_sequences=True), \n",
    "                                   input_shape=(num_sus, cnn_output_size)))\n",
    "    model.add(layers.Bidirectional(layers.LSTM(units)))\n",
    "    model.add(layers.Dense(1, activation='linear'))\n",
    "    return model\n",
    "\n",
    "def create_rnn_encoder_decoder(units, num_sus, cnn_output_size):\n",
    "    # define bi-lstm encoder\n",
    "    encoder_inputs = Input(shape=(None, cnn_output_size))\n",
    "    encoder = layers.Bidirectional(layers.LSTM(units, return_state=True))\n",
    "    encoder_outputs, forward_h, forward_c, backward_h, backward_c = encoder(encoder_inputs)\n",
    "    state_h = layers.Concatenate()([forward_h, backward_h])\n",
    "    state_c = layers.Concatenate()([forward_c, backward_c])\n",
    "    encoder_states = [state_h, state_c]\n",
    "    \n",
    "    # define training decoder\n",
    "    decoder_inputs = Input(shape=(None, 1))\n",
    "    decoder_lstm = layers.LSTM(units * 2, return_sequences=True, return_state=True)\n",
    "    decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "    decoder_dense = layers.Dense(1, activation='linear')\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "    model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def create_inference_model(model, units):\n",
    "    #inference encoder\n",
    "    encoder_inputs = model.input[0] #input_1\n",
    "    encoder_outputs, forward_h, forward_c, backward_h, backward_c = model.layers[1].output # lstm_1\n",
    "    state_h = layers.Concatenate()([forward_h, backward_h])\n",
    "    state_c = layers.Concatenate()([forward_c, backward_c])\n",
    "    encoder_states = [state_h, state_c]\n",
    "    encoder_model = Model(encoder_inputs, encoder_states)\n",
    "    \n",
    "    #inference decoder\n",
    "    decoder_inputs = model.input[1] #input_2\n",
    "    decoder_state_input_h = Input(shape=(units * 2,),name='input_3')\n",
    "    decoder_state_input_c = Input(shape=(units * 2,),name='input_4')\n",
    "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "    decoder_lstm = model.layers[5]\n",
    "    decoder_outputs, state_h_dec, state_c_dec = decoder_lstm(\n",
    "        decoder_inputs, initial_state=decoder_states_inputs)\n",
    "    decoder_states = [state_h_dec, state_c_dec]\n",
    "    decoder_dense = model.layers[6]\n",
    "    decoder_outputs=decoder_dense(decoder_outputs)\n",
    "\n",
    "    decoder_model = Model([decoder_inputs] + decoder_states_inputs,\n",
    "                          [decoder_outputs] + decoder_states)\n",
    "    return encoder_model, decoder_model\n",
    "\n",
    "def predict_su_adjusted_power(inf_enc, inf_dec, num_sus, data):\n",
    "    # encode\n",
    "    state = inf_enc.predict(data)\n",
    "    # start of sequence input\n",
    "    target_seq = np.array([0.0]).reshape(1, 1, 1)\n",
    "    # collect predictions\n",
    "    output = list()\n",
    "    for _ in range(num_sus):\n",
    "        # predict next char\n",
    "        yhat, h, c = inf_dec.predict([target_seq] + state)\n",
    "        # store prediction\n",
    "        output.append(yhat[0,0,:])\n",
    "        # update state\n",
    "        state = [h, c]\n",
    "        # update target sequence\n",
    "        target_seq = yhat\n",
    "    return np.array(output)\n",
    "\n",
    "model_name = \"log_vgg16_max_su_total\"\n",
    "max_su_image_dir = '/'.join(image_dir.split('/')[:-1]) + \"/\" + model_name + \"/images\"\n",
    "number_samples = [128, 256, 512, 1024, 2048, 4096]\n",
    "hyper_metric, mode = \"val_cus_mae\", 'min'\n",
    "MAX_QUEUE_SIZE, WORKERS = 28, 4\n",
    "lstm_units = 128\n",
    "mini_batch, epochs = 32, 120\n",
    "fp_penalty_coef, fn_penalty_coef = 1, 1\n",
    "dataset_path = '/'.join(image_dir.split('/')[:-1]) + \"/\" + model_name\n",
    "fp_penalty_coef, fn_penalty_coef = 1, 1\n",
    "total_power, avg_diff_power, max_min_ratio = [], [], []\n",
    "cnn_output_size = 512\n",
    "dataset_path = '/'.join(image_dir.split('/')[:-1]) + \"/\" + model_name\n",
    "\n",
    "\n",
    "model_path = \"/home/shahrokh/projects/MLSpectrumAllocation/ML/data/pictures_299_299_transfer/\" + \\\n",
    "             \"splat/pu_circle_su_circle_60/raw_power_min_max_norm/color/log_pu5_su5/\" + \\\n",
    "             \"pus_1_sus_3_channels/models/log_vgg16/\"\n",
    "MODEL_PATH = f\"{dataset_path}/bilstm/{lstm_units}_units\"\n",
    "if not os.path.exists(MODEL_PATH):\n",
    "    os.makedirs(MODEL_PATH)\n",
    "for number_sample in number_samples:\n",
    "    cnn_model = models.load_model(f\"{model_path}/{number_sample}/best_model_lambda_0.h5\",\n",
    "                                  custom_objects={ \n",
    "                                      'loss': custom_loss(fp_penalty_coef, fn_penalty_coef),\n",
    "                                      'fp_mae': fp_mae,\n",
    "                                      'mae':'mae', 'mse':'mse'})\n",
    "    feature_output = Model(inputs=cnn_model.layers[0].input, outputs=cnn_model.layers[2].output)\n",
    "    time_start = time.time()\n",
    "    train_generator = LSTMBatchGenerator(\n",
    "        dataset=data_max_su_tot[:number_sample],\n",
    "        batch_size=mini_batch,\n",
    "        start_idx=prev_sample, \n",
    "        float_memory_used=float_memory_used,\n",
    "        cnn_feature_model=feature_output,\n",
    "        num_sus=max_sus_num,\n",
    "        cnn_output_size=cnn_output_size,\n",
    "        image_dir=max_su_image_dir)\n",
    "    \n",
    "\n",
    "    val_size = math.ceil(number_sample * validation_size)\n",
    "#     val_size = data_reg.shape[0] - number_sample\n",
    "    val_generator = LSTMBatchGenerator(\n",
    "        dataset=data_max_su_tot[number_sample:number_sample+val_size],\n",
    "        batch_size=mini_batch,\n",
    "        start_idx=number_sample, \n",
    "        float_memory_used=float_memory_used,\n",
    "        cnn_feature_model=feature_output,\n",
    "        num_sus=max_sus_num,\n",
    "        cnn_output_size=cnn_output_size,\n",
    "        image_dir=max_su_image_dir)\n",
    "  \n",
    "    print('number_samples:', number_sample, \", New samples:\", number_sample - prev_sample)\n",
    "    print(\"Validation size:\", val_size, \", starts:\", number_sample, \", ends:\", number_sample + val_size - 1)\n",
    "#     bilstm_model = create_bilstm_model(128, max_sus_num, cnn_output_size)\n",
    "    train_model = create_rnn_encoder_decoder(units=lstm_units, num_sus=max_sus_num,\n",
    "                                             cnn_output_size=cnn_output_size)\n",
    "    \n",
    "    checkpointer = ModelCheckpoint(filepath=MODEL_PATH + f\"/best_model_{number_sample}.h5\",\n",
    "                                   verbose=1, save_best_only=True, \n",
    "                                   monitor=hyper_metric,\n",
    "                                   mode=mode)\n",
    "    train_model.compile(loss=\"mse\", \n",
    "                        optimizer=optimizers.Adam(), \n",
    "                        metrics=['mse', 'mae', cus_mae])\n",
    "    train_model.fit(train_generator, epochs=epochs, verbose=2,\n",
    "                     validation_data=val_generator, \n",
    "                     shuffle=True, callbacks=[checkpointer], \n",
    "                     workers=WORKERS, max_queue_size=MAX_QUEUE_SIZE, \n",
    "                     use_multiprocessing=False)\n",
    "    # re-loading best model\n",
    "    best_lstm_mode = models.load_model(MODEL_PATH + f\"/best_model_{number_sample}.h5\",\n",
    "                                       custom_objects={'mse':'mse', 'mae': 'mae',\n",
    "                                                       'cus_mae': cus_mae})\n",
    "    inf_enc, inf_dec = create_inference_model(model=best_lstm_mode, units=lstm_units)\n",
    "    \n",
    "    # predicting test-sample one-by-one\n",
    "    test_generator = LSTMBatchGenerator(\n",
    "        dataset=data_max_su_tot[number_sample+val_size:],\n",
    "        batch_size=1,\n",
    "        start_idx=number_sample+val_size, \n",
    "        float_memory_used=float_memory_used,\n",
    "        cnn_feature_model=feature_output,\n",
    "        num_sus=max_sus_num,\n",
    "        cnn_output_size=cnn_output_size,\n",
    "        image_dir=max_su_image_dir)\n",
    "    tot_power_tmp, max_min_ratio_tmp, avg_diff_power_tmp = [], [], []\n",
    "    \n",
    "    for test_idx in range(number_sample+val_size, data_max_su_tot.shape[0]):\n",
    "        X, y = test_generator.__getitem__(test_idx - (number_sample+val_size))\n",
    "        y_hat = predict_su_adjusted_power(inf_enc, inf_dec, max_sus_num, X[0])\n",
    "        \n",
    "        y_hat_tot_power = 10 * np.log10((10 ** (y_hat/10)).sum())\n",
    "        y_tot_power = 10 * np.log10((10 ** (y/10)).sum())\n",
    "        \n",
    "        tot_power_tmp.append(y_hat_tot_power)\n",
    "        max_min_ratio_tmp.append(y_hat.max() - y_hat.min())\n",
    "        avg_diff_power_tmp.append(abs(y_hat_tot_power - y_tot_power))\n",
    "    \n",
    "    total_power.append(sum(tot_power_tmp) / len(tot_power_tmp))\n",
    "    avg_diff_power.append(sum(avg_diff_power_tmp) / len(avg_diff_power_tmp))\n",
    "    max_min_ratio.append(sum(max_min_ratio_tmp) / len(max_min_ratio_tmp))\n",
    "    \n",
    "    print(f\"number_sample: {number_sample}, total_power: {total_power[-1]}, avg_diff_power: {avg_diff_power[-1]}\"\n",
    "          f\"max_min_ratio: {max_min_ratio[-1]}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.load_model(\"/home/shahrokh/projects/MLSpectrumAllocation/ML/data/pictures_299_299_transfer/\" + \\\n",
    "             \"splat/pu_circle_su_circle_60/raw_power_min_max_norm/color/log_pu5_su5/\" + \\\n",
    "             \"pus_1_sus_3_channels/models/log_vgg16/128/best_model_lambda_0.h5\",\n",
    "                              custom_objects={ 'loss': custom_loss(1, 1),\n",
    "                                              'fp_mae': fp_mae,\n",
    "                                              'mae':'mae', 'mse':'mse'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7.018]\n",
      "[3.323]\n",
      "[0]\n"
     ]
    }
   ],
   "source": [
    "print(average_diff_power)\n",
    "print(fp_mean_power)\n",
    "print(best_lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ML/data/pictures_299_299_transfer/splat/pu_circle_su_circle_60/raw_power_min_max_norm/color/log_pu5_su5/20pus_5sus_3channels/log_vgg16_max_su_total/images'"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_su_image_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=model.layers[0].input, outputs=model.layers[2].output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 6.05172634e-01,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 2.06243649e+01, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 2.07075715e-01, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 2.01428680e+01, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 4.87392008e-01, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 9.89619568e-02, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 2.51417980e+01, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 9.18023363e-02, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 7.43128210e-02,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        1.38056672e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 1.83829829e-01, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 6.58319741e-02, 0.00000000e+00, 0.00000000e+00,\n",
       "        6.13626361e-01, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 3.68522148e+01, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 2.10870247e+01,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 2.58923881e-02,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 1.09268618e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        9.08366814e-02, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 8.16915333e-02,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 4.70201635e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 1.14810257e+01, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 1.16485667e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 1.72981784e-01,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 1.12691574e-01,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 4.21375176e-03, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 3.42099953e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 8.19323242e-01, 3.53022516e-01,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 8.62023085e-02,\n",
       "        4.08703610e-02, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 9.75533783e-01, 0.00000000e+00,\n",
       "        2.74893083e-02, 5.17769814e-01, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 2.00749755e-01, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 6.17705891e-03,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 2.82269726e+01, 0.00000000e+00,\n",
       "        0.00000000e+00, 2.16087580e-01, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 3.45836029e+01, 0.00000000e+00,\n",
       "        1.69296622e-01, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        1.24076471e+01, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 4.94096088e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 2.43857594e+01, 0.00000000e+00,\n",
       "        0.00000000e+00, 1.72656417e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        6.86604381e-02, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 1.23992935e-01, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 3.54458475e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        8.68254125e-01, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(read_image(1, max_su_image_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aug, 400 sensors\n",
    "[6.973, 6.9, 6.78, 6.583, 6.51, 6.395]\n",
    "[3.423, 3.234, 3.611, 3.509, 3.391, 3.234]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 400 sensors\n",
    "[7.018, 6.98, 6.88, 6.755, 6.621, 6.497]\n",
    "[3.323, 3.179, 3.93, 3.422, 3.459, 3.519]\n",
    "[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6.847, 6.856]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[6.847, 6.856 (6.608), 6.827 (6.782),7.038 (6.678, 6.621), 6.32 (6.20)]\n",
    "[3.473, 3.691, 3.297, 4.085 (3.8, 3.459), 3.34 (3.01)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aug\n",
    "[6.83, 6.812 , 6.72, 6.51, 6.252]\n",
    "[3.37, 3.583 , 3.14, 3.39, 3.503]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnns[0].save(MODEL_PATH + \"0_fit.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\n",
    "ii, jj = 2, 2\n",
    "b = [a[i][j] for i in range(max(0, ii - 1), min(2, ii+1) + 1) for j in range(max(0, jj-1), min(2, jj+1)+1)\n",
    "     if not (i ==ii and j==jj)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = {1:2, 3:4}\n",
    "[c[i] for i in c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### MULTIPLE - SUS\n",
    "def db_to_dec(db: float):\n",
    "    return 10 ** (db/10)\n",
    "def dec_to_db(dec: float):\n",
    "    return -float('inf') if dec <= 0 else 10 * math.log10(dec)\n",
    "\n",
    "def greedy_sus(pus, requesting_sus, model, pl_info, number_channel = 1):\n",
    "    # pus: (x, y, p), requesting_sus: (x, y)\n",
    "    def best_su_candidate(pus, active_sus, requesting_sus):\n",
    "        if len(requesting_sus) == 1:\n",
    "            return requesting_sus[0][0]\n",
    "        # active_sus: (x, y, allocated_power), requesting_sus: (req_id, x, y)\n",
    "        power_map_cell_size, area_size = 50, 1000\n",
    "        cell_weight, neighbour_weight = 0.5, 0.5\n",
    "        power_map = [[0] * int(area_size // power_map_cell_size) for _ in range(int(area_size // power_map_cell_size))]\n",
    "        \n",
    "        for pu_num in range(int(len(pus)//3)):\n",
    "            x, y, dec_p = pus[pu_num*3], pus[pu_num * 3 + 1], db_to_dec(pus[pu_num*3 + 2])\n",
    "            power_map[int(x * cell_size // power_map_cell_size)][int(y * cell_size // power_map_cell_size)] += dec_p\n",
    "        for su in active_sus:\n",
    "            x, y, dec_p = su[0], su[1], db_to_dec(su[2])\n",
    "            power_map[int(x * cell_size // power_map_cell_size)][int(y * cell_size // power_map_cell_size)] += dec_p\n",
    "        \n",
    "        power_score = [[0] * len(power_map[0]) for _ in range(len(power_map))]\n",
    "        # updating power score\n",
    "        for i in range(len(power_map)):\n",
    "            for j in range(len(power_map[0])):\n",
    "                power_score[i][j] = cell_weight * power_map[i][j] + neighbour_weight * sum(\n",
    "                [power_map[ii][jj] for ii in range(max(0, i - 1), min(i + 1, len(power_map))) \n",
    "                 for jj in range(max(0, j - 1), min(j + 1, len(power_map[0]))) if not (i == ii and j == jj)])\n",
    "        # find su with lowest weight\n",
    "        best_su_req, min_score = -1, float('inf')\n",
    "        for req_id, x, y in requesting_sus:\n",
    "            if power_score[int(x * cell_size // power_map_cell_size)][int(x * cell_size // power_map_cell_size)] < min_score:\n",
    "                min_score = power_score[int(x * cell_size // power_map_cell_size)][int(x * cell_size // power_map_cell_size)]\n",
    "                best_su_req = req_id\n",
    "        return best_su_req         \n",
    "    active_sus = [{} for _ in range(number_channel)] # (request_id: power_allocated)\n",
    "    assigned_req = set()\n",
    "    for _ in range(len(requesting_sus)):\n",
    "        max_allocated_power, best_channel, best_req_id = -float('inf'), -1, -1\n",
    "        for ch in range(number_channel):\n",
    "            nex_req_id = best_su_candidate(pus, [(*requesting_sus[i], active_sus[ch][i]) for i in active_sus[ch]],\n",
    "                                          [(idd, *requesting_sus[idd]) for idd in range(len(requesting_sus))\n",
    "                                           if idd not in assigned_req])\n",
    "            su_lst = []\n",
    "            for i in active_sus[ch]:\n",
    "                su_lst += [*requesting_sus[i]]\n",
    "                su_lst.append(active_sus[ch][i])\n",
    "            requesting_data = [len(pus)//3] + pus + [len(active_sus[ch]) + 1] + su_lst + requesting_sus[nex_req_id]\n",
    "            requesting_image = np.expand_dims(create_image(requesting_data, slope=slope, style=style, \n",
    "                                                           noise_floor=noise_floor, pu_shape=pu_shape, su_shape=su_shape,\n",
    "                                                           su_param=su_param, sensors_num=(sensors_num if sensors else 0), \n",
    "                                                           intensity_degradation=intensity_degradation, \n",
    "                                                           max_pu_power=0.0 if not sensors else -30,\n",
    "                                                           max_su_power=0.0), 0)\n",
    "            predicted_power = model.predict(requesting_image)\n",
    "            if predicted_power[0][0] > max_allocated_power:\n",
    "                max_allocated_power = predicted_power[0][0]\n",
    "                best_channel, best_req_id = ch, nex_req_id\n",
    "        active_sus[best_channel][best_req_id] = max_allocated_power\n",
    "        assigned_req.add(best_req_id)\n",
    "    res = []\n",
    "    for ch in range(number_channel):\n",
    "        for req_id in active_sus[ch]:\n",
    "            res.append((req_id, active_sus[ch][req_id], ch))\n",
    "    return res\n",
    "\n",
    "def fair_sus(pus, requesting_sus, model, pl_info, number_channel = 1):\n",
    "    # pus: (x, y, p), requesting_sus: (x, y)\n",
    "    def best_su_candidate(pus, active_sus, requesting_sus):\n",
    "        if len(requesting_sus) == 1:\n",
    "            return requesting_sus[0][0]\n",
    "        # active_sus: (x, y, allocated_power), requesting_sus: (req_id, x, y)\n",
    "        power_map_cell_size, area_size = 50, 1000\n",
    "        cell_weight, neighbour_weight = 0.5, 0.5\n",
    "        power_map = [[0] * int(area_size // power_map_cell_size) for _ in range(int(area_size // power_map_cell_size))]\n",
    "        \n",
    "        for pu_num in range(int(len(pus)//3)):\n",
    "            x, y, dec_p = pus[pu_num*3], pus[pu_num * 3 + 1], db_to_dec(pus[pu_num*3 + 2])\n",
    "            power_map[int(x * cell_size // power_map_cell_size)][int(y * cell_size // power_map_cell_size)] += dec_p\n",
    "        for su in active_sus:\n",
    "            x, y, dec_p = su[0], su[1], db_to_dec(su[2])\n",
    "            power_map[int(x * cell_size // power_map_cell_size)][int(y * cell_size // power_map_cell_size)] += dec_p\n",
    "        \n",
    "        power_score = [[0] * len(power_map[0]) for _ in range(len(power_map))]\n",
    "        # updating power score\n",
    "        for i in range(len(power_map)):\n",
    "            for j in range(len(power_map[0])):\n",
    "                power_score[i][j] = cell_weight * power_map[i][j] + neighbour_weight * sum(\n",
    "                [power_map[ii][jj] for ii in range(max(0, i - 1), min(i + 1, len(power_map))) \n",
    "                 for jj in range(max(0, j - 1), min(j + 1, len(power_map[0]))) if not (i == ii and j == jj)])\n",
    "        # find su with lowest weight\n",
    "        best_su_req, min_score = -1, float('inf')\n",
    "        for req_id, x, y in requesting_sus:\n",
    "            if power_score[int(x * cell_size // power_map_cell_size)][int(x * cell_size // power_map_cell_size)] < min_score:\n",
    "                min_score = power_score[int(x * cell_size // power_map_cell_size)][int(x * cell_size // power_map_cell_size)]\n",
    "                best_su_req = req_id\n",
    "        return best_su_req         \n",
    "    active_sus = [{} for _ in range(number_channel)] # (request_id: power_allocated)\n",
    "    assigned_req = set()\n",
    "    for _ in range(len(requesting_sus)):\n",
    "        max_allocated_power, best_channel, best_req_id = -float('inf'), -1, -1\n",
    "        for ch in range(number_channel):\n",
    "            nex_req_id = best_su_candidate(pus, [(*requesting_sus[i], active_sus[ch][i]) for i in active_sus[ch]],\n",
    "                                          [(idd, *requesting_sus[idd]) for idd in range(len(requesting_sus))\n",
    "                                           if idd not in assigned_req])\n",
    "            su_lst = []\n",
    "            for i in active_sus[ch]:\n",
    "                su_lst += [*requesting_sus[i]]\n",
    "                su_lst.append(active_sus[ch][i])\n",
    "            requesting_data = [len(pus)//3] + pus + [len(active_sus[ch]) + 1] + su_lst + requesting_sus[nex_req_id]\n",
    "            requesting_image = np.expand_dims(create_image(requesting_data, slope=slope, style=style, \n",
    "                                                           noise_floor=noise_floor, pu_shape=pu_shape, su_shape=su_shape,\n",
    "                                                           su_param=su_param, sensors_num=(sensors_num if sensors else 0), \n",
    "                                                           intensity_degradation=intensity_degradation, \n",
    "                                                           max_pu_power=0.0 if not sensors else -30,\n",
    "                                                           max_su_power=0.0), 0)\n",
    "            predicted_power = model.predict(requesting_image)\n",
    "            if predicted_power[0][0] > max_allocated_power:\n",
    "                max_allocated_power = predicted_power[0][0]\n",
    "                best_channel, best_req_id = ch, nex_req_id\n",
    "        if max_allocated_power < -20:\n",
    "            # it's less than threshold. sort active su w.r.t to best_req_id and try to decrease their power\n",
    "            def dist(p1, p2):\n",
    "                return ((p1[0] - p2[0]) ** 2 + (p1[1] - p2[1]) ** 2) ** 0.5\n",
    "            dist_info = []\n",
    "            for ch in range(number_channel):\n",
    "                for i in active_sus[ch]:\n",
    "                    dist_info.append((ch, i, dist(requesting_sus[i], requesting_sus[best_req_id])))\n",
    "            dist_info.sort(key=lambda x:x[2])\n",
    "            SATISFIED = False\n",
    "            for i in range(len(dist_info)):\n",
    "                candid_ch, candid_su_id  = dist_info[i][0], dist_info[i][1]\n",
    "                candid_old_pow = active_sus[candid_ch][candid_su_id]\n",
    "                if  candid_old_pow > -10:\n",
    "                    candid_new_pow = candid_old_pow\n",
    "                    while candid_new_pow - 5 > -20.0:\n",
    "                        candid_new_pow -= 5\n",
    "                        # try this new power\n",
    "                        su_lst = []\n",
    "                        for ii in active_sus[candid_ch]:\n",
    "                            su_lst += [*requesting_sus[ii]]\n",
    "                            if ii == candid_su_id:\n",
    "                                su_lst.append(candid_new_pow)\n",
    "                            else:\n",
    "                                su_lst.append(active_sus[candid_ch][ii])\n",
    "                        requesting_data = [len(pus)//3] + pus + [len(active_sus[candid_ch]) + 1] + su_lst + requesting_sus[best_req_id]\n",
    "                        requesting_image = np.expand_dims(create_image(requesting_data, slope=slope, style=style, \n",
    "                                                                       noise_floor=noise_floor, pu_shape=pu_shape, su_shape=su_shape,\n",
    "                                                                       su_param=su_param, sensors_num=(sensors_num if sensors else 0), \n",
    "                                                                       intensity_degradation=intensity_degradation, \n",
    "                                                                       max_pu_power=0.0 if not sensors else -30,\n",
    "                                                                       max_su_power=0.0), 0)\n",
    "                        predicted_power = model.predict(requesting_image)[0][0]\n",
    "                        if predicted_power > -20.0:\n",
    "                            SATISFIED = True\n",
    "                            break\n",
    "                    if SATISFIED:\n",
    "                        break\n",
    "            if SATISFIED:\n",
    "                best_channel, max_allocated_power = candid_ch, predicted_power\n",
    "                active_sus[candid_ch][candid_su_id] = candid_new_pow  #update the candid su\n",
    "                    \n",
    "        active_sus[best_channel][best_req_id] = max_allocated_power\n",
    "        assigned_req.add(best_req_id)\n",
    "    res = []\n",
    "    for ch in range(number_channel):\n",
    "        for req_id in active_sus[ch]:\n",
    "            res.append((req_id, active_sus[ch][req_id], ch))\n",
    "    return res\n",
    "\n",
    "#     return [(i, active_sus[ch][i][0][0]) for i in active_sus[ch] for ch in range(number_channel)]\n",
    "#     req_dec_power = [db_to_dec(active_sus[ch][i][0][0]) for i in active_sus[ch] for ch in range(number_channel)]\n",
    "#     return sum(req_dec_power), max(req_dec_power) / min(req_dec_power)\n",
    "\n",
    "def random_sus(pus, requesting_sus, model, pl_info, number_channel = 1):\n",
    "    active_sus = [{}  for _ in range(number_channel)] # (request_id: power_allocated)\n",
    "    assigned_req = set()\n",
    "    for nex_req_id in range(len(requesting_sus)):\n",
    "        max_allocated_power, best_channel, best_req_id = -float('inf'), -1, -1\n",
    "        for ch in range(number_channel):\n",
    "            su_lst = []\n",
    "            for i in active_sus[ch]:\n",
    "                su_lst += [*requesting_sus[i]]\n",
    "                su_lst.append(active_sus[ch][i])\n",
    "            requesting_data = [len(pus)//3] + pus + [len(active_sus[ch]) + 1] + su_lst + requesting_sus[nex_req_id]\n",
    "            requesting_image = np.expand_dims(create_image(requesting_data, slope=slope, style=style, \n",
    "                                                           noise_floor=noise_floor, pu_shape=pu_shape, su_shape=su_shape,\n",
    "                                                           su_param=su_param, sensors_num=(sensors_num if sensors else 0), \n",
    "                                                           intensity_degradation=intensity_degradation, \n",
    "                                                           max_pu_power=0.0 if not sensors else -30,\n",
    "                                                           max_su_power=0.0), 0)\n",
    "            predicted_power = model.predict(requesting_image)\n",
    "            if predicted_power[0][0] > max_allocated_power:\n",
    "                max_allocated_power = predicted_power[0][0]\n",
    "                best_channel, best_req_id = ch, nex_req_id\n",
    "        active_sus[best_channel][best_req_id] = max_allocated_power\n",
    "        assigned_req.add(best_req_id)\n",
    "    res = []\n",
    "    for ch in range(number_channel):\n",
    "        for req_id in active_sus[ch]:\n",
    "            res.append((req_id, active_sus[ch][req_id], ch))\n",
    "    return res\n",
    "#     return [(req_id, active_sus[ch][req_id], ch) for req_id in active_sus[ch] for ch in range(number_channel)]\n",
    "#     req_dec_power = [db_to_dec(active_sus[ch][i][0][0]) for i in active_sus[ch] for ch in range(number_channel)]\n",
    "#     return sum(req_dec_power), max(req_dec_power) / min(req_dec_power)\n",
    "    \n",
    "def multiple_sus(data_reg, train_set_size, pl_info, model_path, number_channel):\n",
    "#     random_sum_power, random_max_min_ratio = [[] for _ in range(len(train_set_size))], [[] for _ in range(len(train_set_size))]\n",
    "#     greedy_sum_power, greedy_max_min_ratio = [[] for _ in range(len(train_set_size))], [[] for _ in range(len(train_set_size))]\n",
    "    random_res, greedy_res = [[] for _ in range(len(train_set_size))], [[] for _ in range(len(train_set_size))]\n",
    "    fair_res = [[] for _ in range(len(train_set_size))]\n",
    "    for ind, train_size in enumerate(train_set_size):\n",
    "        print(f\"Train size: {train_size}\")\n",
    "        model = models.load_model(f\"{model_path}/{train_size}/best_model_lambda_0.1.h5\",\n",
    "                                 custom_objects={ 'loss': custom_loss(1, 1), \n",
    "                                               'fp_mae': fp_mae,\n",
    "                                               'mae':'mae', 'mse':'mse'})\n",
    "        for i in tqdm.tqdm(range(len(data_reg))):\n",
    "            pu_num = int(data_reg[i][0])\n",
    "            pus = np.ndarray.tolist(data_reg[i][1:1 + pu_num * 3])\n",
    "            su_num = int(data_reg[i][1 + pu_num * 3])\n",
    "            if su_num < 4:\n",
    "                continue\n",
    "            sus = []\n",
    "            for su_ind in range(su_num):\n",
    "                sus.append(np.ndarray.tolist(data_reg[i][2 + pu_num * 3 + su_ind * 3:4 + pu_num * 3 + su_ind * 3]))\n",
    "#             random_tot_power, random_ratio = random_sus(pus, sus, model, pl_info, 1)\n",
    "#             print(random_tot_power)\n",
    "#             print(random_ratio)\n",
    "#             greedy_tot_power, greedy_ratio = greedy_sus(pus, sus, model, pl_info, 1)\n",
    "#             print(greedy_tot_power)\n",
    "#             print(greedy_ratio)\n",
    "#             random_sum_power[ind].append(random_tot_power)\n",
    "#             random_max_min_ratio[ind].append(random_ratio)\n",
    "#             greedy_sum_power[ind].append(greedy_tot_power)\n",
    "#             greedy_max_min_ratio[ind].append(greedy_ratio)\n",
    "            fair_res[ind].append(fair_sus(pus, sus, model, pl_info, number_channel))\n",
    "            random_res[ind].append(random_sus(pus, sus, model, pl_info, number_channel))\n",
    "            greedy_res[ind].append(greedy_sus(pus, sus, model, pl_info, number_channel))\n",
    "            var_f = open('/'.join(image_dir.split('/')[:-1]) + \"/multi_res_num_channel\"\n",
    "                     + str(number_channel) + \"_\" + intensity_degradation + '_' + str(slope) + '_' + \n",
    "                     dtime + \".dat\", \"wb\") # file for saving results\n",
    "            pickle.dump([random_res, greedy_res, fair_res],  file=var_f)\n",
    "            var_f.close()\n",
    "            \n",
    "#     return random_sum_power, random_max_min_ratio, greedy_sum_power, greedy_max_min_ratio\n",
    "    return random_res, greedy_res, fair_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rand_res, greedy_res, fair_res = multiple_sus(data_reg[:600], number_samples, None,\n",
    "                                              \"ML/data/pictures_299_299_transfer/splat/pu_circle_su_circle_60/raw_power_min_max_norm/color/log_5/pus_5_sus_3_channels/models/log_vgg16\",\n",
    "                                              4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_res, greedy_res, fair_res = pickle.load( open( \"ML/data/pictures_299_299_transfer/splat/pu_circle_su_circle_60/raw_power_min_max_norm/color/log_pu5_su5/pus_20_sus_3_channels/multi_res_num_channel1_log_5__202110_2115_18.dat\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_calc(data_arr):\n",
    "    min_data, tot_power = [], []\n",
    "    for data in data_arr:\n",
    "#         min_arr = [min(val, key=lambda x:x[1])[1] for val in data]\n",
    "        min_arr = [(sorted(val, key=lambda x:x[1])[-1][1] - sorted(val, key=lambda x:x[1])[0][1]) for val in data]\n",
    "#         print(min_arr)\n",
    "        min_data.append(sum(min_arr)/len(min_arr))\n",
    "        sum_arr = [dec_to_db(sum([db_to_dec(val[1]) for val in fair_res_sng]))for fair_res_sng in data]\n",
    "        tot_power.append(sum(sum_arr)/len(sum_arr))\n",
    "    return [round(min_, 2) for min_ in min_data], [round(tot,2) for tot in tot_power]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fair_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minn, tot = multi_calc(fair_res)\n",
    "print(minn)\n",
    "print(tot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minn, tot = multi_calc(greedy_res)\n",
    "print(minn)\n",
    "print(tot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minn, tot = multi_calc(rand_res)\n",
    "print(minn)\n",
    "print(tot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[dec_to_db(sum([db_to_dec(x) for x in [xx] * 4])) for xx in [6.29, 7.17, 6.3, 7.8, 7.95, 8.07]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "greedy_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[[10609.339866896524],\n",
    " [27745.525760029166],\n",
    " [33981.2176874612],\n",
    " [83093.78437330532],\n",
    " [230199.57169850808],\n",
    " [303634.6974403381]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(number_samples)\n",
    "print(average_diff_power)\n",
    "print(fp_mean_power)\n",
    "print(best_lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[128, 256, 512, 1024, 2048, 4096]\n",
    "[8.678, 7.595, 7.392, 6.968, 6.701, 6.521]\n",
    "[5.619, 4.375, 3.38,  4.032, 3.282, 3.4]\n",
    "[0.1, 0, 1, 10, 0.1 0.1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnns[0].summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnns[0].summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = DataBatchGenerator(dataset=data_reg[0:number_sample], batch_size=16,\n",
    "                                         start_idx=0, number_image_channels=number_image_channels,\n",
    "                                         max_x=max_x, max_y=max_y, float_memory_used=float_memory_used)\n",
    "    \n",
    "\n",
    "# val_size = math.ceil(number_sample * validation_size)\n",
    "val_generator = DataBatchGenerator(dataset=data_reg[number_sample:number_sample+val_size], \n",
    "                                   batch_size=32,\n",
    "                                   start_idx=number_sample,\n",
    "                                   number_image_channels=number_image_channels,\n",
    "                                   max_x=max_x, max_y=max_y, \n",
    "                                   float_memory_used=float_memory_used,\n",
    "                                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = DataBatchGenerator(dataset=data_reg_train[prev_sample:number_sample], batch_size=16,\n",
    "                                         start_idx=0, number_image_channels=number_image_channels,\n",
    "                                         max_x=max_x, max_y=max_y, float_memory_used=float_memory_used)\n",
    "    \n",
    "\n",
    "val_size = math.ceil(number_sample/4 * validation_size)\n",
    "val_generator = DataBatchGenerator(dataset=data_reg[number_sample//4:number_sample//4+val_size], \n",
    "                                       batch_size=mini_batch,\n",
    "                                       start_idx=number_sample//4,\n",
    "                                       number_image_channels=number_image_channels,\n",
    "                                       max_x=max_x, max_y=max_y, \n",
    "                                       float_memory_used=float_memory_used,\n",
    "                                      image_dir=\"/\".join(image_dir.split(\"/\")[:-1]) + \"/images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = models.load_model(MODEL_PATH + str(0) + '.h5', \n",
    "                              custom_objects={ 'loss': custom_loss(fp_penalty_coef, fn_penalty_coef), \n",
    "                                               'fp_mae': fp_mae,\n",
    "                                               'mae':'mae', 'mse':'mse'})\n",
    "best_model.layers[1].trainable = True\n",
    "best_model.compile(loss=custom_loss(fp_penalty_coef, fn_penalty_coef), \n",
    "                        optimizer=optimizers.Adam(1e-5), \n",
    "                        metrics=['mse', 'mae', fp_mae])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.fit(train_generator, epochs=40, verbose=2,\n",
    "                   validation_data=val_generator, \n",
    "                   shuffle=True, callbacks=[checkpointers[0]], \n",
    "                   workers=WORKERS, max_queue_size=MAX_QUEUE_SIZE, \n",
    "                   use_multiprocessing=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.save(MODEL_PATH + \"0.1_fit2.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.save(\"ML/data/pictures_299_299_transfer/log/noisy_std_1/pu_circle_su_circle_60/raw_power_min_max_norm/color/log_5/pus_1_sus_3_channels/models/700000/best_model_lambda_0_fit.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_sample + val_size+320*3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model1 = models.load_model(MODEL_PATH + str(0.1) + '.h5', \n",
    "                                       custom_objects={ 'loss': custom_loss(fp_penalty_coef, fn_penalty_coef), \n",
    "                                                       'fp_mae': fp_mae,\n",
    "                                                      'mae':'mae', 'mse':'mse'})\n",
    "test_generator = DataBatchGenerator(dataset=data_reg[number_sample + val_size:], \n",
    "                                    batch_size=32,\n",
    "                                    start_idx=number_sample + val_size, \n",
    "                                    number_image_channels=number_image_channels,\n",
    "                                    max_x=max_x, max_y=max_y, float_memory_used=float_memory_used,\n",
    "                                   image_dir=\"/\".join(image_dir.split(\"/\")[:-1]) + \"/images\")\n",
    "\n",
    "print(\"Test starts: \", number_sample + val_size, \", ends: \", data_reg.shape[0] - 1)\n",
    "time.sleep(1)\n",
    "test_res = best_model1.evaluate(test_generator, verbose=1, \n",
    "                                workers=WORKERS, max_queue_size=MAX_QUEUE_SIZE, use_multiprocessing=False)\n",
    "        \n",
    "test_mae_idx, test_fp_mae_idx = [best_model1.metrics_names.index(mtrc) \n",
    "                                for mtrc in ['mae','fp_mae']]\n",
    "test_mae, test_fp_mae = test_res[test_mae_idx], test_res[test_fp_mae_idx]\n",
    "average_diff_power.append(round(test_mae, 3))\n",
    "fp_mean_power.append(round(test_fp_mae, 3))\n",
    "print('average_error: ', average_diff_power[-1], ', fp_average_error: ', \n",
    "        fp_mean_power[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:research] *",
   "language": "python",
   "name": "conda-env-research-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
