{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SVM and SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import metrics\n",
    "# from sklearn.cross_validation import *\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC, SVR, LinearSVR\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INIT\n",
    "number_samples = [5] + list(range(10, 101, 10)) + [120, 150, 200, 250, 300, 400, 500, 700] + list(range(1000, 4001, 1000))\n",
    "number_samples = [256, 512, 1024, 2048, 4096, 8192]\n",
    "number_samples = [8192]\n",
    "C_vec = list(np.arange(0.1, 10, 0.5))\n",
    "max_pus_number, max_sus_number, num_sensors = 20, 1, 225\n",
    "IS_SENSORS = True\n",
    "DUMMY_LOC_VALUE, DUMMY_POWER_VALUE = -90.0, -90.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD DATA\n",
    "num_columns = (num_sensors if IS_SENSORS else max_pus_number * 3 + 1) + max_sus_number * 3 + 2\n",
    "cols = [i for i in range(num_columns)]\n",
    "dataframe = pd.read_csv('../../../java_workspace/research/spectrum_allocation/resources/data/' +\n",
    "                        'dynamic_pus_225sensor_60000_min10_max20PUs_1SUs_square100grid_splat_2020_06_19_00_46.txt', \n",
    "                        delimiter=',', header=None, names=cols)\n",
    "dataframe_max = pd.read_csv('../../../java_workspace/research/spectrum_allocation/resources/data/' +\n",
    "                            'dynamic_pus_max_power_60000_min10_max20PUs_1SUs_square100grid_splat_2020_06_19_00_46.txt', delimiter=',', header=None)\n",
    "\n",
    "dataframe.reset_index(drop=True, inplace=True)\n",
    "dataframe_max.reset_index(drop=True, inplace=True)\n",
    "\n",
    "dataframe_tot = pd.concat([dataframe, dataframe_max.iloc[:, dataframe_max.columns.values[-1]]], axis=1,\n",
    "                        ignore_index=True)\n",
    "idx = dataframe_tot[dataframe_tot[dataframe_tot.columns[-1]] == -float('inf')].index\n",
    "dataframe_tot.drop(idx, inplace=True)\n",
    "\n",
    "data_reg = np.concatenate((dataframe_tot.values[:, 0:dataframe_tot.shape[1]-3], \n",
    "                           dataframe_tot.values[:, dataframe_tot.shape[1]-1:dataframe_tot.shape[1]]), axis=1)\n",
    "data_reg[data_reg < -90.0] = -90.0\n",
    "data_class = dataframe_tot.values[:, 0:dataframe_tot.shape[1]-1]\n",
    "y_class_power = dataframe_tot.values[:, -1]\n",
    "del dataframe, dataframe_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39370, 229)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_reg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting data\n",
    "# def split_data(data, train_samples):\n",
    "#     num_inputs = data.shape[1] - 1\n",
    "#     val_samples = round(train_samples/3)\n",
    "#     X_train, y_train = data[0:train_samples, 0: num_inputs], data[0:train_samples, -1]\n",
    "#     X_val, y_val = data[train_samples:train_samples+val_samples, 0: num_inputs],data[train_samples:train_samples+val_samples, -1]\n",
    "#     X_test, y_test = data[train_samples:, 0: num_inputs], data[train_samples:, -1]\n",
    "#     return X_train,X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "def split_data(data: np.ndarray, train_samples):\n",
    "    num_inputs = (max_sus_number - 1) * 3 + 2 + (max_pus_number * 3 \n",
    "                                                 if not IS_SENSORS else num_sensors)\n",
    "    val_samples = round(train_samples/3)\n",
    "    test_samples = data.shape[0] - val_samples - train_samples\n",
    "    #init arrays\n",
    "    X_train = np.ones((train_samples, num_inputs), dtype=float) * DUMMY_LOC_VALUE\n",
    "    X_val = np.ones((val_samples, num_inputs), dtype=float) * DUMMY_LOC_VALUE\n",
    "    X_test = np.ones((test_samples, num_inputs), dtype=float) * DUMMY_LOC_VALUE\n",
    "    # read values\n",
    "    if not IS_SENSORS:\n",
    "        # fill train\n",
    "        for train_sample in range(train_samples):\n",
    "            num_pus = int(data[train_sample, 0])\n",
    "            num_sus = int(data[train_sample, 1 + num_pus * 3])\n",
    "            X_train[train_sample, :num_pus * 3] = data[train_sample, 1:1 + num_pus * 3]#pus\n",
    "            #sus except power of last su\n",
    "            X_train[train_sample, max_pus_number * 3: (max_pus_number + num_sus) * 3 \n",
    "                    - 1] = data[train_sample, 2 + num_pus * 3: \n",
    "                                1 + (num_pus + num_sus) * 3]\n",
    "        # fill validation\n",
    "        for val_sample in range(train_samples, train_samples + val_samples):\n",
    "            num_pus = int(data[val_sample, 0])\n",
    "            num_sus = int(data[val_sample, 1 + num_pus * 3])\n",
    "            X_val[val_sample - train_samples, :num_pus * 3] = data[val_sample, 1:1 + num_pus * 3]\n",
    "            X_val[val_sample - train_samples, max_pus_number * 3: \n",
    "                  (max_pus_number + num_sus) * 3 - 1] = data[val_sample, 2 + num_pus * 3:\n",
    "                                                             1 + (num_pus + num_sus) * 3]\n",
    "        # fill test\n",
    "        for test_sample in range(train_samples + val_samples, \n",
    "                                 train_samples + val_samples + test_samples):\n",
    "            num_pus = int(data[test_sample, 0])\n",
    "            num_sus = int(data[test_sample, 1 + num_pus * 3])\n",
    "            X_test[test_sample - (train_samples + val_samples), :num_pus * 3] = data[\n",
    "                test_sample, 1:1 + num_pus * 3]\n",
    "            X_test[test_sample - (train_samples + val_samples), max_pus_number * 3:\n",
    "                   (max_pus_number + num_sus) * 3 - 1] = data[test_sample, 2 + num_pus * 3:\n",
    "                                                              1 + (num_pus + num_sus) * 3]\n",
    "    else:\n",
    "        # read sensors\n",
    "        X_train[:, :num_sensors] = data[:train_samples, :num_sensors]\n",
    "        X_val[:, :num_sensors] = data[train_samples: train_samples + val_samples,\n",
    "                                         :num_sensors]\n",
    "        X_test[:, :num_sensors] = data[train_samples + val_samples : , :num_sensors]\n",
    "        #read sus\n",
    "        for train_sample in range(train_samples):\n",
    "            num_sus = int(data[train_sample, num_sensors])\n",
    "            X_train[train_sample, num_sensors: num_sensors + num_sus * 3 - 1] = data[\n",
    "                train_sample, num_sensors + 1:num_sensors + num_sus * 3]\n",
    "            \n",
    "        for val_sample in range(train_samples, train_samples + val_samples):\n",
    "            num_sus = int(data[val_sample, num_sensors])\n",
    "            X_val[val_sample - train_samples, num_sensors: num_sensors + num_sus * 3 - 1] =\\\n",
    "                data[val_sample, num_sensors + 1:num_sensors + num_sus * 3]\n",
    "            \n",
    "        for test_sample in range(train_samples + val_samples, \n",
    "                                 train_samples + val_samples + test_samples):\n",
    "            num_sus = int(data[test_sample, num_sensors])\n",
    "            X_test[test_sample - (train_samples + val_samples), num_sensors:\n",
    "                   num_sensors + num_sus * 3 - 1] = data[test_sample, \n",
    "                                                         num_sensors + 1:num_sensors + num_sus * 3]\n",
    "\n",
    "    \n",
    "    y_train = data[0 : train_samples, -1]\n",
    "    y_val = data[train_samples : train_samples + val_samples, -1]\n",
    "    y_test = data[train_samples + val_samples:, -1]\n",
    "    return X_train,X_val, X_test, y_train, y_val, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#utils\n",
    "def false_analysis(y_test, y_pred):\n",
    "    tp = sum(y_pred[y_test==1])\n",
    "    fp = sum(y_pred) - tp\n",
    "    return fp, sum(y_test) - tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-84.351 -83.983 -80.417 -74.961 -69.494 -77.179 -80.206 -77.813 -77.542\n",
      " -84.415 -70.748 -70.611 -77.697 -80.034 -81.164 -84.652 -79.489 -74.59\n",
      " -73.505 -69.336 -72.5   -69.849 -66.903 -63.208 -66.273 -87.81  -83.555\n",
      " -81.786 -76.895 -75.356 -79.321 -81.829 -71.919 -71.254 -70.9   -70.363\n",
      " -70.477 -70.992 -80.925 -71.611 -80.926 -79.621 -74.127 -77.264 -74.003\n",
      " -71.261 -68.828 -64.863 -62.374 -63.464 -83.427 -82.707 -80.311 -80.916\n",
      " -80.554 -72.088 -71.989 -71.566 -71.772 -71.056 -71.321 -70.722 -71.637\n",
      " -71.327 -78.488 -71.977 -72.672 -74.011 -73.343 -73.109 -71.071 -67.947\n",
      " -63.907 -50.179 -61.535 -81.046 -83.264 -81.155 -85.163 -80.61  -72.335\n",
      " -72.718 -72.793 -71.308 -69.005 -67.744 -68.735 -69.445 -70.755 -70.072\n",
      " -69.809 -72.183 -70.061 -70.106 -73.295 -71.533 -69.336 -66.037 -63.263\n",
      " -64.279 -81.02  -82.023 -79.102 -80.711 -72.561 -72.704 -73.057 -71.721\n",
      " -69.929 -66.09  -64.002 -63.418 -66.724 -69.088 -64.974 -65.531 -77.58\n",
      " -81.076 -81.752 -69.688 -72.086 -70.289 -69.003 -68.071 -68.14  -82.719\n",
      " -81.794 -80.856 -82.466 -81.606 -76.155 -73.707 -71.511 -68.048 -64.623\n",
      " -53.81  -55.669 -64.48  -69.289 -71.285 -74.773 -72.229 -73.647 -79.731\n",
      " -69.157 -68.82  -70.364 -71.966 -71.58  -72.112 -84.435 -83.175 -83.736\n",
      " -77.59  -72.058 -72.054 -72.713 -71.795 -64.517 -63.291 -61.056 -60.324\n",
      " -64.999 -69.108 -70.795 -73.592 -69.119 -72.136 -77.061 -77.426 -67.895\n",
      " -67.604 -67.295 -67.308 -70.169 -83.852 -81.218 -79.765 -77.785 -83.868\n",
      " -66.036 -65.937 -65.607 -65.134 -68.479 -66.674 -63.864 -67.868 -69.928\n",
      " -71.191 -68.459 -69.211 -68.465 -73.382 -74.952 -77.116 -73.49  -67.337\n",
      " -75.139 -67.462 -80.331 -80.117 -84.75  -67.63  -67.4   -66.118 -71.382\n",
      " -68.881 -68.259 -67.944 -65.384 -70.3   -67.443 -68.119 -76.529 -68.15\n",
      " -73.728 -75.41  -81.425 -73.536 -74.819 -75.079 -74.989 -76.922 -75.431\n",
      " -89.146 -87.772 -68.213 -67.991 -67.649 -71.78  -68.029 -65.854 -64.235\n",
      " -64.702 -65.359 -66.065 -67.529 -67.832 -66.337 -67.983 -76.567 -75.979\n",
      " -78.202 -77.146 -75.607 -73.815 -74.424 -76.633 -78.538 -89.372 -86.708\n",
      " -68.398 -84.781 -71.983 -70.019 -67.467 -64.022 -60.43  -63.037 -64.565\n",
      " -70.593 -72.388 -66.16  -70.225 -74.831 -73.231 -74.721 -78.203 -75.861\n",
      " -73.122 -68.445 -70.046 -73.843 -76.533 -88.961 -88.577 -84.828 -68.563\n",
      " -72.053 -69.793 -65.669 -59.948 -47.185 -61.545 -66.548 -69.147 -71.139\n",
      " -72.249 -72.273 -72.977 -74.344 -75.702 -78.084 -76.133 -72.901 -60.653\n",
      " -59.617 -73.799 -76.848 -80.725 -68.59  -68.562 -84.764 -72.501 -69.514\n",
      " -66.584 -62.615 -58.381 -63.685 -66.505 -69.561 -71.436 -71.888 -71.76\n",
      " -77.25  -74.647 -74.162 -78.05  -76.548 -74.37  -71.4   -71.106 -75.279\n",
      " -77.14  -80.848 -88.89  -68.319 -68.196 -73.122 -71.218 -68.695 -66.482\n",
      " -65.361 -67.385 -68.7   -71.333 -71.56  -71.582 -79.48  -73.946 -74.224\n",
      " -75.878 -78.493 -77.357 -77.183 -75.159 -75.882 -77.32  -79.023 -77.889\n",
      " -77.808 -77.561 -67.932 -67.583 -67.189 -70.964 -70.136 -69.176 -69.236\n",
      " -70.356 -70.774 -78.332 -71.117 -75.896 -76.879 -74.027 -75.607 -75.731\n",
      " -75.468 -78.171 -78.04  -78.753 -79.67  -80.234 -90.    -77.697 -77.301\n",
      " -76.629 -67.228 -66.848 -70.098 -69.94  -69.462 -69.597 -69.992 -70.614\n",
      " -70.257 -76.677 -75.766 -81.706 -78.639 -84.938 -75.27  -75.226 -77.965\n",
      " -78.07  -78.051 -84.866 -85.036 -82.132 -81.668 -76.849 -76.246 -84.788\n",
      " -75.409 -80.49  -69.613 -69.508 -69.581 -69.44  -76.865 -74.826 -79.8\n",
      " -80.124 -83.763 -80.578 -79.082 -78.261 -78.26  -80.167 -83.981 -83.545\n",
      " -78.128 -87.178 -81.916 -81.686 -80.826 -75.982 -75.709 -85.958 -74.619\n",
      " -74.328 -78.522 -73.27  -72.433 -71.11  -72.013 -77.938 -79.419 -76.29\n",
      " -75.295 -75.438 -80.916 -79.158 -79.037 -78.116 -78.083 -80.591 -86.874\n",
      " -85.066 -81.458 -80.566 -80.462 -80.536 -77.855 -75.649 -74.712 -73.676\n",
      " -71.362 -68.434 -62.98  -71.012 -76.076 -76.643 -77.338 -77.508 -77.456\n",
      " -71.062 -80.508 -71.568 -72.151 -81.491 -80.977 -82.094 -84.464 -83.33\n",
      " -85.562 -83.773 -79.487 -78.473 -77.464 -74.955 -74.475 -75.428 -71.493\n",
      " -61.874 -71.553 -71.755 -72.637 -72.241 -76.303 -69.43  -63.982 -74.385\n",
      " -75.143 -84.455 -83.72  -72.827 -75.146 -87.342 -88.603 -84.135 -81.535\n",
      " -83.855 -80.116 -79.836 -76.441 -75.866 -74.393 -72.525 -71.868 -72.902\n",
      " -74.588 -73.101 -72.152 -70.301 -72.587 -71.312 -72.383 -72.728 -76.048\n",
      " -72.848 -75.348 -73.513 -87.86  -85.252 -83.012 -85.531 -82.794 -79.307\n",
      " -77.499 -77.007 -76.146 -74.57  -72.402 -71.406 -72.832 -71.438 -71.745\n",
      " -74.524 -72.331 -70.902 -70.554 -70.638 -70.698 -72.267 -72.522 -74.895\n",
      " -75.705 -87.15  -85.61  -82.708 -82.564 -83.056 -83.834 -78.012 -76.51\n",
      " -74.289 -72.268 -67.933 -66.73  -70.382 -72.762 -77.297 -74.222 -71.227\n",
      " -70.228 -66.651 -64.576 -67.662 -69.806 -69.842 -70.812 -72.558 -87.502\n",
      " -87.853 -82.876 -73.966 -76.832 -84.896 -78.879 -78.216 -74.658 -71.702\n",
      " -64.781 -51.597 -69.275 -71.585 -73.998 -74.578 -71.087 -70.63  -64.139\n",
      " -49.733 -64.489 -66.182 -63.205 -66.356 -71.432 -90.    -88.78  -82.926\n",
      " -81.758 -84.236 -88.396 -78.419 -78.455 -76.204 -73.695 -68.22  -68.595\n",
      " -71.927 -73.776 -77.241 -73.552 -73.516 -72.018 -68.351 -65.76  -68.277\n",
      " -67.465 -52.164 -63.44  -70.094   1.      0.     85.      2.655]\n",
      "[-84.351 -83.983 -80.417 -74.961 -69.494 -77.179 -80.206 -77.813 -77.542\n",
      " -84.415 -70.748 -70.611 -77.697 -80.034 -81.164 -84.652 -79.489 -74.59\n",
      " -73.505 -69.336 -72.5   -69.849 -66.903 -63.208 -66.273 -87.81  -83.555\n",
      " -81.786 -76.895 -75.356 -79.321 -81.829 -71.919 -71.254 -70.9   -70.363\n",
      " -70.477 -70.992 -80.925 -71.611 -80.926 -79.621 -74.127 -77.264 -74.003\n",
      " -71.261 -68.828 -64.863 -62.374 -63.464 -83.427 -82.707 -80.311 -80.916\n",
      " -80.554 -72.088 -71.989 -71.566 -71.772 -71.056 -71.321 -70.722 -71.637\n",
      " -71.327 -78.488 -71.977 -72.672 -74.011 -73.343 -73.109 -71.071 -67.947\n",
      " -63.907 -50.179 -61.535 -81.046 -83.264 -81.155 -85.163 -80.61  -72.335\n",
      " -72.718 -72.793 -71.308 -69.005 -67.744 -68.735 -69.445 -70.755 -70.072\n",
      " -69.809 -72.183 -70.061 -70.106 -73.295 -71.533 -69.336 -66.037 -63.263\n",
      " -64.279 -81.02  -82.023 -79.102 -80.711 -72.561 -72.704 -73.057 -71.721\n",
      " -69.929 -66.09  -64.002 -63.418 -66.724 -69.088 -64.974 -65.531 -77.58\n",
      " -81.076 -81.752 -69.688 -72.086 -70.289 -69.003 -68.071 -68.14  -82.719\n",
      " -81.794 -80.856 -82.466 -81.606 -76.155 -73.707 -71.511 -68.048 -64.623\n",
      " -53.81  -55.669 -64.48  -69.289 -71.285 -74.773 -72.229 -73.647 -79.731\n",
      " -69.157 -68.82  -70.364 -71.966 -71.58  -72.112 -84.435 -83.175 -83.736\n",
      " -77.59  -72.058 -72.054 -72.713 -71.795 -64.517 -63.291 -61.056 -60.324\n",
      " -64.999 -69.108 -70.795 -73.592 -69.119 -72.136 -77.061 -77.426 -67.895\n",
      " -67.604 -67.295 -67.308 -70.169 -83.852 -81.218 -79.765 -77.785 -83.868\n",
      " -66.036 -65.937 -65.607 -65.134 -68.479 -66.674 -63.864 -67.868 -69.928\n",
      " -71.191 -68.459 -69.211 -68.465 -73.382 -74.952 -77.116 -73.49  -67.337\n",
      " -75.139 -67.462 -80.331 -80.117 -84.75  -67.63  -67.4   -66.118 -71.382\n",
      " -68.881 -68.259 -67.944 -65.384 -70.3   -67.443 -68.119 -76.529 -68.15\n",
      " -73.728 -75.41  -81.425 -73.536 -74.819 -75.079 -74.989 -76.922 -75.431\n",
      " -89.146 -87.772 -68.213 -67.991 -67.649 -71.78  -68.029 -65.854 -64.235\n",
      " -64.702 -65.359 -66.065 -67.529 -67.832 -66.337 -67.983 -76.567 -75.979\n",
      " -78.202 -77.146 -75.607 -73.815 -74.424 -76.633 -78.538 -89.372 -86.708\n",
      " -68.398 -84.781 -71.983 -70.019 -67.467 -64.022 -60.43  -63.037 -64.565\n",
      " -70.593 -72.388 -66.16  -70.225 -74.831 -73.231 -74.721 -78.203 -75.861\n",
      " -73.122 -68.445 -70.046 -73.843 -76.533 -88.961 -88.577 -84.828 -68.563\n",
      " -72.053 -69.793 -65.669 -59.948 -47.185 -61.545 -66.548 -69.147 -71.139\n",
      " -72.249 -72.273 -72.977 -74.344 -75.702 -78.084 -76.133 -72.901 -60.653\n",
      " -59.617 -73.799 -76.848 -80.725 -68.59  -68.562 -84.764 -72.501 -69.514\n",
      " -66.584 -62.615 -58.381 -63.685 -66.505 -69.561 -71.436 -71.888 -71.76\n",
      " -77.25  -74.647 -74.162 -78.05  -76.548 -74.37  -71.4   -71.106 -75.279\n",
      " -77.14  -80.848 -88.89  -68.319 -68.196 -73.122 -71.218 -68.695 -66.482\n",
      " -65.361 -67.385 -68.7   -71.333 -71.56  -71.582 -79.48  -73.946 -74.224\n",
      " -75.878 -78.493 -77.357 -77.183 -75.159 -75.882 -77.32  -79.023 -77.889\n",
      " -77.808 -77.561 -67.932 -67.583 -67.189 -70.964 -70.136 -69.176 -69.236\n",
      " -70.356 -70.774 -78.332 -71.117 -75.896 -76.879 -74.027 -75.607 -75.731\n",
      " -75.468 -78.171 -78.04  -78.753 -79.67  -80.234 -90.    -77.697 -77.301\n",
      " -76.629 -67.228 -66.848 -70.098 -69.94  -69.462 -69.597 -69.992 -70.614\n",
      " -70.257 -76.677 -75.766 -81.706 -78.639 -84.938 -75.27  -75.226 -77.965\n",
      " -78.07  -78.051 -84.866 -85.036 -82.132 -81.668 -76.849 -76.246 -84.788\n",
      " -75.409 -80.49  -69.613 -69.508 -69.581 -69.44  -76.865 -74.826 -79.8\n",
      " -80.124 -83.763 -80.578 -79.082 -78.261 -78.26  -80.167 -83.981 -83.545\n",
      " -78.128 -87.178 -81.916 -81.686 -80.826 -75.982 -75.709 -85.958 -74.619\n",
      " -74.328 -78.522 -73.27  -72.433 -71.11  -72.013 -77.938 -79.419 -76.29\n",
      " -75.295 -75.438 -80.916 -79.158 -79.037 -78.116 -78.083 -80.591 -86.874\n",
      " -85.066 -81.458 -80.566 -80.462 -80.536 -77.855 -75.649 -74.712 -73.676\n",
      " -71.362 -68.434 -62.98  -71.012 -76.076 -76.643 -77.338 -77.508 -77.456\n",
      " -71.062 -80.508 -71.568 -72.151 -81.491 -80.977 -82.094 -84.464 -83.33\n",
      " -85.562 -83.773 -79.487 -78.473 -77.464 -74.955 -74.475 -75.428 -71.493\n",
      " -61.874 -71.553 -71.755 -72.637 -72.241 -76.303 -69.43  -63.982 -74.385\n",
      " -75.143 -84.455 -83.72  -72.827 -75.146 -87.342 -88.603 -84.135 -81.535\n",
      " -83.855 -80.116 -79.836 -76.441 -75.866 -74.393 -72.525 -71.868 -72.902\n",
      " -74.588 -73.101 -72.152 -70.301 -72.587 -71.312 -72.383 -72.728 -76.048\n",
      " -72.848 -75.348 -73.513 -87.86  -85.252 -83.012 -85.531 -82.794 -79.307\n",
      " -77.499 -77.007 -76.146 -74.57  -72.402 -71.406 -72.832 -71.438 -71.745\n",
      " -74.524 -72.331 -70.902 -70.554 -70.638 -70.698 -72.267 -72.522 -74.895\n",
      " -75.705 -87.15  -85.61  -82.708 -82.564 -83.056 -83.834 -78.012 -76.51\n",
      " -74.289 -72.268 -67.933 -66.73  -70.382 -72.762 -77.297 -74.222 -71.227\n",
      " -70.228 -66.651 -64.576 -67.662 -69.806 -69.842 -70.812 -72.558 -87.502\n",
      " -87.853 -82.876 -73.966 -76.832 -84.896 -78.879 -78.216 -74.658 -71.702\n",
      " -64.781 -51.597 -69.275 -71.585 -73.998 -74.578 -71.087 -70.63  -64.139\n",
      " -49.733 -64.489 -66.182 -63.205 -66.356 -71.432 -90.    -88.78  -82.926\n",
      " -81.758 -84.236 -88.396 -78.419 -78.455 -76.204 -73.695 -68.22  -68.595\n",
      " -71.927 -73.776 -77.241 -73.552 -73.516 -72.018 -68.351 -65.76  -68.277\n",
      " -67.465 -52.164 -63.44  -70.094   0.     85.   ]\n",
      "2.655\n",
      "***\n",
      "[-82.762 -82.619 -83.86  -85.303 -84.552 -82.95  -83.218 -84.072 -85.945\n",
      " -87.261 -88.084 -83.105 -83.223 -82.523 -83.584 -83.547 -82.995 -90.\n",
      " -82.56  -82.751 -90.    -83.786 -86.734 -86.451 -89.376 -81.981 -81.956\n",
      " -87.755 -82.306 -87.823 -82.388 -64.719 -80.469 -83.791 -86.606 -87.123\n",
      " -88.197 -84.031 -84.902 -86.154 -85.237 -85.808 -86.61  -85.887 -81.816\n",
      " -84.093 -81.66  -83.207 -83.341 -84.784 -81.211 -74.293 -74.632 -86.479\n",
      " -81.608 -81.911 -78.3   -79.99  -86.154 -83.384 -83.559 -83.481 -84.258\n",
      " -84.052 -83.761 -82.752 -83.321 -82.161 -84.642 -82.737 -81.505 -81.518\n",
      " -82.032 -82.758 -85.635 -74.068 -73.933 -74.548 -75.001 -75.029 -85.262\n",
      " -80.649 -80.968 -81.926 -81.177 -81.654 -88.044 -84.135 -83.932 -80.167\n",
      " -76.863 -77.961 -81.427 -82.401 -81.885 -82.257 -80.288 -81.808 -76.457\n",
      " -79.256 -73.927 -77.61  -73.98  -78.413 -89.659 -78.333 -79.547 -81.17\n",
      " -84.503 -80.795 -84.344 -81.722 -82.181 -81.797 -79.805 -63.687 -76.128\n",
      " -79.661 -80.552 -81.469 -80.542 -82.475 -72.802 -69.697 -73.411 -72.896\n",
      " -71.751 -74.071 -75.494 -80.13  -78.331 -80.468 -80.008 -83.806 -81.777\n",
      " -81.231 -84.63  -82.026 -82.32  -80.041 -78.748 -80.236 -79.935 -81.213\n",
      " -74.521 -82.811 -73.598 -73.279 -82.764 -73.672 -65.535 -66.19  -69.684\n",
      " -76.197 -77.711 -81.263 -78.723 -79.249 -80.99  -79.637 -82.948 -80.938\n",
      " -83.766 -81.231 -81.439 -81.15  -82.706 -80.528 -75.366 -74.704 -86.466\n",
      " -73.792 -74.449 -73.953 -74.607 -59.993 -59.887 -69.176 -73.069 -74.687\n",
      " -75.417 -77.446 -77.879 -76.8   -77.453 -79.725 -81.54  -80.783 -82.84\n",
      " -81.148 -80.933 -81.03  -75.368 -75.179 -82.315 -75.536 -73.974 -72.648\n",
      " -70.375 -71.497 -67.276 -69.137 -71.352 -73.665 -78.506 -74.779 -78.012\n",
      " -77.138 -70.379 -70.371 -79.406 -79.24  -80.366 -84.89  -78.766 -79.117\n",
      " -82.608 -75.463 -75.665 -76.476 -74.706 -72.532 -67.732 -66.845 -67.282\n",
      " -55.606 -67.501 -70.867 -73.679 -75.387 -72.79  -72.913 -79.168 -78.582\n",
      " -75.097 -75.865 -76.326 -76.633 -80.167 -79.099 -79.725 -80.053 -75.894\n",
      " -75.512 -75.984 -74.014 -71.979 -65.367 -54.393 -62.421 -63.212 -66.096\n",
      " -70.291 -72.995 -74.535 -74.091 -77.269 -73.969 -76.556 -79.785 -76.497\n",
      " -76.568 -76.495 -73.302 -73.79  -75.501 -78.622 -75.3   -75.13  -75.375\n",
      " -74.298 -71.641 -67.318 -62.989 -64.678 -52.206 -65.949 -71.204 -72.88\n",
      " -74.472 -80.233 -72.609 -78.441 -76.237 -77.012 -75.818 -75.555 -76.356\n",
      " -77.449 -75.766 -79.477 -75.578 -74.736 -74.812 -74.934 -75.311 -73.752\n",
      " -71.456 -69.612 -70.003 -68.502 -69.581 -72.484 -74.562 -75.69  -73.01\n",
      " -72.828 -76.57  -75.873 -74.4   -73.241 -73.495 -74.884 -75.684 -76.467\n",
      " -75.632 -75.972 -77.345 -72.786 -72.456 -76.187 -74.667 -67.857 -72.982\n",
      " -68.626 -72.687 -73.242 -73.979 -76.155 -74.635 -75.815 -74.592 -77.154\n",
      " -75.784 -71.686 -66.943 -67.977 -72.436 -75.203 -77.113 -74.929 -75.712\n",
      " -74.565 -71.293 -68.656 -70.387 -72.161 -71.68  -69.273 -71.813 -76.414\n",
      " -75.243 -76.163 -75.783 -78.967 -74.141 -77.259 -76.19  -73.907 -70.608\n",
      " -59.158 -58.373 -71.69  -75.068 -76.107 -72.651 -67.49  -69.044 -69.46\n",
      " -72.656 -68.446 -67.302 -67.438 -71.943 -71.329 -72.048 -86.702 -71.926\n",
      " -72.191 -73.018 -75.206 -75.824 -76.387 -74.933 -70.679 -69.035 -69.99\n",
      " -72.205 -73.908 -77.093 -69.663 -67.982 -74.313 -68.333 -68.195 -67.744\n",
      " -67.578 -67.564 -67.916 -69.222 -80.016 -85.298 -80.095 -78.66  -76.639\n",
      " -68.492 -73.809 -77.504 -75.712 -74.188 -74.197 -73.581 -75.712 -73.798\n",
      " -71.104 -71.077 -69.421 -69.88  -69.141 -67.138 -66.933 -65.364 -61.467\n",
      " -65.698 -68.612 -86.296 -79.778 -79.574 -78.714 -77.59  -69.365 -74.393\n",
      " -75.193 -74.053 -76.464 -76.936 -76.628 -74.053 -75.429 -71.323 -69.721\n",
      " -71.398 -68.884 -66.646 -63.811 -63.22  -62.2   -47.272 -63.591 -70.99\n",
      " -83.692 -86.872 -85.    -82.13  -79.605 -79.297 -79.811 -81.739 -73.831\n",
      " -73.125 -73.034 -81.138 -72.219 -72.367 -71.417 -71.81  -69.317 -67.461\n",
      " -64.706 -60.015 -58.083 -62.095 -62.867 -66.555 -69.418 -84.556 -84.467\n",
      " -83.533 -85.937 -83.143 -81.413 -81.542 -79.984 -81.239 -72.909 -72.656\n",
      " -72.156 -73.262 -71.543 -71.524 -70.925 -69.933 -66.446 -62.914 -56.375\n",
      " -49.847 -61.768 -65.713 -67.391 -69.796 -90.    -86.803 -84.6   -85.926\n",
      " -84.387 -83.151 -82.999 -80.77  -81.731 -80.573 -77.863 -68.718 -75.58\n",
      " -79.568 -71.151 -70.884 -70.063 -65.365 -60.565 -63.245 -63.388 -64.403\n",
      " -67.24  -69.834 -70.544 -90.    -89.566 -84.245 -89.07  -83.446 -81.774\n",
      " -81.322 -83.317 -82.902 -79.837 -77.586 -73.416 -74.882 -80.241 -70.971\n",
      " -67.362 -76.811 -69.266 -67.538 -65.876 -67.319 -67.534 -68.76  -71.776\n",
      " -73.738 -87.961 -88.942 -86.237 -86.911 -85.704 -82.896 -84.47  -81.41\n",
      " -81.836 -80.79  -78.576 -78.09  -75.822 -76.061 -75.93  -79.136 -66.705\n",
      " -66.34  -75.926 -69.819 -69.682 -66.232 -65.908 -69.171 -71.16  -90.\n",
      " -90.    -87.988 -88.022 -86.805 -85.024 -83.31  -83.076 -80.845 -78.415\n",
      " -76.27  -79.131 -79.328 -79.677 -75.953 -74.318 -78.071 -79.368 -66.079\n",
      " -65.99  -65.903 -66.086 -66.428 -81.159 -77.966 -90.    -90.    -88.313\n",
      " -88.684 -87.066 -90.    -85.069 -82.827 -80.066 -75.727 -58.768 -76.224\n",
      " -79.576 -79.392 -74.622 -75.51  -75.005 -80.008 -74.966 -74.874 -76.183\n",
      " -76.899 -76.448 -77.455 -77.72    1.     36.     27.      7.71 ]\n",
      "[-82.762 -82.619 -83.86  -85.303 -84.552 -82.95  -83.218 -84.072 -85.945\n",
      " -87.261 -88.084 -83.105 -83.223 -82.523 -83.584 -83.547 -82.995 -90.\n",
      " -82.56  -82.751 -90.    -83.786 -86.734 -86.451 -89.376 -81.981 -81.956\n",
      " -87.755 -82.306 -87.823 -82.388 -64.719 -80.469 -83.791 -86.606 -87.123\n",
      " -88.197 -84.031 -84.902 -86.154 -85.237 -85.808 -86.61  -85.887 -81.816\n",
      " -84.093 -81.66  -83.207 -83.341 -84.784 -81.211 -74.293 -74.632 -86.479\n",
      " -81.608 -81.911 -78.3   -79.99  -86.154 -83.384 -83.559 -83.481 -84.258\n",
      " -84.052 -83.761 -82.752 -83.321 -82.161 -84.642 -82.737 -81.505 -81.518\n",
      " -82.032 -82.758 -85.635 -74.068 -73.933 -74.548 -75.001 -75.029 -85.262\n",
      " -80.649 -80.968 -81.926 -81.177 -81.654 -88.044 -84.135 -83.932 -80.167\n",
      " -76.863 -77.961 -81.427 -82.401 -81.885 -82.257 -80.288 -81.808 -76.457\n",
      " -79.256 -73.927 -77.61  -73.98  -78.413 -89.659 -78.333 -79.547 -81.17\n",
      " -84.503 -80.795 -84.344 -81.722 -82.181 -81.797 -79.805 -63.687 -76.128\n",
      " -79.661 -80.552 -81.469 -80.542 -82.475 -72.802 -69.697 -73.411 -72.896\n",
      " -71.751 -74.071 -75.494 -80.13  -78.331 -80.468 -80.008 -83.806 -81.777\n",
      " -81.231 -84.63  -82.026 -82.32  -80.041 -78.748 -80.236 -79.935 -81.213\n",
      " -74.521 -82.811 -73.598 -73.279 -82.764 -73.672 -65.535 -66.19  -69.684\n",
      " -76.197 -77.711 -81.263 -78.723 -79.249 -80.99  -79.637 -82.948 -80.938\n",
      " -83.766 -81.231 -81.439 -81.15  -82.706 -80.528 -75.366 -74.704 -86.466\n",
      " -73.792 -74.449 -73.953 -74.607 -59.993 -59.887 -69.176 -73.069 -74.687\n",
      " -75.417 -77.446 -77.879 -76.8   -77.453 -79.725 -81.54  -80.783 -82.84\n",
      " -81.148 -80.933 -81.03  -75.368 -75.179 -82.315 -75.536 -73.974 -72.648\n",
      " -70.375 -71.497 -67.276 -69.137 -71.352 -73.665 -78.506 -74.779 -78.012\n",
      " -77.138 -70.379 -70.371 -79.406 -79.24  -80.366 -84.89  -78.766 -79.117\n",
      " -82.608 -75.463 -75.665 -76.476 -74.706 -72.532 -67.732 -66.845 -67.282\n",
      " -55.606 -67.501 -70.867 -73.679 -75.387 -72.79  -72.913 -79.168 -78.582\n",
      " -75.097 -75.865 -76.326 -76.633 -80.167 -79.099 -79.725 -80.053 -75.894\n",
      " -75.512 -75.984 -74.014 -71.979 -65.367 -54.393 -62.421 -63.212 -66.096\n",
      " -70.291 -72.995 -74.535 -74.091 -77.269 -73.969 -76.556 -79.785 -76.497\n",
      " -76.568 -76.495 -73.302 -73.79  -75.501 -78.622 -75.3   -75.13  -75.375\n",
      " -74.298 -71.641 -67.318 -62.989 -64.678 -52.206 -65.949 -71.204 -72.88\n",
      " -74.472 -80.233 -72.609 -78.441 -76.237 -77.012 -75.818 -75.555 -76.356\n",
      " -77.449 -75.766 -79.477 -75.578 -74.736 -74.812 -74.934 -75.311 -73.752\n",
      " -71.456 -69.612 -70.003 -68.502 -69.581 -72.484 -74.562 -75.69  -73.01\n",
      " -72.828 -76.57  -75.873 -74.4   -73.241 -73.495 -74.884 -75.684 -76.467\n",
      " -75.632 -75.972 -77.345 -72.786 -72.456 -76.187 -74.667 -67.857 -72.982\n",
      " -68.626 -72.687 -73.242 -73.979 -76.155 -74.635 -75.815 -74.592 -77.154\n",
      " -75.784 -71.686 -66.943 -67.977 -72.436 -75.203 -77.113 -74.929 -75.712\n",
      " -74.565 -71.293 -68.656 -70.387 -72.161 -71.68  -69.273 -71.813 -76.414\n",
      " -75.243 -76.163 -75.783 -78.967 -74.141 -77.259 -76.19  -73.907 -70.608\n",
      " -59.158 -58.373 -71.69  -75.068 -76.107 -72.651 -67.49  -69.044 -69.46\n",
      " -72.656 -68.446 -67.302 -67.438 -71.943 -71.329 -72.048 -86.702 -71.926\n",
      " -72.191 -73.018 -75.206 -75.824 -76.387 -74.933 -70.679 -69.035 -69.99\n",
      " -72.205 -73.908 -77.093 -69.663 -67.982 -74.313 -68.333 -68.195 -67.744\n",
      " -67.578 -67.564 -67.916 -69.222 -80.016 -85.298 -80.095 -78.66  -76.639\n",
      " -68.492 -73.809 -77.504 -75.712 -74.188 -74.197 -73.581 -75.712 -73.798\n",
      " -71.104 -71.077 -69.421 -69.88  -69.141 -67.138 -66.933 -65.364 -61.467\n",
      " -65.698 -68.612 -86.296 -79.778 -79.574 -78.714 -77.59  -69.365 -74.393\n",
      " -75.193 -74.053 -76.464 -76.936 -76.628 -74.053 -75.429 -71.323 -69.721\n",
      " -71.398 -68.884 -66.646 -63.811 -63.22  -62.2   -47.272 -63.591 -70.99\n",
      " -83.692 -86.872 -85.    -82.13  -79.605 -79.297 -79.811 -81.739 -73.831\n",
      " -73.125 -73.034 -81.138 -72.219 -72.367 -71.417 -71.81  -69.317 -67.461\n",
      " -64.706 -60.015 -58.083 -62.095 -62.867 -66.555 -69.418 -84.556 -84.467\n",
      " -83.533 -85.937 -83.143 -81.413 -81.542 -79.984 -81.239 -72.909 -72.656\n",
      " -72.156 -73.262 -71.543 -71.524 -70.925 -69.933 -66.446 -62.914 -56.375\n",
      " -49.847 -61.768 -65.713 -67.391 -69.796 -90.    -86.803 -84.6   -85.926\n",
      " -84.387 -83.151 -82.999 -80.77  -81.731 -80.573 -77.863 -68.718 -75.58\n",
      " -79.568 -71.151 -70.884 -70.063 -65.365 -60.565 -63.245 -63.388 -64.403\n",
      " -67.24  -69.834 -70.544 -90.    -89.566 -84.245 -89.07  -83.446 -81.774\n",
      " -81.322 -83.317 -82.902 -79.837 -77.586 -73.416 -74.882 -80.241 -70.971\n",
      " -67.362 -76.811 -69.266 -67.538 -65.876 -67.319 -67.534 -68.76  -71.776\n",
      " -73.738 -87.961 -88.942 -86.237 -86.911 -85.704 -82.896 -84.47  -81.41\n",
      " -81.836 -80.79  -78.576 -78.09  -75.822 -76.061 -75.93  -79.136 -66.705\n",
      " -66.34  -75.926 -69.819 -69.682 -66.232 -65.908 -69.171 -71.16  -90.\n",
      " -90.    -87.988 -88.022 -86.805 -85.024 -83.31  -83.076 -80.845 -78.415\n",
      " -76.27  -79.131 -79.328 -79.677 -75.953 -74.318 -78.071 -79.368 -66.079\n",
      " -65.99  -65.903 -66.086 -66.428 -81.159 -77.966 -90.    -90.    -88.313\n",
      " -88.684 -87.066 -90.    -85.069 -82.827 -80.066 -75.727 -58.768 -76.224\n",
      " -79.576 -79.392 -74.622 -75.51  -75.005 -80.008 -74.966 -74.874 -76.183\n",
      " -76.899 -76.448 -77.455 -77.72   36.     27.   ]\n",
      "7.71\n",
      "***\n",
      "[-90.    -90.    -90.    -90.    -87.817 -86.524 -84.114 -82.186 -84.897\n",
      " -84.446 -85.5   -85.967 -87.064 -86.496 -87.676 -90.    -86.429 -90.\n",
      " -90.    -88.266 -89.372 -88.887 -89.146 -90.    -90.    -90.    -90.\n",
      " -88.704 -88.315 -87.09  -85.984 -81.722 -75.097 -77.102 -83.672 -86.651\n",
      " -82.016 -83.959 -81.16  -81.577 -87.441 -85.158 -85.337 -87.254 -87.223\n",
      " -90.    -89.328 -89.757 -90.    -90.    -90.    -89.463 -87.438 -87.555\n",
      " -85.149 -83.816 -79.934 -64.894 -74.046 -83.538 -83.519 -80.575 -80.428\n",
      " -82.036 -81.398 -87.319 -90.    -88.06  -90.    -90.    -88.448 -88.661\n",
      " -88.959 -89.793 -89.206 -89.819 -87.113 -89.011 -86.805 -88.303 -83.953\n",
      " -83.903 -79.916 -80.468 -80.286 -71.992 -71.581 -71.543 -71.416 -71.626\n",
      " -71.938 -87.297 -88.682 -88.691 -86.16  -85.038 -88.589 -89.555 -90.\n",
      " -89.976 -90.    -83.271 -82.715 -83.226 -82.652 -81.43  -82.966 -81.875\n",
      " -73.584 -73.286 -72.232 -71.901 -71.467 -71.367 -71.552 -72.251 -84.637\n",
      " -73.445 -84.518 -83.522 -83.642 -83.479 -86.148 -90.    -89.606 -83.921\n",
      " -83.236 -82.57  -87.244 -82.615 -82.407 -83.538 -74.701 -74.158 -73.79\n",
      " -74.063 -73.068 -71.115 -70.906 -70.182 -72.422 -73.405 -74.164 -74.529\n",
      " -83.889 -84.042 -86.749 -87.478 -87.873 -90.    -86.69  -86.431 -85.94\n",
      " -85.054 -82.797 -86.902 -83.96  -74.949 -86.616 -73.788 -72.148 -70.213\n",
      " -67.568 -67.239 -66.434 -70.078 -73.331 -74.487 -75.105 -85.836 -86.655\n",
      " -86.579 -88.002 -90.    -90.    -85.71  -82.503 -81.175 -83.344 -83.769\n",
      " -83.192 -75.539 -75.371 -75.231 -73.379 -71.333 -67.848 -62.953 -58.412\n",
      " -65.452 -69.955 -72.339 -74.976 -86.845 -85.449 -86.385 -85.37  -86.903\n",
      " -87.109 -87.953 -83.    -79.843 -74.159 -80.026 -81.269 -83.165 -75.395\n",
      " -83.491 -75.432 -73.496 -70.646 -66.296 -60.415 -45.743 -64.819 -69.975\n",
      " -73.127 -75.235 -75.282 -84.691 -90.    -90.    -87.727 -87.389 -88.343\n",
      " -82.461 -76.784 -60.168 -78.907 -81.85  -84.305 -85.319 -75.104 -75.142\n",
      " -74.072 -71.608 -69.14  -65.466 -66.073 -66.542 -70.685 -73.268 -75.287\n",
      " -75.072 -75.217 -85.16  -85.034 -88.521 -90.    -87.653 -82.094 -80.433\n",
      " -78.226 -79.412 -82.035 -82.648 -86.82  -74.705 -74.649 -74.183 -73.305\n",
      " -71.342 -70.64  -70.199 -70.185 -72.396 -74.072 -74.447 -85.653 -81.345\n",
      " -85.006 -90.    -81.398 -81.899 -82.674 -80.602 -82.999 -80.898 -80.654\n",
      " -81.346 -81.293 -81.512 -82.89  -82.438 -85.364 -73.144 -72.429 -73.024\n",
      " -72.958 -87.062 -72.587 -86.524 -73.342 -79.156 -80.986 -81.192 -88.689\n",
      " -90.    -81.698 -82.456 -72.782 -82.783 -81.728 -78.714 -78.489 -84.515\n",
      " -78.131 -73.632 -76.775 -83.336 -72.668 -85.363 -71.796 -71.889 -72.113\n",
      " -86.574 -83.081 -81.05  -81.278 -79.286 -82.672 -81.131 -80.614 -80.788\n",
      " -81.936 -79.11  -79.262 -85.638 -78.317 -78.2   -78.606 -79.05  -76.227\n",
      " -77.922 -85.446 -82.319 -84.66  -71.801 -78.232 -79.015 -88.501 -81.468\n",
      " -90.    -80.96  -82.808 -79.494 -77.131 -77.118 -79.058 -80.814 -81.395\n",
      " -79.951 -84.836 -73.475 -73.196 -73.106 -73.011 -73.198 -73.521 -87.31\n",
      " -89.051 -77.868 -78.891 -78.745 -78.388 -80.732 -84.871 -83.294 -82.738\n",
      " -80.443 -76.914 -71.691 -72.696 -76.241 -79.001 -74.921 -87.292 -74.116\n",
      " -73.518 -86.66  -73.065 -73.156 -73.423 -74.11  -74.417 -82.038 -81.824\n",
      " -82.422 -82.461 -84.033 -84.428 -84.825 -84.619 -82.569 -79.163 -76.743\n",
      " -69.028 -63.083 -75.231 -75.295 -75.221 -87.85  -75.927 -75.316 -73.145\n",
      " -72.405 -72.903 -74.386 -75.566 -75.732 -75.991 -76.542 -81.001 -81.332\n",
      " -84.534 -86.454 -84.626 -82.557 -83.681 -80.748 -79.335 -76.162 -75.603\n",
      " -73.899 -75.671 -75.818 -75.805 -75.431 -72.217 -70.044 -68.22  -69.128\n",
      " -71.893 -74.627 -76.184 -74.515 -74.706 -82.029 -82.514 -82.47  -84.17\n",
      " -86.428 -81.419 -83.684 -78.035 -77.11  -76.506 -75.776 -76.405 -76.489\n",
      " -76.195 -76.55  -74.128 -70.948 -67.21  -60.409 -64.565 -69.523 -73.613\n",
      " -76.076 -88.527 -74.925 -74.932 -89.6   -87.671 -85.646 -85.092 -81.649\n",
      " -85.445 -78.415 -82.086 -77.932 -76.475 -74.539 -73.788 -90.    -76.592\n",
      " -74.807 -71.271 -65.71  -50.463 -64.522 -69.042 -72.564 -75.589 -74.975\n",
      " -88.16  -74.984 -82.545 -82.566 -89.623 -84.855 -85.196 -82.216 -82.287\n",
      " -80.14  -76.598 -74.241 -72.411 -69.645 -76.    -75.888 -74.493 -71.789\n",
      " -69.357 -66.972 -67.877 -71.856 -74.477 -76.197 -74.813 -74.926 -84.077\n",
      " -82.534 -82.377 -86.446 -83.051 -79.951 -76.788 -80.363 -79.421 -76.131\n",
      " -74.78  -69.797 -53.764 -72.019 -90.    -75.819 -74.317 -72.414 -71.738\n",
      " -72.44  -72.78  -75.453 -73.989 -74.458 -74.563 -83.862 -83.387 -83.261\n",
      " -83.739 -83.516 -83.821 -80.136 -88.686 -79.525 -77.92  -74.929 -71.309\n",
      " -66.691 -90.    -90.    -71.067 -90.    -75.125 -74.729 -74.658 -70.554\n",
      " -89.86  -73.56  -73.868 -82.042 -82.026 -90.    -86.025 -90.    -84.687\n",
      " -84.425 -80.138 -80.271 -90.    -79.091 -76.75  -73.373 -72.913 -86.964\n",
      " -81.245 -70.729 -70.472 -70.251 -89.954 -70.161 -70.447 -90.    -73.206\n",
      " -90.    -81.683 -90.    -86.997 -88.882 -86.384 -84.993 -90.    -90.\n",
      " -89.3   -89.465 -90.    -79.015 -77.65  -77.174 -90.    -90.    -80.657\n",
      " -80.336 -80.012 -79.968 -70.151 -90.    -90.    -83.097 -83.337 -83.489\n",
      " -86.283 -86.861 -86.928 -87.022 -87.066 -89.699 -85.217 -90.    -78.139\n",
      " -90.    -74.249 -73.902 -73.724   1.     76.     26.     10.587]\n",
      "[-90.    -90.    -90.    -90.    -87.817 -86.524 -84.114 -82.186 -84.897\n",
      " -84.446 -85.5   -85.967 -87.064 -86.496 -87.676 -90.    -86.429 -90.\n",
      " -90.    -88.266 -89.372 -88.887 -89.146 -90.    -90.    -90.    -90.\n",
      " -88.704 -88.315 -87.09  -85.984 -81.722 -75.097 -77.102 -83.672 -86.651\n",
      " -82.016 -83.959 -81.16  -81.577 -87.441 -85.158 -85.337 -87.254 -87.223\n",
      " -90.    -89.328 -89.757 -90.    -90.    -90.    -89.463 -87.438 -87.555\n",
      " -85.149 -83.816 -79.934 -64.894 -74.046 -83.538 -83.519 -80.575 -80.428\n",
      " -82.036 -81.398 -87.319 -90.    -88.06  -90.    -90.    -88.448 -88.661\n",
      " -88.959 -89.793 -89.206 -89.819 -87.113 -89.011 -86.805 -88.303 -83.953\n",
      " -83.903 -79.916 -80.468 -80.286 -71.992 -71.581 -71.543 -71.416 -71.626\n",
      " -71.938 -87.297 -88.682 -88.691 -86.16  -85.038 -88.589 -89.555 -90.\n",
      " -89.976 -90.    -83.271 -82.715 -83.226 -82.652 -81.43  -82.966 -81.875\n",
      " -73.584 -73.286 -72.232 -71.901 -71.467 -71.367 -71.552 -72.251 -84.637\n",
      " -73.445 -84.518 -83.522 -83.642 -83.479 -86.148 -90.    -89.606 -83.921\n",
      " -83.236 -82.57  -87.244 -82.615 -82.407 -83.538 -74.701 -74.158 -73.79\n",
      " -74.063 -73.068 -71.115 -70.906 -70.182 -72.422 -73.405 -74.164 -74.529\n",
      " -83.889 -84.042 -86.749 -87.478 -87.873 -90.    -86.69  -86.431 -85.94\n",
      " -85.054 -82.797 -86.902 -83.96  -74.949 -86.616 -73.788 -72.148 -70.213\n",
      " -67.568 -67.239 -66.434 -70.078 -73.331 -74.487 -75.105 -85.836 -86.655\n",
      " -86.579 -88.002 -90.    -90.    -85.71  -82.503 -81.175 -83.344 -83.769\n",
      " -83.192 -75.539 -75.371 -75.231 -73.379 -71.333 -67.848 -62.953 -58.412\n",
      " -65.452 -69.955 -72.339 -74.976 -86.845 -85.449 -86.385 -85.37  -86.903\n",
      " -87.109 -87.953 -83.    -79.843 -74.159 -80.026 -81.269 -83.165 -75.395\n",
      " -83.491 -75.432 -73.496 -70.646 -66.296 -60.415 -45.743 -64.819 -69.975\n",
      " -73.127 -75.235 -75.282 -84.691 -90.    -90.    -87.727 -87.389 -88.343\n",
      " -82.461 -76.784 -60.168 -78.907 -81.85  -84.305 -85.319 -75.104 -75.142\n",
      " -74.072 -71.608 -69.14  -65.466 -66.073 -66.542 -70.685 -73.268 -75.287\n",
      " -75.072 -75.217 -85.16  -85.034 -88.521 -90.    -87.653 -82.094 -80.433\n",
      " -78.226 -79.412 -82.035 -82.648 -86.82  -74.705 -74.649 -74.183 -73.305\n",
      " -71.342 -70.64  -70.199 -70.185 -72.396 -74.072 -74.447 -85.653 -81.345\n",
      " -85.006 -90.    -81.398 -81.899 -82.674 -80.602 -82.999 -80.898 -80.654\n",
      " -81.346 -81.293 -81.512 -82.89  -82.438 -85.364 -73.144 -72.429 -73.024\n",
      " -72.958 -87.062 -72.587 -86.524 -73.342 -79.156 -80.986 -81.192 -88.689\n",
      " -90.    -81.698 -82.456 -72.782 -82.783 -81.728 -78.714 -78.489 -84.515\n",
      " -78.131 -73.632 -76.775 -83.336 -72.668 -85.363 -71.796 -71.889 -72.113\n",
      " -86.574 -83.081 -81.05  -81.278 -79.286 -82.672 -81.131 -80.614 -80.788\n",
      " -81.936 -79.11  -79.262 -85.638 -78.317 -78.2   -78.606 -79.05  -76.227\n",
      " -77.922 -85.446 -82.319 -84.66  -71.801 -78.232 -79.015 -88.501 -81.468\n",
      " -90.    -80.96  -82.808 -79.494 -77.131 -77.118 -79.058 -80.814 -81.395\n",
      " -79.951 -84.836 -73.475 -73.196 -73.106 -73.011 -73.198 -73.521 -87.31\n",
      " -89.051 -77.868 -78.891 -78.745 -78.388 -80.732 -84.871 -83.294 -82.738\n",
      " -80.443 -76.914 -71.691 -72.696 -76.241 -79.001 -74.921 -87.292 -74.116\n",
      " -73.518 -86.66  -73.065 -73.156 -73.423 -74.11  -74.417 -82.038 -81.824\n",
      " -82.422 -82.461 -84.033 -84.428 -84.825 -84.619 -82.569 -79.163 -76.743\n",
      " -69.028 -63.083 -75.231 -75.295 -75.221 -87.85  -75.927 -75.316 -73.145\n",
      " -72.405 -72.903 -74.386 -75.566 -75.732 -75.991 -76.542 -81.001 -81.332\n",
      " -84.534 -86.454 -84.626 -82.557 -83.681 -80.748 -79.335 -76.162 -75.603\n",
      " -73.899 -75.671 -75.818 -75.805 -75.431 -72.217 -70.044 -68.22  -69.128\n",
      " -71.893 -74.627 -76.184 -74.515 -74.706 -82.029 -82.514 -82.47  -84.17\n",
      " -86.428 -81.419 -83.684 -78.035 -77.11  -76.506 -75.776 -76.405 -76.489\n",
      " -76.195 -76.55  -74.128 -70.948 -67.21  -60.409 -64.565 -69.523 -73.613\n",
      " -76.076 -88.527 -74.925 -74.932 -89.6   -87.671 -85.646 -85.092 -81.649\n",
      " -85.445 -78.415 -82.086 -77.932 -76.475 -74.539 -73.788 -90.    -76.592\n",
      " -74.807 -71.271 -65.71  -50.463 -64.522 -69.042 -72.564 -75.589 -74.975\n",
      " -88.16  -74.984 -82.545 -82.566 -89.623 -84.855 -85.196 -82.216 -82.287\n",
      " -80.14  -76.598 -74.241 -72.411 -69.645 -76.    -75.888 -74.493 -71.789\n",
      " -69.357 -66.972 -67.877 -71.856 -74.477 -76.197 -74.813 -74.926 -84.077\n",
      " -82.534 -82.377 -86.446 -83.051 -79.951 -76.788 -80.363 -79.421 -76.131\n",
      " -74.78  -69.797 -53.764 -72.019 -90.    -75.819 -74.317 -72.414 -71.738\n",
      " -72.44  -72.78  -75.453 -73.989 -74.458 -74.563 -83.862 -83.387 -83.261\n",
      " -83.739 -83.516 -83.821 -80.136 -88.686 -79.525 -77.92  -74.929 -71.309\n",
      " -66.691 -90.    -90.    -71.067 -90.    -75.125 -74.729 -74.658 -70.554\n",
      " -89.86  -73.56  -73.868 -82.042 -82.026 -90.    -86.025 -90.    -84.687\n",
      " -84.425 -80.138 -80.271 -90.    -79.091 -76.75  -73.373 -72.913 -86.964\n",
      " -81.245 -70.729 -70.472 -70.251 -89.954 -70.161 -70.447 -90.    -73.206\n",
      " -90.    -81.683 -90.    -86.997 -88.882 -86.384 -84.993 -90.    -90.\n",
      " -89.3   -89.465 -90.    -79.015 -77.65  -77.174 -90.    -90.    -80.657\n",
      " -80.336 -80.012 -79.968 -70.151 -90.    -90.    -83.097 -83.337 -83.489\n",
      " -86.283 -86.861 -86.928 -87.022 -87.066 -89.699 -85.217 -90.    -78.139\n",
      " -90.    -74.249 -73.902 -73.724  76.     26.   ]\n",
      "10.587\n"
     ]
    }
   ],
   "source": [
    "X_train, X_val, X_test, y_train, y_val, y_test = split_data(data_reg, 256)\n",
    "print(data_reg[0])\n",
    "print(X_train[0])\n",
    "print(y_train[0])\n",
    "print(\"***\")\n",
    "print(data_reg[256])\n",
    "print(X_val[0])\n",
    "print(y_val[0])\n",
    "print(\"***\")\n",
    "print(data_reg[256+85])\n",
    "print(X_test[0])\n",
    "print(y_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_reg[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-16.501,  87.   ,  52.   , ...,  10.   ,  34.   , -21.746],\n",
       "       [ -4.746,  74.   ,  71.   , ...,     nan,     nan,   5.038],\n",
       "       [-22.775,  97.   ,  57.   , ...,   0.   ,     nan,   2.342],\n",
       "       ...,\n",
       "       [ -2.106,   1.   ,  62.   , ...,     nan,     nan,  16.766],\n",
       "       [-26.324,  77.   ,   2.   , ...,  71.   ,  10.   ,   7.17 ],\n",
       "       [-23.057,   5.   ,  82.   , ...,     nan,     nan,  14.104]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## SVM(SVC)\n",
    "average_class_diff_power = []\n",
    "fp_mean_power = []\n",
    "accuracy, f_score, false_positive, false_negative = [], [], [], []\n",
    "best_c_lst = []\n",
    "fp_penalty_coef, fn_penalty_coef = 1, 1\n",
    "metric = \"fp_min\"  # {\"accuracy\", \"fp_min\"}\n",
    "class_weight = {0:fp_penalty_coef/(fp_penalty_coef + fn_penalty_coef), 1:fn_penalty_coef/(fp_penalty_coef + fn_penalty_coef)}\n",
    "best_c, bestsvcclassifier = None, None\n",
    "for number_sample in number_samples:\n",
    "    best_accuracy = -float('inf')\n",
    "    best_fp = float('inf')\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test = split_data(data_class, number_sample)\n",
    "    scaler_x = StandardScaler()\n",
    "    scaler_x.fit(X_train)\n",
    "    X_train = scaler_x.transform(X_train)\n",
    "    X_val = scaler_x.transform(X_val)\n",
    "    for c in C_vec:\n",
    "        svclassifier = SVC(kernel='rbf', C=c, class_weight = class_weight)\n",
    "        svclassifier.fit(X_train, y_train)\n",
    "        \n",
    "        #validating\n",
    "        y_pred_val = svclassifier.predict(X_val)\n",
    "        if metric == \"accuracy\":\n",
    "            accuracy_tmp = metrics.accuracy_score(y_val, y_pred_val)\n",
    "            if accuracy_tmp > best_accuracy:\n",
    "                best_accuracy = accuracy_tmp\n",
    "                best_c = c\n",
    "                bestsvcclassifier = svclassifier\n",
    "        elif metric == \"fp_min\":\n",
    "            conf_mat = metrics.confusion_matrix(y_val, y_pred_val)\n",
    "            fp_tmp = conf_mat[0][1] if len(conf_mat) == 2 else 0\n",
    "            if fp_tmp < best_fp:\n",
    "                best_fp = fp_tmp\n",
    "                best_c = c\n",
    "                bestsvcclassifier = svclassifier\n",
    "                    \n",
    "            \n",
    "    best_c_lst.append(best_c)\n",
    "    #predicting\n",
    "    X_test = scaler_x.transform(X_test)\n",
    "    y_pred = bestsvcclassifier.predict(X_test)\n",
    "    \n",
    "    #evaluating\n",
    "    accuracy.append(round(metrics.accuracy_score(y_test, y_pred), 3))\n",
    "    f_score.append(round(metrics.f1_score(y_true=y_test, y_pred=y_pred), 3))\n",
    "    fp, fn = false_analysis(y_test, y_pred)\n",
    "    false_positive.append(int(fp))\n",
    "    false_negative.append(int(fn))\n",
    "    \n",
    "    #Power max calculations\n",
    "    y_class_power_test = y_class_power[len(y_class_power)-X_test.shape[0]:]\n",
    "    y_class_power_pred = np.zeros(y_class_power_test.size)\n",
    "    max_power = max(y_class_power_test) + 10  # 10 is added to increase higher bound\n",
    "    min_power = min(y_class_power_test) - 10  # 10 is subtracted to decrease lower bound\n",
    "    for i in range(len(y_class_power_test)):\n",
    "        h = max_power\n",
    "        l = min_power\n",
    "        while h - l > 0.5:\n",
    "            mid = l + (h - l)/2;\n",
    "            mid_norm = (mid - scaler_x.mean_[-1])/scaler_x.scale_[-1]\n",
    "            X_test[i][-1] = mid_norm\n",
    "            res_tmp = bestsvcclassifier.predict(X_test[i:i+1])\n",
    "            if res_tmp[0]:\n",
    "                l = mid\n",
    "            else:\n",
    "                h = mid\n",
    "        y_class_power_pred[i] = l + (h - l)/2\n",
    "    average_class_diff_power.append(round(np.mean(np.absolute(y_class_power_pred - y_class_power_test)), 3))\n",
    "    fp_samples = np.zeros(len(y_class_power_pred), dtype=float)\n",
    "    fp_samples[y_class_power_pred > y_class_power_test] = (y_class_power_pred - y_class_power_test)[y_class_power_pred > \n",
    "                                                                                                    y_class_power_test]\n",
    "    fp_mean_power.append(round(np.mean(fp_samples), 3))\n",
    "    print('Number_samples:', number_sample, ', accuracy:', accuracy[-1], ', f_score:', f_score[-1], \n",
    "          ', fp:', fp,', fn:', fn, ', error:', average_class_diff_power[-1], 'fp_error:', fp_mean_power[-1])\n",
    "del svclassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(average_class_diff_power)\n",
    "print(fp_mean_power)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MAX_POWER ANAlysis\n",
    "fig = plt.figure(figsize=(15,8))\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "plt.plot(number_samples, accuracy)\n",
    "plt.xlabel('# training samples')\n",
    "plt.ylabel('%')\n",
    "plt.title('SVM: Classification Accuracy(Dynamic PUs, Using PUs, Test_size=40k)')\n",
    "plt.grid(True)\n",
    "plt.savefig('ML\\\\results\\\\changing_training_test40k_4kx4k_smallVal_compare_dynamicPUS_svmAcc.png')\n",
    "\n",
    "fig = plt.figure(figsize=(15,8))\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "plt.plot(number_samples, f_score)\n",
    "plt.xlabel('# training samples')\n",
    "plt.ylabel('%')\n",
    "plt.title('SVM: Classification F_score(Dynamic PUs, Using PUs, Test_size=40k)')\n",
    "plt.grid(True)\n",
    "plt.savefig('ML\\\\results\\\\changing_training_test40k_4kx4k_smallVal_compare_dynamicPUS_svmfscore.png')\n",
    "\n",
    "fig = plt.figure(figsize=(15,8))\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "plt.plot(number_samples, false_positive)\n",
    "plt.xlabel('# training samples')\n",
    "plt.ylabel('#')\n",
    "plt.title('SVM: Classification FP(Dynamic PUs, Using PUs, Test_size=40k)')\n",
    "plt.grid(True)\n",
    "plt.savefig('ML\\\\results\\\\changing_training_test40k_4kx4k_smallVal_compare_dynamicPUS_svmfp.png')\n",
    "\n",
    "fig = plt.figure(figsize=(15,8))\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "plt.plot(number_samples, false_negative)\n",
    "plt.xlabel('# training samples')\n",
    "plt.ylabel('#')\n",
    "plt.title('SVM: Classification FN(Dynamic PUs, Using PUs, Test_size=40k)')\n",
    "plt.grid(True)\n",
    "plt.savefig('ML\\\\results\\\\changing_training_test40k_4kx4k_smallVal_compare_dynamicPUS_svmfn.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number_samples:  8192  error:  7.029 , fp_error: 3.266\n"
     ]
    }
   ],
   "source": [
    "# SVR\n",
    "average_reg_diff_power, best_c_reg_lst, fp_mean_power = [], [], []\n",
    "best_c_reg, bestsvrclassifier =  None, None\n",
    "# TODO: Having different penalties for fp and fn\n",
    "for number_sample in number_samples:\n",
    "    min_err = float('inf')\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test = split_data(data_reg, number_sample)\n",
    "    scaler_x = StandardScaler()\n",
    "    scaler_x.fit(X_train)\n",
    "    X_train = scaler_x.transform(X_train)\n",
    "    X_val = scaler_x.transform(X_val)\n",
    "    for c in C_vec:\n",
    "        svrlassifier = SVR(kernel='rbf', C=c, degree=X_train.shape[1]+1)\n",
    "        svrlassifier.fit(X_train, y_train)\n",
    "        \n",
    "        #validating\n",
    "        y_pred_val = svrlassifier.predict(X_val)\n",
    "        err_tmp = np.mean(np.absolute(y_pred_val - y_val))\n",
    "        if err_tmp < min_err:\n",
    "            min_err = err_tmp\n",
    "            best_c_reg = c\n",
    "            bestsvrclassifier = svrlassifier\n",
    "            \n",
    "    best_c_reg_lst.append(best_c_reg)\n",
    "    #predicting\n",
    "    X_test = scaler_x.transform(X_test)\n",
    "    y_pred = bestsvrclassifier.predict(X_test)\n",
    "    \n",
    "    \n",
    "    #evaluating\n",
    "    average_reg_diff_power.append(round(np.mean(np.absolute(y_test - y_pred)), 3))\n",
    "    fp_samples = np.zeros(len(y_test), dtype=float)\n",
    "    fp_samples[y_pred > y_test] = (y_pred - y_test)[y_pred > y_test]\n",
    "    fp_mean_power.append(round(np.mean(fp_samples), 3))\n",
    "    print('Number_samples: ', number_sample, ' error: ', average_reg_diff_power[-1], \n",
    "          ', fp_error:', fp_mean_power[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shahrokh/miniconda3/envs/research/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/home/shahrokh/miniconda3/envs/research/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/home/shahrokh/miniconda3/envs/research/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/home/shahrokh/miniconda3/envs/research/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/home/shahrokh/miniconda3/envs/research/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/home/shahrokh/miniconda3/envs/research/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/home/shahrokh/miniconda3/envs/research/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/home/shahrokh/miniconda3/envs/research/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/home/shahrokh/miniconda3/envs/research/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/home/shahrokh/miniconda3/envs/research/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/home/shahrokh/miniconda3/envs/research/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/home/shahrokh/miniconda3/envs/research/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/home/shahrokh/miniconda3/envs/research/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/home/shahrokh/miniconda3/envs/research/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/home/shahrokh/miniconda3/envs/research/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/home/shahrokh/miniconda3/envs/research/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/home/shahrokh/miniconda3/envs/research/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/home/shahrokh/miniconda3/envs/research/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/home/shahrokh/miniconda3/envs/research/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number_samples:  8192  error:  7.574 , fp_error: 3.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shahrokh/miniconda3/envs/research/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "# Linear SVR\n",
    "average_reg_diff_power, best_c_reg_lst, fp_mean_power = [], [], []\n",
    "best_c_reg, bestsvrclassifier =  None, None\n",
    "for number_sample in number_samples:\n",
    "    min_err = float('inf')\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test = split_data(data_reg, number_sample)\n",
    "    scaler_x = StandardScaler()\n",
    "    scaler_x.fit(X_train)\n",
    "    X_train = scaler_x.transform(X_train)\n",
    "    X_val = scaler_x.transform(X_val)\n",
    "    for c in C_vec:\n",
    "        svrlassifier = LinearSVR(C=c, loss='epsilon_insensitive')\n",
    "        svrlassifier.fit(X_train, y_train)\n",
    "        \n",
    "        #validating\n",
    "        y_pred_val = svrlassifier.predict(X_val)\n",
    "        err_tmp = np.mean(np.absolute(y_pred_val - y_val))\n",
    "        if err_tmp < min_err:\n",
    "            min_err = err_tmp\n",
    "            best_c_reg = c\n",
    "            bestsvrclassifier = svrlassifier\n",
    "            \n",
    "    best_c_reg_lst.append(best_c_reg)\n",
    "    #predicting\n",
    "    X_test = scaler_x.transform(X_test)\n",
    "    y_pred = bestsvrclassifier.predict(X_test)\n",
    "    \n",
    "    \n",
    "    #evaluating\n",
    "    average_reg_diff_power.append(round(np.mean(np.absolute(y_test - y_pred)), 3))\n",
    "    fp_samples = np.zeros(len(y_test), dtype=float)\n",
    "    fp_samples[y_pred > y_test] = (y_pred - y_test)[y_pred > y_test]\n",
    "    fp_mean_power.append(round(np.mean(fp_samples), 3))\n",
    "    print('Number_samples: ', number_sample, ' error: ', average_reg_diff_power[-1], ', fp_error:', fp_mean_power[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7.145, 7.294, 7.1, 7.084, 7.096, 7.087]\n",
      "[3.023, 3.267, 3.306, 3.406, 3.405, 3.472]\n"
     ]
    }
   ],
   "source": [
    "print(average_reg_diff_power)\n",
    "print(fp_mean_power)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,8))\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "plt.plot(number_samples, average_class_diff_power)\n",
    "plt.plot(number_samples, average_reg_diff_power, 'r--')\n",
    "plt.xlabel('# training samples')\n",
    "plt.ylabel('Diff(dB)')\n",
    "plt.title('Average absolute difference power(Dynamic PUs, Using PUs, Test_size=40k)')\n",
    "plt.grid(True)\n",
    "\n",
    "ax.set_yticks(np.arange(0,20, 2))\n",
    "ax.set_ylim([2,20])\n",
    "ax.set_xticks(np.arange(5,4100, 500))\n",
    "# plt.grid(which='minor')\n",
    "# plt.text(40, 50, '# Validation = 34k')\n",
    "# plt.text(400, 45, '# Test = 34k')\n",
    "plt.legend(['SVM', 'SVR'])\n",
    "plt.savefig('ML\\\\results\\\\changing_training_test40k_4kx4k_smallVal_compare_dynamicPUS_averag_powerSVMSVR.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_reg_diff_power_tot = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(number_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_reg_diff_power_tot.append(average_reg_diff_power)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_samples = [5, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 120, 150, 200, 250, 300, 400, 500, 700, 1000, 2000, 3000, 4000]\n",
    "average_reg_diff_power_tot = [[4.91, 4.915, 5.02, 7.274, 5.21, 5.476, 5.547, 5.448, 5.358, 5.486, 5.424, 5.585, 5.423, 5.369,\n",
    "                               5.275, 5.22, 5.065, 5.058, 4.915, 4.873, 4.8, 4.787, 4.782], \n",
    "                              [4.739, 4.728, 4.858, 4.9, 4.761, 4.835, 4.898, 4.959, 4.806, 4.913, 4.945, 4.88, 4.8, 4.812,\n",
    "                               4.756, 4.74, 4.764, 4.832, 4.729, 4.745, 4.728, 4.728, 4.731], \n",
    "                              [5.59, 5.33, 4.764, 4.731, 4.728, 4.923, 5.086, 4.798, 4.816, 5.035, 4.827, 4.842, 4.757, 4.796,\n",
    "                               4.734, 4.748, 4.74, 4.734, 4.728, 4.729, 4.726, 4.726, 4.728], \n",
    "                              [4.791, 4.781, 4.732, 5.677, 4.757, 4.837, 4.841, 5.242, 4.81, 4.955, 4.972, 5.106, 4.801, 4.754,\n",
    "                               4.764, 4.748, 4.757, 4.763, 4.737, 4.735, 4.737, 4.74, 4.744]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,8))\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "plt.plot(number_samples, average_reg_diff_power_tot[0])\n",
    "plt.plot(number_samples, average_reg_diff_power_tot[1], 'r--')\n",
    "plt.plot(number_samples, average_reg_diff_power_tot[2], 'g.-')\n",
    "plt.plot(number_samples, average_reg_diff_power_tot[3], 'y->')\n",
    "plt.xlabel('# training samples')\n",
    "plt.ylabel('Diff(dB)')\n",
    "plt.title('Average absolute difference power(Dynamic PUs, Using PUs, Test_size=40k)')\n",
    "plt.grid(True)\n",
    "\n",
    "ax.set_yticks(np.arange(0,8, 2))\n",
    "ax.set_ylim([2,8])\n",
    "ax.set_xticks(np.arange(5,4100, 500))\n",
    "# plt.grid(which='minor')\n",
    "# plt.text(40, 50, '# Validation = 34k')\n",
    "# plt.text(400, 45, '# Test = 34k')\n",
    "plt.legend(['linear', 'rbf', 'poly', 'sigmoid'])\n",
    "# plt.savefig('ML\\\\results\\\\changing_training_test40k_4kx4k_smallVal_compare_dynamicPUS_averag_powerSVMSVR.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
