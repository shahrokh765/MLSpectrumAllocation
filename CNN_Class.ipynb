{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# IMPORTS\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras import layers, models\n",
    "from keras import backend as K\n",
    "from keras import regularizers\n",
    "from keras.layers.noise import AlphaDropout\n",
    "from collections import namedtuple\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import datetime\n",
    "import os\n",
    "import seaborn as sns\n",
    "Point = namedtuple('Point', ('x', 'y'))\n",
    "Circle = namedtuple('Circle', ('r'))\n",
    "Square = namedtuple('Square', ('side'))\n",
    "Rectangle = namedtuple('Rectangle', ('length', 'width'))\n",
    "PointWithDistance = namedtuple('PointWithDistance', ('p', 'dist'))\n",
    "float_memory_used = 'float16'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INIT\n",
    "number_samples = [5] + list(range(10, 101, 10)) + [120, 150, 200, 250, 300, 400, 500, 700] + list(range(1000, 1001, 1000))\n",
    "number_samples = [120, 150, 200, 250, 300, 400, 500, 700] + list(range(1000, 1001, 1000))\n",
    "number_samples = [500]\n",
    "\n",
    "cnn_type = \"classification\"  # {\"classification\", \"regression\"}\n",
    "validation_size, noise_floor = 0.33, -90.0\n",
    "su_power = 0 # this is not actually su power just a number to show there is an SU in its image\n",
    "max_x, max_y, number_image_channels, su_szie = 200, 200, 2, None\n",
    "pu_shape, su_shape = 'circle', 'circle' # shape = {'circle', 'square', 'point'}\n",
    "style = \"raw_power_min_max_norm\"  # {\"raw_power_zscore_norm\", \"image_intensity\", \"raw_power_min_max_norm\"}\n",
    "pus_num, intensity_degradation, slope = 15, 'log', 6\n",
    "if su_shape == 'circle':\n",
    "    su_param = Circle(su_szie)\n",
    "elif su_shape == 'square':\n",
    "    su_param = Square(su_szie)\n",
    "else:\n",
    "    su_param = None\n",
    "# num_pus = (data_reg.shape[1] - 3)//3\n",
    "dtime = datetime.datetime.now().strftime('_%Y%m_%d%H_%M')\n",
    "color = \"color\" if number_image_channels > 1 else \"grey\"\n",
    "image_dir = 'ML/data/'+ cnn_type + '/pictures_' + str(max_x) + '_' + str(max_y) + '/pu_' + pu_shape + '_su_' + su_shape + '_'  + \"/\" + style + \"/\" + color +'/' + (\n",
    "    \"\" if pu_shape == 'point' and su_shape == 'point' else (intensity_degradation + '_' + str(slope)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD DATA\n",
    "dataframe = pd.read_csv('ML/data/static_pus_using_pus50000_15PUs_201912_2315_33_200.txt', delimiter=',', header=None)\n",
    "dataframe_max = pd.read_csv('ML/data/static_pus_max_power50000_15PUs_201912_2315_33_200.txt', delimiter=',', header=None)\n",
    "\n",
    "dataframe.reset_index(drop=True, inplace=True)\n",
    "dataframe_max.reset_index(drop=True, inplace=True)\n",
    "\n",
    "dataframe_tot = pd.concat([dataframe, dataframe_max.iloc[:, dataframe_max.columns.values[-1]]], axis=1,\n",
    "                        ignore_index=True)\n",
    "idx = dataframe_tot[dataframe_tot[dataframe_tot.columns[-1]] == -float('inf')].index\n",
    "dataframe_tot.drop(idx, inplace=True)\n",
    "\n",
    "# data_reg = np.concatenate((dataframe_tot.values[:, 0:dataframe_tot.shape[1]-3], \n",
    "#                            dataframe_tot.values[:, dataframe_tot.shape[1]-1:dataframe_tot.shape[1]]), axis=1)\n",
    "data_class = dataframe_tot.values[:, 0:dataframe_tot.shape[1]-1]\n",
    "y_class_power = dataframe_tot.values[:, -1]\n",
    "del dataframe, dataframe_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_su_power = -float('inf')\n",
    "for i in range(data_reg.shape[0]):\n",
    "    max_su_power = max(su_max_powe, data_reg[i][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidian_distance(p1: Point, p2: Point):\n",
    "    return ((p1.x - p2.x) ** 2 + (p1.y - p2.y) ** 2) ** 0.5\n",
    "\n",
    "def calculate_mu_sigma(data, num_pus):\n",
    "    sum_non_noise = 0\n",
    "    for pu_n in range(num_pus): # calculate mu\n",
    "        sum_non_noise += data[pu_n*3+2]\n",
    "    mu = ((max_x * max_y - num_pus) * noise_floor + sum_non_noise)/(max_x * max_y)\n",
    "    sum_square = 0\n",
    "    for pu_n in range(num_pus): # calculate sigma\n",
    "        sum_square += (data[pu_n*3+2]-mu)**2\n",
    "    sum_square += (max_x * max_y - num_pus) * (noise_floor - mu)**2\n",
    "    sigma = math.sqrt(sum_square/(max_x * max_y))\n",
    "    return mu, sigma\n",
    "\n",
    "def get_pu_param(pu_shape: str, intensity_degradation: str, pu_p: float, noise_floor: float, slope: float):\n",
    "    pu_param = None\n",
    "    if pu_shape == 'circle':\n",
    "        if intensity_degradation == \"linear\":\n",
    "            pu_param = Circle(int((pu_p - noise_floor) / slope)) # linear\n",
    "        elif intensity_degradation == \"log\":\n",
    "            pu_param = Circle(int(10 ** ((pu_p - noise_floor) / (10 *slope)))) # log_based\n",
    "    elif pu_shape == 'square':\n",
    "        if intensity_degradation == \"linear\":\n",
    "            pu_param = Square(int(2 ** 0.5 * (pu_p - noise_floor) / slope)) # linear\n",
    "        elif intensity_degradation == \"log\":\n",
    "            pu_param = Square(int(2 ** 0.5 * 10 ** ((pu_p - noise_floor) / (10 *slope)))) # log_based\n",
    "    elif pu_shape == 'point':\n",
    "        pu_param = None\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported PU shape(create_image)! \", pu_shape)\n",
    "    return pu_param\n",
    "\n",
    "def create_image(data, pus_num, slope, style=\"raw_power_z_score\", noise_floor=-90, pu_shape= 'circle', pu_param=None, \n",
    "                 su_shape='circle', su_param=None, intensity_degradation=\"log\", max_pu_power: float=0.0, max_su_power=0.0):  \n",
    "    # style = {\"raw_power_zscore_norm\", \"image_intensity\", \"raw_power_min_max_norm\"}\n",
    "    # intensity_degradation= {\"log\", \"linear\"}\n",
    "    # if param is None, it's automatically calculated. Highest brightness(or power value) (255 or 1.) would\n",
    "    # assigned to the center(PU location) and radius(side) would be calculated based on its power, slope, and noise floor.\n",
    "    # If it is given, intensity(power) of pixel beside center would be calculated in the same fashin with an exception that \n",
    "    # intensity below zero(noise_floor) would be replaced by zero(noise_floor)\n",
    "    if style == \"raw_power_min_max_norm\":\n",
    "        # In this way, PUs' location are replaced with their power(dBm) and the power would fade with \n",
    "        # slope till gets noise_floor(in circle shape)\n",
    "        \n",
    "        # creating pu matrix\n",
    "        image = np.zeros((1,number_image_channels,max_x, max_y), dtype=float_memory_used)\n",
    "        for pu_i in range(pus_num):\n",
    "            pu_x, pu_y, pu_p = max(0, min(max_x-1, int(data[pu_i*3]))), max(0, min(max_x-1, int(data[pu_i*3+1]))), data[\n",
    "                pu_i*3+2]\n",
    "            if not pu_param:\n",
    "                pu_param_p = get_pu_param(pu_shape, intensity_degradation, pu_p, noise_floor, slope)\n",
    "            else:\n",
    "                pu_param_p = pu_param\n",
    "            points = points_inside_shape(center=Point(pu_x, pu_y), shape=pu_shape, param=pu_param_p)\n",
    "            for point in points:\n",
    "                if 0 <= point.p.x < max_x and 0 <= point.p.y < max_y: # TODO should pass image size\n",
    "                    if intensity_degradation == \"linear\":\n",
    "                        image[0][0][point.p.x][point.p.y] += (pu_p - slope * point.dist - noise_floor)/(\n",
    "                            max_pu_power - noise_floor)\n",
    "                    elif intensity_degradation == \"log\":\n",
    "                        if point.dist < 1:\n",
    "                            image[0][0][point.p.x][point.p.y] += (pu_p - noise_floor) / (max_pu_power - noise_floor)\n",
    "                        else:\n",
    "                            image[0][0][point.p.x][point.p.y] += (pu_p - slope * 10*math.log10(point.dist) - noise_floor)/(\n",
    "                                max_pu_power - noise_floor)\n",
    "        # creating su matrix\n",
    "        su_num = (len(data) - pus_num * 3 - 1) // 3\n",
    "        if not (len(data) - pus_num * 3) % 3:\n",
    "            raise ValueError(\"Data provided is not correct; can't get SUs' information(create_image)\")\n",
    "#         if su_param is None:\n",
    "#             # if su_param is unavailable, a circle(square) with radius(side) 1 is created\n",
    "#             if su_shape == 'circle':\n",
    "#                 su_param = Circle(1)\n",
    "#             elif su_shape == 'square':\n",
    "#                 su_param = Square(1)\n",
    "#             elif su_shape == 'point':\n",
    "#                 su_param = None\n",
    "#             else:\n",
    "#                 raise ValueError(\"Unsupported SU shape(create_image)! \", su_shape)\n",
    "#         su_intensity = 1.\n",
    "        su_channel = 0 if number_image_channels == 1 else 1\n",
    "        for su_i in range(su_num):\n",
    "            su_x, su_y, su_p = max(0, min(max_x-1, int(data[pus_num*3+su_i*2]))), max(\n",
    "                0, min(max_x-1, int(data[pus_num*3+su_i*2+1]))), data[pus_num*3+su_i*2+2]\n",
    "            if not su_param:\n",
    "                su_param_s = get_pu_param(su_shape, intensity_degradation, su_p, noise_floor, slope)\n",
    "            else:\n",
    "                su_param_s = su_param\n",
    "            points = points_inside_shape(center=Point(su_x, su_y), param=su_param_s, shape=su_shape)\n",
    "            for point in points:\n",
    "                if 0 <= point.p.x < max_x and 0 <= point.p.y < max_y: # TODO should pass image size\n",
    "                    if intensity_degradation == \"linear\":\n",
    "                        image[0][su_channel][point.p.x][point.p.y] += (su_p - slope * point.dist - noise_floor)/(\n",
    "                            max_su_power - noise_floor)\n",
    "                    elif intensity_degradation == \"log\":\n",
    "                        if point.dist < 1:\n",
    "                            image[0][su_channel][point.p.x][point.p.y] += (su_p - noise_floor) / (max_su_power - noise_floor)\n",
    "                        else:\n",
    "                            image[0][su_channel][point.p.x][point.p.y] += (su_p - slope * 10*math.log10(point.dist) - noise_floor)/(\n",
    "                                max_su_power - noise_floor)\n",
    "        return image\n",
    "        \n",
    "#         pu_image = [[(noise_floor - mu)/sigma] * max_y for _ in range(max_x)]\n",
    "    elif style == \"image_intensity\":\n",
    "        # creating PU image\n",
    "        image = np.zeros((1,number_image_channels,max_x, max_y), dtype=float_memory_used)\n",
    "        for pu_i in range(pus_num):\n",
    "            pu_x, pu_y, pu_p = max(0, min(max_x-1, int(data[pu_i*3]))), max(0, min(max_x-1, int(data[pu_i*3+1]))), data[pu_i*3+2]\n",
    "            if pu_param is None:\n",
    "                pu_param = get_pu_param(pu_shape, intensity_degradation, pu_p, noise_floor, slope)\n",
    "            points = points_inside_shape(center=Point(pu_x, pu_y), shape=pu_shape, param=pu_param)\n",
    "            for point in points:\n",
    "                if 0 <= point.p.x < max_x and 0 <= point.p.y < max_y: # TODO should pass image size\n",
    "                    if intensity_degradation == \"linear\":\n",
    "                        image[0][0][point.p.x][point.p.y] += max((pu_p - slope * point.dist + abs(noise_floor))\n",
    "                                                              /(pu_p + abs(noise_floor)), 0)\n",
    "                    elif intensity_degradation == \"log\":\n",
    "                        if point.dist < 1:\n",
    "                            image[0][0][point.p.x][point.p.y] = 1\n",
    "                        else:\n",
    "                            image[0][0][point.p.x][point.p.y] += max((pu_p - slope * 10*math.log10(point.dist) + abs(noise_floor))\n",
    "                                                                 /(pu_p + abs(noise_floor)), 0)\n",
    "                    image[0][0][point.p.x][point.p.y] = min(image[0][0][point.p.x][point.p.y], 1.0)\n",
    "                        \n",
    "        # creating SU image\n",
    "        su_num = (len(data) - pus_num * 3) // 3\n",
    "        if not (len(data) - pus_num * 3) % 3:\n",
    "            raise ValueError(\"Data provided is not correct; can't get SUs' information(create_image)\")\n",
    "#         su_image = np.zeros((max_x, max_y), dtype=float_memory_used)\n",
    "#         if su_param is None:\n",
    "#             # if su_param is unavailable, a circle(square) with radius(side) 1 is created\n",
    "#             if su_shape == 'circle':\n",
    "#                 su_param = Circle(1)\n",
    "#             elif su_shape == 'square':\n",
    "#                 su_param = Square(1)\n",
    "#             elif su_shape == 'point':\n",
    "#                 su_param = None\n",
    "#             else:\n",
    "#                 raise ValueError(\"Unsupported SU shape(create_image)! \", su_shape)\n",
    "        su_channel = 0 if number_image_channels == 1 else 1\n",
    "#         su_intensity = 1.\n",
    "        for su_i in range(su_num):\n",
    "            su_x, su_y, su_p = max(0, min(max_x-1, int(data[pus_num*3+su_i*2]))\n",
    "                                  ), max(0, min(max_x-1, int(data[pus_num*3+su_i*2+1]))), su_intensity\n",
    "            points = points_inside_shape(center=Point(su_x, su_y), param=su_param, shape=su_shape)\n",
    "            for point in points:\n",
    "                if 0 <= point.p.x < max_x and 0 <= point.p.y < max_y: # TODO should pass image size\n",
    "                    if number_image_channels > 1:\n",
    "                        image[0][1][point.p.x][point.p.y] = su_intensity\n",
    "                    elif number_image_channels == 1:\n",
    "                        image[0][0][point.p.x][point.p.y] = su_intensity\n",
    "#         return np.array([pu_image, su_image, [[0.] * max_y for _ in range(max_x)]], dtype='float32') # return like this to be able to display as an RGB image with pyplot.imshow(imsave)\n",
    "#         return np.append(pu_image, su_image, axis=0)\n",
    "        return image\n",
    "        \n",
    "            \n",
    "    else:\n",
    "        raise ValueError(\"Unsupported style(create_image)! \", style)\n",
    "        \n",
    "def points_inside_shape(center: Point, shape: str, param)-> list:\n",
    "    # This function returns points+distance around center with defined shape\n",
    "    if shape == 'circle':\n",
    "        # First creates points inside a square(around orgigin) with 2*r side and then remove those with distance > r.\n",
    "        # Shift all remaining around center. O(4r^2)\n",
    "        r, origin = param.r, Point(0, 0)\n",
    "        square_points = set([Point(x, y) for x in range(-r, r+1) for y in range(-r, r+1)])\n",
    "        points = []\n",
    "        while square_points:\n",
    "            p = square_points.pop()\n",
    "            dist = euclidian_distance(p, origin)\n",
    "            if dist <= r:\n",
    "                points.append(PointWithDistance(Point(p.x + center.x, p.y + center.y), dist))\n",
    "                if p.x != 0:\n",
    "                    points.append(PointWithDistance(Point(-p.x + center.x, p.y + center.y), dist))\n",
    "                    square_points.remove(Point(-p.x, p.y))\n",
    "                if p.y != 0:\n",
    "                    points.append(PointWithDistance(Point(p.x + center.x, -p.y + center.y), dist))\n",
    "                    square_points.remove(Point(p.x, -p.y))\n",
    "                if p.x != 0 and p.y != 0:\n",
    "                    points.append(PointWithDistance(Point(-p.x + center.x, -p.y + center.y), dist))\n",
    "                    square_points.remove(Point(-p.x, -p.y))\n",
    "        return points\n",
    "    elif shape == 'square':\n",
    "        half_side = param.side // 2\n",
    "        return [PointWithDistance(Point(x, y), euclidian_distance(Point(x, y), center)) for x in range(-half_side + center.x,\n",
    "                                                                                               half_side + center.x+1) \n",
    "                         for y in range(-half_side + center.y, half_side + center.y + 1)]\n",
    "    elif shape == 'point':\n",
    "        return [PointWithDistance(center, 0)]\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported shape(points_inside_shape)! \", shape)\n",
    "    \n",
    "# TODO: Consider using min_max normalization becasue difference between values using\n",
    "# z-score is huge since most of the pixels have the same value, noise floor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving images once to save time\n",
    "# run this cell just for creating images\n",
    "if not os.path.exists(image_dir):\n",
    "    os.makedirs(image_dir)\n",
    "# for image_num in range(115, data_reg.shape[0]):\n",
    "for image_num in range(0, 1330):\n",
    "    image = create_image(data=data_class[image_num], slope=slope, style=style, noise_floor=noise_floor,\n",
    "                         pu_shape=pu_shape, su_shape=su_shape, su_param=None, pus_num=pus_num, \n",
    "                         intensity_degradation=intensity_degradation, max_su_power=50.0)\n",
    "    if style == \"image_intensity\":\n",
    "        if number_image_channels != 3:\n",
    "            image = np.append(np.array(image[0]), np.zeros((3-number_image_channels,max_x, max_y), \n",
    "                                                           dtype=float_memory_used), axis=0)\n",
    "        image_save = np.swapaxes(image, 0, 2)\n",
    "        plt.imsave(image_dir + '/image' + str(image_num)+'.png', image_save)\n",
    "    elif style == \"raw_power_min_max_norm\" or style == \"raw_power_zscore_norm\":\n",
    "        np.save(image_dir + '/image' + str(image_num), image)\n",
    "    if image_num + 1 % 100 == 0:\n",
    "        print(image_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(70,70))         # Sample figsize in inches\n",
    "sns.heatmap(image[0][0], ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO, CNN Model here\n",
    "def cnn_model(num_filters, kernel_lam, bias_lam):\n",
    "#     num_filters, lam = 5, 5\n",
    "    data_format = 'channels_first'\n",
    "    convolution_filter, dense_filter = 'selu', 'sigmoid' #softsign, sigmoid; relu, linear\n",
    "    filter_shape, pool_size = (3, 3), (2,2)\n",
    "    cnn = models.Sequential()\n",
    "    cnn.add(layers.Conv2D(num_filters, filter_shape, padding='same', activation=convolution_filter, \n",
    "                          input_shape=(number_image_channels, max_x, max_y), data_format=data_format, \n",
    "                          kernel_regularizer=regularizers.l2(kernel_lam), bias_regularizer=regularizers.l2(bias_lam),\n",
    "                          kernel_initializer='lecun_normal'))\n",
    "    cnn.add(layers.MaxPooling2D(pool_size=pool_size, data_format=data_format))\n",
    "    \n",
    "    cnn.add(layers.Conv2D(num_filters, filter_shape,padding='same', activation=convolution_filter, data_format=data_format, \n",
    "                         kernel_regularizer=regularizers.l2(kernel_lam), bias_regularizer=regularizers.l2(bias_lam),\n",
    "                         kernel_initializer='lecun_normal'))\n",
    "    cnn.add(layers.MaxPooling2D(pool_size=pool_size, data_format=data_format))\n",
    "    \n",
    "    cnn.add(layers.Conv2D(num_filters, filter_shape, padding='same', activation=convolution_filter, data_format=data_format, \n",
    "                         kernel_regularizer=regularizers.l2(kernel_lam), bias_regularizer=regularizers.l2(bias_lam),\n",
    "                         kernel_initializer='lecun_normal'))\n",
    "    cnn.add(layers.MaxPooling2D(pool_size, data_format=data_format))\n",
    "    \n",
    "    cnn.add(layers.Conv2D(num_filters, filter_shape, padding='same', activation=convolution_filter, data_format=data_format, \n",
    "                         kernel_regularizer=regularizers.l2(kernel_lam), bias_regularizer=regularizers.l2(bias_lam),\n",
    "                         kernel_initializer='lecun_normal'))\n",
    "    cnn.add(layers.MaxPooling2D(pool_size, data_format=data_format))\n",
    "    \n",
    "    cnn.add(layers.Conv2D(num_filters, filter_shape, padding='same', activation=convolution_filter, data_format=data_format, \n",
    "                         kernel_regularizer=regularizers.l2(kernel_lam), bias_regularizer=regularizers.l2(bias_lam),\n",
    "                         kernel_initializer='lecun_normal'))\n",
    "    cnn.add(layers.MaxPooling2D(pool_size, data_format=data_format))\n",
    "    \n",
    "#     cnn.add(layers.Conv2D(num_filters, filter_shape, padding='same', activation=convolution_filter, data_format=data_format, \n",
    "#                          kernel_regularizer=regularizers.l2(kernel_lam), bias_regularizer=regularizers.l2(bias_lam),\n",
    "#                          kernel_initializer='lecun_normal'))\n",
    "#     cnn.add(layers.MaxPooling2D(pool_size, data_format=data_format))\n",
    "    \n",
    "#     cnn.add(layers.Conv2D(num_filters, filter_shape, padding='same', activation=convolution_filter, data_format=data_format, \n",
    "#                          kernel_regularizer=regularizers.l2(kernel_lam), bias_regularizer=regularizers.l2(bias_lam),\n",
    "#                           kernel_initializer='lecun_normal'))\n",
    "#     cnn.add(layers.MaxPooling2D(pool_size, data_format=data_format))\n",
    "#     cnn.add(AlphaDropout(0.25))\n",
    "    cnn.add(layers.Flatten())\n",
    "#     cnn.add(layers.Dense(20, activation=convolution_filter, kernel_regularizer=regularizers.l2(lam), bias_regularizer=regularizers.l2(lam)))\n",
    "    cnn.add(layers.Dense(20, activation=convolution_filter, kernel_regularizer=regularizers.l2(kernel_lam),\n",
    "                         bias_regularizer=regularizers.l2(bias_lam), kernel_initializer='lecun_normal'))\n",
    "#     cnn.add(AlphaDropout(0.25))\n",
    "    cnn.add(layers.Dense(1, activation=dense_filter, kernel_regularizer=regularizers.l2(kernel_lam),\n",
    "                         bias_regularizer=regularizers.l2(bias_lam), kernel_initializer='lecun_normal'))\n",
    "    return cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = cnn_model(7, 0, 0)\n",
    "\n",
    "cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# CNN\n",
    "prev_sample = 0\n",
    "lambda_vec = [0,0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1, 3]\n",
    "max_train_samples = math.ceil(number_samples[-1] * (1 + validation_size))\n",
    "x_train = np.empty((max_train_samples, number_image_channels, max_x, max_y), dtype=float_memory_used)\n",
    "y_train = np.empty((max_train_samples), dtype=float_memory_used)\n",
    "average_diff_power, fp_mean_power = [], []\n",
    "for number_sample in number_samples:\n",
    "    sample = math.ceil(number_sample * (1 + validation_size))\n",
    "    for image_num in range(prev_sample, sample):\n",
    "        prev_sample = sample\n",
    "        if style == \"image_intensity\":\n",
    "            image = plt.imread(image_dir + '/image' + str(image_num)+'.png')\n",
    "            image = np.swapaxes(image, 0, 2)\n",
    "            x_train[image_num] = np.array(image[:number_image_channels], dtype=float_memory_used).reshape(1, number_image_channels, max_x, max_y)\n",
    "            del image\n",
    "        elif  style == \"raw_power_min_max_norm\" or style == \"raw_power_zscore_norm\":\n",
    "            x_train[image_num] = np.load(image_dir + '/image' + str(image_num)+'.npy')\n",
    "        y_train[image_num] = np.asarray(data_class[image_num][-1], dtype=float_memory_used)\n",
    "        if image_num + 1 % 100 == 0:\n",
    "            print(image_num)\n",
    "    best_acc = -float('inf')\n",
    "    best_model, best_lam = None, None\n",
    "    for lamb in lambda_vec:\n",
    "#         print(\"Lambda:\", lamb)\n",
    "        cnn = cnn_model(7, lamb, 0)\n",
    "        cnn.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        cnn.fit(x_train[:sample], y_train[:sample].astype(int), epochs=1, verbose=0, batch_size=1, \n",
    "                validation_split=validation_size/(validation_size + 1))\n",
    "        if cnn.history.history['val_acc'][-1] > best_acc:\n",
    "            best_acc = cnn.history.history['val_acc'][-1]\n",
    "            best_model = cnn\n",
    "            best_lam = lamb\n",
    "    print(\"best lambda\", best_lam, \"best_acc\", best_acc)\n",
    "    # evaluating test images\n",
    "    sum_mae, sum_fp_mae = 0, 0\n",
    "    test_size = 0\n",
    "#     for test_num in range(max_train_samples, data_reg.shape[0]):\n",
    "    for test_num in range(1330, 5000):\n",
    "        test_size += 1\n",
    "        #Power max calculations\n",
    "        test_y = y_class_power[test_num]\n",
    "        test_yp = -90\n",
    "        max_power = 55  # 10 is added to increase higher bound\n",
    "        min_power = -90  # 10 is subtracted to decrease lower bound\n",
    "        h = max_power\n",
    "        l = min_power\n",
    "        while h - l > 0.5:\n",
    "            mid = l + (h - l)/2;\n",
    "            X_test = data_class[test_num]\n",
    "            X_test[-2] = mid\n",
    "            if style == \"image_intensity\":\n",
    "                pass\n",
    "            else:\n",
    "                test_image = create_image(data=X_test, slope=slope, style=style, noise_floor=noise_floor,\n",
    "                        pu_shape=pu_shape, su_shape=su_shape, su_param=None, pus_num=pus_num, \n",
    "                        intensity_degradation=intensity_degradation, max_su_power=50.0)\n",
    "            res_tmp = best_model.predict_classes(test_image)[0][0]\n",
    "            if res_tmp:\n",
    "                l = mid\n",
    "                test_yp = max(test_yp, mid)\n",
    "            else:\n",
    "                h = mid\n",
    "        sum_mae += abs(test_yp - test_y)\n",
    "        if test_yp > test_y:\n",
    "            sum_fp_mae += abs(test_yp - test_y)\n",
    "        if test_num % 500 == 0:\n",
    "            print('test: ', test_num)\n",
    "        \n",
    "#         print('Number_samples:', number_sample, ', accuracy:', accuracy[-1], ', f_score:', f_score[-1], \n",
    "#               ', fp:', fp,', fn:', fn, ', error:', average_class_diff_power[-1], 'fp_error:', fp_mean_power[-1])\n",
    "        \n",
    "    fp_mean_power.append(round(sum_fp_mae/ test_size, 3))\n",
    "    average_diff_power.append(round(sum_mae / test_size, 3))\n",
    "    print('number_samples: ', number_sample, ', average_error: ', average_diff_power[-1], ' fp_average_error: ', fp_mean_power[-1])\n",
    "    \n",
    "    var_f = open('/'.join(image_dir.split('/')[:-1]) +  '/' + intensity_degradation + '_' + str(slope) + '_' + dtime + \".dat\", \"wb\") # file for saving results\n",
    "    pickle.dump([average_diff_power, fp_mean_power, number_samples], file=var_f)\n",
    "    var_f.close()       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = data_class[3000]\n",
    "X_test[-2] = 40\n",
    "test_image = create_image(data=X_test, slope=slope, style=style, noise_floor=noise_floor,\n",
    "                          pu_shape=pu_shape, su_shape=su_shape, su_param=None, pus_num=pus_num, \n",
    "                          intensity_degradation=intensity_degradation, max_su_power=50.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_class_power[3000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.predict_classes(test_image)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(70,70))         # Sample figsize in inches\n",
    "sns.heatmap(x_train[200][0], ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lambda_vec = [0,0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1, 3]\n",
    "best_acc = -float('inf')\n",
    "best_model, best_lam = None, None\n",
    "for lamb in lambda_vec:\n",
    "    print(\"Lambda:\", lamb)\n",
    "    cnn = cnn_model(15, lamb, 0)\n",
    "    cnn.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    cnn.fit(x_train[:sample], y_train[:sample].astype(int), epochs=1, verbose=1, batch_size=1, validation_split=validation_size/\n",
    "            (validation_size + 1))\n",
    "    if cnn.history.history['acc'][-1] > best_acc:\n",
    "        min_error = cnn.history.history['acc'][-1]\n",
    "        best_model = cnn\n",
    "        best_lam = lamb\n",
    "    print(\"\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_mae, sum_fp_mae = 0, 0\n",
    "test_size = 0\n",
    "for test_num in range(1330, 1700):\n",
    "        test_size += 1\n",
    "        #Power max calculations\n",
    "        test_y = y_class_power[test_num]\n",
    "        test_yp = -float('inf')\n",
    "        max_power = 55  # 10 is added to increase higher bound\n",
    "        min_power = -90  # 10 is subtracted to decrease lower bound\n",
    "        h = max_power\n",
    "        l = min_power\n",
    "        while h - l > 0.5:\n",
    "            mid = l + (h - l)/2;\n",
    "            X_test = data_class[image_num]\n",
    "            X_test[-2] = mid\n",
    "            if style == \"image_intensity\":\n",
    "                pass\n",
    "            else:\n",
    "                test_image = create_image(data=X_test, slope=slope, style=style, noise_floor=noise_floor,\n",
    "                        pu_shape=pu_shape, su_shape=su_shape, su_param=None, pus_num=pus_num, \n",
    "                        intensity_degradation=intensity_degradation, max_su_power=50.0)\n",
    "            res_tmp = cnn.predict_classes(test_image)[0][0]\n",
    "            if res_tmp:\n",
    "                l = mid\n",
    "                test_yp = max(test_yp, mid)\n",
    "            else:\n",
    "                h = mid\n",
    "        sum_mae += abs(test_yp - test_y)\n",
    "        if test_yp > test_y:\n",
    "            sum_fp_mae += abs(test_yp - test_y)\n",
    "        if test_num % 500 == 0:\n",
    "            print('test: ', test_num)\n",
    "fp_mean_power.append(round(sum_fp_mae/ test_size, 3))\n",
    "average_diff_power.append(round(sum_mae / test_size, 3))\n",
    "print('number_samples: ', number_sample, ', average_error: ', average_diff_power[-1], ' fp_average_error: ', fp_mean_power[-1])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.predict_classes(test_image)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
